{
  "last_updated": "2025-02-28T00:46:48.896828",
  "papers": [
    {
      "title": "Towards a robust approach to infer causality in molecular systems satisfying detailed balance",
      "authors": [
        "Vittorio Del Tatto",
        "Debarshi Banerjee",
        "Ali Hassanali",
        "Alessandro Laio"
      ],
      "abstract": "The ability to distinguish between correlation and causation of variables in\nmolecular systems remains an interesting and open area of investigation. In\nthis work, we probe causality in a molecular system using two independent\ncomputational methods that infer the causal direction through the language of\ninformation transfer. Specifically, we demonstrate that a molecular dynamics\nsimulation involving a single Tryptophan in liquid water displays asymmetric\ninformation transfer between specific collective variables, such as solute and\nsolvent coordinates. Analyzing a discrete Markov-state and Langevin dynamics on\na 2D free energy surface, we show that the same kind of asymmetries can emerge\neven in extremely simple systems, undergoing equilibrium and time-reversible\ndynamics. We use these model systems to rationalize the unidirectional\ninformation transfer in the molecular system in terms of asymmetries in the\nunderlying free energy landscape and/or relaxation dynamics of the relevant\ncoordinates. Finally, we propose a computational experiment that allows one to\ndecide if an asymmetric information transfer between two variables corresponds\nto a genuine causal link.",
      "pdf_url": "http://arxiv.org/pdf/2502.19384v1",
      "arxiv_url": "http://arxiv.org/abs/2502.19384v1",
      "published": "2025-02-26",
      "categories": [
        "physics.chem-ph",
        "cond-mat.stat-mech",
        "physics.bio-ph"
      ]
    },
    {
      "title": "Long-term Causal Inference via Modeling Sequential Latent Confounding",
      "authors": [
        "Weilin Chen",
        "Ruichu Cai",
        "Yuguang Yan",
        "Zhifeng Hao",
        "José Miguel Hernández-Lobato"
      ],
      "abstract": "Long-term causal inference is an important but challenging problem across\nvarious scientific domains. To solve the latent confounding problem in\nlong-term observational studies, existing methods leverage short-term\nexperimental data. Ghassami et al. propose an approach based on the Conditional\nAdditive Equi-Confounding Bias (CAECB) assumption, which asserts that the\nconfounding bias in the short-term outcome is equal to that in the long-term\noutcome, so that the long-term confounding bias and the causal effects can be\nidentified. While effective in certain cases, this assumption is limited to\nscenarios with a one-dimensional short-term outcome. In this paper, we\nintroduce a novel assumption that extends the CAECB assumption to accommodate\ntemporal short-term outcomes. Our proposed assumption states a functional\nrelationship between sequential confounding biases across temporal short-term\noutcomes, under which we theoretically establish the identification of\nlong-term causal effects. Based on the identification result, we develop an\nestimator and conduct a theoretical analysis of its asymptotic properties.\nExtensive experiments validate our theoretical results and demonstrate the\neffectiveness of the proposed method.",
      "pdf_url": "http://arxiv.org/pdf/2502.18994v1",
      "arxiv_url": "http://arxiv.org/abs/2502.18994v1",
      "published": "2025-02-26",
      "categories": [
        "cs.LG"
      ]
    },
    {
      "title": "Nonparametric Heterogeneous Long-term Causal Effect Estimation via Data Combination",
      "authors": [
        "Weilin Chen",
        "Ruichu Cai",
        "Junjie Wan",
        "Zeqin Yang",
        "José Miguel Hernández-Lobato"
      ],
      "abstract": "Long-term causal inference has drawn increasing attention in many scientific\ndomains. Existing methods mainly focus on estimating average long-term causal\neffects by combining long-term observational data and short-term experimental\ndata. However, it is still understudied how to robustly and effectively\nestimate heterogeneous long-term causal effects, significantly limiting\npractical applications. In this paper, we propose several two-stage style\nnonparametric estimators for heterogeneous long-term causal effect estimation,\nincluding propensity-based, regression-based, and multiple robust estimators.\nWe conduct a comprehensive theoretical analysis of their asymptotic properties\nunder mild assumptions, with the ultimate goal of building a better\nunderstanding of the conditions under which some estimators can be expected to\nperform better. Extensive experiments across several semi-synthetic and\nreal-world datasets validate the theoretical results and demonstrate the\neffectiveness of the proposed estimators.",
      "pdf_url": "http://arxiv.org/pdf/2502.18960v1",
      "arxiv_url": "http://arxiv.org/abs/2502.18960v1",
      "published": "2025-02-26",
      "categories": [
        "cs.LG"
      ]
    },
    {
      "title": "Enhancing DNA Foundation Models to Address Masking Inefficiencies",
      "authors": [
        "Monireh Safari",
        "Pablo Millan Arias",
        "Scott C. Lowe",
        "Lila Kari",
        "Angel X. Chang",
        "Graham W. Taylor"
      ],
      "abstract": "Masked language modelling (MLM) as a pretraining objective has been widely\nadopted in genomic sequence modelling. While pretrained models can successfully\nserve as encoders for various downstream tasks, the distribution shift between\npretraining and inference detrimentally impacts performance, as the pretraining\ntask is to map [MASK] tokens to predictions, yet the [MASK] is absent during\ndownstream applications. This means the encoder does not prioritize its\nencodings of non-[MASK] tokens, and expends parameters and compute on work only\nrelevant to the MLM task, despite this being irrelevant at deployment time. In\nthis work, we propose a modified encoder-decoder architecture based on the\nmasked autoencoder framework, designed to address this inefficiency within a\nBERT-based transformer. We empirically show that the resulting mismatch is\nparticularly detrimental in genomic pipelines where models are often used for\nfeature extraction without fine-tuning. We evaluate our approach on the\nBIOSCAN-5M dataset, comprising over 2 million unique DNA barcodes. We achieve\nsubstantial performance gains in both closed-world and open-world\nclassification tasks when compared against causal models and bidirectional\narchitectures pretrained with MLM tasks.",
      "pdf_url": "http://arxiv.org/pdf/2502.18405v1",
      "arxiv_url": "http://arxiv.org/abs/2502.18405v1",
      "published": "2025-02-25",
      "categories": [
        "cs.LG"
      ]
    },
    {
      "title": "Multi-class Seismic Building Damage Assessment from InSAR Imagery using Quadratic Variational Causal Bayesian Inference",
      "authors": [
        "Xuechun Li",
        "Susu Xu"
      ],
      "abstract": "Interferometric Synthetic Aperture Radar (InSAR) technology uses satellite\nradar to detect surface deformation patterns and monitor earthquake impacts on\nbuildings. While vital for emergency response planning, extracting multi-class\nbuilding damage classifications from InSAR data faces challenges: overlapping\ndamage signatures with environmental noise, computational complexity in\nmulti-class scenarios, and the need for rapid regional-scale processing. Our\nnovel multi-class variational causal Bayesian inference framework with\nquadratic variational bounds provides rigorous approximations while ensuring\nefficiency. By integrating InSAR observations with USGS ground failure models\nand building fragility functions, our approach separates building damage\nsignals while maintaining computational efficiency through strategic pruning.\nEvaluation across five major earthquakes (Haiti 2021, Puerto Rico 2020, Zagreb\n2020, Italy 2016, Ridgecrest 2019) shows improved damage classification\naccuracy (AUC: 0.94-0.96), achieving up to 35.7% improvement over existing\nmethods. Our approach maintains high accuracy (AUC > 0.93) across all damage\ncategories while reducing computational overhead by over 40% without requiring\nextensive ground truth data.",
      "pdf_url": "http://arxiv.org/pdf/2502.18546v1",
      "arxiv_url": "http://arxiv.org/abs/2502.18546v1",
      "published": "2025-02-25",
      "categories": [
        "cs.CV",
        "cs.LG"
      ]
    },
    {
      "title": "An Overview of Large Language Models for Statisticians",
      "authors": [
        "Wenlong Ji",
        "Weizhe Yuan",
        "Emily Getzen",
        "Kyunghyun Cho",
        "Michael I. Jordan",
        "Song Mei",
        "Jason E Weston",
        "Weijie J. Su",
        "Jing Xu",
        "Linjun Zhang"
      ],
      "abstract": "Large Language Models (LLMs) have emerged as transformative tools in\nartificial intelligence (AI), exhibiting remarkable capabilities across diverse\ntasks such as text generation, reasoning, and decision-making. While their\nsuccess has primarily been driven by advances in computational power and deep\nlearning architectures, emerging problems -- in areas such as uncertainty\nquantification, decision-making, causal inference, and distribution shift --\nrequire a deeper engagement with the field of statistics. This paper explores\npotential areas where statisticians can make important contributions to the\ndevelopment of LLMs, particularly those that aim to engender trustworthiness\nand transparency for human users. Thus, we focus on issues such as uncertainty\nquantification, interpretability, fairness, privacy, watermarking and model\nadaptation. We also consider possible roles for LLMs in statistical analysis.\nBy bridging AI and statistics, we aim to foster a deeper collaboration that\nadvances both the theoretical foundations and practical applications of LLMs,\nultimately shaping their role in addressing complex societal challenges.",
      "pdf_url": "http://arxiv.org/pdf/2502.17814v1",
      "arxiv_url": "http://arxiv.org/abs/2502.17814v1",
      "published": "2025-02-25",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ]
    },
    {
      "title": "Joint Value Estimation and Bidding in Repeated First-Price Auctions",
      "authors": [
        "Yuxiao Wen",
        "Yanjun Han",
        "Zhengyuan Zhou"
      ],
      "abstract": "We study regret minimization in repeated first-price auctions (FPAs), where a\nbidder observes only the realized outcome after each auction -- win or loss.\nThis setup reflects practical scenarios in online display advertising where the\nactual value of an impression depends on the difference between two potential\noutcomes, such as clicks or conversion rates, when the auction is won versus\nlost. We analyze three outcome models: (1) adversarial outcomes without\nfeatures, (2) linear potential outcomes with features, and (3) linear treatment\neffects in features. For each setting, we propose algorithms that jointly\nestimate private values and optimize bidding strategies, achieving near-optimal\nregret bounds. Notably, our framework enjoys a unique feature that the\ntreatments are also actively chosen, and hence eliminates the need for the\noverlap condition commonly required in causal inference.",
      "pdf_url": "http://arxiv.org/pdf/2502.17292v1",
      "arxiv_url": "http://arxiv.org/abs/2502.17292v1",
      "published": "2025-02-24",
      "categories": [
        "cs.LG",
        "cs.GT",
        "cs.IT",
        "math.IT",
        "stat.ME",
        "stat.ML"
      ]
    },
    {
      "title": "Teleology-Driven Affective Computing: A Causal Framework for Sustained Well-Being",
      "authors": [
        "Bin Yin",
        "Chong-Yi Liu",
        "Liya Fu",
        "Jinkun Zhang"
      ],
      "abstract": "Affective computing has made significant strides in emotion recognition and\ngeneration, yet current approaches mainly focus on short-term pattern\nrecognition and lack a comprehensive framework to guide affective agents toward\nlong-term human well-being. To address this, we propose a teleology-driven\naffective computing framework that unifies major emotion theories (basic\nemotion, appraisal, and constructivist approaches) under the premise that\naffect is an adaptive, goal-directed process that facilitates survival and\ndevelopment. Our framework emphasizes aligning agent responses with both\npersonal/individual and group/collective well-being over extended timescales.\nWe advocate for creating a \"dataverse\" of personal affective events, capturing\nthe interplay between beliefs, goals, actions, and outcomes through real-world\nexperience sampling and immersive virtual reality. By leveraging causal\nmodeling, this \"dataverse\" enables AI systems to infer individuals' unique\naffective concerns and provide tailored interventions for sustained well-being.\nAdditionally, we introduce a meta-reinforcement learning paradigm to train\nagents in simulated environments, allowing them to adapt to evolving affective\nconcerns and balance hierarchical goals - from immediate emotional needs to\nlong-term self-actualization. This framework shifts the focus from statistical\ncorrelations to causal reasoning, enhancing agents' ability to predict and\nrespond proactively to emotional challenges, and offers a foundation for\ndeveloping personalized, ethically aligned affective systems that promote\nmeaningful human-AI interactions and societal well-being.",
      "pdf_url": "http://arxiv.org/pdf/2502.17172v1",
      "arxiv_url": "http://arxiv.org/abs/2502.17172v1",
      "published": "2025-02-24",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CY",
        "q-bio.NC",
        "H.1.2, J.4",
        "H.1.2; J.4"
      ]
    },
    {
      "title": "A tutorial on optimal dynamic treatment regimes",
      "authors": [
        "Chunyu Wang",
        "Brian DM Tom"
      ],
      "abstract": "A dynamic treatment regime is a sequence of treatment decision rules tailored\nto an individual's evolving status over time. In precision medicine, much focus\nhas been placed on finding an optimal dynamic treatment regime which, if\nfollowed by everyone in the population, would yield the best outcome on\naverage; and extensive investigation has been conducted from both\nmethodological and applications standpoints. The aim of this tutorial is to\nprovide readers who are interested in optimal dynamic treatment regimes with a\nsystematic, detailed but accessible introduction, including the formal\ndefinition and formulation of this topic within the framework of causal\ninference, identification assumptions required to link the causal quantity of\ninterest to the observed data, existing statistical models and estimation\nmethods to learn the optimal regime from data, and application of these methods\nto both simulated and real data.",
      "pdf_url": "http://arxiv.org/pdf/2502.16988v1",
      "arxiv_url": "http://arxiv.org/abs/2502.16988v1",
      "published": "2025-02-24",
      "categories": [
        "stat.OT",
        "stat.AP"
      ]
    },
    {
      "title": "Time Series Domain Adaptation via Latent Invariant Causal Mechanism",
      "authors": [
        "Ruichu Cai",
        "Junxian Huang",
        "Zhenhui Yang",
        "Zijian Li",
        "Emadeldeen Eldele",
        "Min Wu",
        "Fuchun Sun"
      ],
      "abstract": "Time series domain adaptation aims to transfer the complex temporal\ndependence from the labeled source domain to the unlabeled target domain.\nRecent advances leverage the stable causal mechanism over observed variables to\nmodel the domain-invariant temporal dependence. However, modeling precise\ncausal structures in high-dimensional data, such as videos, remains\nchallenging. Additionally, direct causal edges may not exist among observed\nvariables (e.g., pixels). These limitations hinder the applicability of\nexisting approaches to real-world scenarios. To address these challenges, we\nfind that the high-dimension time series data are generated from the\nlow-dimension latent variables, which motivates us to model the causal\nmechanisms of the temporal latent process. Based on this intuition, we propose\na latent causal mechanism identification framework that guarantees the\nuniqueness of the reconstructed latent causal structures. Specifically, we\nfirst identify latent variables by utilizing sufficient changes in historical\ninformation. Moreover, by enforcing the sparsity of the relationships of latent\nvariables, we can achieve identifiable latent causal structures. Built on the\ntheoretical results, we develop the Latent Causality Alignment (LCA) model that\nleverages variational inference, which incorporates an intra-domain latent\nsparsity constraint for latent structure reconstruction and an inter-domain\nlatent sparsity constraint for domain-invariant structure reconstruction.\nExperiment results on eight benchmarks show a general improvement in the\ndomain-adaptive time series classification and forecasting tasks, highlighting\nthe effectiveness of our method in real-world scenarios. Codes are available at\nhttps://github.com/DMIRLAB-Group/LCA.",
      "pdf_url": "http://arxiv.org/pdf/2502.16637v1",
      "arxiv_url": "http://arxiv.org/abs/2502.16637v1",
      "published": "2025-02-23",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ME"
      ]
    }
  ]
}