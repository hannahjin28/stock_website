{
  "last_updated": "2025-07-18T00:57:50.650092",
  "papers": [
    {
      "title": "Targeted Deep Architectures: A TMLE-Based Framework for Robust Causal Inference in Neural Networks",
      "authors": [
        "Yi Li",
        "David Mccoy",
        "Nolan Gunter",
        "Kaitlyn Lee",
        "Alejandro Schuler",
        "Mark van der Laan"
      ],
      "abstract": "Modern deep neural networks are powerful predictive tools yet often lack\nvalid inference for causal parameters, such as treatment effects or entire\nsurvival curves. While frameworks like Double Machine Learning (DML) and\nTargeted Maximum Likelihood Estimation (TMLE) can debias machine-learning fits,\nexisting neural implementations either rely on \"targeted losses\" that do not\nguarantee solving the efficient influence function equation or computationally\nexpensive post-hoc \"fluctuations\" for multi-parameter settings. We propose\nTargeted Deep Architectures (TDA), a new framework that embeds TMLE directly\ninto the network's parameter space with no restrictions on the backbone\narchitecture. Specifically, TDA partitions model parameters - freezing all but\na small \"targeting\" subset - and iteratively updates them along a targeting\ngradient, derived from projecting the influence functions onto the span of the\ngradients of the loss with respect to weights. This procedure yields plug-in\nestimates that remove first-order bias and produce asymptotically valid\nconfidence intervals. Crucially, TDA easily extends to multi-dimensional causal\nestimands (e.g., entire survival curves) by merging separate targeting\ngradients into a single universal targeting update. Theoretically, TDA inherits\nclassical TMLE properties, including double robustness and semiparametric\nefficiency. Empirically, on the benchmark IHDP dataset (average treatment\neffects) and simulated survival data with informative censoring, TDA reduces\nbias and improves coverage relative to both standard neural-network estimators\nand prior post-hoc approaches. In doing so, TDA establishes a direct, scalable\npathway toward rigorous causal inference within modern deep architectures for\ncomplex multi-parameter targets.",
      "pdf_url": "http://arxiv.org/pdf/2507.12435v1",
      "arxiv_url": "http://arxiv.org/abs/2507.12435v1",
      "published": "2025-07-16",
      "categories": [
        "cs.LG"
      ]
    },
    {
      "title": "Robust Causal Discovery in Real-World Time Series with Power-Laws",
      "authors": [
        "Matteo Tusoni",
        "Giuseppe Masi",
        "Andrea Coletta",
        "Aldo Glielmo",
        "Viviana Arrigoni",
        "Novella Bartolini"
      ],
      "abstract": "Exploring causal relationships in stochastic time series is a challenging yet\ncrucial task with a vast range of applications, including finance, economics,\nneuroscience, and climate science. Many algorithms for Causal Discovery (CD)\nhave been proposed, but they often exhibit a high sensitivity to noise,\nresulting in misleading causal inferences when applied to real data. In this\npaper, we observe that the frequency spectra of typical real-world time series\nfollow a power-law distribution, notably due to an inherent self-organizing\nbehavior. Leveraging this insight, we build a robust CD method based on the\nextraction of power -law spectral features that amplify genuine causal signals.\nOur method consistently outperforms state-of-the-art alternatives on both\nsynthetic benchmarks and real-world datasets with known causal structures,\ndemonstrating its robustness and practical relevance.",
      "pdf_url": "http://arxiv.org/pdf/2507.12257v1",
      "arxiv_url": "http://arxiv.org/abs/2507.12257v1",
      "published": "2025-07-16",
      "categories": [
        "cs.LG",
        "physics.data-an",
        "stat.ML",
        "stat.OT"
      ]
    },
    {
      "title": "PRISM: Distributed Inference for Foundation Models at Edge",
      "authors": [
        "Muhammad Azlan Qazi",
        "Alexandros Iosifidis",
        "Qi Zhang"
      ],
      "abstract": "Foundation models (FMs) have achieved remarkable success across a wide range\nof applications, from image classification to natural langurage processing, but\npose significant challenges for deployment at edge. This has sparked growing\ninterest in developing practical and efficient strategies for bringing\nfoundation models to edge environments. In this work, we propose PRISM, a\ncommunication-efficient and compute-aware strategy for distributed Transformer\ninference on edge devices. Our method leverages a Segment Means representation\nto approximate intermediate output features, drastically reducing inter-device\ncommunication. Additionally, we restructure the self-attention mechanism to\neliminate redundant computations caused by per-device Key/Value calculation in\nposition-wise partitioning and design a partition-aware causal masking scheme\ntailored for autoregressive models. We evaluate PRISM on ViT, BERT, and GPT-2\nacross diverse datasets, namely CIFAR-10, CIFAR-100, ImageNet-1k, GLUE, and\nCBT. Our results demonstrate substantial reductions in communication overhead\n(up to 99.2% for BERT at compression rate CR = 128) and per-device computation\n(51.24% for BERT at the same setting), with only minor accuracy degradation.\nThis method offers a scalable and practical solution for deploying foundation\nmodels in distributed resource-constrained environments.",
      "pdf_url": "http://arxiv.org/pdf/2507.12145v1",
      "arxiv_url": "http://arxiv.org/abs/2507.12145v1",
      "published": "2025-07-16",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "Inference on Optimal Policy Values and Other Irregular Functionals via Smoothing",
      "authors": [
        "Justin Whitehouse",
        "Morgane Austern",
        "Vasilis Syrgkanis"
      ],
      "abstract": "Constructing confidence intervals for the value of an optimal treatment\npolicy is an important problem in causal inference. Insight into the optimal\npolicy value can guide the development of reward-maximizing, individualized\ntreatment regimes. However, because the functional that defines the optimal\nvalue is non-differentiable, standard semi-parametric approaches for performing\ninference fail to be directly applicable. Existing approaches for handling this\nnon-differentiability fall roughly into two camps. In one camp are estimators\nbased on constructing smooth approximations of the optimal value. These\napproaches are computationally lightweight, but typically place unrealistic\nparametric assumptions on outcome regressions. In another camp are approaches\nthat directly de-bias the non-smooth objective. These approaches don't place\nparametric assumptions on nuisance functions, but they either require the\ncomputation of intractably-many nuisance estimates, assume unrealistic\n$L^\\infty$ nuisance convergence rates, or make strong margin assumptions that\nprohibit non-response to a treatment. In this paper, we revisit the problem of\nconstructing smooth approximations of non-differentiable functionals. By\ncarefully controlling first-order bias and second-order remainders, we show\nthat a softmax smoothing-based estimator can be used to estimate parameters\nthat are specified as a maximum of scores involving nuisance components. In\nparticular, this includes the value of the optimal treatment policy as a\nspecial case. Our estimator obtains $\\sqrt{n}$ convergence rates, avoids\nparametric restrictions/unrealistic margin assumptions, and is often\nstatistically efficient.",
      "pdf_url": "http://arxiv.org/pdf/2507.11780v1",
      "arxiv_url": "http://arxiv.org/abs/2507.11780v1",
      "published": "2025-07-15",
      "categories": [
        "econ.EM",
        "cs.LG",
        "math.ST",
        "stat.ME",
        "stat.TH"
      ]
    },
    {
      "title": "Constructing targeted minimum loss/maximum likelihood estimators: a simple illustration to build intuition",
      "authors": [
        "Rachael K. Ross",
        "Lina M. Montoya",
        "Dana E. Goin",
        "Ivan Diaz",
        "Audrey Renson"
      ],
      "abstract": "Use of machine learning to estimate nuisance functions (e.g. outcomes models,\npropensity score models) in estimators used in causal inference is increasingly\ncommon, as it can mitigate bias due to model misspecification. However, it can\nbe challenging to achieve valid inference (e.g., estimate valid confidence\nintervals). The efficient influence function (EIF) provides a recipe to go from\na statistical estimand relevant to our causal question, to an estimator that\ncan validly incorporate machine learning. Our companion paper, Renson et al.\n2025 (arXiv:2502.05363), provides a thorough but approachable description of\nthe EIF, along with a guide through the steps to go from a unique statistical\nestimand to development of one type of EIF-based estimator, the so-called\none-step estimator. Another commonly used estimator based on the EIF is the\ntargeted maximum likelihood/minimum loss estimator (TMLE). Construction of\nTMLEs is well-discussed in the statistical literature, but there remains a gap\nin translation to a more applied audience. In this letter, which supplements\nRenson et al., we provide a more accessible illustration of how to construct a\nTMLE.",
      "pdf_url": "http://arxiv.org/pdf/2507.11680v1",
      "arxiv_url": "http://arxiv.org/abs/2507.11680v1",
      "published": "2025-07-15",
      "categories": [
        "stat.ME"
      ]
    },
    {
      "title": "Streaming 4D Visual Geometry Transformer",
      "authors": [
        "Dong Zhuo",
        "Wenzhao Zheng",
        "Jiahe Guo",
        "Yuqi Wu",
        "Jie Zhou",
        "Jiwen Lu"
      ],
      "abstract": "Perceiving and reconstructing 4D spatial-temporal geometry from videos is a\nfundamental yet challenging computer vision task. To facilitate interactive and\nreal-time applications, we propose a streaming 4D visual geometry transformer\nthat shares a similar philosophy with autoregressive large language models. We\nexplore a simple and efficient design and employ a causal transformer\narchitecture to process the input sequence in an online manner. We use temporal\ncausal attention and cache the historical keys and values as implicit memory to\nenable efficient streaming long-term 4D reconstruction. This design can handle\nreal-time 4D reconstruction by incrementally integrating historical information\nwhile maintaining high-quality spatial consistency. For efficient training, we\npropose to distill knowledge from the dense bidirectional visual geometry\ngrounded transformer (VGGT) to our causal model. For inference, our model\nsupports the migration of optimized efficient attention operator (e.g.,\nFlashAttention) from the field of large language models. Extensive experiments\non various 4D geometry perception benchmarks demonstrate that our model\nincreases the inference speed in online scenarios while maintaining competitive\nperformance, paving the way for scalable and interactive 4D vision systems.\nCode is available at: https://github.com/wzzheng/StreamVGGT.",
      "pdf_url": "http://arxiv.org/pdf/2507.11539v1",
      "arxiv_url": "http://arxiv.org/abs/2507.11539v1",
      "published": "2025-07-15",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Gradient Regularization-based Neural Granger Causality",
      "authors": [
        "Meiliang Liu",
        "Huiwen Dong",
        "Xiaoxiao Yang",
        "Yunfang Xu",
        "Zijin Li",
        "Zhengye Si",
        "Xinyue Yang",
        "Zhiwen Zhao"
      ],
      "abstract": "With the advancement of deep learning technologies, various neural\nnetwork-based Granger causality models have been proposed. Although these\nmodels have demonstrated notable improvements, several limitations remain. Most\nexisting approaches adopt the component-wise architecture, necessitating the\nconstruction of a separate model for each time series, which results in\nsubstantial computational costs. In addition, imposing the sparsity-inducing\npenalty on the first-layer weights of the neural network to extract causal\nrelationships weakens the model's ability to capture complex interactions. To\naddress these limitations, we propose Gradient Regularization-based Neural\nGranger Causality (GRNGC), which requires only one time series prediction model\nand applies $L_{1}$ regularization to the gradient between model's input and\noutput to infer Granger causality. Moreover, GRNGC is not tied to a specific\ntime series forecasting model and can be implemented with diverse architectures\nsuch as KAN, MLP, and LSTM, offering enhanced flexibility. Numerical\nsimulations on DREAM, Lorenz-96, fMRI BOLD, and CausalTime show that GRNGC\noutperforms existing baselines and significantly reduces computational\noverhead. Meanwhile, experiments on real-world DNA, Yeast, HeLa, and bladder\nurothelial carcinoma datasets further validate the model's effectiveness in\nreconstructing gene regulatory networks.",
      "pdf_url": "http://arxiv.org/pdf/2507.11178v1",
      "arxiv_url": "http://arxiv.org/abs/2507.11178v1",
      "published": "2025-07-15",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Uncovering Causal Relation Shifts in Event Sequences under Out-of-Domain Interventions",
      "authors": [
        "Kazi Tasnim Zinat",
        "Yun Zhou",
        "Xiang Lyu",
        "Yawei Wang",
        "Zhicheng Liu",
        "Panpan Xu"
      ],
      "abstract": "Inferring causal relationships between event pairs in a temporal sequence is\napplicable in many domains such as healthcare, manufacturing, and\ntransportation. Most existing work on causal inference primarily focuses on\nevent types within the designated domain, without considering the impact of\nexogenous out-of-domain interventions. In real-world settings, these\nout-of-domain interventions can significantly alter causal dynamics. To address\nthis gap, we propose a new causal framework to define average treatment effect\n(ATE), beyond independent and identically distributed (i.i.d.) data in classic\nRubin's causal framework, to capture the causal relation shift between events\nof temporal process under out-of-domain intervention. We design an unbiased ATE\nestimator, and devise a Transformer-based neural network model to handle both\nlong-range temporal dependencies and local patterns while integrating\nout-of-domain intervention information into process modeling. Extensive\nexperiments on both simulated and real-world datasets demonstrate that our\nmethod outperforms baselines in ATE estimation and goodness-of-fit under\nout-of-domain-augmented point processes.",
      "pdf_url": "http://arxiv.org/pdf/2507.10809v1",
      "arxiv_url": "http://arxiv.org/abs/2507.10809v1",
      "published": "2025-07-14",
      "categories": [
        "cs.LG",
        "stat.ML"
      ]
    },
    {
      "title": "Propensity score weighting across counterfactual worlds: longitudinal effects under positivity violations",
      "authors": [
        "Alec McClean",
        "Iván Díaz"
      ],
      "abstract": "When examining a contrast between two interventions, longitudinal causal\ninference studies frequently encounter positivity violations when one or both\nregimes are impossible to observe for some subjects. Existing weighting methods\neither assume positivity holds or produce effects that conflate interventions'\nimpacts on ultimate outcomes with their effects on intermediate treatments and\ncovariates. We propose a novel class of estimands -- cumulative cross-world\nweighted effects -- that weights potential outcome differences using propensity\nscores adapting to positivity violations cumulatively across timepoints and\nsimultaneously across both counterfactual treatment histories. This new\nestimand isolates mechanistic differences between treatment regimes, is\nidentifiable without positivity assumptions, and circumvents the limitations of\nexisting longitudinal methods. Further, our analysis reveals two fundamental\ninsights about longitudinal causal inference under positivity violations.\nFirst, while mechanistically meaningful, these effects correspond to\nnon-implementable interventions, exposing a core\ninterpretability-implementability tradeoff. Second, the identified effects\nfaithfully capture mechanistic differences only under a partial common support\nassumption; violations cause the identified functional to collapse to zero,\neven when the causal effect is non-zero. We develop doubly robust-style\nestimators that achieve asymptotic normality and parametric convergence under\nnonparametric assumptions on the nuisance estimators. To this end, we\nreformulate challenging density ratio estimation as regression function\nestimation, which is achievable with standard machine learning methods. We\nillustrate our methods through analysis of union membership's effect on\nearnings.",
      "pdf_url": "http://arxiv.org/pdf/2507.10774v1",
      "arxiv_url": "http://arxiv.org/abs/2507.10774v1",
      "published": "2025-07-14",
      "categories": [
        "stat.ME"
      ]
    },
    {
      "title": "Causal Discovery for Linear Non-Gaussian Models with Disjoint Cycles",
      "authors": [
        "Mathias Drton",
        "Marina Garrote-López",
        "Niko Nikov",
        "Elina Robeva",
        "Y. Samuel Wang"
      ],
      "abstract": "The paradigm of linear structural equation modeling readily allows one to\nincorporate causal feedback loops in the model specification. These appear as\ndirected cycles in the common graphical representation of the models. However,\nthe presence of cycles entails difficulties such as the fact that models need\nno longer be characterized by conditional independence relations. As a result,\nlearning cyclic causal structures remains a challenging problem. In this paper,\nwe offer new insights on this problem in the context of linear non-Gaussian\nmodels. First, we precisely characterize when two directed graphs determine the\nsame linear non-Gaussian model. Next, we take up a setting of cycle-disjoint\ngraphs, for which we are able to show that simple quadratic and cubic\npolynomial relations among low-order moments of a non-Gaussian distribution\nallow one to locate source cycles. Complementing this with a strategy of\ndecorrelating cycles and multivariate regression allows one to infer a\nblock-topological order among the directed cycles, which leads to a {consistent\nand computationally efficient algorithm} for learning causal structures with\ndisjoint cycles.",
      "pdf_url": "http://arxiv.org/pdf/2507.10767v1",
      "arxiv_url": "http://arxiv.org/abs/2507.10767v1",
      "published": "2025-07-14",
      "categories": [
        "math.ST",
        "stat.ME",
        "stat.ML",
        "stat.TH",
        "62H22, 62D20, 62R01"
      ]
    }
  ]
}