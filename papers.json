{
  "last_updated": "2025-10-24T00:47:59.045873",
  "papers": [
    {
      "title": "Benchmarking World-Model Learning",
      "authors": [
        "Archana Warrier",
        "Dat Nguyen",
        "Michelangelo Naim",
        "Moksh Jain",
        "Yichao Liang",
        "Karen Schroeder",
        "Cambridge Yang",
        "Joshua B. Tenenbaum",
        "Sebastian Vollmer",
        "Kevin Ellis",
        "Zenna Tavares"
      ],
      "abstract": "Model-learning agents should gather information to learn world models that\nsupport many downstream tasks and inferences, such as predicting unobserved\nstates, estimating near- and far-term consequences of actions, planning action\nsequences, and detecting changes in dynamics. Current methods for learning and\nevaluating world models diverge from this goal: training and evaluation are\nanchored to next-frame prediction, and success is scored by reward maximization\nin the same environment. We propose WorldTest, a protocol to evaluate\nmodel-learning agents that separates reward-free interaction from a scored test\nphase in a different but related environment. WorldTest is\nopen-ended$\\unicode{x2014}$models should support many different tasks unknown\nahead of time$\\unicode{x2014}$and agnostic to model representation, allowing\ncomparison across approaches. We instantiated WorldTest with AutumnBench, a\nsuite of 43 interactive grid-world environments and 129 tasks across three\nfamilies: masked-frame prediction, planning, and predicting changes to the\ncausal dynamics. We compared 517 human participants and three frontier models\non AutumnBench. We found that humans outperform the models, and scaling compute\nimproves performance only in some environments but not others. WorldTest\nprovides a novel template$\\unicode{x2014}$reward-free exploration, derived\ntests, and behavior-based scoring$\\unicode{x2014}$to evaluate what agents learn\nabout environment dynamics, and AutumnBench exposes significant headroom in\nworld-model learning.",
      "pdf_url": "http://arxiv.org/pdf/2510.19788v2",
      "arxiv_url": "http://arxiv.org/abs/2510.19788v2",
      "published": "2025-10-22",
      "categories": [
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Lookahead Routing for Large Language Models",
      "authors": [
        "Canbin Huang",
        "Tianyuan Shi",
        "Yuhua Zhu",
        "Ruijun Chen",
        "Xiaojun Quan"
      ],
      "abstract": "Large language model (LLM) routers improve the efficiency of multi-model\nsystems by directing each query to the most appropriate model while leveraging\nthe diverse strengths of heterogeneous LLMs. Most existing approaches frame\nrouting as a classification problem based solely on the input query. While this\nreduces overhead by avoiding inference across all models, it overlooks valuable\ninformation that could be gleaned from potential outputs and fails to capture\nimplicit intent or contextual nuances that often emerge only during response\ngeneration. These limitations can result in suboptimal routing decisions,\nparticularly for complex or ambiguous queries that require deeper semantic\nunderstanding. To address this challenge, we propose Lookahead, a routing\nframework that \"foresees\" potential model outputs by predicting their latent\nrepresentations and uses these predictions to guide model selection, thus\nenabling more informed routing without full inference. Within this framework,\nwe implement two approaches based on causal and masked language models.\nEmpirical evaluations across seven public benchmarks - spanning instruction\nfollowing, mathematical reasoning, and code generation - show that Lookahead\nconsistently outperforms existing routing baselines, achieving an average\nperformance gain of 7.7% over the state-of-the-art. Our code is available at\nhttps://github.com/huangcb01/lookahead-routing.",
      "pdf_url": "http://arxiv.org/pdf/2510.19506v1",
      "arxiv_url": "http://arxiv.org/abs/2510.19506v1",
      "published": "2025-10-22",
      "categories": [
        "cs.CL"
      ]
    },
    {
      "title": "A New Targeted-Federated Learning Framework for Estimating Heterogeneity of Treatment Effects: A Robust Framework with Applications in Aging Cohorts",
      "authors": [
        "Rong Zhao",
        "Jason Falvey",
        "Xu Shi",
        "Vernon M. Chinchilli",
        "Chixiang Chen"
      ],
      "abstract": "Analyzing data from multiple sources offers valuable opportunities to improve\nthe estimation efficiency of causal estimands. However, this analysis also\nposes many challenges due to population heterogeneity and data privacy\nconstraints. While several advanced methods for causal inference in federated\nsettings have been developed in recent years, many focus on difference-based\naveraged causal effects and are not designed to study effect modification. In\nthis study, we introduce a novel targeted-federated learning framework to study\nthe heterogeneity of treatment effects (HTEs) for a targeted population by\nproposing a projection-based estimand. This HTE framework integrates\ninformation from multiple data sources without sharing raw data, while\naccounting for covariate distribution shifts among sources. Our proposed\napproach is shown to be doubly robust, conveniently supporting both\ndifference-based estimands for continuous outcomes and odds ratio-based\nestimands for binary outcomes. Furthermore, we develop a\ncommunication-efficient bootstrap-based selection procedure to detect\nnon-transportable data sources, thereby enhancing robust information\naggregation without introducing bias. The superior performance of the proposed\nestimator over existing methods is demonstrated through extensive simulation\nstudies, and the utility of our approach has been shown in a real-world data\napplication using nationwide Medicare-linked data.",
      "pdf_url": "http://arxiv.org/pdf/2510.19243v1",
      "arxiv_url": "http://arxiv.org/abs/2510.19243v1",
      "published": "2025-10-22",
      "categories": [
        "stat.ME"
      ]
    },
    {
      "title": "No Intelligence Without Statistics: The Invisible Backbone of Artificial Intelligence",
      "authors": [
        "Ernest Fokoué"
      ],
      "abstract": "The rapid ascent of artificial intelligence (AI) is often portrayed as a\nrevolution born from computer science and engineering. This narrative, however,\nobscures a fundamental truth: the theoretical and methodological core of AI is,\nand has always been, statistical. This paper systematically argues that the\nfield of statistics provides the indispensable foundation for machine learning\nand modern AI. We deconstruct AI into nine foundational pillars-Inference,\nDensity Estimation, Sequential Learning, Generalization, Representation\nLearning, Interpretability, Causality, Optimization, and\nUnification-demonstrating that each is built upon century-old statistical\nprinciples. From the inferential frameworks of hypothesis testing and\nestimation that underpin model evaluation, to the density estimation roots of\nclustering and generative AI; from the time-series analysis inspiring recurrent\nnetworks to the causal models that promise true understanding, we trace an\nunbroken statistical lineage. While celebrating the computational engines that\npower modern AI, we contend that statistics provides the brain-the theoretical\nframeworks, uncertainty quantification, and inferential goals-while computer\nscience provides the brawn-the scalable algorithms and hardware. Recognizing\nthis statistical backbone is not merely an academic exercise, but a necessary\nstep for developing more robust, interpretable, and trustworthy intelligent\nsystems. We issue a call to action for education, research, and practice to\nre-embrace this statistical foundation. Ignoring these roots risks building a\nfragile future; embracing them is the path to truly intelligent machines. There\nis no machine learning without statistical learning; no artificial intelligence\nwithout statistical thought.",
      "pdf_url": "http://arxiv.org/pdf/2510.19212v1",
      "arxiv_url": "http://arxiv.org/abs/2510.19212v1",
      "published": "2025-10-22",
      "categories": [
        "stat.ME",
        "cs.AI",
        "62-02, 68T01",
        "G.3; I.2.6"
      ]
    },
    {
      "title": "Desirable Effort Fairness and Optimality Trade-offs in Strategic Learning",
      "authors": [
        "Valia Efthymiou",
        "Ekaterina Fedorova",
        "Chara Podimata"
      ],
      "abstract": "Strategic learning studies how decision rules interact with agents who may\nstrategically change their inputs/features to achieve better outcomes. In\nstandard settings, models assume that the decision-maker's sole scope is to\nlearn a classifier that maximizes an objective (e.g., accuracy) assuming that\nagents best respond. However, real decision-making systems' goals do not align\nexclusively with producing good predictions. They may consider the external\neffects of inducing certain incentives, which translates to the change of\ncertain features being more desirable for the decision maker. Further, the\nprincipal may also need to incentivize desirable feature changes fairly across\nheterogeneous agents. How much does this constrained optimization (i.e.,\nmaximize the objective, but restrict agents' incentive disparity) cost the\nprincipal? We propose a unified model of principal-agent interaction that\ncaptures this trade-off under three additional components: (1) causal\ndependencies between features, such that changes in one feature affect others;\n(2) heterogeneous manipulation costs between agents; and (3) peer learning,\nthrough which agents infer the principal's rule. We provide theoretical\nguarantees on the principal's optimality loss constrained to a particular\ndesirability fairness tolerance for multiple broad classes of fairness\nmeasures. Finally, through experiments on real datasets, we show the explicit\ntradeoff between maximizing accuracy and fairness in desirability effort.",
      "pdf_url": "http://arxiv.org/pdf/2510.19098v1",
      "arxiv_url": "http://arxiv.org/abs/2510.19098v1",
      "published": "2025-10-21",
      "categories": [
        "cs.GT",
        "cs.CY"
      ]
    },
    {
      "title": "A Hybrid Enumeration Framework for Optimal Counterfactual Generation in Post-Acute COVID-19 Heart Failure",
      "authors": [
        "Jingya Cheng",
        "Alaleh Azhir",
        "Jiazi Tian",
        "Hossein Estiri"
      ],
      "abstract": "Counterfactual inference provides a mathematical framework for reasoning\nabout hypothetical outcomes under alternative interventions, bridging causal\nreasoning and predictive modeling. We present a counterfactual inference\nframework for individualized risk estimation and intervention analysis,\nillustrated through a clinical application to post-acute sequelae of COVID-19\n(PASC) among patients with pre-existing heart failure (HF). Using longitudinal\ndiagnosis, laboratory, and medication data from a large health-system cohort,\nwe integrate regularized predictive modeling with counterfactual search to\nidentify actionable pathways to PASC-related HF hospital admissions. The\nframework combines exact enumeration with optimization-based methods, including\nthe Nearest Instance Counterfactual Explanations (NICE) and Multi-Objective\nCounterfactuals (MOC) algorithms, to efficiently explore high-dimensional\nintervention spaces. Applied to more than 2700 individuals with confirmed\nSARS-CoV-2 infection and prior HF, the model achieved strong discriminative\nperformance (AUROC: 0.88, 95% CI: 0.84-0.91) and generated interpretable,\npatient-specific counterfactuals that quantify how modifying comorbidity\npatterns or treatment factors could alter predicted outcomes. This work\ndemonstrates how counterfactual reasoning can be formalized as an optimization\nproblem over predictive functions, offering a rigorous, interpretable, and\ncomputationally efficient approach to personalized inference in complex\nbiomedical systems.",
      "pdf_url": "http://arxiv.org/pdf/2510.18841v1",
      "arxiv_url": "http://arxiv.org/abs/2510.18841v1",
      "published": "2025-10-21",
      "categories": [
        "cs.LG"
      ]
    },
    {
      "title": "Improving the Generation and Evaluation of Synthetic Data for Downstream Medical Causal Inference",
      "authors": [
        "Harry Amad",
        "Zhaozhi Qian",
        "Dennis Frauen",
        "Julianna Piskorz",
        "Stefan Feuerriegel",
        "Mihaela van der Schaar"
      ],
      "abstract": "Causal inference is essential for developing and evaluating medical\ninterventions, yet real-world medical datasets are often difficult to access\ndue to regulatory barriers. This makes synthetic data a potentially valuable\nasset that enables these medical analyses, along with the development of new\ninference methods themselves. Generative models can produce synthetic data that\nclosely approximate real data distributions, yet existing methods do not\nconsider the unique challenges that downstream causal inference tasks, and\nspecifically those focused on treatments, pose. We establish a set of\ndesiderata that synthetic data containing treatments should satisfy to maximise\ndownstream utility: preservation of (i) the covariate distribution, (ii) the\ntreatment assignment mechanism, and (iii) the outcome generation mechanism.\nBased on these desiderata, we propose a set of evaluation metrics to assess\nsuch synthetic data. Finally, we present STEAM: a novel method for generating\nSynthetic data for Treatment Effect Analysis in Medicine that mimics the\ndata-generating process of data containing treatments and optimises for our\ndesiderata. We empirically demonstrate that STEAM achieves state-of-the-art\nperformance across our metrics as compared to existing generative models,\nparticularly as the complexity of the true data-generating process increases.",
      "pdf_url": "http://arxiv.org/pdf/2510.18768v1",
      "arxiv_url": "http://arxiv.org/abs/2510.18768v1",
      "published": "2025-10-21",
      "categories": [
        "cs.LG"
      ]
    },
    {
      "title": "Causally Perturbed Fairness Testing",
      "authors": [
        "Chengwen Du",
        "Tao Chen"
      ],
      "abstract": "To mitigate unfair and unethical discrimination over sensitive features\n(e.g., gender, age, or race), fairness testing plays an integral role in\nengineering systems that leverage AI models to handle tabular data. A key\nchallenge therein is how to effectively reveal fairness bugs under an\nintractable sample size using perturbation. Much current work has been focusing\non designing the test sample generators, ignoring the valuable knowledge about\ndata characteristics that can help guide the perturbation and hence limiting\ntheir full potential. In this paper, we seek to bridge such a gap by proposing\na generic framework of causally perturbed fairness testing, dubbed CausalFT.\nThrough causal inference, the key idea of CausalFT is to extract the most\ndirectly and causally relevant non-sensitive feature to its sensitive\ncounterpart, which can jointly influence the prediction of the label. Such a\ncausal relationship is then seamlessly injected into the perturbation to guide\na test sample generator. Unlike existing generator-level work, CausalFT serves\nas a higher-level framework that can be paired with diverse base generators.\nExtensive experiments on 1296 cases confirm that CausalFT can considerably\nimprove arbitrary base generators in revealing fairness bugs over 93% of the\ncases with acceptable extra runtime overhead. Compared with a state-of-the-art\napproach that ranks the non-sensitive features solely based on correlation,\nCausalFT performs significantly better on 64% cases while being much more\nefficient. Further, CausalFT can better improve bias resilience in nearly all\ncases.",
      "pdf_url": "http://arxiv.org/pdf/2510.18719v1",
      "arxiv_url": "http://arxiv.org/abs/2510.18719v1",
      "published": "2025-10-21",
      "categories": [
        "cs.SE",
        "cs.AI"
      ]
    },
    {
      "title": "C-SWAP: Explainability-Aware Structured Pruning for Efficient Neural Networks Compression",
      "authors": [
        "Baptiste Bauvin",
        "Loïc Baret",
        "Ola Ahmad"
      ],
      "abstract": "Neural network compression has gained increasing attention in recent years,\nparticularly in computer vision applications, where the need for model\nreduction is crucial for overcoming deployment constraints. Pruning is a widely\nused technique that prompts sparsity in model structures, e.g. weights,\nneurons, and layers, reducing size and inference costs. Structured pruning is\nespecially important as it allows for the removal of entire structures, which\nfurther accelerates inference time and reduces memory overhead. However, it can\nbe computationally expensive, requiring iterative retraining and optimization.\nTo overcome this problem, recent methods considered one-shot setting, which\napplies pruning directly at post-training. Unfortunately, they often lead to a\nconsiderable drop in performance. In this paper, we focus on this issue by\nproposing a novel one-shot pruning framework that relies on explainable deep\nlearning. First, we introduce a causal-aware pruning approach that leverages\ncause-effect relations between model predictions and structures in a\nprogressive pruning process. It allows us to efficiently reduce the size of the\nnetwork, ensuring that the removed structures do not deter the performance of\nthe model. Then, through experiments conducted on convolution neural network\nand vision transformer baselines, pre-trained on classification tasks, we\ndemonstrate that our method consistently achieves substantial reductions in\nmodel size, with minimal impact on performance, and without the need for\nfine-tuning. Overall, our approach outperforms its counterparts, offering the\nbest trade-off. Our code is available on GitHub.",
      "pdf_url": "http://arxiv.org/pdf/2510.18636v1",
      "arxiv_url": "http://arxiv.org/abs/2510.18636v1",
      "published": "2025-10-21",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.RO"
      ]
    },
    {
      "title": "A Multi-Evidence Framework Rescues Low-Power Prognostic Signals and Rejects Statistical Artifacts in Cancer Genomics",
      "authors": [
        "Gokturk Aytug Akarlar"
      ],
      "abstract": "Motivation: Standard genome-wide association studies in cancer genomics rely\non statistical significance with multiple testing correction, but\nsystematically fail in underpowered cohorts. In TCGA breast cancer (n=967, 133\ndeaths), low event rates (13.8%) create severe power limitations, producing\nfalse negatives for known drivers and false positives for large passenger\ngenes. Results: We developed a five-criteria computational framework\nintegrating causal inference (inverse probability weighting, doubly robust\nestimation) with orthogonal biological validation (expression, mutation\npatterns, literature evidence). Applied to TCGA-BRCA mortality analysis,\nstandard Cox+FDR detected zero genes at FDR<0.05, confirming complete failure\nin underpowered settings. Our framework correctly identified RYR2 -- a cardiac\ngene with no cancer function -- as a false positive despite nominal\nsignificance (p=0.024), while identifying KMT2C as a complex candidate\nrequiring validation despite marginal significance (p=0.047, q=0.954). Power\nanalysis revealed median power of 15.1% across genes, with KMT2C achieving only\n29.8% power (HR=1.55), explaining borderline statistical significance despite\nstrong biological evidence. The framework distinguished true signals from\nartifacts through mutation pattern analysis: RYR2 showed 29.8% silent mutations\n(passenger signature) with no hotspots, while KMT2C showed 6.7% silent\nmutations with 31.4% truncating variants (driver signature). This\nmulti-evidence approach provides a template for analyzing underpowered cohorts,\nprioritizing biological interpretability over purely statistical significance.\n  Availability: All code and analysis pipelines available at\ngithub.com/akarlaraytu/causal-inference-for-cancer-genomics",
      "pdf_url": "http://arxiv.org/pdf/2510.18571v1",
      "arxiv_url": "http://arxiv.org/abs/2510.18571v1",
      "published": "2025-10-21",
      "categories": [
        "q-bio.GN",
        "cs.LG",
        "92B15, 62P10, 62F10, 62H17",
        "J.3; I.2.6; G.3"
      ]
    }
  ]
}