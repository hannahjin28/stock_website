{
  "last_updated": "2025-12-10T00:57:16.172488",
  "papers": [
    {
      "title": "Large Causal Models from Large Language Models",
      "authors": [
        "Sridhar Mahadevan"
      ],
      "abstract": "We introduce a new paradigm for building large causal models (LCMs) that exploits the enormous potential latent in today's large language models (LLMs). We describe our ongoing experiments with an implemented system called DEMOCRITUS (Decentralized Extraction of Manifold Ontologies of Causal Relations Integrating Topos Universal Slices) aimed at building, organizing, and visualizing LCMs that span disparate domains extracted from carefully targeted textual queries to LLMs. DEMOCRITUS is methodologically distinct from traditional narrow domain and hypothesis centered causal inference that builds causal models from experiments that produce numerical data. A high-quality LLM is used to propose topics, generate causal questions, and extract plausible causal statements from a diverse range of domains. The technical challenge is then to take these isolated, fragmented, potentially ambiguous and possibly conflicting causal claims, and weave them into a coherent whole, converting them into relational causal triples and embedding them into a LCM. Addressing this technical challenge required inventing new categorical machine learning methods, which we can only briefly summarize in this paper, as it is focused more on the systems side of building DEMOCRITUS. We describe the implementation pipeline for DEMOCRITUS comprising of six modules, examine its computational cost profile to determine where the current bottlenecks in scaling the system to larger models. We describe the results of using DEMOCRITUS over a wide range of domains, spanning archaeology, biology, climate change, economics, medicine and technology. We discuss the limitations of the current DEMOCRITUS system, and outline directions for extending its capabilities.",
      "pdf_url": "https://arxiv.org/pdf/2512.07796v1",
      "arxiv_url": "http://arxiv.org/abs/2512.07796v1",
      "published": "2025-12-08",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Generating Storytelling Images with Rich Chains-of-Reasoning",
      "authors": [
        "Xiujie Song",
        "Qi Jia",
        "Shota Watanabe",
        "Xiaoyi Pang",
        "Ruijie Chen",
        "Mengyue Wu",
        "Kenny Q. Zhu"
      ],
      "abstract": "An image can convey a compelling story by presenting rich, logically connected visual clues. These connections form Chains-of-Reasoning (CoRs) within the image, enabling viewers to infer events, causal relationships, and other information, thereby understanding the underlying story. In this paper, we focus on these semantically rich images and define them as Storytelling Images. Such images have diverse applications beyond illustration creation and cognitive screening, leveraging their ability to convey multi-layered information visually and inspire active interpretation. However, due to their complex semantic nature, Storytelling Images are inherently challenging to create, and thus remain relatively scarce. To address this challenge, we introduce the Storytelling Image Generation task, which explores how generative AI models can be leveraged to create such images. Specifically, we propose a two-stage pipeline, StorytellingPainter, which combines the creative reasoning abilities of Large Language Models (LLMs) with the visual synthesis capabilities of Text-to-Image (T2I) models to generate Storytelling Images. Alongside this pipeline, we develop a dedicated evaluation framework comprising three main evaluators: a Semantic Complexity Evaluator, a KNN-based Diversity Evaluator and a Story-Image Alignment Evaluator. Given the critical role of story generation in the Storytelling Image Generation task and the performance disparity between open-source and proprietary LLMs, we further explore tailored training strategies to reduce this gap, resulting in a series of lightweight yet effective models named Mini-Storytellers. Experimental results demonstrate the feasibility and effectiveness of our approaches. The code is available at https://github.com/xiujiesong/StorytellingImageGeneration.",
      "pdf_url": "https://arxiv.org/pdf/2512.07198v1",
      "arxiv_url": "http://arxiv.org/abs/2512.07198v1",
      "published": "2025-12-08",
      "categories": [
        "cs.CV",
        "cs.CL"
      ]
    },
    {
      "title": "SLOACI: Surrogate-Leveraged Online Adaptive Causal Inference",
      "authors": [
        "Yingying Fan",
        "Zihan Wang",
        "Waverly Wei"
      ],
      "abstract": "Adaptive experimental designs have gained increasing attention across a range of domains. In this paper, we propose a new methodological framework, surrogate-leveraged online adaptive causal inference (SLOACI), which integrates predictive surrogate outcomes into adaptive designs to enhance efficiency. For downstream analysis, we construct the adaptive augmented inverse probability weighting estimator for the average treatment effect using collected data. Our procedure remains robust even when surrogates are noisy or weak. We provide a comprehensive theoretical foundation for SLOACI. Under the asymptotic regime, we show that the proposed estimator attains the semiparametric efficiency bound. From a non-asymptotic perspective, we derive a regret bound to provide practical insights. We also develop a toolbox of sequential testing procedures that accommodates both asymptotic and non-asymptotic regimes, allowing experimenters to choose the perspective that best aligns with their practical needs. Extensive simulations and a synthetic case study are conducted to showcase the superior finite-sample performance of our method.",
      "pdf_url": "https://arxiv.org/pdf/2512.06872v1",
      "arxiv_url": "http://arxiv.org/abs/2512.06872v1",
      "published": "2025-12-07",
      "categories": [
        "stat.ME"
      ]
    },
    {
      "title": "Making Event Study Plots Honest: A Functional Data Approach to Causal Inference",
      "authors": [
        "Chencheng Fang",
        "Dominik Liebl"
      ],
      "abstract": "Event study plots are the centerpiece of Difference-in-Differences (DiD) analysis, but current plotting methods cannot provide honest causal inference when the parallel trends and/or no-anticipation assumptions fail. We introduce a novel functional data approach to DiD that directly enables honest causal inference via event study plots. Our DiD estimator converges to a Gaussian process in the Banach space of continuous functions, enabling fast and powerful simultaneous confidence bands. This theoretical contribution allows us to turn an event study plot into a rigorous honest causal inference tool through equivalence and relevance testing: Honest reference bands can be validated using equivalence testing in the pre-anticipation period, and honest causal effects can be tested using relevance testing in the post-treatment period. We demonstrate the performance of the method in simulations and two case studies.",
      "pdf_url": "https://arxiv.org/pdf/2512.06804v1",
      "arxiv_url": "http://arxiv.org/abs/2512.06804v1",
      "published": "2025-12-07",
      "categories": [
        "econ.EM"
      ]
    },
    {
      "title": "From Next-Token to Next-Block: A Principled Adaptation Path for Diffusion LLMs",
      "authors": [
        "Yuchuan Tian",
        "Yuchen Liang",
        "Jiacheng Sun",
        "Shuo Zhang",
        "Guangwen Yang",
        "Yingte Shu",
        "Sibo Fang",
        "Tianyu Guo",
        "Kai Han",
        "Chao Xu",
        "Hanting Chen",
        "Xinghao Chen",
        "Yunhe Wang"
      ],
      "abstract": "Large language models (LLMs) excel at generation but dominant autoregressive (AR) decoding is inherently sequential, creating a throughput bottleneck. Diffusion Language Models (DLMs)--especially block-wise variants--enable parallel generation and intra-block bidirectional reasoning, yet training large DLMs from scratch is costly and wastes the knowledge in mature AR checkpoints. Prior \"adaptation\" attempts either modify logits or randomly grow attention masks to full-sequence diffusion, or simply transplant AR weights into a block-diffusion recipe, leaving a fundamental mismatch between AR causality and block-wise bidirectionality unaddressed. We reframe adaptation as a intra-paradigm path from AR to Block-Diffusion by viewing AR as Block-Diffusion with blocksize=1. Concretely, we design the pathway of adaptation as follows: we use a context-causal attention mask (causal in context, bidirectional only within the active block), an efficient parallel adaptation procedure, an auxiliary AR loss to maximize data utilization and retain pretrained knowledge, and gradual increment of the generation block size. The recipe integrates cleanly with masked block-diffusion and maintains train-inference consistency. Built on these components, NBDiff-7B (Base and Instruct) could inherit the long-context modeling and reasoning capabilities, and achieve state-of-the-art performance among the 7B-class DLMs, delivering strong gains on general-knowledge, math, and code benchmarks over strong baselines. These results demonstrate that principled AR-to-block-diffusion adaptation is an effective and compute-efficient alternative to training DLMs from scratch. Codes: https://github.com/YuchuanTian/NBDiff.",
      "pdf_url": "https://arxiv.org/pdf/2512.06776v1",
      "arxiv_url": "http://arxiv.org/abs/2512.06776v1",
      "published": "2025-12-07",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "WAM-Flow: Parallel Coarse-to-Fine Motion Planning via Discrete Flow Matching for Autonomous Driving",
      "authors": [
        "Yifang Xu",
        "Jiahao Cui",
        "Feipeng Cai",
        "Zhihao Zhu",
        "Hanlin Shang",
        "Shan Luan",
        "Mingwang Xu",
        "Neng Zhang",
        "Yaoyi Li",
        "Jia Cai",
        "Siyu Zhu"
      ],
      "abstract": "We introduce WAM-Flow, a vision-language-action (VLA) model that casts ego-trajectory planning as discrete flow matching over a structured token space. In contrast to autoregressive decoders, WAM-Flow performs fully parallel, bidirectional denoising, enabling coarse-to-fine refinement with a tunable compute-accuracy trade-off. Specifically, the approach combines a metric-aligned numerical tokenizer that preserves scalar geometry via triplet-margin learning, a geometry-aware flow objective and a simulator-guided GRPO alignment that integrates safety, ego progress, and comfort rewards while retaining parallel generation. A multi-stage adaptation converts a pre-trained auto-regressive backbone (Janus-1.5B) from causal decoding to non-causal flow model and strengthens road-scene competence through continued multimodal pretraining. Thanks to the inherent nature of consistency model training and parallel decoding inference, WAM-Flow achieves superior closed-loop performance against autoregressive and diffusion-based VLA baselines, with 1-step inference attaining 89.1 PDMS and 5-step inference reaching 90.3 PDMS on NAVSIM v1 benchmark. These results establish discrete flow matching as a new promising paradigm for end-to-end autonomous driving. The code will be publicly available soon.",
      "pdf_url": "https://arxiv.org/pdf/2512.06112v1",
      "arxiv_url": "http://arxiv.org/abs/2512.06112v1",
      "published": "2025-12-05",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    {
      "title": "Standard and stressed value at risk forecasting using dynamic Bayesian networks",
      "authors": [
        "Eden Gross",
        "Ryan Kruger",
        "Francois Toerien"
      ],
      "abstract": "This study introduces a dynamic Bayesian network (DBN) framework for forecasting value at risk (VaR) and stressed VaR (SVaR) and compares its performance to several commonly applied models. Using daily S&P 500 index returns from 1991 to 2020, we produce 10-day 99% VaR and SVaR forecasts using a rolling period and historical returns for the traditional models, while three DBNs use both historical and forecasted returns. We evaluate the models' forecasting accuracy using standard backtests and forecasting error measures. Results show that autoregressive models deliver the most accurate VaR forecasts, while the DBNs achieve comparable performance to the historical simulation model, despite incorporating forward-looking return forecasts. For SVaR, all models produce highly conservative forecasts, with minimal breaches and limited differentiation in accuracy. While DBNs do not outperform traditional models, they demonstrate feasibility as a forward-looking approach to provide a foundation for future research on integrating causal inference into financial risk forecasting.",
      "pdf_url": "https://arxiv.org/pdf/2512.05661v1",
      "arxiv_url": "http://arxiv.org/abs/2512.05661v1",
      "published": "2025-12-05",
      "categories": [
        "q-fin.RM"
      ]
    },
    {
      "title": "ShaRP: SHAllow-LayeR Pruning for Video Large Language Models Acceleration",
      "authors": [
        "Yingjie Xia",
        "Tao Liu",
        "Jinglei Shi",
        "Qingsong Xie",
        "Heng Guo",
        "Jian Yang",
        "Xi Wang"
      ],
      "abstract": "Video Large Language Models (VLLMs) face the challenge of high computational load during the pre-filling stage due to the processing of an enormous number of visual tokens. Although attention-based pruning methods are widely used to accelerate inference, trials at early decoder layers often result in significant performance degradation, especially under high compression rates. We argue that while attention-based pruning inherently holds the potential to identify the most relevant visual tokens, its effectiveness in shallow decoder layers is limited by factors such as positional encoding bias and insufficient information interaction. In this paper, we propose an improved attention-based pruning framework, termed ShaRP, that integrates segment-aware causal masking, positional debiasing, and token deduplication for enhanced token selection. It enables effective pruning at shallow layers while maintaining stable performance under high compression rates without retraining. Extensive experiments demonstrate that ShaRP achieves competitive performance across multiple video understanding benchmarks, establishing a new paradigm for accelerating VLLM inference.",
      "pdf_url": "https://arxiv.org/pdf/2512.05385v1",
      "arxiv_url": "http://arxiv.org/abs/2512.05385v1",
      "published": "2025-12-05",
      "categories": [
        "cs.CV"
      ]
    },
    {
      "title": "Text Rationalization for Robust Causal Effect Estimation",
      "authors": [
        "Lijinghua Zhang",
        "Hengrui Cai"
      ],
      "abstract": "Recent advances in natural language processing have enabled the increasing use of text data in causal inference, particularly for adjusting confounding factors in treatment effect estimation. Although high-dimensional text can encode rich contextual information, it also poses unique challenges for causal identification and estimation. In particular, the positivity assumption, which requires sufficient treatment overlap across confounder values, is often violated at the observational level, when massive text is represented in feature spaces. Redundant or spurious textual features inflate dimensionality, producing extreme propensity scores, unstable weights, and inflated variance in effect estimates. We address these challenges with Confounding-Aware Token Rationalization (CATR), a framework that selects a sparse necessary subset of tokens using a residual-independence diagnostic designed to preserve confounding information sufficient for unconfoundedness. By discarding irrelevant texts while retaining key signals, CATR mitigates observational-level positivity violations and stabilizes downstream causal effect estimators. Experiments on synthetic data and a real-world study using the MIMIC-III database demonstrate that CATR yields more accurate, stable, and interpretable causal effect estimates than existing baselines.",
      "pdf_url": "https://arxiv.org/pdf/2512.05373v1",
      "arxiv_url": "http://arxiv.org/abs/2512.05373v1",
      "published": "2025-12-05",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "stat.ME",
        "stat.ML"
      ]
    },
    {
      "title": "Learning Causality for Longitudinal Data",
      "authors": [
        "Mouad EL Bouchattaoui"
      ],
      "abstract": "This thesis develops methods for causal inference and causal representation learning (CRL) in high-dimensional, time-varying data.\n  The first contribution introduces the Causal Dynamic Variational Autoencoder (CDVAE), a model for estimating Individual Treatment Effects (ITEs) by capturing unobserved heterogeneity in treatment response driven by latent risk factors that affect only outcomes. CDVAE comes with theoretical guarantees on valid latent adjustment and generalization bounds for ITE error. Experiments on synthetic and real datasets show that CDVAE outperforms baselines, and that state-of-the-art models greatly improve when augmented with its latent substitutes, approaching oracle performance without access to true adjustment variables.\n  The second contribution proposes an efficient framework for long-term counterfactual regression based on RNNs enhanced with Contrastive Predictive Coding (CPC) and InfoMax. It captures long-range dependencies under time-varying confounding while avoiding the computational cost of transformers, achieving state-of-the-art results and introducing CPC into causal inference.\n  The third contribution advances CRL by addressing how latent causes manifest in observed variables. We introduce a model-agnostic interpretability layer based on the geometry of the decoder Jacobian. A sparse self-expression prior induces modular, possibly overlapping groups of observed features aligned with shared latent influences. We provide recovery guarantees in both disjoint and overlapping settings and show that meaningful latent-to-observed structure can be recovered without anchor features or single-parent assumptions. Scalable Jacobian-based regularization techniques are also developed.",
      "pdf_url": "https://arxiv.org/pdf/2512.04980v1",
      "arxiv_url": "http://arxiv.org/abs/2512.04980v1",
      "published": "2025-12-04",
      "categories": [
        "stat.ML",
        "cs.IT",
        "cs.LG"
      ]
    }
  ]
}