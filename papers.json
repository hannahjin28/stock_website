{
  "last_updated": "2025-12-02T00:55:22.060935",
  "papers": [
    {
      "title": "A Design-Based Matching Framework for Staggered Adoption with Time-Varying Confounding",
      "authors": [
        "Suehyun Kim",
        "Kwonsang Lee"
      ],
      "abstract": "Causal inference in longitudinal datasets has long been challenging due to dynamic treatment adoption and confounding by time-varying covariates. Prior work either fails to account for heterogeneity across treatment adoption cohorts and treatment timings or relies on modeling assumptions. In this paper, we develop a novel design-based framework for inference on group- and time-specific treatment effects in panel data with staggered treatment adoption. We establish identification results for causal effects under this structure and introduce corresponding estimators, together with a block bootstrap procedure for estimating the covariance matrix and testing the homogeneity of group-time treatment effects. To implement the framework in practice, we propose the Reverse-Time Nested Matching algorithm, which constructs matched strata by pairing units from different adoption cohorts in a way that ensures comparability of covariate histories at each treatment time. Applying the algorithm to the Netflix-IPTV dataset, we find that while Netflix subscription does not significantly affect total IPTV viewing time, it does negatively affect VoD usage. We also provide statistical evidence that the causal effects of Netflix subscription may vary even within the same treatment cohort or across the same outcome and event times.",
      "pdf_url": "https://arxiv.org/pdf/2511.23208v1",
      "arxiv_url": "http://arxiv.org/abs/2511.23208v1",
      "published": "2025-11-28",
      "categories": [
        "stat.ME"
      ]
    },
    {
      "title": "A General Bayesian Nonparametric Approach for Estimating Population-Level and Conditional Causal Effects",
      "authors": [
        "Yongseok Hur",
        "Joonhyuk Jung",
        "Juhee Lee"
      ],
      "abstract": "We propose a Bayesian nonparametric (BNP) approach to causal inference using observational data consisting of outcome, treatment, and a set of confounders. The conditional distribution of the outcome given treatment and confounders is modeled flexibly using a dependent nonparametric mixture model, in which both the atoms and the weights vary with the confounders. The proposed BNP model is well suited for causal inference problems, as it does not rely on parametric assumptions about how the conditional distribution depends on the confounders. In particular, the model effectively adjusts for confounding and improves the modeling of treatment effect heterogeneity, leading to more accurate estimation of both the average treatment effect (ATE) and heterogeneous treatment effects (HTE). Posterior inference under the proposed model is computationally efficient due to the use of data augmentation. Extensive evaluations demonstrate that the proposed model offers competitive or superior performance compared to a wide range of recent methods spanning various statistical approaches, including Bayesian additive regression tree (BART) models, which are well known for their strong empirical performance. More importantly, the model provides fully probabilistic inference on quantities of interest that other methods cannot easily provide, using their posterior distributions.",
      "pdf_url": "https://arxiv.org/pdf/2511.23085v1",
      "arxiv_url": "http://arxiv.org/abs/2511.23085v1",
      "published": "2025-11-28",
      "categories": [
        "stat.ME"
      ]
    },
    {
      "title": "An empirical estimate of the electricity supply curve from market outcomes",
      "authors": [
        "Jorge SÃ¡nchez Canales",
        "Alice Lixuan Xu",
        "Chiara Fusar Bassini",
        "Lynn H. Kaack",
        "Lion Hirth"
      ],
      "abstract": "Researchers and electricity sector practitioners frequently require the supply curve of electricity markets and the price elasticity of supply for purposes such as price forecasting, policy analyses or market power assessment. It is common practice to construct supply curves from engineering data such as installed capacity and fuel prices. In this study, we propose a data-driven methodology to estimate the supply curve of electricity market empirically, i.e. from observed prices and quantities without further modeling assumptions. Due to the massive swings in fuel prices during the European energy crisis, a central task is detecting periods of stable supply curves. To this end, we implement two alternative clustering methods, one based on the fundamental drivers of electricity supply and the other directly on observed market outcomes. We apply our methods to the German electricity market between 2019 and 2024. We find that both approaches identify almost identical regimes shifts, supporting the idea of stable supply regimes stemming from stable drivers. Supply conditions are often stable for extended periods, but evolved rapidly during the energy crisis, triggering a rapid succession of regimes. Fuel prices were the dominant drivers of regime shifts, while conventional plant availability and the nuclear phase-out play a comparatively minor role. Our approach produces empirical supply curves suitable for causal inference and counterfactual analysis of market outcomes.",
      "pdf_url": "https://arxiv.org/pdf/2511.23068v1",
      "arxiv_url": "http://arxiv.org/abs/2511.23068v1",
      "published": "2025-11-28",
      "categories": [
        "econ.EM"
      ]
    },
    {
      "title": "Time Extrapolation with Graph Convolutional Autoencoder and Tensor Train Decomposition",
      "authors": [
        "Yuanhong Chen",
        "Federico Pichi",
        "Zhen Gao",
        "Gianluigi Rozza"
      ],
      "abstract": "Graph autoencoders have gained attention in nonlinear reduced-order modeling of parameterized partial differential equations defined on unstructured grids. Despite they provide a geometrically consistent way of treating complex domains, applying such architectures to parameterized dynamical systems for temporal prediction beyond the training data, i.e. the extrapolation regime, is still a challenging task due to the simultaneous need of temporal causality and generalizability in the parametric space. In this work, we explore the integration of graph convolutional autoencoders (GCAs) with tensor train (TT) decomposition and Operator Inference (OpInf) to develop a time-consistent reduced-order model. In particular, high-fidelity snapshots are represented as a combination of parametric, spatial, and temporal cores via TT decomposition, while OpInf is used to learn the evolution of the latter. Moreover, we enhance the generalization performance by developing a multi-fidelity two-stages approach in the framework of Deep Operator Networks (DeepONet), treating the spatial and temporal cores as the trunk networks, and the parametric core as the branch network. Numerical results, including heat-conduction, advection-diffusion and vortex-shedding phenomena, demonstrate great performance in effectively learning the dynamic in the extrapolation regime for complex geometries, also in comparison with state-of-the-art approaches e.g. MeshGraphNets.",
      "pdf_url": "https://arxiv.org/pdf/2511.23037v1",
      "arxiv_url": "http://arxiv.org/abs/2511.23037v1",
      "published": "2025-11-28",
      "categories": [
        "math.NA",
        "cs.LG"
      ]
    },
    {
      "title": "Seeing before Observable: Potential Risk Reasoning in Autonomous Driving via Vision Language Models",
      "authors": [
        "Jiaxin Liu",
        "Xiangyu Yan",
        "Liang Peng",
        "Lei Yang",
        "Lingjun Zhang",
        "Yuechen Luo",
        "Yueming Tao",
        "Ashton Yu Xuan Tan",
        "Mu Li",
        "Lei Zhang",
        "Ziqi Zhan",
        "Sai Guo",
        "Hong Wang",
        "Jun Li"
      ],
      "abstract": "Ensuring safety remains a key challenge for autonomous vehicles (AVs), especially in rare and complex scenarios. One critical but understudied aspect is the \\textbf{potential risk} situations, where the risk is \\textbf{not yet observable} but can be inferred from subtle precursors, such as anomalous behaviors or commonsense violations. Recognizing these precursors requires strong semantic understanding and reasoning capabilities, which are often absent in current AV systems due to the scarcity of such cases in existing driving or risk-centric datasets. Moreover, current autonomous driving accident datasets often lack annotations of the causal reasoning chains behind incidents, which are essential for identifying potential risks before they become observable. To address these gaps, we introduce PotentialRiskQA, a novel vision-language dataset designed for reasoning about potential risks prior to observation. Each sample is annotated with structured scene descriptions, semantic precursors, and inferred risk outcomes. Based on this dataset, we further propose PR-Reasoner, a vision-language-model-based framework tailored for onboard potential risk reasoning. Experimental results show that fine-tuning on PotentialRiskQA enables PR-Reasoner to significantly enhance its performance on the potential risk reasoning task compared to baseline VLMs. Together, our dataset and model provide a foundation for developing autonomous systems with improved foresight and proactive safety capabilities, moving toward more intelligent and resilient AVs.",
      "pdf_url": "https://arxiv.org/pdf/2511.22928v1",
      "arxiv_url": "http://arxiv.org/abs/2511.22928v1",
      "published": "2025-11-28",
      "categories": [
        "cs.RO"
      ]
    },
    {
      "title": "CRAwDAD: Causal Reasoning Augmentation with Dual-Agent Debate",
      "authors": [
        "Finn G. Vamosi",
        "Nils D. Forkert"
      ],
      "abstract": "When people reason about cause and effect, they often consider many competing \"what if\" scenarios before deciding which explanation fits best. Analogously, advanced language models capable of causal inference can consider multiple interventions and counterfactuals to judge the validity of causal claims. Crucially, this type of reasoning is less like a single calculation and more like an internal dialogue between alternative hypotheses. In this paper, we make this dialogue explicit through a dual-agent debate framework where one model provides a structured causal inference, and the other critically examines this reasoning for logical flaws. When disagreements arise, agents attempt to persuade each other, challenging each other's logic and revising their conclusions until they converge on a mutually agreed answer. To take advantage of this deliberative process, we specifically use reasoning language models, whose strengths in both causal inference and adversarial debate remain under-explored relative to standard large language models. We evaluate our approach on the CLadder dataset, a benchmark linking natural language questions to formally defined causal graphs across all three rungs of Pearl's ladder of causation. With Qwen3 and DeepSeek-R1 as debater agents, we demonstrate that multi-agent debate improves DeepSeek-R1's overall accuracy in causal inference from 78.03% to 87.45%, with the counterfactual category specifically improving from 67.94% to 80.04% accuracy. Similarly, Qwen3's overall accuracy improves from 84.16% to 89.41%, and counterfactual questions from 71.53% to 80.35%, showing that strong models can still benefit greatly from debate with weaker agents. Our results highlight the potential of reasoning models as building blocks for multi-agent systems in causal inference, and demonstrate the importance of diverse perspectives in causal problem-solving.",
      "pdf_url": "https://arxiv.org/pdf/2511.22854v1",
      "arxiv_url": "http://arxiv.org/abs/2511.22854v1",
      "published": "2025-11-28",
      "categories": [
        "cs.LG",
        "cs.MA"
      ]
    },
    {
      "title": "The Causal Uncertainty Principle",
      "authors": [
        "Daniel D. Reidpath"
      ],
      "abstract": "This paper explains why internal and external validity cannot be simultaneously maximised. It introduces \"evidential states\" to represent the information available for causal inference and shows that routine study operations (restriction, conditioning, and intervention) transform these states in ways that do not commute. Because each operation removes or reorganises information differently, changing their order yields evidential states that support different causal claims. This non-commutativity creates a structural trade-off: the steps that secure precise causal identification also eliminate the heterogeneity required for generalisation. Small model, observational and experimental examples illustrate how familiar failures of transportability arise from this order dependence. The result is a concise structural account of why increasing causal precision necessarily narrows the world to which findings apply.",
      "pdf_url": "https://arxiv.org/pdf/2511.22649v1",
      "arxiv_url": "http://arxiv.org/abs/2511.22649v1",
      "published": "2025-11-27",
      "categories": [
        "stat.AP",
        "cs.IT"
      ]
    },
    {
      "title": "CoT4AD: A Vision-Language-Action Model with Explicit Chain-of-Thought Reasoning for Autonomous Driving",
      "authors": [
        "Zhaohui Wang",
        "Tengbo Yu",
        "Hao Tang"
      ],
      "abstract": "Vision-Language-Action (VLA) models have recently attracted growing attention in end-to-end autonomous driving for their strong reasoning capabilities and rich world knowledge. However, existing VLAs often suffer from limited numerical reasoning ability and overly simplified input-output mappings, which hinder their performance in complex driving scenarios requiring step-by-step causal reasoning. To address these challenges, we propose CoT4AD, a novel VLA framework that introduces Chain-of-Thought (CoT) reasoning for autonomous driving to enhance both numerical and causal reasoning in Vision-Language Models (VLMs). CoT4AD integrates visual observations and language instructions to perform semantic reasoning, scene understanding, and trajectory planning. During training, it explicitly models a perception-question-prediction-action CoT to align the reasoning space with the action space across multiple driving tasks. During inference, it performs implicit CoT reasoning to enable consistent numerical reasoning and robust decision-making in dynamic environments. Extensive experiments on both real-world and simulated benchmarks, including nuScenes and Bench2Drive, demonstrate that CoT4AD achieves state-of-the-art performance in both open-loop and closed-loop evaluations. Code will be released upon paper acceptance.",
      "pdf_url": "https://arxiv.org/pdf/2511.22532v1",
      "arxiv_url": "http://arxiv.org/abs/2511.22532v1",
      "published": "2025-11-27",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Design-based theory for causal inference",
      "authors": [
        "Xin Lu",
        "Wanjia Fu",
        "Hongzi Li",
        "Haoyang Yu",
        "Honghao Zhang",
        "Ke Zhu",
        "Hanzhong Liu"
      ],
      "abstract": "Causal inference, as a major research area in statistics and data science, plays a central role across diverse fields such as medicine, economics, education, and the social sciences. Design-based causal inference begins with randomized experiments and emphasizes conducting statistical inference by leveraging the known randomization mechanism, thereby enabling identification and estimation of causal effects under weak model dependence. Grounded in the seminal works of Fisher and Neyman, this paradigm has evolved to include various design strategies, such as stratified randomization and rerandomization, and analytical methods including Fisher randomization tests, Neyman-style asymptotic inference, and regression adjustment. In recent years, with the emergence of complex settings involving high-dimensional data, individual noncompliance, and network interference, design-based causal inference has witnessed remarkable theoretical and methodological advances. This paper provides a systematic review of recent progress in this field, focusing on covariate-balanced randomization designs, design-based statistical inference methods, and their extensions to high-dimensional, noncompliance, and network interference scenarios. It concludes with a comprehensive perspective on future directions for the theoretical development and practical applications of causal inference.",
      "pdf_url": "https://arxiv.org/pdf/2511.22518v1",
      "arxiv_url": "http://arxiv.org/abs/2511.22518v1",
      "published": "2025-11-27",
      "categories": [
        "stat.ME"
      ]
    },
    {
      "title": "A Sensitivity Approach to Causal Inference Under Limited Overlap",
      "authors": [
        "Yuanzhe Ma",
        "Hongseok Namkoong"
      ],
      "abstract": "Limited overlap between treated and control groups is a key challenge in observational analysis. Standard approaches like trimming importance weights can reduce variance but introduce a fundamental bias. We propose a sensitivity framework for contextualizing findings under limited overlap, where we assess how irregular the outcome function has to be in order for the main finding to be invalidated. Our approach is based on worst-case confidence bounds on the bias introduced by standard trimming practices, under explicit assumptions necessary to extrapolate counterfactual estimates from regions of overlap to those without. Empirically, we demonstrate how our sensitivity framework protects against spurious findings by quantifying uncertainty in regions with limited overlap.",
      "pdf_url": "https://arxiv.org/pdf/2511.22003v1",
      "arxiv_url": "http://arxiv.org/abs/2511.22003v1",
      "published": "2025-11-27",
      "categories": [
        "stat.ML",
        "cs.LG",
        "stat.ME"
      ]
    }
  ]
}