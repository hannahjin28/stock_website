{
  "last_updated": "2025-08-09T00:55:03.533336",
  "papers": [
    {
      "title": "MolSnap: Snap-Fast Molecular Generation with Latent Variational Mean Flow",
      "authors": [
        "Md Atik Ahamed",
        "Qiang Ye",
        "Qiang Cheng"
      ],
      "abstract": "Molecular generation conditioned on textual descriptions is a fundamental\ntask in computational chemistry and drug discovery. Existing methods often\nstruggle to simultaneously ensure high-quality, diverse generation and fast\ninference. In this work, we propose a novel causality-aware framework that\naddresses these challenges through two key innovations. First, we introduce a\nCausality-Aware Transformer (CAT) that jointly encodes molecular graph tokens\nand text instructions while enforcing causal dependencies during generation.\nSecond, we develop a Variational Mean Flow (VMF) framework that generalizes\nexisting flow-based methods by modeling the latent space as a mixture of\nGaussians, enhancing expressiveness beyond unimodal priors. VMF enables\nefficient one-step inference while maintaining strong generation quality and\ndiversity. Extensive experiments on four standard molecular benchmarks\ndemonstrate that our model outperforms state-of-the-art baselines, achieving\nhigher novelty (up to 74.5\\%), diversity (up to 70.3\\%), and 100\\% validity\nacross all datasets. Moreover, VMF requires only one number of function\nevaluation (NFE) during conditional generation and up to five NFEs for\nunconditional generation, offering substantial computational efficiency over\ndiffusion-based methods.",
      "pdf_url": "http://arxiv.org/pdf/2508.05411v1",
      "arxiv_url": "http://arxiv.org/abs/2508.05411v1",
      "published": "2025-08-07",
      "categories": [
        "cs.LG"
      ]
    },
    {
      "title": "UNCAGE: Contrastive Attention Guidance for Masked Generative Transformers in Text-to-Image Generation",
      "authors": [
        "Wonjun Kang",
        "Byeongkeun Ahn",
        "Minjae Lee",
        "Kevin Galim",
        "Seunghyuk Oh",
        "Hyung Il Koo",
        "Nam Ik Cho"
      ],
      "abstract": "Text-to-image (T2I) generation has been actively studied using Diffusion\nModels and Autoregressive Models. Recently, Masked Generative Transformers have\ngained attention as an alternative to Autoregressive Models to overcome the\ninherent limitations of causal attention and autoregressive decoding through\nbidirectional attention and parallel decoding, enabling efficient and\nhigh-quality image generation. However, compositional T2I generation remains\nchallenging, as even state-of-the-art Diffusion Models often fail to accurately\nbind attributes and achieve proper text-image alignment. While Diffusion Models\nhave been extensively studied for this issue, Masked Generative Transformers\nexhibit similar limitations but have not been explored in this context. To\naddress this, we propose Unmasking with Contrastive Attention Guidance\n(UNCAGE), a novel training-free method that improves compositional fidelity by\nleveraging attention maps to prioritize the unmasking of tokens that clearly\nrepresent individual objects. UNCAGE consistently improves performance in both\nquantitative and qualitative evaluations across multiple benchmarks and\nmetrics, with negligible inference overhead. Our code is available at\nhttps://github.com/furiosa-ai/uncage.",
      "pdf_url": "http://arxiv.org/pdf/2508.05399v1",
      "arxiv_url": "http://arxiv.org/abs/2508.05399v1",
      "published": "2025-08-07",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "A Novel Architecture for Symbolic Reasoning with Decision Trees and LLM Agents",
      "authors": [
        "Andrew Kiruluta"
      ],
      "abstract": "We propose a hybrid architecture that integrates decision tree-based symbolic\nreasoning with the generative capabilities of large language models (LLMs)\nwithin a coordinated multi-agent framework. Unlike prior approaches that\nloosely couple symbolic and neural modules, our design embeds decision trees\nand random forests as callable oracles within a unified reasoning system.\nTree-based modules enable interpretable rule inference and causal logic, while\nLLM agents handle abductive reasoning, generalization, and interactive\nplanning. A central orchestrator maintains belief state consistency and\nmediates communication across agents and external tools, enabling reasoning\nover both structured and unstructured inputs.\n  The system achieves strong performance on reasoning benchmarks. On\n\\textit{ProofWriter}, it improves entailment consistency by +7.2\\% through\nlogic-grounded tree validation. On GSM8k, it achieves +5.3\\% accuracy gains in\nmultistep mathematical problems via symbolic augmentation. On \\textit{ARC}, it\nboosts abstraction accuracy by +6.0\\% through integration of symbolic oracles.\nApplications in clinical decision support and scientific discovery show how the\nsystem encodes domain rules symbolically while leveraging LLMs for contextual\ninference and hypothesis generation. This architecture offers a robust,\ninterpretable, and extensible solution for general-purpose neuro-symbolic\nreasoning.",
      "pdf_url": "http://arxiv.org/pdf/2508.05311v1",
      "arxiv_url": "http://arxiv.org/abs/2508.05311v1",
      "published": "2025-08-07",
      "categories": [
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Physics-Informed Time-Integrated DeepONet: Temporal Tangent Space Operator Learning for High-Accuracy Inference",
      "authors": [
        "Luis Mandl",
        "Dibyajyoti Nayak",
        "Tim Ricken",
        "Somdatta Goswami"
      ],
      "abstract": "Accurately modeling and inferring solutions to time-dependent partial\ndifferential equations (PDEs) over extended horizons remains a core challenge\nin scientific machine learning. Traditional full rollout (FR) methods, which\npredict entire trajectories in one pass, often fail to capture the causal\ndependencies and generalize poorly outside the training time horizon.\nAutoregressive (AR) approaches, evolving the system step by step, suffer from\nerror accumulation, limiting long-term accuracy. These shortcomings limit the\nlong-term accuracy and reliability of both strategies. To address these issues,\nwe introduce the Physics-Informed Time-Integrated Deep Operator Network\n(PITI-DeepONet), a dual-output architecture trained via fully physics-informed\nor hybrid physics- and data-driven objectives to ensure stable, accurate\nlong-term evolution well beyond the training horizon. Instead of forecasting\nfuture states, the network learns the time-derivative operator from the current\nstate, integrating it using classical time-stepping schemes to advance the\nsolution in time. Additionally, the framework can leverage residual monitoring\nduring inference to estimate prediction quality and detect when the system\ntransitions outside the training domain. Applied to benchmark problems,\nPITI-DeepONet shows improved accuracy over extended inference time horizons\nwhen compared to traditional methods. Mean relative $\\mathcal{L}_2$ errors\nreduced by 84% (vs. FR) and 79% (vs. AR) for the one-dimensional heat equation;\nby 87% (vs. FR) and 98% (vs. AR) for the one-dimensional Burgers equation; and\nby 42% (vs. FR) and 89% (vs. AR) for the two-dimensional Allen-Cahn equation.\nBy moving beyond classic FR and AR schemes, PITI-DeepONet paves the way for\nmore reliable, long-term integration of complex, time-dependent PDEs.",
      "pdf_url": "http://arxiv.org/pdf/2508.05190v1",
      "arxiv_url": "http://arxiv.org/abs/2508.05190v1",
      "published": "2025-08-07",
      "categories": [
        "cs.LG"
      ]
    },
    {
      "title": "Multimodal Causal-Driven Representation Learning for Generalizable Medical Image Segmentation",
      "authors": [
        "Xusheng Liang",
        "Lihua Zhou",
        "Nianxin Li",
        "Miao Xu",
        "Ziyang Song",
        "Dong Yi",
        "Jinlin Wu",
        "Hongbin Liu",
        "Jiebo Luo",
        "Zhen Lei"
      ],
      "abstract": "Vision-Language Models (VLMs), such as CLIP, have demonstrated remarkable\nzero-shot capabilities in various computer vision tasks. However, their\napplication to medical imaging remains challenging due to the high variability\nand complexity of medical data. Specifically, medical images often exhibit\nsignificant domain shifts caused by various confounders, including equipment\ndifferences, procedure artifacts, and imaging modes, which can lead to poor\ngeneralization when models are applied to unseen domains. To address this\nlimitation, we propose Multimodal Causal-Driven Representation Learning\n(MCDRL), a novel framework that integrates causal inference with the VLM to\ntackle domain generalization in medical image segmentation. MCDRL is\nimplemented in two steps: first, it leverages CLIP's cross-modal capabilities\nto identify candidate lesion regions and construct a confounder dictionary\nthrough text prompts, specifically designed to represent domain-specific\nvariations; second, it trains a causal intervention network that utilizes this\ndictionary to identify and eliminate the influence of these domain-specific\nvariations while preserving the anatomical structural information critical for\nsegmentation tasks. Extensive experiments demonstrate that MCDRL consistently\noutperforms competing methods, yielding superior segmentation accuracy and\nexhibiting robust generalizability.",
      "pdf_url": "http://arxiv.org/pdf/2508.05008v1",
      "arxiv_url": "http://arxiv.org/abs/2508.05008v1",
      "published": "2025-08-07",
      "categories": [
        "cs.CV"
      ]
    },
    {
      "title": "Causal Reflection with Language Models",
      "authors": [
        "Abi Aryan",
        "Zac Liu"
      ],
      "abstract": "While LLMs exhibit impressive fluency and factual recall, they struggle with\nrobust causal reasoning, often relying on spurious correlations and brittle\npatterns. Similarly, traditional Reinforcement Learning agents also lack causal\nunderstanding, optimizing for rewards without modeling why actions lead to\noutcomes. We introduce Causal Reflection, a framework that explicitly models\ncausality as a dynamic function over state, action, time, and perturbation,\nenabling agents to reason about delayed and nonlinear effects. Additionally, we\ndefine a formal Reflect mechanism that identifies mismatches between predicted\nand observed outcomes and generates causal hypotheses to revise the agent's\ninternal model. In this architecture, LLMs serve not as black-box reasoners,\nbut as structured inference engines translating formal causal outputs into\nnatural language explanations and counterfactuals. Our framework lays the\ntheoretical groundwork for Causal Reflective agents that can adapt,\nself-correct, and communicate causal understanding in evolving environments.",
      "pdf_url": "http://arxiv.org/pdf/2508.04495v1",
      "arxiv_url": "http://arxiv.org/abs/2508.04495v1",
      "published": "2025-08-06",
      "categories": [
        "cs.LG",
        "cs.CL"
      ]
    },
    {
      "title": "Boosting Visual Knowledge-Intensive Training for LVLMs Through Causality-Driven Visual Object Completion",
      "authors": [
        "Qingguo Hu",
        "Ante Wang",
        "Jia Song",
        "Delai Qiu",
        "Qingsong Liu",
        "Jinsong Su"
      ],
      "abstract": "Large Vision-Language Models (LVLMs) have experienced significant\nadvancements in recent years. However, their performance still falls short in\ntasks requiring deep visual perception, such as identifying subtle differences\nbetween images. A potential cause is the scarcity of visual knowledge in\npopular instruction-tuning corpora, resulting in inadequate visual perception\nand reasoning capabilities. To address this challenge, we introduce a\nself-improvement framework grounded in a novel visual knowledge-intensive task,\n\\underline{C}ausality-driven \\underline{V}isual object \\underline{C}ompletion\n(CVC). This task requires LVLMs to infer the masked object in an image based on\nits \\textit{causal} relationships with the other visible information. We first\nobtain rich examples cheaply through our automated instance construction\npipeline, without relying on sophisticated LVLMs (\\textit{e.g.}, GPT-4V) or\nhuman assistance. Then, LVLMs effectively self-improve through trial and error\nlearning using these created instances. Our experiments demonstrate substantial\ngains across four challenging specialized tasks and four widely-used\ncomprehensive benchmarks. Especially on specialized tasks, our method achieves\nan average improvement of 5.4\\% and 4.0\\% compared to the corresponding\nbaselines when utilizing LLaVA-1.5-7B and LLaVA-1.5-13B, respectively. The code\nis available at https://github.com/XMUDeepLIT/CVC.",
      "pdf_url": "http://arxiv.org/pdf/2508.04453v1",
      "arxiv_url": "http://arxiv.org/abs/2508.04453v1",
      "published": "2025-08-06",
      "categories": [
        "cs.CV"
      ]
    },
    {
      "title": "Causal Reward Adjustment: Mitigating Reward Hacking in External Reasoning via Backdoor Correction",
      "authors": [
        "Ruike Song",
        "Zeen Song",
        "Huijie Guo",
        "Wenwen Qiang"
      ],
      "abstract": "External reasoning systems combine language models with process reward models\n(PRMs) to select high-quality reasoning paths for complex tasks such as\nmathematical problem solving. However, these systems are prone to reward\nhacking, where high-scoring but logically incorrect paths are assigned high\nscores by the PRMs, leading to incorrect answers. From a causal inference\nperspective, we attribute this phenomenon primarily to the presence of\nconfounding semantic features. To address it, we propose Causal Reward\nAdjustment (CRA), a method that mitigates reward hacking by estimating the true\nreward of a reasoning path. CRA trains sparse autoencoders on the PRM's\ninternal activations to recover interpretable features, then corrects\nconfounding by using backdoor adjustment. Experiments on math solving datasets\ndemonstrate that CRA mitigates reward hacking and improves final accuracy,\nwithout modifying the policy model or retraining PRM.",
      "pdf_url": "http://arxiv.org/pdf/2508.04216v1",
      "arxiv_url": "http://arxiv.org/abs/2508.04216v1",
      "published": "2025-08-06",
      "categories": [
        "cs.LG"
      ]
    },
    {
      "title": "Robust estimation of causal dose-response relationship using exposure data with dose as an instrumental variable",
      "authors": [
        "Jixian Wang",
        "Zhiwei Zhang",
        "Ram Tiwari"
      ],
      "abstract": "An accurate estimation of the dose-response relationship is important to\ndetermine the optimal dose. For this purpose, a dose finding trial in which\nsubjects are randomized to a few fixed dose levels is the most commonly used\ndesign. Often, the estimation uses response data only, although drug exposure\ndata are often obtained during the trial. The use of exposure data to improve\nthis estimation is difficult, as exposure-response relationships are typically\nsubject to confounding bias even in a randomized trial. We propose a robust\napproach to estimate the dose-response relationship without assuming a true\nexposure-response model, using dose as an instrumental variable. Our approach\ncombines the control variable approach in causal inference with unobserved\nconfounding factors and the ANCOVA adjustment of randomized trials. The\napproach presented uses working models for dose-exposure-response data, but\nthey are robust to model misspecification and remain consistent when the\nworking models are far from correct. The asymptotic properties of the proposed\napproach are also examined. A simulation study is performed to evaluate the\nperformance of the proposed approach. For illustration, the approach is used to\na Car-T trial with randomized doses.",
      "pdf_url": "http://arxiv.org/pdf/2508.04215v1",
      "arxiv_url": "http://arxiv.org/abs/2508.04215v1",
      "published": "2025-08-06",
      "categories": [
        "stat.ME"
      ]
    },
    {
      "title": "Can Large Language Models Adequately Perform Symbolic Reasoning Over Time Series?",
      "authors": [
        "Zewen Liu",
        "Juntong Ni",
        "Xianfeng Tang",
        "Max S. Y. Lau",
        "Wei Jin"
      ],
      "abstract": "Uncovering hidden symbolic laws from time series data, as an aspiration\ndating back to Kepler's discovery of planetary motion, remains a core challenge\nin scientific discovery and artificial intelligence. While Large Language\nModels show promise in structured reasoning tasks, their ability to infer\ninterpretable, context-aligned symbolic structures from time series data is\nstill underexplored. To systematically evaluate this capability, we introduce\nSymbolBench, a comprehensive benchmark designed to assess symbolic reasoning\nover real-world time series across three tasks: multivariate symbolic\nregression, Boolean network inference, and causal discovery. Unlike prior\nefforts limited to simple algebraic equations, SymbolBench spans a diverse set\nof symbolic forms with varying complexity. We further propose a unified\nframework that integrates LLMs with genetic programming to form a closed-loop\nsymbolic reasoning system, where LLMs act both as predictors and evaluators.\nOur empirical results reveal key strengths and limitations of current models,\nhighlighting the importance of combining domain knowledge, context alignment,\nand reasoning structure to improve LLMs in automated scientific discovery.",
      "pdf_url": "http://arxiv.org/pdf/2508.03963v1",
      "arxiv_url": "http://arxiv.org/abs/2508.03963v1",
      "published": "2025-08-05",
      "categories": [
        "cs.AI"
      ]
    }
  ]
}