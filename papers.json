{
  "last_updated": "2025-08-12T00:54:54.880746",
  "papers": [
    {
      "title": "Decorrelated feature importance from local sample weighting",
      "authors": [
        "Benedikt Fröhlich",
        "Alison Durst",
        "Merle Behr"
      ],
      "abstract": "Feature importance (FI) statistics provide a prominent and valuable method of\ninsight into the decision process of machine learning (ML) models, but their\neffectiveness has well-known limitations when correlation is present among the\nfeatures in the training data. In this case, the FI often tends to be\ndistributed among all features which are in correlation with the\nresponse-generating signal features. Even worse, if multiple signal features\nare in strong correlation with a noise feature, while being only modestly\ncorrelated with one another, this can result in a noise feature having a\ndistinctly larger FI score than any signal feature. Here we propose local\nsample weighting (losaw) which can flexibly be integrated into many ML\nalgorithms to improve FI scores in the presence of feature correlation in the\ntraining data. Our approach is motivated from inverse probability weighting in\ncausal inference and locally, within the ML model, uses a sample weighting\nscheme to decorrelate a target feature from the remaining features. This\nreduces model bias locally, whenever the effect of a potential signal feature\nis evaluated and compared to others. Moreover, losaw comes with a natural\ntuning parameter, the minimum effective sample size of the weighted population,\nwhich corresponds to an interpretation-prediction-tradeoff, analog to a\nbias-variance-tradeoff as for classical ML tuning parameters. We demonstrate\nhow losaw can be integrated within decision tree-based ML methods and within\nmini-batch training of neural networks. We investigate losaw for random forest\nand convolutional neural networks in a simulation study on settings showing\ndiverse correlation patterns. We found that losaw improves FI consistently.\nMoreover, it often improves prediction accuracy for out-of-distribution, while\nmaintaining a similar accuracy for in-distribution test data.",
      "pdf_url": "http://arxiv.org/pdf/2508.06337v1",
      "arxiv_url": "http://arxiv.org/abs/2508.06337v1",
      "published": "2025-08-08",
      "categories": [
        "stat.ML",
        "cs.LG",
        "stat.ME"
      ]
    },
    {
      "title": "Llasa+: Free Lunch for Accelerated and Streaming Llama-Based Speech Synthesis",
      "authors": [
        "Wenjie Tian",
        "Xinfa Zhu",
        "Hanke Xie",
        "Zhen Ye",
        "Wei Xue",
        "Lei Xie"
      ],
      "abstract": "Recent progress in text-to-speech (TTS) has achieved impressive naturalness\nand flexibility, especially with the development of large language model\n(LLM)-based approaches. However, existing autoregressive (AR) structures and\nlarge-scale models, such as Llasa, still face significant challenges in\ninference latency and streaming synthesis. To deal with the limitations, we\nintroduce Llasa+, an accelerated and streaming TTS model built on Llasa.\nSpecifically, to accelerate the generation process, we introduce two\nplug-and-play Multi-Token Prediction (MTP) modules following the frozen\nbackbone. These modules allow the model to predict multiple tokens in one AR\nstep. Additionally, to mitigate potential error propagation caused by\ninaccurate MTP, we design a novel verification algorithm that leverages the\nfrozen backbone to validate the generated tokens, thus allowing Llasa+ to\nachieve speedup without sacrificing generation quality. Furthermore, we design\na causal decoder that enables streaming speech reconstruction from tokens.\nExtensive experiments show that Llasa+ achieves a 1.48X speedup without\nsacrificing generation quality, despite being trained only on LibriTTS.\nMoreover, the MTP-and-verification framework can be applied to accelerate any\nLLM-based model. All codes and models are publicly available at\nhttps://github.com/ASLP-lab/LLaSA_Plus.",
      "pdf_url": "http://arxiv.org/pdf/2508.06262v1",
      "arxiv_url": "http://arxiv.org/abs/2508.06262v1",
      "published": "2025-08-08",
      "categories": [
        "cs.SD",
        "eess.AS"
      ]
    },
    {
      "title": "InfoCausalQA:Can Models Perform Non-explicit Causal Reasoning Based on Infographic?",
      "authors": [
        "Keummin Ka",
        "Junhyeong Park",
        "Jahyun Jeon",
        "Youngjae Yu"
      ],
      "abstract": "Recent advances in Vision-Language Models (VLMs) have demonstrated impressive\ncapabilities in perception and reasoning. However, the ability to perform\ncausal inference -- a core aspect of human cognition -- remains underexplored,\nparticularly in multimodal settings. In this study, we introduce InfoCausalQA,\na novel benchmark designed to evaluate causal reasoning grounded in\ninfographics that combine structured visual data with textual context. The\nbenchmark comprises two tasks: Task 1 focuses on quantitative causal reasoning\nbased on inferred numerical trends, while Task 2 targets semantic causal\nreasoning involving five types of causal relations: cause, effect,\nintervention, counterfactual, and temporal. We manually collected 494\ninfographic-text pairs from four public sources and used GPT-4o to generate\n1,482 high-quality multiple-choice QA pairs. These questions were then\ncarefully revised by humans to ensure they cannot be answered based on\nsurface-level cues alone but instead require genuine visual grounding. Our\nexperimental results reveal that current VLMs exhibit limited capability in\ncomputational reasoning and even more pronounced limitations in semantic causal\nreasoning. Their significantly lower performance compared to humans indicates a\nsubstantial gap in leveraging infographic-based information for causal\ninference. Through InfoCausalQA, we highlight the need for advancing the causal\nreasoning abilities of multimodal AI systems.",
      "pdf_url": "http://arxiv.org/pdf/2508.06220v1",
      "arxiv_url": "http://arxiv.org/abs/2508.06220v1",
      "published": "2025-08-08",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "DKG-LLM : A Framework for Medical Diagnosis and Personalized Treatment Recommendations via Dynamic Knowledge Graph and Large Language Model Integration",
      "authors": [
        "Ali Sarabadani",
        "Maryam Abdollahi Shamami",
        "Hamidreza Sadeghsalehi",
        "Borhan Asadi",
        "Saba Hesaraki"
      ],
      "abstract": "Large Language Models (LLMs) have grown exponentially since the release of\nChatGPT. These models have gained attention due to their robust performance on\nvarious tasks, including language processing tasks. These models achieve\nunderstanding and comprehension of tasks by training billions of parameters.\nThe development of these models is a transformative force in enhancing natural\nlanguage understanding and has taken a significant step towards artificial\ngeneral intelligence (AGI). In this study, we aim to present the DKG-LLM\nframework. The DKG-LLM framework introduces a groundbreaking approach to\nmedical diagnosis and personalized treatment recommendations by integrating a\ndynamic knowledge graph (DKG) with the Grok 3 large language model. Using the\nAdaptive Semantic Fusion Algorithm (ASFA), heterogeneous medical data\n(including clinical reports and PubMed articles) and patient records\ndynamically generate a knowledge graph consisting of 15,964 nodes in 13\ndistinct types (e.g., diseases, symptoms, treatments, patient profiles) and\n127,392 edges in 26 relationship types (e.g., causal, therapeutic,\nassociation). ASFA utilizes advanced probabilistic models, Bayesian inference,\nand graph optimization to extract semantic information, dynamically updating\nthe graph with approximately 150 new nodes and edges in each data category\nwhile maintaining scalability with up to 987,654 edges. Real-world datasets,\nincluding MIMIC-III and PubMed, were utilized to evaluate the proposed\narchitecture. The evaluation results show that DKG-LLM achieves a diagnostic\naccuracy of 84.19%. The model also has a treatment recommendation accuracy of\n89.63% and a semantic coverage of 93.48%. DKG-LLM is a reliable and\ntransformative tool that handles noisy data and complex multi-symptom diseases,\nalong with feedback-based learning from physician input.",
      "pdf_url": "http://arxiv.org/pdf/2508.06186v1",
      "arxiv_url": "http://arxiv.org/abs/2508.06186v1",
      "published": "2025-08-08",
      "categories": [
        "cs.CL"
      ]
    },
    {
      "title": "NanoCodec: Towards High-Quality Ultra Fast Speech LLM Inference",
      "authors": [
        "Edresson Casanova",
        "Paarth Neekhara",
        "Ryan Langman",
        "Shehzeen Hussain",
        "Subhankar Ghosh",
        "Xuesong Yang",
        "Ante Jukić",
        "Jason Li",
        "Boris Ginsburg"
      ],
      "abstract": "Large Language Models (LLMs) have significantly advanced audio processing by\nleveraging audio codecs to discretize audio into tokens, enabling the\napplication of language modeling techniques to speech data. However, existing\naudio codecs often operate at high frame rates, leading to slow training and\ninference, particularly for autoregressive models. To address this, there is\ngrowing interest in low frame-rate audio codecs, which reduce the number of\nautoregressive steps required to generate one second of audio. In this paper,\nwe conduct ablation studies to examine the impact of frame rate, bitrate, and\ncausality on codec reconstruction quality. Based on our findings, we introduce\nNanoCodec, a state-of-the-art audio codec that achieves high-quality\ncompression at just 12.5 frames per second (FPS). NanoCodec outperforms related\nworks across various bitrate ranges, establishing a new benchmark for\nlow-latency and efficient Speech LLM training and inference.",
      "pdf_url": "http://arxiv.org/pdf/2508.05835v1",
      "arxiv_url": "http://arxiv.org/abs/2508.05835v1",
      "published": "2025-08-07",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.SD"
      ]
    },
    {
      "title": "MolSnap: Snap-Fast Molecular Generation with Latent Variational Mean Flow",
      "authors": [
        "Md Atik Ahamed",
        "Qiang Ye",
        "Qiang Cheng"
      ],
      "abstract": "Molecular generation conditioned on textual descriptions is a fundamental\ntask in computational chemistry and drug discovery. Existing methods often\nstruggle to simultaneously ensure high-quality, diverse generation and fast\ninference. In this work, we propose a novel causality-aware framework that\naddresses these challenges through two key innovations. First, we introduce a\nCausality-Aware Transformer (CAT) that jointly encodes molecular graph tokens\nand text instructions while enforcing causal dependencies during generation.\nSecond, we develop a Variational Mean Flow (VMF) framework that generalizes\nexisting flow-based methods by modeling the latent space as a mixture of\nGaussians, enhancing expressiveness beyond unimodal priors. VMF enables\nefficient one-step inference while maintaining strong generation quality and\ndiversity. Extensive experiments on four standard molecular benchmarks\ndemonstrate that our model outperforms state-of-the-art baselines, achieving\nhigher novelty (up to 74.5\\%), diversity (up to 70.3\\%), and 100\\% validity\nacross all datasets. Moreover, VMF requires only one number of function\nevaluation (NFE) during conditional generation and up to five NFEs for\nunconditional generation, offering substantial computational efficiency over\ndiffusion-based methods.",
      "pdf_url": "http://arxiv.org/pdf/2508.05411v1",
      "arxiv_url": "http://arxiv.org/abs/2508.05411v1",
      "published": "2025-08-07",
      "categories": [
        "cs.LG"
      ]
    },
    {
      "title": "UNCAGE: Contrastive Attention Guidance for Masked Generative Transformers in Text-to-Image Generation",
      "authors": [
        "Wonjun Kang",
        "Byeongkeun Ahn",
        "Minjae Lee",
        "Kevin Galim",
        "Seunghyuk Oh",
        "Hyung Il Koo",
        "Nam Ik Cho"
      ],
      "abstract": "Text-to-image (T2I) generation has been actively studied using Diffusion\nModels and Autoregressive Models. Recently, Masked Generative Transformers have\ngained attention as an alternative to Autoregressive Models to overcome the\ninherent limitations of causal attention and autoregressive decoding through\nbidirectional attention and parallel decoding, enabling efficient and\nhigh-quality image generation. However, compositional T2I generation remains\nchallenging, as even state-of-the-art Diffusion Models often fail to accurately\nbind attributes and achieve proper text-image alignment. While Diffusion Models\nhave been extensively studied for this issue, Masked Generative Transformers\nexhibit similar limitations but have not been explored in this context. To\naddress this, we propose Unmasking with Contrastive Attention Guidance\n(UNCAGE), a novel training-free method that improves compositional fidelity by\nleveraging attention maps to prioritize the unmasking of tokens that clearly\nrepresent individual objects. UNCAGE consistently improves performance in both\nquantitative and qualitative evaluations across multiple benchmarks and\nmetrics, with negligible inference overhead. Our code is available at\nhttps://github.com/furiosa-ai/uncage.",
      "pdf_url": "http://arxiv.org/pdf/2508.05399v1",
      "arxiv_url": "http://arxiv.org/abs/2508.05399v1",
      "published": "2025-08-07",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "A Novel Architecture for Symbolic Reasoning with Decision Trees and LLM Agents",
      "authors": [
        "Andrew Kiruluta"
      ],
      "abstract": "We propose a hybrid architecture that integrates decision tree-based symbolic\nreasoning with the generative capabilities of large language models (LLMs)\nwithin a coordinated multi-agent framework. Unlike prior approaches that\nloosely couple symbolic and neural modules, our design embeds decision trees\nand random forests as callable oracles within a unified reasoning system.\nTree-based modules enable interpretable rule inference and causal logic, while\nLLM agents handle abductive reasoning, generalization, and interactive\nplanning. A central orchestrator maintains belief state consistency and\nmediates communication across agents and external tools, enabling reasoning\nover both structured and unstructured inputs.\n  The system achieves strong performance on reasoning benchmarks. On\n\\textit{ProofWriter}, it improves entailment consistency by +7.2\\% through\nlogic-grounded tree validation. On GSM8k, it achieves +5.3\\% accuracy gains in\nmultistep mathematical problems via symbolic augmentation. On \\textit{ARC}, it\nboosts abstraction accuracy by +6.0\\% through integration of symbolic oracles.\nApplications in clinical decision support and scientific discovery show how the\nsystem encodes domain rules symbolically while leveraging LLMs for contextual\ninference and hypothesis generation. This architecture offers a robust,\ninterpretable, and extensible solution for general-purpose neuro-symbolic\nreasoning.",
      "pdf_url": "http://arxiv.org/pdf/2508.05311v1",
      "arxiv_url": "http://arxiv.org/abs/2508.05311v1",
      "published": "2025-08-07",
      "categories": [
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Physics-Informed Time-Integrated DeepONet: Temporal Tangent Space Operator Learning for High-Accuracy Inference",
      "authors": [
        "Luis Mandl",
        "Dibyajyoti Nayak",
        "Tim Ricken",
        "Somdatta Goswami"
      ],
      "abstract": "Accurately modeling and inferring solutions to time-dependent partial\ndifferential equations (PDEs) over extended horizons remains a core challenge\nin scientific machine learning. Traditional full rollout (FR) methods, which\npredict entire trajectories in one pass, often fail to capture the causal\ndependencies and generalize poorly outside the training time horizon.\nAutoregressive (AR) approaches, evolving the system step by step, suffer from\nerror accumulation, limiting long-term accuracy. These shortcomings limit the\nlong-term accuracy and reliability of both strategies. To address these issues,\nwe introduce the Physics-Informed Time-Integrated Deep Operator Network\n(PITI-DeepONet), a dual-output architecture trained via fully physics-informed\nor hybrid physics- and data-driven objectives to ensure stable, accurate\nlong-term evolution well beyond the training horizon. Instead of forecasting\nfuture states, the network learns the time-derivative operator from the current\nstate, integrating it using classical time-stepping schemes to advance the\nsolution in time. Additionally, the framework can leverage residual monitoring\nduring inference to estimate prediction quality and detect when the system\ntransitions outside the training domain. Applied to benchmark problems,\nPITI-DeepONet shows improved accuracy over extended inference time horizons\nwhen compared to traditional methods. Mean relative $\\mathcal{L}_2$ errors\nreduced by 84% (vs. FR) and 79% (vs. AR) for the one-dimensional heat equation;\nby 87% (vs. FR) and 98% (vs. AR) for the one-dimensional Burgers equation; and\nby 42% (vs. FR) and 89% (vs. AR) for the two-dimensional Allen-Cahn equation.\nBy moving beyond classic FR and AR schemes, PITI-DeepONet paves the way for\nmore reliable, long-term integration of complex, time-dependent PDEs.",
      "pdf_url": "http://arxiv.org/pdf/2508.05190v1",
      "arxiv_url": "http://arxiv.org/abs/2508.05190v1",
      "published": "2025-08-07",
      "categories": [
        "cs.LG"
      ]
    },
    {
      "title": "Multimodal Causal-Driven Representation Learning for Generalizable Medical Image Segmentation",
      "authors": [
        "Xusheng Liang",
        "Lihua Zhou",
        "Nianxin Li",
        "Miao Xu",
        "Ziyang Song",
        "Dong Yi",
        "Jinlin Wu",
        "Hongbin Liu",
        "Jiebo Luo",
        "Zhen Lei"
      ],
      "abstract": "Vision-Language Models (VLMs), such as CLIP, have demonstrated remarkable\nzero-shot capabilities in various computer vision tasks. However, their\napplication to medical imaging remains challenging due to the high variability\nand complexity of medical data. Specifically, medical images often exhibit\nsignificant domain shifts caused by various confounders, including equipment\ndifferences, procedure artifacts, and imaging modes, which can lead to poor\ngeneralization when models are applied to unseen domains. To address this\nlimitation, we propose Multimodal Causal-Driven Representation Learning\n(MCDRL), a novel framework that integrates causal inference with the VLM to\ntackle domain generalization in medical image segmentation. MCDRL is\nimplemented in two steps: first, it leverages CLIP's cross-modal capabilities\nto identify candidate lesion regions and construct a confounder dictionary\nthrough text prompts, specifically designed to represent domain-specific\nvariations; second, it trains a causal intervention network that utilizes this\ndictionary to identify and eliminate the influence of these domain-specific\nvariations while preserving the anatomical structural information critical for\nsegmentation tasks. Extensive experiments demonstrate that MCDRL consistently\noutperforms competing methods, yielding superior segmentation accuracy and\nexhibiting robust generalizability.",
      "pdf_url": "http://arxiv.org/pdf/2508.05008v1",
      "arxiv_url": "http://arxiv.org/abs/2508.05008v1",
      "published": "2025-08-07",
      "categories": [
        "cs.CV"
      ]
    }
  ]
}