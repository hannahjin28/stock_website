{
  "last_updated": "2026-02-03T01:12:47.529220",
  "papers": [
    {
      "title": "RAudit: A Blind Auditing Protocol for Large Language Model Reasoning",
      "authors": [
        "Edward Y. Chang",
        "Longling Geng"
      ],
      "abstract": "Inference-time scaling can amplify reasoning pathologies: sycophancy, rung collapse, and premature certainty. We present RAudit, a diagnostic protocol for auditing LLM reasoning without ground truth access. The key constraint is blindness: the auditor evaluates only whether derivation steps support conclusions, enabling detection of trace-output inconsistency and, when latent competence exists, its recovery. RAudit measures process quality via CRIT-based reasonableness scores and varies critique formulation to study how social framing affects model response. We prove bounded correction and $O(\\log(1/Îµ))$ termination. Experiments on mathematical reasoning (CAP-GSM8K) and causal judgment (CausalL2) reveal four mechanisms explaining model unreliability: (1) Latent Competence Suppression, where models derive correct answers then overwrite them under social pressure; (2) The False Competence Trap, where weaker judges mask sycophancy that stronger judges expose; (3) The Complexity-Vulnerability Tradeoff, where causal tasks induce more than 10 times higher sycophancy than mathematical tasks; and (4) Iatrogenic Critique, where authoritative correction harms weaker models. These findings challenge assumptions that capability implies robustness and that stronger feedback yields better outputs.",
      "pdf_url": "https://arxiv.org/pdf/2601.23133v1",
      "arxiv_url": "http://arxiv.org/abs/2601.23133v1",
      "published": "2026-01-30",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Decomposing Epistemic Uncertainty for Causal Decision Making",
      "authors": [
        "Md Musfiqur Rahman",
        "Ziwei Jiang",
        "Hilaf Hasson",
        "Murat Kocaoglu"
      ],
      "abstract": "Causal inference from observational data provides strong evidence for the best action in decision-making without performing expensive randomized trials. The effect of an action is usually not identifiable under unobserved confounding, even with an infinite amount of data. Recent work uses neural networks to obtain practical bounds to such causal effects, which is often an intractable problem. However, these approaches may overfit to the dataset and be overconfident in their causal effect estimates. Moreover, there is currently no systematic approach to disentangle how much of the width of causal effect bounds is due to fundamental non-identifiability versus how much is due to finite-sample limitations. We propose a novel framework to address this problem by considering a confidence set around the empirical observational distribution and obtaining the intersection of causal effect bounds for all distributions in this confidence set. This allows us to distinguish the part of the interval that can be reduced by collecting more samples, which we call sample uncertainty, from the part that can only be reduced by observing more variables, such as latent confounders or instrumental variables, but not with more data, which we call non-ID uncertainty. The upper and lower bounds to this intersection are obtained by solving min-max and max-min problems with neural causal models by searching over all distributions that the dataset might have been sampled from, and all SCMs that entail the corresponding distribution. We demonstrate via extensive experiments on synthetic and real-world datasets that our algorithm can determine when collecting more samples will not help determine the best action. This can guide practitioners to collect more variables or lean towards a randomized study for best action identification.",
      "pdf_url": "https://arxiv.org/pdf/2601.22736v1",
      "arxiv_url": "http://arxiv.org/abs/2601.22736v1",
      "published": "2026-01-30",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Score-based Integrated Gradient for Root Cause Explanations of Outliers",
      "authors": [
        "Phuoc Nguyen",
        "Truyen Tran",
        "Sunil Gupta",
        "Svetha Venkatesh"
      ],
      "abstract": "Identifying the root causes of outliers is a fundamental problem in causal inference and anomaly detection. Traditional approaches based on heuristics or counterfactual reasoning often struggle under uncertainty and high-dimensional dependencies. We introduce SIREN, a novel and scalable method that attributes the root causes of outliers by estimating the score functions of the data likelihood. Attribution is computed via integrated gradients that accumulate score contributions along paths from the outlier toward the normal data distribution. Our method satisfies three of the four classic Shapley value axioms - dummy, efficiency, and linearity - as well as an asymmetry axiom derived from the underlying causal structure. Unlike prior work, SIREN operates directly on the score function, enabling tractable and uncertainty-aware root cause attribution in nonlinear, high-dimensional, and heteroscedastic causal models. Extensive experiments on synthetic random graphs and real-world cloud service and supply chain datasets show that SIREN outperforms state-of-the-art baselines in both attribution accuracy and computational efficiency.",
      "pdf_url": "https://arxiv.org/pdf/2601.22399v1",
      "arxiv_url": "http://arxiv.org/abs/2601.22399v1",
      "published": "2026-01-29",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Causal Imitation Learning Under Measurement Error and Distribution Shift",
      "authors": [
        "Shi Bo",
        "AmirEmad Ghassami"
      ],
      "abstract": "We study offline imitation learning (IL) when part of the decision-relevant state is observed only through noisy measurements and the distribution may change between training and deployment. Such settings induce spurious state-action correlations, so standard behavioral cloning (BC) -- whether conditioning on raw measurements or ignoring them -- can converge to systematically biased policies under distribution shift. We propose a general framework for IL under measurement error, inspired by explicitly modeling the causal relationships among the variables, yielding a target that retains a causal interpretation and is robust to distribution shift. Building on ideas from proximal causal inference, we introduce \\texttt{CausIL}, which treats noisy state observations as proxy variables, and we provide identification conditions under which the target policy is recoverable from demonstrations without rewards or interactive expert queries. We develop estimators for both discrete and continuous state spaces; for continuous settings, we use an adversarial procedure over RKHS function classes to learn the required parameters. We evaluate \\texttt{CausIL} on semi-simulated longitudinal data from the PhysioNet/Computing in Cardiology Challenge 2019 cohort and demonstrate improved robustness to distribution shift compared to BC baselines.",
      "pdf_url": "https://arxiv.org/pdf/2601.22206v1",
      "arxiv_url": "http://arxiv.org/abs/2601.22206v1",
      "published": "2026-01-29",
      "categories": [
        "cs.LG",
        "stat.ME",
        "stat.ML"
      ]
    },
    {
      "title": "Causal Autoregressive Diffusion Language Model",
      "authors": [
        "Junhao Ruan",
        "Bei Li",
        "Yongjing Yin",
        "Pengcheng Huang",
        "Xin Chen",
        "Jingang Wang",
        "Xunliang Cai",
        "Tong Xiao",
        "JingBo Zhu"
      ],
      "abstract": "In this work, we propose Causal Autoregressive Diffusion (CARD), a novel framework that unifies the training efficiency of ARMs with the high-throughput inference of diffusion models. CARD reformulates the diffusion process within a strictly causal attention mask, enabling dense, per-token supervision in a single forward pass. To address the optimization instability of causal diffusion, we introduce a soft-tailed masking schema to preserve local context and a context-aware reweighting mechanism derived from signal-to-noise principles. This design enables dynamic parallel decoding, where the model leverages KV-caching to adaptively generate variable-length token sequences based on confidence. Empirically, CARD outperforms existing discrete diffusion baselines while reducing training latency by 3 $\\times$ compared to block diffusion methods. Our results demonstrate that CARD achieves ARM-level data efficiency while unlocking the latency benefits of parallel generation, establishing a robust paradigm for next-generation efficient LLMs.",
      "pdf_url": "https://arxiv.org/pdf/2601.22031v1",
      "arxiv_url": "http://arxiv.org/abs/2601.22031v1",
      "published": "2026-01-29",
      "categories": [
        "cs.CL"
      ]
    },
    {
      "title": "Causal World Modeling for Robot Control",
      "authors": [
        "Lin Li",
        "Qihang Zhang",
        "Yiming Luo",
        "Shuai Yang",
        "Ruilin Wang",
        "Fei Han",
        "Mingrui Yu",
        "Zelin Gao",
        "Nan Xue",
        "Xing Zhu",
        "Yujun Shen",
        "Yinghao Xu"
      ],
      "abstract": "This work highlights that video world modeling, alongside vision-language pre-training, establishes a fresh and independent foundation for robot learning. Intuitively, video world models provide the ability to imagine the near future by understanding the causality between actions and visual dynamics. Inspired by this, we introduce LingBot-VA, an autoregressive diffusion framework that learns frame prediction and policy execution simultaneously. Our model features three carefully crafted designs: (1) a shared latent space, integrating vision and action tokens, driven by a Mixture-of-Transformers (MoT) architecture, (2) a closed-loop rollout mechanism, allowing for ongoing acquisition of environmental feedback with ground-truth observations, (3) an asynchronous inference pipeline, parallelizing action prediction and motor execution to support efficient control. We evaluate our model on both simulation benchmarks and real-world scenarios, where it shows significant promise in long-horizon manipulation, data efficiency in post-training, and strong generalizability to novel configurations. The code and model are made publicly available to facilitate the community.",
      "pdf_url": "https://arxiv.org/pdf/2601.21998v1",
      "arxiv_url": "http://arxiv.org/abs/2601.21998v1",
      "published": "2026-01-29",
      "categories": [
        "cs.CV",
        "cs.RO"
      ]
    },
    {
      "title": "FBS: Modeling Native Parallel Reading inside a Transformer",
      "authors": [
        "Tongxi Wang"
      ],
      "abstract": "Large language models (LLMs) excel across many tasks, yet inference is still dominated by strictly token-by-token autoregression. Existing acceleration methods largely patch this pipeline and miss core human-reading ingredients: content-adaptive foresight, chunk-structure-aware compute allocation, and train--test consistency for preview/skimming. We propose the \\textbf{Fovea-Block-Skip Transformer} (FBS), which injects a causal, trainable loop into Transformers via Parafovea-Attention Window (PAW), Chunk-Head (CH), and Skip-Gate (SG). Across diverse benchmarks, FBS improves the quality-efficiency trade-off without increasing parameters, and ablations show the three modules are complementary.",
      "pdf_url": "https://arxiv.org/pdf/2601.21708v1",
      "arxiv_url": "http://arxiv.org/abs/2601.21708v1",
      "published": "2026-01-29",
      "categories": [
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Differential Dynamic Causal Nets: Model Construction, Identification and Group Comparisons",
      "authors": [
        "Kang You",
        "Gary Green",
        "Jian Zhang"
      ],
      "abstract": "Pathophysiolpgical modelling of brain systems from microscale to macroscale remains difficult in group comparisons partly because of the infeasibility of modelling the interactions of thousands of neurons at the scales involved. Here, to address the challenge, we present a novel approach to construct differential causal networks directly from electroencephalogram (EEG) data. The proposed network is based on conditionally coupled neuronal circuits which describe the average behaviour of interacting neuron populations that contribute to observed EEG data. In the network, each node represents a parameterised local neural system while directed edges stand for node-wise connections with transmission parameters. The network is hierarchically structured in the sense that node and edge parameters are varying in subjects but follow a mixed-effects model. A novel evolutionary optimisation algorithm for parameter inference in the proposed method is developed using a loss function derived from Chen-Fliess expansions of stochastic differential equations. The method is demonstrated by application to the fitting of coupled Jansen-Rit local models. The performance of the proposed method is evaluated on both synthetic and real EEG data. In the real EEG data analysis, we track changes in the parameters that characterise dynamic causality within brains that demonstrate epileptic activity. We show evidence of network functional disruptions, due to imbalance of excitatory-inhibitory interneurons and altered epileptic brain connectivity, before and during seizure periods.",
      "pdf_url": "https://arxiv.org/pdf/2601.21478v1",
      "arxiv_url": "http://arxiv.org/abs/2601.21478v1",
      "published": "2026-01-29",
      "categories": [
        "q-bio.NC",
        "stat.AP",
        "stat.ML"
      ]
    },
    {
      "title": "Modeling Endogenous Logic: Causal Neuro-Symbolic Reasoning Model for Explainable Multi-Behavior Recommendation",
      "authors": [
        "Yuzhe Chen",
        "Jie Cao",
        "Youquan Wang",
        "Haicheng Tao",
        "Darko B. Vukovic",
        "Jia Wu"
      ],
      "abstract": "Existing multi-behavior recommendations tend to prioritize performance at the expense of explainability, while current explainable methods suffer from limited generalizability due to their reliance on external information. Neuro-Symbolic integration offers a promising avenue for explainability by combining neural networks with symbolic logic rule reasoning. Concurrently, we posit that user behavior chains inherently embody an endogenous logic suitable for explicit reasoning. However, these observational multiple behaviors are plagued by confounders, causing models to learn spurious correlations. By incorporating causal inference into this Neuro-Symbolic framework, we propose a novel Causal Neuro-Symbolic Reasoning model for Explainable Multi-Behavior Recommendation (CNRE). CNRE operationalizes the endogenous logic by simulating a human-like decision-making process. Specifically, CNRE first employs hierarchical preference propagation to capture heterogeneous cross-behavior dependencies. Subsequently, it models the endogenous logic rule implicit in the user's behavior chain based on preference strength, and adaptively dispatches to the corresponding neural-logic reasoning path (e.g., conjunction, disjunction). This process generates an explainable causal mediator that approximates an ideal state isolated from confounding effects. Extensive experiments on three large-scale datasets demonstrate CNRE's significant superiority over state-of-the-art baselines, offering multi-level explainability from model design and decision process to recommendation results.",
      "pdf_url": "https://arxiv.org/pdf/2601.21335v1",
      "arxiv_url": "http://arxiv.org/abs/2601.21335v1",
      "published": "2026-01-29",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Data-Driven Generation of Neutron Star Equations of State Using Variational Autoencoders",
      "authors": [
        "Alex Ross",
        "Tianqi Zhao",
        "Sanjay Reddy"
      ],
      "abstract": "We develop a machine learning model based on a structured variational autoencoder (VAE) framework to reconstruct and generate neutron star (NS) equations of state (EOS). The VAE consists of an encoder network that maps high-dimensional EOS data into a lower-dimensional latent space and a decoder network that reconstructs the full EOS from the latent representation. The latent space includes supervised NS observables derived from the training EOS data, as well as latent random variables corresponding to additional unspecified EOS features learned automatically. Sampling the latent space enables the generation of new, causal, and stable EOS models that satisfy astronomical constraints on the supervised NS observables, while allowing Bayesian inference of the EOS incorporating additional multimessenger data, including gravitational waves from LIGO/Virgo and mass and radius measurements of pulsars. Based on a VAE trained on a Skyrme EOS dataset, we find that a latent space with two supervised NS observables, the maximum mass $(M_{\\max})$ and the canonical radius $(R_{1.4})$, together with one latent random variable controlling the EOS near the crust--core transition, can already reconstruct Skyrme EOSs with high fidelity, achieving mean absolute percentage errors of approximately $(0.15\\%)$ for $(M_{\\max})$ and $(R_{1.4})$ derived from the decoder-reconstructed EOS.",
      "pdf_url": "https://arxiv.org/pdf/2601.21231v1",
      "arxiv_url": "http://arxiv.org/abs/2601.21231v1",
      "published": "2026-01-29",
      "categories": [
        "astro-ph.HE",
        "astro-ph.IM",
        "astro-ph.SR",
        "cs.LG"
      ]
    }
  ]
}