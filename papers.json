{
  "last_updated": "2025-09-05T00:49:45.199312",
  "papers": [
    {
      "title": "The super learner for time-to-event outcomes: A tutorial",
      "authors": [
        "Ruth H. Keogh",
        "Karla Diaz-Ordaz",
        "Nan van Geloven",
        "Jon Michael Gran",
        "Kamaryn T. Tanner"
      ],
      "abstract": "Estimating risks or survival probabilities conditional on individual\ncharacteristics based on censored time-to-event data is a commonly faced task.\nThis may be for the purpose of developing a prediction model or may be part of\na wider estimation procedure, such as in causal inference. A challenge is that\nit is impossible to know at the outset which of a set of candidate models will\nprovide the best predictions. The super learner is a powerful approach for\nfinding the best model or combination of models ('ensemble') among a\npre-specified set of candidate models or 'learners', which can include\nparametric and machine learning models. Super learners for time-to-event\noutcomes have been developed, but the literature is technical and a reader may\nfind it challenging to gather together the full details of how these methods\nwork and can be implemented. In this paper we provide a practical tutorial on\nsuper learner methods for time-to-event outcomes. An overview of the general\nsteps involved in the super learner is given, followed by details of three\nspecific implementations for time-to-event outcomes. We cover discrete-time and\ncontinuous-time versions of the super learner, as described by Polley and van\nder Laan (2011), Westling et al. (2023) and Munch and Gerds (2024). We compare\nthe properties of the methods and provide information on how they can be\nimplemented in R. The methods are illustrated using an open access data set and\nR code is provided.",
      "pdf_url": "http://arxiv.org/pdf/2509.03315v1",
      "arxiv_url": "http://arxiv.org/abs/2509.03315v1",
      "published": "2025-09-03",
      "categories": [
        "stat.ME"
      ]
    },
    {
      "title": "Bayesian Network Propensity Score to Evaluate Treatment Effects in Observational Studies",
      "authors": [
        "Clelia Di Serio",
        "Federica Cugnata",
        "Pier Luigi Conti",
        "Alberto Briganti",
        "Fulvia Mecatti",
        "Paola Vicard",
        "Paola Maria Vittoria Rancoita"
      ],
      "abstract": "This paper focuses on the Bayesian Network Propensity Score (BNPS), a novel\napproach for estimating treatment effects in observational studies\ncharacterized by unknown (and likely unbalanced) designs and complex dependency\nstructures among covariates. Traditional methods, such as logistic regression,\noften impose rigid parametric assumptions that may lead to misspecification\nerrors, compromising causal inference. Recent classical and machine learning\nalternatives, such as boosted CART, random forests, and Stable Balancing\nWeights, seem to be attractive in a predictive perspective, but they typically\nlack asymptotic properties, such as consistency, efficiency, and valid variance\nestimation. In contrast, the recently proposed BNPS to estimate propensity\nscores uses Bayesian Networks to flexibly model conditional dependencies while\npreserving essential statistical properties such as consistency, asymptotic\nnormality and asymptotic efficiency. Combined with the H\\'ajek estimator, BNPS\nenables robust estimation of the Average Treatment Effect (ATE) in scenarios\nwith strong covariate interactions and unknown data-generating mechanisms.\nThrough extensive simulations across fifteen realistic scenarios and varying\nsample sizes, BNPS consistently outperforms benchmark methods in both empirical\nrejection rates and coverage accuracy. Finally, an application to a real-world\ndataset of 7,162 prostate cancer patients from San Raffaele Hospital (Milan,\nItaly) demonstrates BNPS's practical value in assessing the impact of pelvic\nlymph node dissection on hospitalization duration and biochemical recurrence.\nThe findings support BNPS as a statistically robust, interpretable and\ntransparent alternative for causal inference in complex observational settings,\nenhancing the reliability of evidence from real-world biomedical data.",
      "pdf_url": "http://arxiv.org/pdf/2509.03194v1",
      "arxiv_url": "http://arxiv.org/abs/2509.03194v1",
      "published": "2025-09-03",
      "categories": [
        "stat.ME",
        "stat.AP"
      ]
    },
    {
      "title": "Distribution-valued Causal Machine Learning: Implications of Credit on Spending Patterns",
      "authors": [
        "Cheuk Hang Leung",
        "Yijun Li",
        "Qi Wu"
      ],
      "abstract": "Fintech lending has become a central mechanism through which digital\nplatforms stimulate consumption, offering dynamic, personalized credit limits\nthat directly shape the purchasing power of consumers. Although prior research\nshows that higher limits increase average spending, scalar-based outcomes\nobscure the heterogeneous distributional nature of consumer responses. This\npaper addresses this gap by proposing a new causal inference framework that\nestimates how continuous changes in the credit limit affect the entire\ndistribution of consumer spending. We formalize distributional causal effects\nwithin the Wasserstein space and introduce a robust Distributional Double\nMachine Learning estimator, supported by asymptotic theory to ensure\nconsistency and validity. To implement this estimator, we design a deep\nlearning architecture comprising two components: a Neural Functional Regression\nNet to capture complex, nonlinear relationships between treatments, covariates,\nand distributional outcomes, and a Conditional Normalizing Flow Net to estimate\ngeneralized propensity scores under continuous treatment. Numerical experiments\ndemonstrate that the proposed estimator accurately recovers distributional\neffects in a range of data-generating scenarios. Applying our framework to\ntransaction-level data from a major BigTech platform, we find that increased\ncredit limits primarily shift consumers towards higher-value purchases rather\nthan uniformly increasing spending, offering new insights for personalized\nmarketing strategies and digital consumer finance.",
      "pdf_url": "http://arxiv.org/pdf/2509.03063v1",
      "arxiv_url": "http://arxiv.org/abs/2509.03063v1",
      "published": "2025-09-03",
      "categories": [
        "econ.GN",
        "q-fin.EC"
      ]
    },
    {
      "title": "Covariate Adjustment Cannot Hurt: Treatment Effect Estimation under Interference with Low-Order Outcome Interactions",
      "authors": [
        "Xinyi Wang",
        "Shuangning Li"
      ],
      "abstract": "In randomized experiments, covariates are often used to reduce variance and\nimprove the precision of treatment effect estimates. However, in many real\nworld settings, interference between units, where one unit's treatment affects\nanother's outcome, complicates causal inference. This raises a key question:\nhow can covariates be effectively used in the presence of interference?\nAddressing this challenge is nontrivial, as direct covariate adjustment, such\nas through regression, can sometimes increase variance due to dependencies\nacross units. In this paper, we study how to use covariate information to\nreduce the variance of treatment effect estimators under interference. We focus\non the total treatment effect (TTE), defined as the difference in average\noutcomes when all units are treated versus when all are controlled. Our\nanalysis is conducted under the neighborhood interference model and a low order\ninteraction outcome model. Building on the SNIPE estimator from\nCortez-Rodriguez et al. (2023), we propose a covariate adjusted SNIPE estimator\nand show that, under sparsity conditions on the interference network, the\nproposed estimator is asymptotically unbiased and has asymptotic variance no\ngreater than that of the original SNIPE estimator. This parallels the classical\nresult of Lin (2013) under the no interference assumption, where covariate\nadjustment does not worsen estimation precision. Importantly, our variance\nimprovement result does not rely on strong assumptions about the covariates:\nthe covariates may be arbitrarily dependent, affect outcomes across units, and\ndepend on the interference network itself.",
      "pdf_url": "http://arxiv.org/pdf/2509.03050v1",
      "arxiv_url": "http://arxiv.org/abs/2509.03050v1",
      "published": "2025-09-03",
      "categories": [
        "stat.ME"
      ]
    },
    {
      "title": "Improving Generative Methods for Causal Evaluation via Simulation-Based Inference",
      "authors": [
        "Pracheta Amaranath",
        "Vinitra Muralikrishnan",
        "Amit Sharma",
        "David D. Jensen"
      ],
      "abstract": "Generating synthetic datasets that accurately reflect real-world\nobservational data is critical for evaluating causal estimators, but remains a\nchallenging task. Existing generative methods offer a solution by producing\nsynthetic datasets anchored in the observed data (source data) while allowing\nvariation in key parameters such as the treatment effect and amount of\nconfounding bias. However, existing methods typically require users to provide\npoint estimates of such parameters (rather than distributions) and fixed\nestimates (rather than estimates that can be improved with reference to the\nsource data). This denies users the ability to express uncertainty over\nparameter values and removes the potential for posterior inference, potentially\nleading to unreliable estimator comparisons. We introduce simulation-based\ninference for causal evaluation (SBICE), a framework that models generative\nparameters as uncertain and infers their posterior distribution given a source\ndataset. Leveraging techniques in simulation-based inference, SBICE identifies\nparameter configurations that produce synthetic datasets closely aligned with\nthe source data distribution. Empirical results demonstrate that SBICE improves\nthe reliability of estimator evaluations by generating more realistic datasets,\nwhich supports a robust and data-consistent approach to causal benchmarking\nunder uncertainty.",
      "pdf_url": "http://arxiv.org/pdf/2509.02892v1",
      "arxiv_url": "http://arxiv.org/abs/2509.02892v1",
      "published": "2025-09-02",
      "categories": [
        "cs.LG",
        "stat.ME"
      ]
    },
    {
      "title": "Causal Spatial Quantile Regression",
      "authors": [
        "Yan Gong",
        "Reetam Majumder",
        "Brian J. Reich",
        "Raphaël Huser"
      ],
      "abstract": "Treatment effects in a wide range of economic, environmental, and\nepidemiological applications often vary across space, and understanding the\nheterogeneity of causal effects across space and outcome quantiles is a\ncritical challenge in spatial causal inference. To effectively capture spatial\nheterogeneity in distributional treatment effects, we propose a novel\nsemiparametric neural network-based causal framework leveraging deep spatial\nquantile regression and then construct a plug-in estimator for spatial quantile\ntreatment effects (SQTE). This framework incorporates an efficient adjustment\nprocedure to mitigate the impact of spatial hidden confounders. Extensive\nsimulations across various scenarios demonstrate that our methodology can\naccurately estimate SQTE, even with the presence of spatial hidden confounders.\nAdditionally, the spatial confounding adjustment procedure effectively reduces\nneighborhood spatial patterns in the residuals. We apply this method to assess\nthe spatially varying quantile treatment effects of maternal smoking on newborn\nbirth weight in North Carolina, United States. Our findings consistently show\nnegative effects across all birth weight quantiles, with particularly severe\nimpacts observed in the lower quantile regions.",
      "pdf_url": "http://arxiv.org/pdf/2509.02294v1",
      "arxiv_url": "http://arxiv.org/abs/2509.02294v1",
      "published": "2025-09-02",
      "categories": [
        "stat.ME"
      ]
    },
    {
      "title": "Interpretational errors with instrumental variables",
      "authors": [
        "Luca Locher",
        "Mats J. Stensrud",
        "Aaron L. Sarvet"
      ],
      "abstract": "Instrumental variables (IV) are often used to identify causal effects in\nobservational settings and experiments subject to non-compliance. Under\ncanonical assumptions, IVs allow us to identify a so-called local average\ntreatment effect (LATE). The use of IVs is often accompanied by a pragmatic\ndecision to abandon the identification of the causal parameter that corresponds\nto the original research question and target the LATE instead. This pragmatic\ndecision presents a potential source of error: an investigator mistakenly\ninterprets findings as if they had made inference on their original causal\nparameter of interest. We conducted a systematic review and meta-analysis of\npatterns of pragmatism and interpretational errors in the applied IV literature\npublished in leading journals of economics, political science, epidemiology,\nand clinical medicine (n = 309 unique studies). We found that a large fraction\nof studies targeted the LATE, although specific interest in this parameter was\nrare. Of these studies, 61% contained claims that mistakenly suggested that\nanother parameter was targeted -- one whose value likely differs, and could\neven have the opposite sign, from the parameter actually estimated. Our\nfindings suggest that the validity of conclusions drawn from IV applications is\noften compromised by interpretational errors.",
      "pdf_url": "http://arxiv.org/pdf/2509.02045v1",
      "arxiv_url": "http://arxiv.org/abs/2509.02045v1",
      "published": "2025-09-02",
      "categories": [
        "econ.EM",
        "stat.AP"
      ]
    },
    {
      "title": "Any-Order Flexible Length Masked Diffusion",
      "authors": [
        "Jaeyeon Kim",
        "Lee Cheuk-Kit",
        "Carles Domingo-Enrich",
        "Yilun Du",
        "Sham Kakade",
        "Timothy Ngotiaoco",
        "Sitan Chen",
        "Michael Albergo"
      ],
      "abstract": "Masked diffusion models (MDMs) have recently emerged as a promising\nalternative to autoregressive models over discrete domains. MDMs generate\nsequences in an any-order, parallel fashion, enabling fast inference and strong\nperformance on non-causal tasks. However, a crucial limitation is that they do\nnot support token insertions and are thus limited to fixed-length generations.\nTo this end, we introduce Flexible Masked Diffusion Models (FlexMDMs), a\ndiscrete diffusion paradigm that simultaneously can model sequences of flexible\nlength while provably retaining MDMs' flexibility of any-order inference.\nGrounded in an extension of the stochastic interpolant framework, FlexMDMs\ngenerate sequences by inserting mask tokens and unmasking them. Empirically, we\nshow that FlexMDMs match MDMs in perplexity while modeling length statistics\nwith much higher fidelity. On a synthetic maze planning task, they achieve\n$\\approx 60 \\%$ higher success rate than MDM baselines. Finally, we show\npretrained MDMs can easily be retrofitted into FlexMDMs: on 16 H100s, it takes\nonly three days to fine-tune LLaDA-8B into a FlexMDM, achieving superior\nperformance on math (GSM8K, $58\\% \\to 67\\%$) and code infilling performance\n($52\\% \\to 65\\%$).",
      "pdf_url": "http://arxiv.org/pdf/2509.01025v1",
      "arxiv_url": "http://arxiv.org/abs/2509.01025v1",
      "published": "2025-08-31",
      "categories": [
        "cs.LG"
      ]
    },
    {
      "title": "ProCause: Generating Counterfactual Outcomes to Evaluate Prescriptive Process Monitoring Methods",
      "authors": [
        "Jakob De Moor",
        "Hans Weytjens",
        "Johannes De Smedt"
      ],
      "abstract": "Prescriptive Process Monitoring (PresPM) is the subfield of Process Mining\nthat focuses on optimizing processes through real-time interventions based on\nevent log data. Evaluating PresPM methods is challenging due to the lack of\nground-truth outcomes for all intervention actions in datasets. A generative\ndeep learning approach from the field of Causal Inference (CI), RealCause, has\nbeen commonly used to estimate the outcomes for proposed intervention actions\nto evaluate a new policy. However, RealCause overlooks the temporal\ndependencies in process data, and relies on a single CI model architecture,\nTARNet, limiting its effectiveness. To address both shortcomings, we introduce\nProCause, a generative approach that supports both sequential (e.g., LSTMs) and\nnon-sequential models while integrating multiple CI architectures (S-Learner,\nT-Learner, TARNet, and an ensemble). Our research using a simulator with known\nground truths reveals that TARNet is not always the best choice; instead, an\nensemble of models offers more consistent reliability, and leveraging LSTMs\nshows potential for improved evaluations when temporal dependencies are\npresent. We further validate ProCause's practical effectiveness through a\nreal-world data analysis, ensuring a more reliable evaluation of PresPM\nmethods.",
      "pdf_url": "http://arxiv.org/pdf/2509.00797v1",
      "arxiv_url": "http://arxiv.org/abs/2509.00797v1",
      "published": "2025-08-31",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ME"
      ]
    },
    {
      "title": "Quantum Causality: Resolving Simpson's Paradox with $\\mathcal{DO}$-Calculus",
      "authors": [
        "Pilsung Kang"
      ],
      "abstract": "Distinguishing correlation from causation is a fundamental challenge in\nmachine intelligence, often representing a critical barrier to building robust\nand trustworthy systems. While Pearl's $\\mathcal{DO}$-calculus provides a\nrigorous framework for causal inference, a parallel challenge lies in its\nphysical implementation. Here, we apply and experimentally validate a quantum\nalgorithmic framework for performing causal interventions. Our approach maps\ncausal networks onto quantum circuits where probabilistic links are encoded by\ncontrolled-rotation gates, and interventions are realized by a structural\nremodeling of the circuit -- a physical analogue to Pearl's ``graph surgery''.\nWe demonstrate the method's efficacy by resolving Simpson's Paradox in a\n3-qubit model, and show its scalability by quantifying confounding bias in a\n10-qubit healthcare simulation. Critically, we provide a proof-of-principle\nexperimental validation on an IonQ Aria quantum computer, successfully\nreproducing the paradox and its resolution in the presence of real-world noise.\nThis work establishes a practical pathway for quantum causal inference,\noffering a new computational tool to address deep-rooted challenges in\nalgorithmic fairness and explainable AI (XAI).",
      "pdf_url": "http://arxiv.org/pdf/2509.00744v1",
      "arxiv_url": "http://arxiv.org/abs/2509.00744v1",
      "published": "2025-08-31",
      "categories": [
        "quant-ph",
        "cs.AI",
        "cs.LG"
      ]
    }
  ]
}