{
  "last_updated": "2025-10-16T00:51:32.318777",
  "papers": [
    {
      "title": "A Quantum Generative Framework for Modeling Single-Cell Transcriptomes with Gene-Gene and Cell-Cell Interactions",
      "authors": [
        "Selim Romero",
        "Vignesh Kumar",
        "Robert S. Chapkin",
        "James J. Cai"
      ],
      "abstract": "Single-cell RNA sequencing (scRNA-seq) data simulation is limited by\nclassical methods that rely on linear correlations, failing to capture the\nintrinsic, nonlinear dependencies and the simultaneous gene-gene and cell-cell\ninteractions. We introduce qSimCells, a novel hybrid quantum-classical\nsimulator that leverages quantum entanglement to model single-cell\ntranscriptomes. The core innovation is a quantum kernel that uses a\nparameterized quantum circuit with CNOT gates to encode complex, nonlinear gene\nregulatory network (GRN) and cell-cell communication topologies with explicit\ndirectionality (causality). The synthetic data exhibits non-classical\ndependencies that challenge standard analysis. We demonstrated that classical\ncorrelation methods (Pearson and Spearman) failed to reconstruct the complete\nprogrammed quantum causal paths, instead reporting spurious statistical\nartifacts driven by high base-gene expression probabilities. Applying\nCellChat2.0 to the simulated cell-cell communication validated the true\nmechanistic links by showing a robust, relative increase in communication\nprobability (up to 75-fold) only when the quantum entanglement was active. This\nwork confirms that the quantum kernel is essential for creating high-fidelity\nground truth data, highlighting the need for advanced inference techniques to\ncapture the complex, non-classical dependencies inherent in gene regulation.",
      "pdf_url": "http://arxiv.org/pdf/2510.12776v1",
      "arxiv_url": "http://arxiv.org/abs/2510.12776v1",
      "published": "2025-10-14",
      "categories": [
        "q-bio.QM",
        "cs.ET",
        "physics.bio-ph",
        "q-bio.GN"
      ]
    },
    {
      "title": "Causal inference of post-transcriptional regulation timelines from long-read sequencing in Arabidopsis thaliana",
      "authors": [
        "Rub√©n Martos",
        "Christophe Ambroise",
        "Guillem Rigaill"
      ],
      "abstract": "We propose a novel framework for reconstructing the chronology of genetic\nregulation using causal inference based on Pearl's theory. The approach\nproceeds in three main stages: causal discovery, causal inference, and\nchronology construction. We apply it to the ndhB and ndhD genes of the\nchloroplast in Arabidopsis thaliana, generating four alternative maturation\ntimeline models per gene, each derived from a different causal discovery\nalgorithm (HC, PC, LiNGAM, or NOTEARS). Two methodological challenges are\naddressed: the presence of missing data, handled via an EM algorithm that\njointly imputes missing values and estimates the Bayesian network, and the\nselection of the $\\ell_1$-regularization parameter in NOTEARS, for which we\nintroduce a stability selection strategy. The resulting causal models\nconsistently outperform reference chronologies in terms of both reliability and\nmodel fit. Moreover, by combining causal reasoning with domain expertise, the\nframework enables the formulation of testable hypotheses and the design of\ntargeted experimental interventions grounded in theoretical predictions.",
      "pdf_url": "http://arxiv.org/pdf/2510.12504v1",
      "arxiv_url": "http://arxiv.org/abs/2510.12504v1",
      "published": "2025-10-14",
      "categories": [
        "stat.ME"
      ]
    },
    {
      "title": "The Robustness of Differentiable Causal Discovery in Misspecified Scenarios",
      "authors": [
        "Huiyang Yi",
        "Yanyan He",
        "Duxin Chen",
        "Mingyu Kang",
        "He Wang",
        "Wenwu Yu"
      ],
      "abstract": "Causal discovery aims to learn causal relationships between variables from\ntargeted data, making it a fundamental task in machine learning. However,\ncausal discovery algorithms often rely on unverifiable causal assumptions,\nwhich are usually difficult to satisfy in real-world data, thereby limiting the\nbroad application of causal discovery in practical scenarios. Inspired by these\nconsiderations, this work extensively benchmarks the empirical performance of\nvarious mainstream causal discovery algorithms, which assume i.i.d. data, under\neight model assumption violations. Our experimental results show that\ndifferentiable causal discovery methods exhibit robustness under the metrics of\nStructural Hamming Distance and Structural Intervention Distance of the\ninferred graphs in commonly used challenging scenarios, except for scale\nvariation. We also provide the theoretical explanations for the performance of\ndifferentiable causal discovery methods. Finally, our work aims to\ncomprehensively benchmark the performance of recent differentiable causal\ndiscovery methods under model assumption violations, and provide the standard\nfor reasonable evaluation of causal discovery, as well as to further promote\nits application in real-world scenarios.",
      "pdf_url": "http://arxiv.org/pdf/2510.12503v1",
      "arxiv_url": "http://arxiv.org/abs/2510.12503v1",
      "published": "2025-10-14",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ME",
        "stat.ML"
      ]
    },
    {
      "title": "On the permutation invariance principle for causal estimands",
      "authors": [
        "Jiaqi Tong",
        "Fan Li"
      ],
      "abstract": "In many causal inference problems, multiple action variables share the same\ncausal role, such as mediators, factors, network units, or genotypes, yet lack\na natural ordering. To avoid ambiguity in interpretation, causal estimands\nshould remain unchanged under relabeling, an implicit principle we refer to as\npermutation invariance. We formally characterize this principle, analyze its\nalgebraic and combinatorial structure for verification, and present a class of\nweighted estimands that are permutation-invariant while capturing interactions\nof all orders. We further provide guidance on selecting weights that yield\nresidual-free estimands, whose inclusion-exclusion sums capture the maximal\neffect, and extend our results to ratio effect measures.",
      "pdf_url": "http://arxiv.org/pdf/2510.11863v1",
      "arxiv_url": "http://arxiv.org/abs/2510.11863v1",
      "published": "2025-10-13",
      "categories": [
        "stat.ME"
      ]
    },
    {
      "title": "WaveletDiff: Multilevel Wavelet Diffusion For Time Series Generation",
      "authors": [
        "Yu-Hsiang Wang",
        "Olgica Milenkovic"
      ],
      "abstract": "Time series are ubiquitous in many applications that involve forecasting,\nclassification and causal inference tasks, such as healthcare, finance, audio\nsignal processing and climate sciences. Still, large, high-quality time series\ndatasets remain scarce. Synthetic generation can address this limitation;\nhowever, current models confined either to the time or frequency domains\nstruggle to reproduce the inherently multi-scaled structure of real-world time\nseries. We introduce WaveletDiff, a novel framework that trains diffusion\nmodels directly on wavelet coefficients to exploit the inherent\nmulti-resolution structure of time series data. The model combines dedicated\ntransformers for each decomposition level with cross-level attention mechanisms\nthat enable selective information exchange between temporal and frequency\nscales through adaptive gating. It also incorporates energy preservation\nconstraints for individual levels based on Parseval's theorem to preserve\nspectral fidelity throughout the diffusion process. Comprehensive tests across\nsix real-world datasets from energy, finance, and neuroscience domains\ndemonstrate that WaveletDiff consistently outperforms state-of-the-art\ntime-domain and frequency-domain generative methods on both short and long time\nseries across five diverse performance metrics. For example, WaveletDiff\nachieves discriminative scores and Context-FID scores that are $3\\times$\nsmaller on average than the second-best baseline across all datasets.",
      "pdf_url": "http://arxiv.org/pdf/2510.11839v1",
      "arxiv_url": "http://arxiv.org/abs/2510.11839v1",
      "published": "2025-10-13",
      "categories": [
        "cs.LG"
      ]
    },
    {
      "title": "The Role of Congeniality in Multiple Imputation for Doubly Robust Causal Estimation",
      "authors": [
        "Lucy D'Agostino McGowan"
      ],
      "abstract": "This paper provides clear and practical guidance on the specification of\nimputation models when multiple imputation is used in conjunction with doubly\nrobust estimation methods for causal inference. Through theoretical arguments\nand targeted simulations, we show that when a confounder has missing data the\ncorresponding imputation model must include all variables used in either the\npropensity score model or the outcome model, and that these variables must\nappear in the same functional form as in the final analysis. Violating these\nconditions can lead to biased treatment effect estimates, even when both\ncomponents of the doubly robust estimator are correctly specified. We present a\nmathematical framework for doubly robust estimation combined with multiple\nimputation, establish the theoretical requirements for proper imputation in\nthis setting, and demonstrate the consequences of misspecification through\nsimulation. Based on these findings, we offer concrete recommendations to\nensure valid inference when using multiple imputation with doubly robust\nmethods in applied causal analyses.",
      "pdf_url": "http://arxiv.org/pdf/2510.11633v1",
      "arxiv_url": "http://arxiv.org/abs/2510.11633v1",
      "published": "2025-10-13",
      "categories": [
        "stat.ME",
        "stat.AP"
      ]
    },
    {
      "title": "Causal Disentanglement Learning for Accurate Anomaly Detection in Multivariate Time Series",
      "authors": [
        "Wonah Kim",
        "Jeonghyeon Park",
        "Dongsan Jun",
        "Jungkyu Han",
        "Sejin Chun"
      ],
      "abstract": "Disentangling complex causal relationships is important for accurate\ndetection of anomalies. In multivariate time series analysis, dynamic\ninteractions among data variables over time complicate the interpretation of\ncausal relationships. Traditional approaches assume statistical independence\nbetween variables in unsupervised settings, whereas recent methods capture\nfeature correlations through graph representation learning. However, their\nrepresentations fail to explicitly infer the causal relationships over\ndifferent time periods. To solve the problem, we propose Causally Disentangled\nRepresentation Learning for Anomaly Detection (CDRL4AD) to detect anomalies and\nidentify their causal relationships in multivariate time series. First, we\ndesign the causal process as model input, the temporal heterogeneous graph, and\ncausal relationships. Second, our representation identifies causal\nrelationships over different time periods and disentangles latent variables to\ninfer the corresponding causal factors. Third, our experiments on real-world\ndatasets demonstrate that CDRL4AD outperforms state-of-the-art methods in terms\nof accuracy and root cause analysis. Fourth, our model analysis validates\nhyperparameter sensitivity and the time complexity of CDRL4AD. Last, we conduct\na case study to show how our approach assists human experts in diagnosing the\nroot causes of anomalies.",
      "pdf_url": "http://arxiv.org/pdf/2510.11084v1",
      "arxiv_url": "http://arxiv.org/abs/2510.11084v1",
      "published": "2025-10-13",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "FBS Model-based Maintenance Record Accumulation for Failure-Cause Inference in Manufacturing Systems",
      "authors": [
        "Takuma Fujiu",
        "Sho Okazaki",
        "Kohei Kaminishi",
        "Yuji Nakata",
        "Shota Hamamoto",
        "Kenshin Yokose",
        "Tatsunori Hara",
        "Yasushi Umeda",
        "Jun Ota"
      ],
      "abstract": "In manufacturing systems, identifying the causes of failures is crucial for\nmaintaining and improving production efficiency. In knowledge-based\nfailure-cause inference, it is important that the knowledge base (1) explicitly\nstructures knowledge about the target system and about failures, and (2)\ncontains sufficiently long causal chains of failures. In this study, we\nconstructed Diagnostic Knowledge Ontology and proposed a\nFunction-Behavior-Structure (FBS) model-based maintenance-record accumulation\nmethod based on it. Failure-cause inference using the maintenance records\naccumulated by the proposed method showed better agreement with the set of\ncandidate causes enumerated by experts, especially in difficult cases where the\nnumber of related cases is small and the vocabulary used differs. In the\nfuture, it will be necessary to develop inference methods tailored to these\nmaintenance records, build a user interface, and carry out validation on larger\nand more diverse systems. Additionally, this approach leverages the\nunderstanding and knowledge of the target in the design phase to support\nknowledge accumulation and problem solving during the maintenance phase, and it\nis expected to become a foundation for knowledge sharing across the entire\nengineering chain in the future.",
      "pdf_url": "http://arxiv.org/pdf/2510.11003v1",
      "arxiv_url": "http://arxiv.org/abs/2510.11003v1",
      "published": "2025-10-13",
      "categories": [
        "cs.AI",
        "cs.IR"
      ]
    },
    {
      "title": "Identifying treatment effects on categorical outcomes in IV models",
      "authors": [
        "Onil Boussim"
      ],
      "abstract": "This paper provides a nonparametric framework for causal inference with\ncategorical outcomes under binary treatment and binary instrument settings. We\ndecompose the observed joint probability of outcomes and treatment into\nmarginal probabilities of potential outcomes and treatment, and association\nparameters that capture selection bias due to unobserved heterogeneity. Under a\nnovel identifying assumption, association similarity, which requires the\ndependence between unobserved factors and potential outcomes to be invariant\nacross treatment states, we achieve point identification of the full\ndistribution of potential outcomes. Recognizing that this assumption may be\nstrong in some contexts, we propose two weaker alternatives: monotonic\nassociation, which restricts the direction of selection heterogeneity, and\nbounded association, which constrains its magnitude. These relaxed assumptions\ndeliver sharp partial identification bounds that nest point identification as a\nspecial case and facilitate transparent sensitivity analysis. We illustrate the\nframework in an empirical application, estimating the causal effect of private\nhealth insurance on health outcomes.",
      "pdf_url": "http://arxiv.org/pdf/2510.10946v1",
      "arxiv_url": "http://arxiv.org/abs/2510.10946v1",
      "published": "2025-10-13",
      "categories": [
        "econ.EM"
      ]
    },
    {
      "title": "Large Language Models for Full-Text Methods Assessment: A Case Study on Mediation Analysis",
      "authors": [
        "Wenqing Zhang",
        "Trang Nguyen",
        "Elizabeth A. Stuart",
        "Yiqun T. Chen"
      ],
      "abstract": "Systematic reviews are crucial for synthesizing scientific evidence but\nremain labor-intensive, especially when extracting detailed methodological\ninformation. Large language models (LLMs) offer potential for automating\nmethodological assessments, promising to transform evidence synthesis. Here,\nusing causal mediation analysis as a representative methodological domain, we\nbenchmarked state-of-the-art LLMs against expert human reviewers across 180\nfull-text scientific articles. Model performance closely correlated with human\njudgments (accuracy correlation 0.71; F1 correlation 0.97), achieving\nnear-human accuracy on straightforward, explicitly stated methodological\ncriteria. However, accuracy sharply declined on complex, inference-intensive\nassessments, lagging expert reviewers by up to 15%. Errors commonly resulted\nfrom superficial linguistic cues -- for instance, models frequently\nmisinterpreted keywords like \"longitudinal\" or \"sensitivity\" as automatic\nevidence of rigorous methodological approache, leading to systematic\nmisclassifications. Longer documents yielded lower model accuracy, whereas\npublication year showed no significant effect. Our findings highlight an\nimportant pattern for practitioners using LLMs for methods review and synthesis\nfrom full texts: current LLMs excel at identifying explicit methodological\nfeatures but require human oversight for nuanced interpretations. Integrating\nautomated information extraction with targeted expert review thus provides a\npromising approach to enhance efficiency and methodological rigor in evidence\nsynthesis across diverse scientific fields.",
      "pdf_url": "http://arxiv.org/pdf/2510.10762v1",
      "arxiv_url": "http://arxiv.org/abs/2510.10762v1",
      "published": "2025-10-12",
      "categories": [
        "cs.CL",
        "stat.AP"
      ]
    }
  ]
}