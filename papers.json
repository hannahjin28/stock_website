{
  "last_updated": "2025-05-15T00:52:44.059540",
  "papers": [
    {
      "title": "AC-Reason: Towards Theory-Guided Actual Causality Reasoning with Large Language Models",
      "authors": [
        "Yanxi Zhang",
        "Xin Cong",
        "Zhong Zhang",
        "Xiao Liu",
        "Dongyan Zhao",
        "Yesai Wu"
      ],
      "abstract": "Actual causality (AC), a fundamental aspect of causal reasoning (CR), is\nresponsible for attribution and responsibility assignment in real-world\nscenarios. However, existing LLM-based methods lack grounding in formal AC\ntheory, resulting in limited interpretability. Therefore, we propose AC-Reason,\na semi-formal reasoning framework that identifies causally relevant events\nwithin an AC scenario, infers the values of their formal causal factors (e.g.,\nsufficiency, necessity, and normality), and answers AC queries via a\ntheory-guided algorithm with explanations. While AC-Reason does not explicitly\nconstruct a causal graph, it operates over variables in the underlying causal\nstructure to support principled reasoning. To enable comprehensive evaluation,\nwe introduce AC-Bench, a new benchmark built upon and substantially extending\nBig-Bench Hard Causal Judgment (BBH-CJ). AC-Bench comprises ~1K carefully\nannotated samples, each with detailed reasoning steps and focuses solely on\nactual causation. The case study shows that synthesized samples in AC-Bench\npresent greater challenges for LLMs. Extensive experiments on BBH-CJ and\nAC-Bench show that AC-Reason consistently improves LLM performance over\nbaselines. On BBH-CJ, all tested LLMs surpass the average human rater accuracy\nof 69.60%, with GPT-4 + AC-Reason achieving 75.04%. On AC-Bench, GPT-4 +\nAC-Reason again achieves the highest accuracy of 71.82%. AC-Bench further\nenables fine-grained analysis of reasoning faithfulness, revealing that only\nQwen-2.5-72B-Instruct, Claude-3.5-Sonnet, and GPT-4o exhibit faithful\nreasoning, whereas GPT-4 tends to exploit shortcuts. Finally, our ablation\nstudy proves that integrating AC theory into LLMs is highly effective, with the\nproposed algorithm contributing the most significant performance gains.",
      "pdf_url": "http://arxiv.org/pdf/2505.08750v1",
      "arxiv_url": "http://arxiv.org/abs/2505.08750v1",
      "published": "2025-05-13",
      "categories": [
        "cs.CL"
      ]
    },
    {
      "title": "Assumption-robust Causal Inference",
      "authors": [
        "Aditya Ghosh",
        "Dominik Rothenhäusler"
      ],
      "abstract": "In observational causal inference, it is common to encounter multiple\nadjustment sets that appear equally plausible. It is often untestable which of\nthese adjustment sets are valid to adjust for (i.e., satisfies ignorability).\nThis discrepancy can pose practical challenges as it is typically unclear how\nto reconcile multiple, possibly conflicting estimates of the average treatment\neffect (ATE). A naive approach is to report the whole range (convex hull of the\nunion) of the resulting confidence intervals. However, the width of this\ninterval might not shrink to zero in large samples and can be unnecessarily\nwide in real applications. To address this issue, we propose a summary\nprocedure that generates a single estimate, one confidence interval, and\nidentifies a set of units for which the causal effect estimate remains valid,\nprovided at least one adjustment set is valid. The width of our proposed\nconfidence interval shrinks to zero with sample size at $n^{-1/2}$ rate, unlike\nthe original range which is of constant order. Thus, our assumption-robust\napproach enables reliable causal inference on the ATE even in scenarios where\nmost of the adjustment sets are invalid. Admittedly, this robustness comes at a\ncost: our inferential guarantees apply to a target population close to, but\ndifferent from, the one originally intended. We use synthetic and real-data\nexamples to demonstrate that our proposed procedure provides substantially\ntighter confidence intervals for the ATE as compared to the whole range. In\nparticular, for a real-world dataset on 401(k) retirement plans our method\nproduces a confidence interval 50\\% shorter than the whole range of confidence\nintervals based on multiple adjustment sets.",
      "pdf_url": "http://arxiv.org/pdf/2505.08729v1",
      "arxiv_url": "http://arxiv.org/abs/2505.08729v1",
      "published": "2025-05-13",
      "categories": [
        "stat.ME",
        "econ.EM"
      ]
    },
    {
      "title": "Bayesian Estimation of Causal Effects Using Proxies of a Latent Interference Network",
      "authors": [
        "Bar Weinstein",
        "Daniel Nevo"
      ],
      "abstract": "Network interference occurs when treatments assigned to some units affect the\noutcomes of others. Traditional approaches often assume that the observed\nnetwork correctly specifies the interference structure. However, in practice,\nresearchers frequently only have access to proxy measurements of the\ninterference network due to limitations in data collection or potential\nmismatches between measured networks and actual interference pathways. In this\npaper, we introduce a framework for estimating causal effects when only proxy\nnetworks are available. Our approach leverages a structural causal model that\naccommodates diverse proxy types, including noisy measurements, multiple data\nsources, and multilayer networks, and defines causal effects as interventions\non population-level treatments. Since the true interference network is latent,\nestimation poses significant challenges. To overcome them, we develop a\nBayesian inference framework. We propose a Block Gibbs sampler with Locally\nInformed Proposals to update the latent network, thereby efficiently exploring\nthe high-dimensional posterior space composed of both discrete and continuous\nparameters. We illustrate the performance of our method through numerical\nexperiments, demonstrating its accuracy in recovering causal effects even when\nonly proxies of the interference network are available.",
      "pdf_url": "http://arxiv.org/pdf/2505.08395v1",
      "arxiv_url": "http://arxiv.org/abs/2505.08395v1",
      "published": "2025-05-13",
      "categories": [
        "stat.ME",
        "stat.AP",
        "stat.CO",
        "stat.ML",
        "stat.OT"
      ]
    },
    {
      "title": "Empowering Vision Transformers with Multi-Scale Causal Intervention for Long-Tailed Image Classification",
      "authors": [
        "Xiaoshuo Yan",
        "Zhaochuan Li",
        "Lei Meng",
        "Zhuang Qi",
        "Wei Wu",
        "Zixuan Li",
        "Xiangxu Meng"
      ],
      "abstract": "Causal inference has emerged as a promising approach to mitigate long-tail\nclassification by handling the biases introduced by class imbalance. However,\nalong with the change of advanced backbone models from Convolutional Neural\nNetworks (CNNs) to Visual Transformers (ViT), existing causal models may not\nachieve an expected performance gain. This paper investigates the influence of\nexisting causal models on CNNs and ViT variants, highlighting that ViT's global\nfeature representation makes it hard for causal methods to model associations\nbetween fine-grained features and predictions, which leads to difficulties in\nclassifying tail classes with similar visual appearance. To address these\nissues, this paper proposes TSCNet, a two-stage causal modeling method to\ndiscover fine-grained causal associations through multi-scale causal\ninterventions. Specifically, in the hierarchical causal representation learning\nstage (HCRL), it decouples the background and objects, applying backdoor\ninterventions at both the patch and feature level to prevent model from using\nclass-irrelevant areas to infer labels which enhances fine-grained causal\nrepresentation. In the counterfactual logits bias calibration stage (CLBC), it\nrefines the optimization of model's decision boundary by adaptive constructing\ncounterfactual balanced data distribution to remove the spurious associations\nin the logits caused by data distribution. Extensive experiments conducted on\nvarious long-tail benchmarks demonstrate that the proposed TSCNet can eliminate\nmultiple biases introduced by data imbalance, which outperforms existing\nmethods.",
      "pdf_url": "http://arxiv.org/pdf/2505.08173v1",
      "arxiv_url": "http://arxiv.org/abs/2505.08173v1",
      "published": "2025-05-13",
      "categories": [
        "cs.CV"
      ]
    },
    {
      "title": "Asymptotically Efficient Data-adaptive Penalized Shrinkage Estimation with Application to Causal Inference",
      "authors": [
        "Herbert P. Susmann",
        "Yiting Li",
        "Mara A. McAdams-DeMarco",
        "Wenbo Wu",
        "Iván Díaz"
      ],
      "abstract": "A rich literature exists on constructing non-parametric estimators with\noptimal asymptotic properties. In addition to asymptotic guarantees, it is\noften of interest to design estimators with desirable finite-sample properties;\nsuch as reduced mean-squared error of a large set of parameters. We provide\nexamples drawn from causal inference where this may be the case, such as\nestimating a large number of group-specific treatment effects. We show how\nfinite-sample properties of non-parametric estimators, particularly their\nvariance, can be improved by careful application of penalization. Given a\ntarget parameter of interest we derive a novel penalized parameter defined as\nthe solution to an optimization problem that balances fidelity to the original\nparameter against a penalty term. By deriving the non-parametric efficiency\nbound for the penalized parameter, we are able to propose simple data-adaptive\nchoices for the L1 and L2 tuning parameters designed to minimize finite-sample\nmean-squared error while preserving optimal asymptotic properties. The L1 and\nL2 penalization amounts to an adjustment that can be performed as a\npost-processing step applied to any asymptotically normal and efficient\nestimator. We show in extensive simulations that this adjustment yields\nestimators with lower MSE than the unpenalized estimators. Finally, we apply\nour approach to estimate provider quality measures of kidney dialysis providers\nwithin a causal inference framework.",
      "pdf_url": "http://arxiv.org/pdf/2505.08065v1",
      "arxiv_url": "http://arxiv.org/abs/2505.08065v1",
      "published": "2025-05-12",
      "categories": [
        "stat.ME"
      ]
    },
    {
      "title": "rd2d: Causal Inference in Boundary Discontinuity Designs",
      "authors": [
        "Matias D. Cattaneo",
        "Rocio Titiunik",
        "Ruiqi Rae Yu"
      ],
      "abstract": "Boundary discontinuity designs -- also known as Multi-Score Regression\nDiscontinuity (RD) designs, with Geographic RD designs as a prominent example\n-- are often used in empirical research to learn about causal treatment effects\nalong a continuous assignment boundary defined by a bivariate score. This\narticle introduces the R package rd2d, which implements and extends the\nmethodological results developed in Cattaneo, Titiunik and Yu (2025) for\nboundary discontinuity designs. The package employs local polynomial estimation\nand inference using either the bivariate score or a univariate\ndistance-to-boundary metric. It features novel data-driven bandwidth selection\nprocedures, and offers both pointwise and uniform estimation and inference\nalong the assignment boundary. The numerical performance of the package is\ndemonstrated through a simulation study.",
      "pdf_url": "http://arxiv.org/pdf/2505.07989v1",
      "arxiv_url": "http://arxiv.org/abs/2505.07989v1",
      "published": "2025-05-12",
      "categories": [
        "stat.ME",
        "econ.EM",
        "stat.CO"
      ]
    },
    {
      "title": "Nonparametric Instrumental Variable Inference with Many Weak Instruments",
      "authors": [
        "Lars van der Laan",
        "Nathan Kallus",
        "Aurélien Bibaut"
      ],
      "abstract": "We study inference on linear functionals in the nonparametric instrumental\nvariable (NPIV) problem with a discretely-valued instrument under a\nmany-weak-instruments asymptotic regime, where the number of instrument values\ngrows with the sample size. A key motivating example is estimating long-term\ncausal effects in a new experiment with only short-term outcomes, using past\nexperiments to instrument for the effect of short- on long-term outcomes. Here,\nthe assignment to a past experiment serves as the instrument: we have many past\nexperiments but only a limited number of units in each. Since the structural\nfunction is nonparametric but constrained by only finitely many moment\nrestrictions, point identification typically fails. To address this, we\nconsider linear functionals of the minimum-norm solution to the moment\nrestrictions, which is always well-defined. As the number of instrument levels\ngrows, these functionals define an approximating sequence to a target\nfunctional, replacing point identification with a weaker asymptotic notion\nsuited to discrete instruments. Extending the Jackknife Instrumental Variable\nEstimator (JIVE) beyond the classical parametric setting, we propose npJIVE, a\nnonparametric estimator for solutions to linear inverse problems with many weak\ninstruments. We construct automatic debiased machine learning estimators for\nlinear functionals of both the structural function and its minimum-norm\nprojection, and establish their efficiency in the many-weak-instruments regime.",
      "pdf_url": "http://arxiv.org/pdf/2505.07729v1",
      "arxiv_url": "http://arxiv.org/abs/2505.07729v1",
      "published": "2025-05-12",
      "categories": [
        "stat.ME",
        "math.ST",
        "stat.ML",
        "stat.TH"
      ]
    },
    {
      "title": "Generative Pre-trained Autoregressive Diffusion Transformer",
      "authors": [
        "Yuan Zhang",
        "Jiacheng Jiang",
        "Guoqing Ma",
        "Zhiying Lu",
        "Haoyang Huang",
        "Jianlong Yuan",
        "Nan Duan"
      ],
      "abstract": "In this work, we present GPDiT, a Generative Pre-trained Autoregressive\nDiffusion Transformer that unifies the strengths of diffusion and\nautoregressive modeling for long-range video synthesis, within a continuous\nlatent space. Instead of predicting discrete tokens, GPDiT autoregressively\npredicts future latent frames using a diffusion loss, enabling natural modeling\nof motion dynamics and semantic consistency across frames. This continuous\nautoregressive framework not only enhances generation quality but also endows\nthe model with representation capabilities. Additionally, we introduce a\nlightweight causal attention variant and a parameter-free rotation-based\ntime-conditioning mechanism, improving both the training and inference\nefficiency. Extensive experiments demonstrate that GPDiT achieves strong\nperformance in video generation quality, video representation ability, and\nfew-shot learning tasks, highlighting its potential as an effective framework\nfor video modeling in continuous space.",
      "pdf_url": "http://arxiv.org/pdf/2505.07344v1",
      "arxiv_url": "http://arxiv.org/abs/2505.07344v1",
      "published": "2025-05-12",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "GMM with Many Weak Moment Conditions and Nuisance Parameters: General Theory and Applications to Causal Inference",
      "authors": [
        "Rui Wang",
        "Kwun Chuen Gary Chan",
        "Ting Ye"
      ],
      "abstract": "Weak identification is a common issue for many statistical problems -- for\nexample, when instrumental variables are weakly correlated with treatment, or\nwhen proxy variables are weakly correlated with unmeasured confounders. Under\nweak identification, standard estimation methods, such as the generalized\nmethod of moments (GMM), can have sizeable bias in finite samples or even\nasymptotically. In addition, many practical settings involve a growing number\nof nuisance parameters, adding further complexity to the problem. In this\npaper, we study estimation and inference under a general nonlinear moment model\nwith many weak moment conditions and many nuisance parameters. To obtain\ndebiased inference for finite-dimensional target parameters, we demonstrate\nthat Neyman orthogonality plays a stronger role than in conventional settings\nwith strong identification. We study a general two-step debiasing estimator\nthat allows for possibly nonparametric first-step estimation of nuisance\nparameters, and we establish its consistency and asymptotic normality under a\nmany weak moment asymptotic regime. Our theory accommodates both\nhigh-dimensional moment conditions and infinite-dimensional nuisance\nparameters. We provide high-level assumptions for a general setting and discuss\nspecific applications to the problems of estimation and inference with weak\ninstruments and weak proxies.",
      "pdf_url": "http://arxiv.org/pdf/2505.07295v1",
      "arxiv_url": "http://arxiv.org/abs/2505.07295v1",
      "published": "2025-05-12",
      "categories": [
        "math.ST",
        "stat.ME",
        "stat.TH"
      ]
    },
    {
      "title": "Causal View of Time Series Imputation: Some Identification Results on Missing Mechanism",
      "authors": [
        "Ruichu Cai",
        "Kaitao Zheng",
        "Junxian Huang",
        "Zijian Li",
        "Zhengming Chen",
        "Boyan Xu",
        "Zhifeng Hao"
      ],
      "abstract": "Time series imputation is one of the most challenge problems and has broad\napplications in various fields like health care and the Internet of Things.\nExisting methods mainly aim to model the temporally latent dependencies and the\ngeneration process from the observed time series data. In real-world scenarios,\ndifferent types of missing mechanisms, like MAR (Missing At Random), and MNAR\n(Missing Not At Random) can occur in time series data. However, existing\nmethods often overlook the difference among the aforementioned missing\nmechanisms and use a single model for time series imputation, which can easily\nlead to misleading results due to mechanism mismatching. In this paper, we\npropose a framework for time series imputation problem by exploring Different\nMissing Mechanisms (DMM in short) and tailoring solutions accordingly.\nSpecifically, we first analyze the data generation processes with temporal\nlatent states and missing cause variables for different mechanisms.\nSequentially, we model these generation processes via variational inference and\nestimate prior distributions of latent variables via normalizing flow-based\nneural architecture. Furthermore, we establish identifiability results under\nthe nonlinear independent component analysis framework to show that latent\nvariables are identifiable. Experimental results show that our method surpasses\nexisting time series imputation techniques across various datasets with\ndifferent missing mechanisms, demonstrating its effectiveness in real-world\napplications.",
      "pdf_url": "http://arxiv.org/pdf/2505.07180v1",
      "arxiv_url": "http://arxiv.org/abs/2505.07180v1",
      "published": "2025-05-12",
      "categories": [
        "cs.LG",
        "stat.ML"
      ]
    }
  ]
}