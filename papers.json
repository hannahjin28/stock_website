{
  "last_updated": "2025-10-10T00:50:00.537809",
  "papers": [
    {
      "title": "Incorporating Expert Knowledge into Bayesian Causal Discovery of Mixtures of Directed Acyclic Graphs",
      "authors": [
        "Zachris Björkman",
        "Jorge Loría",
        "Sophie Wharrie",
        "Samuel Kaski"
      ],
      "abstract": "Bayesian causal discovery benefits from prior information elicited from\ndomain experts, and in heterogeneous domains any prior knowledge would be badly\nneeded. However, so far prior elicitation approaches have assumed a single\ncausal graph and hence are not suited to heterogeneous domains. We propose a\ncausal elicitation strategy for heterogeneous settings, based on Bayesian\nexperimental design (BED) principles, and a variational mixture structure\nlearning (VaMSL) method -- extending the earlier differentiable Bayesian\nstructure learning (DiBS) method -- to iteratively infer mixtures of causal\nBayesian networks (CBNs). We construct an informative graph prior incorporating\nelicited expert feedback in the inference of mixtures of CBNs. Our proposed\nmethod successfully produces a set of alternative causal models (mixture\ncomponents or clusters), and achieves an improved structure learning\nperformance on heterogeneous synthetic data when informed by a simulated\nexpert. Finally, we demonstrate that our approach is capable of capturing\ncomplex distributions in a breast cancer database.",
      "pdf_url": "http://arxiv.org/pdf/2510.06735v1",
      "arxiv_url": "http://arxiv.org/abs/2510.06735v1",
      "published": "2025-10-08",
      "categories": [
        "cs.LG",
        "stat.ME"
      ]
    },
    {
      "title": "Stable central limit theorems for discrete-time lag martingale difference arrays",
      "authors": [
        "Easton Huch",
        "Walter Dempsey"
      ],
      "abstract": "Recent work in dynamic causal inference introduced a class of discrete-time\nstochastic processes that generalize martingale difference sequences and arrays\nas follows: the random variates in each sequence have expectation zero given\ncertain lagged filtrations but not given the natural filtration. We formalize\nthis class of stochastic processes and prove a stable central limit theorem\n(CLT) via a Bernstein blocking scheme and an application of the classical\nmartingale CLT. We generalize our limit theorem to vector-valued processes via\nthe Cram\\'er-Wold device and develop a simple form for the limiting variance.\nWe demonstrate the application of these results to a problem in dynamic causal\ninference and present a simulation study supporting their validity.",
      "pdf_url": "http://arxiv.org/pdf/2510.06524v1",
      "arxiv_url": "http://arxiv.org/abs/2510.06524v1",
      "published": "2025-10-07",
      "categories": [
        "math.ST",
        "math.PR",
        "stat.TH",
        "60G48, 60F05 (Primary), 60G42, 60B12 (Secondary)"
      ]
    },
    {
      "title": "BlockGPT: Spatio-Temporal Modelling of Rainfall via Frame-Level Autoregression",
      "authors": [
        "Cristian Meo",
        "Varun Sarathchandran",
        "Avijit Majhi",
        "Shao Hung",
        "Carlo Saccardi",
        "Ruben Imhoff",
        "Roberto Deidda",
        "Remko Uijlenhoet",
        "Justin Dauwels"
      ],
      "abstract": "Predicting precipitation maps is a highly complex spatiotemporal modeling\ntask, critical for mitigating the impacts of extreme weather events. Short-term\nprecipitation forecasting, or nowcasting, requires models that are not only\naccurate but also computationally efficient for real-time applications. Current\nmethods, such as token-based autoregressive models, often suffer from flawed\ninductive biases and slow inference, while diffusion models can be\ncomputationally intensive. To address these limitations, we introduce BlockGPT,\na generative autoregressive transformer using batched tokenization (Block)\nmethod that predicts full two-dimensional fields (frames) at each time step.\nConceived as a model-agnostic paradigm for video prediction, BlockGPT\nfactorizes space-time by using self-attention within each frame and causal\nattention across frames; in this work, we instantiate it for precipitation\nnowcasting. We evaluate BlockGPT on two precipitation datasets, viz. KNMI\n(Netherlands) and SEVIR (U.S.), comparing it to state-of-the-art baselines\nincluding token-based (NowcastingGPT) and diffusion-based (DiffCast+Phydnet)\nmodels. The results show that BlockGPT achieves superior accuracy, event\nlocalization as measured by categorical metrics, and inference speeds up to 31x\nfaster than comparable baselines.",
      "pdf_url": "http://arxiv.org/pdf/2510.06293v1",
      "arxiv_url": "http://arxiv.org/abs/2510.06293v1",
      "published": "2025-10-07",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Can language models boost the power of randomized experiments without statistical bias?",
      "authors": [
        "Xinrui Ruan",
        "Xinwei Ma",
        "Yingfei Wang",
        "Waverly Wei",
        "Jingshen Wang"
      ],
      "abstract": "Randomized experiments or randomized controlled trials (RCTs) are gold\nstandards for causal inference, yet cost and sample-size constraints limit\npower. Meanwhile, modern RCTs routinely collect rich, unstructured data that\nare highly prognostic of outcomes but rarely used in causal analyses. We\nintroduce CALM (Causal Analysis leveraging Language Models), a statistical\nframework that integrates large language models (LLMs) predictions with\nestablished causal estimators to increase precision while preserving\nstatistical validity. CALM treats LLM outputs as auxiliary prognostic\ninformation and corrects their potential bias via a heterogeneous calibration\nstep that residualizes and optimally reweights predictions. We prove that CALM\nremains consistent even when LLM predictions are biased and achieves efficiency\ngains over augmented inverse probability weighting estimators for various\ncausal effects. In particular, CALM develops a few-shot variant that aggregates\npredictions across randomly sampled demonstration sets. The resulting\nU-statistic-like predictor restores i.i.d. structure and also mitigates\nprompt-selection variability. Empirically, in simulations calibrated to a\nmobile-app depression RCT, CALM delivers lower variance relative to other\nbenchmarking methods, is effective in zero- and few-shot settings, and remains\nstable across prompt designs. By principled use of LLMs to harness unstructured\ndata and external knowledge learned during pretraining, CALM provides a\npractical path to more precise causal analyses in RCTs.",
      "pdf_url": "http://arxiv.org/pdf/2510.05545v1",
      "arxiv_url": "http://arxiv.org/abs/2510.05545v1",
      "published": "2025-10-07",
      "categories": [
        "stat.ME",
        "econ.EM"
      ]
    },
    {
      "title": "Rivaling Transformers: Multi-Scale Structured State-Space Mixtures for Agentic 6G O-RAN",
      "authors": [
        "Farhad Rezazadeh",
        "Hatim Chergui",
        "Merouane Debbah",
        "Houbing Song",
        "Dusit Niyato",
        "Lingjia Liu"
      ],
      "abstract": "In sixth-generation (6G) Open Radio Access Networks (O-RAN), proactive\ncontrol is preferable. A key open challenge is delivering control-grade\npredictions within Near-Real-Time (Near-RT) latency and computational\nconstraints under multi-timescale dynamics. We therefore cast RAN Intelligent\nController (RIC) analytics as an agentic perceive-predict xApp that turns\nnoisy, multivariate RAN telemetry into short-horizon per-User Equipment (UE)\nkey performance indicator (KPI) forecasts to drive anticipatory control. In\nthis regard, Transformers are powerful for sequence learning and time-series\nforecasting, but they are memory-intensive, which limits Near-RT RIC use.\nTherefore, we need models that maintain accuracy while reducing latency and\ndata movement. To this end, we propose a lightweight Multi-Scale Structured\nState-Space Mixtures (MS3M) forecaster that mixes HiPPO-LegS kernels to capture\nmulti-timescale radio dynamics. We develop stable discrete state-space models\n(SSMs) via bilinear (Tustin) discretization and apply their causal impulse\nresponses as per-feature depthwise convolutions. Squeeze-and-Excitation gating\ndynamically reweights KPI channels as conditions change, and a compact gated\nchannel-mixing layer models cross-feature nonlinearities without\nTransformer-level cost. The model is KPI-agnostic -- Reference Signal Received\nPower (RSRP) serves as a canonical use case -- and is trained on sliding\nwindows to predict the immediate next step. Empirical evaluations conducted\nusing our bespoke O-RAN testbed KPI time-series dataset (59,441 windows across\n13 KPIs). Crucially for O-RAN constraints, MS3M achieves a 0.057 s\nper-inference latency with 0.70M parameters, yielding 3-10x lower latency than\nthe Transformer baselines evaluated on the same hardware, while maintaining\ncompetitive accuracy.",
      "pdf_url": "http://arxiv.org/pdf/2510.05255v1",
      "arxiv_url": "http://arxiv.org/abs/2510.05255v1",
      "published": "2025-10-06",
      "categories": [
        "cs.NI"
      ]
    },
    {
      "title": "Structural Identifiability of Graphical Continuous Lyapunov Models",
      "authors": [
        "Carlos Améndola",
        "Tobias Boege",
        "Benjamin Hollering",
        "Pratik Misra"
      ],
      "abstract": "We prove two characterizations of model equivalence of acyclic graphical\ncontinuous Lyapunov models (GCLMs) with uncorrelated noise. The first result\nshows that two graphs are model equivalent if and only if they have the same\nskeleton and equivalent induced 4-node subgraphs. We also give a\ntransformational characterization via structured edge reversals. The two\ntheorems are Lyapunov analogues of celebrated results for Bayesian networks by\nVerma and Pearl, and Chickering, respectively. Our results have broad\nconsequences for the theory of causal inference of GCLMs. First, we find that\nmodel equivalence classes of acyclic GCLMs refine the corresponding classes of\nBayesian networks. Furthermore, we obtain polynomial-time algorithms to test\nmodel equivalence and structural identifiability of given directed acyclic\ngraphs.",
      "pdf_url": "http://arxiv.org/pdf/2510.04985v1",
      "arxiv_url": "http://arxiv.org/abs/2510.04985v1",
      "published": "2025-10-06",
      "categories": [
        "math.ST",
        "stat.TH",
        "62H22, 60J60 (Primary) 15A24, 62R01, 60J70 (Secondary)"
      ]
    },
    {
      "title": "On Predicting Post-Click Conversion Rate via Counterfactual Inference",
      "authors": [
        "Junhyung Ahn",
        "Sanghack Lee"
      ],
      "abstract": "Accurately predicting conversion rate (CVR) is essential in various\nrecommendation domains such as online advertising systems and e-commerce. These\nsystems utilize user interaction logs, which consist of exposures, clicks, and\nconversions. CVR prediction models are typically trained solely based on\nclicked samples, as conversions can only be determined following clicks.\nHowever, the sparsity of clicked instances necessitates the collection of a\nsubstantial amount of logs for effective model training. Recent works address\nthis issue by devising frameworks that leverage non-clicked samples. While\nthese frameworks aim to reduce biases caused by the discrepancy between clicked\nand non-clicked samples, they often rely on heuristics. Against this\nbackground, we propose a method to counterfactually generate conversion labels\nfor non-clicked samples by using causality as a guiding principle, attempting\nto answer the question, \"Would the user have converted if he or she had clicked\nthe recommended item?\" Our approach is named the Entire Space Counterfactual\nInference Multi-task Model (ESCIM). We initially train a structural causal\nmodel (SCM) of user sequential behaviors and conduct a hypothetical\nintervention (i.e., click) on non-clicked items to infer counterfactual CVRs.\nWe then introduce several approaches to transform predicted counterfactual CVRs\ninto binary counterfactual conversion labels for the non-clicked samples.\nFinally, the generated samples are incorporated into the training process.\nExtensive experiments on public datasets illustrate the superiority of the\nproposed algorithm. Online A/B testing further empirically validates the\neffectiveness of our proposed algorithm in real-world scenarios. In addition,\nwe demonstrate the improved performance of the proposed method on latent\nconversion data, showcasing its robustness and superior generalization\ncapabilities.",
      "pdf_url": "http://arxiv.org/pdf/2510.04816v1",
      "arxiv_url": "http://arxiv.org/abs/2510.04816v1",
      "published": "2025-10-06",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Causality-aware Graph Aggregation Weight Estimator for Popularity Debiasing in Top-K Recommendation",
      "authors": [
        "Yue Que",
        "Yingyi Zhang",
        "Xiangyu Zhao",
        "Chen Ma"
      ],
      "abstract": "Graph-based recommender systems leverage neighborhood aggregation to generate\nnode representations, which is highly sensitive to popularity bias, resulting\nin an echo effect during information propagation. Existing graph-based\ndebiasing solutions refine the aggregation process with attempts such as edge\nreconstruction or weight adjustment. However, these methods remain inadequate\nin fully alleviating popularity bias. Specifically, this is because 1) they\nprovide no insights into graph aggregation rationality, thus lacking an\noptimality guarantee; 2) they fail to well balance the training and debiasing\nprocess, which undermines the effectiveness. In this paper, we propose a novel\napproach to mitigate popularity bias through rational modeling of the graph\naggregation process. We reveal that graph aggregation is a special form of\nbackdoor adjustment in causal inference, where the aggregation weight\ncorresponds to the historical interaction likelihood distribution. Based on\nthis insight, we devise an encoder-decoder architecture, namely Causality-aware\nGraph Aggregation Weight Estimator for Debiasing (CAGED), to approximate the\nunbiased aggregation weight by optimizing the evidence lower bound of the\ninteraction likelihood. In order to enhance the debiasing effectiveness during\nearly training stages, we further design a momentum update strategy that\nincrementally refines the aggregation weight matrix. Extensive experiments on\nthree datasets demonstrate that CAGED outperforms existing graph-based\ndebiasing methods. Our implementation is available at\nhttps://github.com/QueYork/CAGED.",
      "pdf_url": "http://arxiv.org/pdf/2510.04502v1",
      "arxiv_url": "http://arxiv.org/abs/2510.04502v1",
      "published": "2025-10-06",
      "categories": [
        "cs.IR",
        "cs.LG"
      ]
    },
    {
      "title": "REAR: Rethinking Visual Autoregressive Models via Generator-Tokenizer Consistency Regularization",
      "authors": [
        "Qiyuan He",
        "Yicong Li",
        "Haotian Ye",
        "Jinghao Wang",
        "Xinyao Liao",
        "Pheng-Ann Heng",
        "Stefano Ermon",
        "James Zou",
        "Angela Yao"
      ],
      "abstract": "Visual autoregressive (AR) generation offers a promising path toward unifying\nvision and language models, yet its performance remains suboptimal against\ndiffusion models. Prior work often attributes this gap to tokenizer limitations\nand rasterization ordering. In this work, we identify a core bottleneck from\nthe perspective of generator-tokenizer inconsistency, i.e., the AR-generated\ntokens may not be well-decoded by the tokenizer. To address this, we propose\nreAR, a simple training strategy introducing a token-wise regularization\nobjective: when predicting the next token, the causal transformer is also\ntrained to recover the visual embedding of the current token and predict the\nembedding of the target token under a noisy context. It requires no changes to\nthe tokenizer, generation order, inference pipeline, or external models.\nDespite its simplicity, reAR substantially improves performance. On ImageNet,\nit reduces gFID from 3.02 to 1.86 and improves IS to 316.9 using a standard\nrasterization-based tokenizer. When applied to advanced tokenizers, it achieves\na gFID of 1.42 with only 177M parameters, matching the performance with larger\nstate-of-the-art diffusion models (675M).",
      "pdf_url": "http://arxiv.org/pdf/2510.04450v1",
      "arxiv_url": "http://arxiv.org/abs/2510.04450v1",
      "published": "2025-10-06",
      "categories": [
        "cs.CV"
      ]
    },
    {
      "title": "Efficient Latent Variable Causal Discovery: Combining Score Search and Targeted Testing",
      "authors": [
        "Joseph Ramsey",
        "Bryan Andrews"
      ],
      "abstract": "Learning causal structure from observational data is especially challenging\nwhen latent variables or selection bias are present. The Fast Causal Inference\n(FCI) algorithm addresses this setting but often performs exhaustive\nconditional independence tests across many subsets, leading to spurious\nindependence claims, extra or missing edges, and unreliable orientations. We\npresent a family of score-guided mixed-strategy causal search algorithms that\nbuild on this tradition. First, we introduce BOSS-FCI and GRaSP-FCI,\nstraightforward variants of GFCI that substitute BOSS or GRaSP for FGES,\nthereby retaining correctness while incurring different scalability tradeoffs.\nSecond, we develop FCI Targeted-testing (FCIT), a novel mixed-strategy method\nthat improves upon these variants by replacing exhaustive all-subsets testing\nwith targeted tests guided by BOSS, yielding well-formed PAGs with higher\nprecision and efficiency. Finally, we propose a simple heuristic, LV-Dumb (also\nknown as BOSS-POD), which bypasses latent-variable-specific reasoning and\ndirectly returns the PAG of the BOSS DAG. Although not strictly correct in the\nFCI sense, it scales better and often achieves superior accuracy in practice.\nSimulations and real-data analyses demonstrate that BOSS-FCI and GRaSP-FCI\nprovide sound baselines, FCIT improves both efficiency and reliability, and\nLV-Dumb offers a practical heuristic with strong empirical performance.\nTogether, these method highlight the value of score-guided and targeted\nstrategies for scalable latent-variable causal discovery.",
      "pdf_url": "http://arxiv.org/pdf/2510.04263v1",
      "arxiv_url": "http://arxiv.org/abs/2510.04263v1",
      "published": "2025-10-05",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    }
  ]
}