{
  "last_updated": "2025-08-27T00:51:30.624822",
  "papers": [
    {
      "title": "Estimating the average treatment effect in cluster-randomized trials with misclassified outcomes and non-random validation subsets",
      "authors": [
        "Dane Isenberg",
        "Nandita Mitra",
        "Steven C. Marcus",
        "Rinad S. Beidas",
        "Kristin A. Linn"
      ],
      "abstract": "Randomized trials are viewed as the benchmark for assessing causal effects of\ntreatments on outcomes of interest. Nonetheless, challenges such as measurement\nerror can undermine the standard causal assumptions for randomized trials. In\nASPIRE, a cluster-randomized trial, pediatric primary care clinics were\nassigned to one of two treatments aimed at promoting clinician delivery of a\nsecure firearm program to parents during well-child visits. A key outcome of\ninterest is thus parent receipt of the program at each visit. Clinicians\ndocumented program delivery in patients' electronic health records for all\nvisits, but their reporting is a proxy measure for the parent receipt outcome.\nParents were also surveyed to report directly on program receipt after their\nchild's visit; however, only a small subset of them completed the survey. Here,\nwe develop a causal inference framework for a binary outcome that is subject to\nmisclassification through silver-standard measures (clinician reports), but\ngold-standard measures (parent reports) are only available for a non-random\ninternal validation subset. We propose a method for identifying the average\ntreatment effect (ATE) that addresses the risk of bias due to misclassification\nand non-random validation selection, even when the outcome (parent receipt) may\ndirectly impact selection propensity (survey responsiveness). We show that ATE\nestimation relies on specifying the relationship between the gold- and\nsilver-standard outcome measures in the validation subset, which may depend on\ntreatment and covariates. Additionally, the clustered design is reflected in\nour causal assumptions and in our cluster-robust approach to estimation of the\nATE. Simulation studies demonstrate acceptable finite-sample operating\ncharacteristics of our ATE estimator, supporting its application to ASPIRE.",
      "pdf_url": "http://arxiv.org/pdf/2508.18137v2",
      "arxiv_url": "http://arxiv.org/abs/2508.18137v2",
      "published": "2025-08-25",
      "categories": [
        "stat.ME"
      ]
    },
    {
      "title": "CausalSent: Interpretable Sentiment Classification with RieszNet",
      "authors": [
        "Daniel Frees",
        "Martin Pollack"
      ],
      "abstract": "Despite the overwhelming performance improvements offered by recent natural\nlanguage processing (NLP) models, the decisions made by these models are\nlargely a black box. Towards closing this gap, the field of causal NLP combines\ncausal inference literature with modern NLP models to elucidate causal effects\nof text features. We replicate and extend Bansal et al's work on regularizing\ntext classifiers to adhere to estimated effects, focusing instead on model\ninterpretability. Specifically, we focus on developing a two-headed\nRieszNet-based neural network architecture which achieves better treatment\neffect estimation accuracy. Our framework, CausalSent, accurately predicts\ntreatment effects in semi-synthetic IMDB movie reviews, reducing MAE of effect\nestimates by 2-3x compared to Bansal et al's MAE on synthetic Civil Comments\ndata. With an ensemble of validated models, we perform an observational case\nstudy on the causal effect of the word \"love\" in IMDB movie reviews, finding\nthat the presence of the word \"love\" causes a +2.9% increase in the probability\nof a positive sentiment.",
      "pdf_url": "http://arxiv.org/pdf/2508.17576v2",
      "arxiv_url": "http://arxiv.org/abs/2508.17576v2",
      "published": "2025-08-25",
      "categories": [
        "cs.CL",
        "cs.LG",
        "68T50"
      ]
    },
    {
      "title": "Visual Analytics for Causal Reasoning from Real-World Health Data",
      "authors": [
        "Arran Zeyu Wang",
        "David Borland",
        "David Gotz"
      ],
      "abstract": "The increasing capture and analysis of large-scale longitudinal health data\noffer opportunities to improve healthcare and advance medical understanding.\nHowever, a critical gap exists between (a) -- the observation of patterns and\ncorrelations, versus (b) -- the understanding of true causal mechanisms that\ndrive outcomes. An accurate understanding of the underlying mechanisms that\ncause various changes in medical status is crucial for decision-makers across\nvarious healthcare domains and roles, yet inferring causality from real-world\nobservational data is difficult for both methodological and practical\nchallenges. This Grand Challenge advocates increased Visual Analytics (VA)\nresearch on this topic to empower people with the tool for sound causal\nreasoning from health data. We note this is complicated by the complex nature\nof medical data -- the volume, variety, sparsity, and temporality of health\ndata streams make the use of causal inference algorithms difficult. Combined\nwith challenges imposed by the realities of health-focused settings, including\ntime constraints and traditional medical work practices, existing causal\nreasoning approaches are valuable but insufficient. We argue that advances in\nresearch can lead to new VA tools that augment human expertise with intuitive\nand robust causal inference capabilities, which can help realize a new paradigm\nof data-driven, causality-aware healthcare practices that improve human health\noutcomes.",
      "pdf_url": "http://arxiv.org/pdf/2508.17474v1",
      "arxiv_url": "http://arxiv.org/abs/2508.17474v1",
      "published": "2025-08-24",
      "categories": [
        "cs.HC"
      ]
    },
    {
      "title": "Mimicking the Physicist's Eye:A VLM-centric Approach for Physics Formula Discovery",
      "authors": [
        "Jiaqi Liu",
        "Songning Lai",
        "Pengze Li",
        "Di Yu",
        "Wenjie Zhou",
        "Yiyang Zhou",
        "Peng Xia",
        "Zijun Wang",
        "Xi Chen",
        "Shixiang Tang",
        "Lei Bai",
        "Wanli Ouyang",
        "Mingyu Ding",
        "Huaxiu Yao",
        "Aoran Wang"
      ],
      "abstract": "Automated discovery of physical laws from observational data in the real\nworld is a grand challenge in AI. Current methods, relying on symbolic\nregression or LLMs, are limited to uni-modal data and overlook the rich, visual\nphenomenological representations of motion that are indispensable to\nphysicists. This \"sensory deprivation\" severely weakens their ability to\ninterpret the inherent spatio-temporal patterns within dynamic phenomena. To\naddress this gap, we propose VIPER-R1, a multimodal model that performs Visual\nInduction for Physics-based Equation Reasoning to discover fundamental symbolic\nformulas. It integrates visual perception, trajectory data, and symbolic\nreasoning to emulate the scientific discovery process. The model is trained via\na curriculum of Motion Structure Induction (MSI), using supervised fine-tuning\nto interpret kinematic phase portraits and to construct hypotheses guided by a\nCausal Chain of Thought (C-CoT), followed by Reward-Guided Symbolic Calibration\n(RGSC) to refine the formula structure with reinforcement learning. During\ninference, the trained VIPER-R1 acts as an agent: it first posits a\nhigh-confidence symbolic ansatz, then proactively invokes an external symbolic\nregression tool to perform Symbolic Residual Realignment (SR^2). This final\nstep, analogous to a physicist's perturbation analysis, reconciles the\ntheoretical model with empirical data. To support this research, we introduce\nPhysSymbol, a new 5,000-instance multimodal corpus. Experiments show that\nVIPER-R1 consistently outperforms state-of-the-art VLM baselines in accuracy\nand interpretability, enabling more precise discovery of physical laws. Project\npage: https://jiaaqiliu.github.io/VIPER-R1/",
      "pdf_url": "http://arxiv.org/pdf/2508.17380v1",
      "arxiv_url": "http://arxiv.org/abs/2508.17380v1",
      "published": "2025-08-24",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Challenges in Statistics: A Dozen Challenges in Causality and Causal Inference",
      "authors": [
        "Carlos Cinelli",
        "Avi Feller",
        "Guido Imbens",
        "Edward Kennedy",
        "Sara Magliacane",
        "Jose Zubizarreta"
      ],
      "abstract": "Causality and causal inference have emerged as core research areas at the\ninterface of modern statistics and domains including biomedical sciences,\nsocial sciences, computer science, and beyond. The field's inherently\ninterdisciplinary nature -- particularly the central role of incorporating\ndomain knowledge -- creates a rich and varied set of statistical challenges.\nMuch progress has been made, especially in the last three decades, but there\nremain many open questions. Our goal in this discussion is to outline research\ndirections and open problems we view as particularly promising for future work.\nThroughout we emphasize that advancing causal research requires a wide range of\ncontributions, from novel theory and methodological innovations to improved\nsoftware tools and closer engagement with domain scientists and practitioners.",
      "pdf_url": "http://arxiv.org/pdf/2508.17099v1",
      "arxiv_url": "http://arxiv.org/abs/2508.17099v1",
      "published": "2025-08-23",
      "categories": [
        "stat.ME"
      ]
    },
    {
      "title": "Individualized Treatment Effects in Advanced Prostate Cancer: A Causal-Survival Modeling Approach to Risk-Guided Therapy",
      "authors": [
        "J. T. Korley"
      ],
      "abstract": "We conducted a proof-of-concept evaluation of individualized treatment effect\n(ITE) estimation using survival data from a randomized trial of 475 men with\nadvanced prostate cancer treated with high- versus low-dose diethylstilbestrol\n(DES). A Weibull accelerated failure time (AFT) model with interaction terms\nfor treatment-by-age and treatment-by-log tumor size was used to capture\nsubgroup-specific treatment effects. The estimated main effect of high-dose DES\nindicated a time ratio of 0.582 (95% CI: [0.306, 1.110]), reflecting reduced\nsurvival at the reference levels of age and tumor size. However,\ninteraction-adjusted ITEs revealed marked effect modification: younger patients\n(e.g., age 50 years) had over fourfold expected survival gains (time ratio\n4.09), whereas older patients (e.g., age 80 years) experienced reduced benefit\n(time ratio 0.71). Similarly, patients with larger tumors (log size $\\sim$4.25,\n$\\sim$70 $cm^2$) derived a stronger benefit (time ratio 1.89) than those with\nsmaller tumors. To evaluate the reliability of these individualized estimates,\nboth the delta method and bootstrap resampling were applied for uncertainty\nquantification, producing closely aligned intervals across the risk spectrum.\nThis analysis illustrates how parametric survival models with clinically\nmotivated interactions and robust inference procedures can yield interpretable\npatient-level treatment effect estimates, even in moderately sized oncology\ntrials.",
      "pdf_url": "http://arxiv.org/pdf/2508.16894v1",
      "arxiv_url": "http://arxiv.org/abs/2508.16894v1",
      "published": "2025-08-23",
      "categories": [
        "stat.AP"
      ]
    },
    {
      "title": "A coalgebraic perspective on predictive processing",
      "authors": [
        "Manuel Baltieri",
        "Filippo Torresan",
        "Tomoya Nakai"
      ],
      "abstract": "Predictive processing and active inference posit that the brain is a system\nperforming Bayesian inference on the environment. By virtue of this, a\nprominent interpretation of predictive processing states that the generative\nmodel (a POMDP) encoded by the brain synchronises with the generative process\n(another POMDP) representing the environment while trying to explain what\nhidden properties of the world generated its sensory input. In this view, the\nbrain is thought to become a copy of the environment. This claim has however\nbeen disputed, stressing the fact that a structural copy, or isomorphism as it\nis at times invoked to be, is not an accurate description of this process since\nthe environment is necessarily more complex than the brain, and what matters is\nnot the capacity to exactly recapitulate the veridical causal structure of the\nworld. In this work, we make parts of this counterargument formal by using\nideas from the theory of coalgebras, an abstract mathematical framework for\ndynamical systems that brings together work from automata theory, concurrency\ntheory, probabilistic processes and other fields. To do so, we cast generative\nmodel and process, in the form of POMDPs, as coalgebras, and use maps between\nthem to describe a form of consistency that goes beyond mere structural\nsimilarity, giving the necessary mathematical background to describe how\ndifferent processes can be seen as behaviourally, rather than structurally,\nequivalent, i.e. how they can be seen as emitting the same observations, and\nthus minimise prediction error, over time without strict assumptions about\nstructural similarity. In particular, we will introduce three standard notions\nof equivalence from the literature on coalgebras, evaluating them in the\ncontext of predictive processing and identifying the one closest to claims made\nby proponents of this framework.",
      "pdf_url": "http://arxiv.org/pdf/2508.16877v1",
      "arxiv_url": "http://arxiv.org/abs/2508.16877v1",
      "published": "2025-08-23",
      "categories": [
        "q-bio.NC"
      ]
    },
    {
      "title": "Heterogeneous Quantile Treatment Effect Estimation for Longitudinal Data with High-Dimensional Confounding",
      "authors": [
        "Zhixin Qiu",
        "Huichen Zhu",
        "Wenjie Wang",
        "Yanlin Tang"
      ],
      "abstract": "Causal inference plays a fundamental role in various real-world applications.\nHowever, in the motivating non-small cell lung cancer (NSCLC) study, it is\nchallenging to estimate the treatment effect of chemotherapy on circulating\ntumor DNA (ctDNA). First, the heterogeneous treatment effects vary across\npatient subgroups defined by baseline characteristics. Second, there exists a\nbroad set of demographic, clinical and molecular variables act as potential\nconfounders. Third, ctDNA trajectories over time show heavy-tailed non-Gaussian\nbehavior. Finally, repeated measurements within subjects introduce unknown\ncorrelation. Combining convolution-smoothed quantile regression and orthogonal\nrandom forest, we propose a framework to estimate heterogeneous quantile\ntreatment effects in the presence of high-dimensional confounding, which not\nonly captures effect heterogeneity across covariates, but also behaves robustly\nto nuisance parameter estimation error. We establish the theoretical properties\nof the proposed estimator and demonstrate its finite-sample performance through\ncomprehensive simulations. We illustrate its practical utility in the motivated\nNSCLC study.",
      "pdf_url": "http://arxiv.org/pdf/2508.16326v1",
      "arxiv_url": "http://arxiv.org/abs/2508.16326v1",
      "published": "2025-08-22",
      "categories": [
        "stat.ME"
      ]
    },
    {
      "title": "Quasi Instrumental Variable Methods for Stable Hidden Confounding and Binary Outcome",
      "authors": [
        "Zhonghua Liu",
        "Baoluo Sun",
        "Ting Ye",
        "David Richardson",
        "Eric Tchetgen Tchetgen"
      ],
      "abstract": "Instrumental variable (IV) methods are central to causal inference from\nobservational data, particularly when a randomized experiment is not feasible.\nHowever, of the three conventional core IV identification conditions, only one,\nIV relevance, is empirically verifiable; often one or both of the other\nconditions, exclusion restriction and IV independence from unmeasured\nconfounders, are unmet in real-world applications. These challenges are\ncompounded when the outcome is binary, a setting for which robust IV methods\nremain underdeveloped. A fundamental contribution of this paper is the\ndevelopment of a general identification strategy justified under a structural\nequilibrium dynamic generative model of so-called stable confounding and a\nquasi instrumental variable (QIV), i.e. a variable that is only assumed to be\npredictive of the outcome. Such a model implies (a) stability of confounding on\nthe multiplicative scale, and (b) stability of the additive average treatment\neffect among the treated (ATT), across levels of that QIV. The former is all\nthat is necessary to ensure a valid test of the causal null hypothesis;\ntogether those two conditions establish nonparametric identification and\nestimation of the conditional and marginal ATT. To address the statistical\nchallenges posed by the need for boundedness in binary outcomes, we introduce a\ngeneralized odds product re-parametrization of the observed data distribution,\nand we develop both a principled maximum likelihood estimator and a triply\nrobust semiparametric locally efficient estimator, which we evaluate through\nsimulations and an empirical application to the UK Biobank.",
      "pdf_url": "http://arxiv.org/pdf/2508.16096v1",
      "arxiv_url": "http://arxiv.org/abs/2508.16096v1",
      "published": "2025-08-22",
      "categories": [
        "stat.ME"
      ]
    },
    {
      "title": "Transforming Causality: Transformer-Based Temporal Causal Discovery with Prior Knowledge Integration",
      "authors": [
        "Jihua Huang",
        "Yi Yao",
        "Ajay Divakaran"
      ],
      "abstract": "We introduce a novel framework for temporal causal discovery and inference\nthat addresses two key challenges: complex nonlinear dependencies and spurious\ncorrelations. Our approach employs a multi-layer Transformer-based time-series\nforecaster to capture long-range, nonlinear temporal relationships among\nvariables. After training, we extract the underlying causal structure and\nassociated time lags from the forecaster using gradient-based analysis,\nenabling the construction of a causal graph. To mitigate the impact of spurious\ncausal relationships, we introduce a prior knowledge integration mechanism\nbased on attention masking, which consistently enforces user-excluded causal\nlinks across multiple Transformer layers. Extensive experiments show that our\nmethod significantly outperforms other state-of-the-art approaches, achieving a\n12.8% improvement in F1-score for causal discovery and 98.9% accuracy in\nestimating causal lags.",
      "pdf_url": "http://arxiv.org/pdf/2508.15928v1",
      "arxiv_url": "http://arxiv.org/abs/2508.15928v1",
      "published": "2025-08-21",
      "categories": [
        "cs.LG",
        "stat.ML"
      ]
    }
  ]
}