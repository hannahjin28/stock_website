{
  "last_updated": "2025-12-18T00:54:18.251235",
  "papers": [
    {
      "title": "Fast and Accurate Causal Parallel Decoding using Jacobi Forcing",
      "authors": [
        "Lanxiang Hu",
        "Siqi Kou",
        "Yichao Fu",
        "Samyam Rajbhandari",
        "Tajana Rosing",
        "Yuxiong He",
        "Zhijie Deng",
        "Hao Zhang"
      ],
      "abstract": "Multi-token generation has emerged as a promising paradigm for accelerating transformer-based large model inference. Recent efforts primarily explore diffusion Large Language Models (dLLMs) for parallel decoding to reduce inference latency. To achieve AR-level generation quality, many techniques adapt AR models into dLLMs to enable parallel decoding. However, they suffer from limited speedup compared to AR models due to a pretrain-to-posttrain mismatch. Specifically, the masked data distribution in post-training deviates significantly from the real-world data distribution seen during pretraining, and dLLMs rely on bidirectional attention, which conflicts with the causal prior learned during pretraining and hinders the integration of exact KV cache reuse. To address this, we introduce Jacobi Forcing, a progressive distillation paradigm where models are trained on their own generated parallel decoding trajectories, smoothly shifting AR models into efficient parallel decoders while preserving their pretrained causal inference property. The models trained under this paradigm, Jacobi Forcing Model, achieves 3.8x wall-clock speedup on coding and math benchmarks with minimal loss in performance. Based on Jacobi Forcing Models' trajectory characteristics, we introduce multi-block decoding with rejection recycling, which enables up to 4.5x higher token acceptance count per iteration and nearly 4.0x wall-clock speedup, effectively trading additional compute for lower inference latency. Our code is available at https://github.com/hao-ai-lab/JacobiForcing.",
      "pdf_url": "https://arxiv.org/pdf/2512.14681v1",
      "arxiv_url": "http://arxiv.org/abs/2512.14681v1",
      "published": "2025-12-16",
      "categories": [
        "cs.CL"
      ]
    },
    {
      "title": "PrivATE: Differentially Private Average Treatment Effect Estimation for Observational Data",
      "authors": [
        "Quan Yuan",
        "Xiaochen Li",
        "Linkang Du",
        "Min Chen",
        "Mingyang Sun",
        "Yunjun Gao",
        "Shibo He",
        "Jiming Chen",
        "Zhikun Zhang"
      ],
      "abstract": "Causal inference plays a crucial role in scientific research across multiple disciplines. Estimating causal effects, particularly the average treatment effect (ATE), from observational data has garnered significant attention. However, computing the ATE from real-world observational data poses substantial privacy risks to users. Differential privacy, which offers strict theoretical guarantees, has emerged as a standard approach for privacy-preserving data analysis. However, existing differentially private ATE estimation works rely on specific assumptions, provide limited privacy protection, or fail to offer comprehensive information protection.\n  To this end, we introduce PrivATE, a practical ATE estimation framework that ensures differential privacy. In fact, various scenarios require varying levels of privacy protection. For example, only test scores are generally sensitive information in education evaluation, while all types of medical record data are usually private. To accommodate different privacy requirements, we design two levels (i.e., label-level and sample-level) of privacy protection in PrivATE. By deriving an adaptive matching limit, PrivATE effectively balances noise-induced error and matching error, leading to a more accurate estimate of ATE. Our evaluation validates the effectiveness of PrivATE. PrivATE outperforms the baselines on all datasets and privacy budgets.",
      "pdf_url": "https://arxiv.org/pdf/2512.14557v1",
      "arxiv_url": "http://arxiv.org/abs/2512.14557v1",
      "published": "2025-12-16",
      "categories": [
        "cs.CR"
      ]
    },
    {
      "title": "Causal Secondary Analysis of Linked Data in the Presence of Mismatch Error",
      "authors": [
        "Martin Slawski"
      ],
      "abstract": "The increased prevalence of observational data and the need to integrate information from multiple sources are critical challenges in contemporary data analysis. Record linkage is a widely used tool for combining datasets in the absence of unique identifiers. The presence of linkage errors such as mismatched records, however, often hampers the analysis of data sets obtained in this way. This issue is more difficult to address in secondary analysis settings, where linkage and subsequent analysis are performed separately, and analysts have limited information about linkage quality. In this paper, we investigate the estimation of average treatment effects in the conventional potential outcome-based causal inference framework under linkage uncertainty. To mitigate the bias that would be incurred with naive analyses, we propose an approach based on estimating equations that treats the unknown match status indicators as missing data. Leveraging a variant of the Expectation-Maximization algorithm, these indicators are imputed based on a corresponding two-component mixture model. The approach is amenable to asymptotic inference. Simulation studies and a case study highlight the importance of accounting for linkage uncertainty and demonstrate the effectiveness of the proposed approach.",
      "pdf_url": "https://arxiv.org/pdf/2512.14492v1",
      "arxiv_url": "http://arxiv.org/abs/2512.14492v1",
      "published": "2025-12-16",
      "categories": [
        "stat.ME"
      ]
    },
    {
      "title": "Causal Structure Learning for Dynamical Systems with Theoretical Score Analysis",
      "authors": [
        "Nicholas Tagliapietra",
        "Katharina Ensinger",
        "Christoph Zimmer",
        "Osman Mian"
      ],
      "abstract": "Real world systems evolve in continuous-time according to their underlying causal relationships, yet their dynamics are often unknown. Existing approaches to learning such dynamics typically either discretize time -- leading to poor performance on irregularly sampled data -- or ignore the underlying causality. We propose CaDyT, a novel method for causal discovery on dynamical systems addressing both these challenges. In contrast to state-of-the-art causal discovery methods that model the problem using discrete-time Dynamic Bayesian networks, our formulation is grounded in Difference-based causal models, which allow milder assumptions for modeling the continuous nature of the system. CaDyT leverages exact Gaussian Process inference for modeling the continuous-time dynamics which is more aligned with the underlying dynamical process. We propose a practical instantiation that identifies the causal structure via a greedy search guided by the Algorithmic Markov Condition and Minimum Description Length principle. Our experiments show that CaDyT outperforms state-of-the-art methods on both regularly and irregularly-sampled data, discovering causal networks closer to the true underlying dynamics.",
      "pdf_url": "https://arxiv.org/pdf/2512.14361v1",
      "arxiv_url": "http://arxiv.org/abs/2512.14361v1",
      "published": "2025-12-16",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.DS"
      ]
    },
    {
      "title": "A comparative overview of win ratio and joint frailty models for recurrent event endpoints with applications in oncology and cardiology",
      "authors": [
        "Adrien Orué",
        "Derek Dinart",
        "Laurent Billot",
        "Carine Bellera",
        "Virginie Rondeau"
      ],
      "abstract": "Composite endpoints that combine recurrent non-fatal events with a terminal event are increasingly used in randomized clinical trials, yet conventional time-to-first event analyses may obscure clinically relevant information. We compared two statistical frameworks tailored to such endpoints: the joint frailty model (JFM) and the last-event assisted recurrent-event win ratio (LWR). The JFM specifies proportional hazards for the recurrent and terminal events linked through a shared frailty, yielding covariate-adjusted, component-specific hazard ratios that account for informative recurrences and dependence with death. The LWR is a nonparametric, prioritized pairwise comparison that incorporates all observed events over follow-up and summarizes a population-level benefit of treatment while respecting a pre-specified hierarchy between death and recurrences. We first assessed the performance of the methods using simulations that varied both the gamma-frailty variance and the event rates. Next, we investigated these two frameworks using practical clinical applications, to assess the performance of the methods and to estimate the sample size required to achieve adequate power. These two approaches delivered complementary insights. The JFM provided component-specific estimates, while the LWR led to a summary measure of treatment effect with direction. Power was systematically improved with JFM, which thus appeared as the most reliable approach for inference and sample size estimation. Methodological extensions of the LWR to appropriately handle censoring and to formalize causal estimands remain a promising direction for future research.",
      "pdf_url": "https://arxiv.org/pdf/2512.13629v1",
      "arxiv_url": "http://arxiv.org/abs/2512.13629v1",
      "published": "2025-12-15",
      "categories": [
        "stat.ME"
      ]
    },
    {
      "title": "ReFusion: A Diffusion Large Language Model with Parallel Autoregressive Decoding",
      "authors": [
        "Jia-Nan Li",
        "Jian Guan",
        "Wei Wu",
        "Chongxuan Li"
      ],
      "abstract": "Autoregressive models (ARMs) are hindered by slow sequential inference. While masked diffusion models (MDMs) offer a parallel alternative, they suffer from critical drawbacks: high computational overhead from precluding Key-Value (KV) caching, and incoherent generation arising from learning dependencies over an intractable space of token combinations. To address these limitations, we introduce ReFusion, a novel masked diffusion model that achieves superior performance and efficiency by elevating parallel decoding from the token level to a higher slot level, where each slot is a fixed-length, contiguous sub-sequence. This is achieved through an iterative ``plan-and-infill'' decoding process: a diffusion-based planning step first identifies a set of weakly dependent slots, and an autoregressive infilling step then decodes these selected slots in parallel. The slot-based design simultaneously unlocks full KV cache reuse with a unified causal framework and reduces the learning complexity from the token combination space to a manageable slot-level permutation space. Extensive experiments on seven diverse benchmarks show that ReFusion not only overwhelmingly surpasses prior MDMs with 34% performance gains and an over 18$\\times$ speedup on average, but also bridges the performance gap to strong ARMs while maintaining a 2.33$\\times$ average speedup.",
      "pdf_url": "https://arxiv.org/pdf/2512.13586v1",
      "arxiv_url": "http://arxiv.org/abs/2512.13586v1",
      "published": "2025-12-15",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "CausalCLIP: Causally-Informed Feature Disentanglement and Filtering for Generalizable Detection of Generated Images",
      "authors": [
        "Bo Liu",
        "Qiao Qin",
        "Qinghui He"
      ],
      "abstract": "The rapid advancement of generative models has increased the demand for generated image detectors capable of generalizing across diverse and evolving generation techniques. However, existing methods, including those leveraging pre-trained vision-language models, often produce highly entangled representations, mixing task-relevant forensic cues (causal features) with spurious or irrelevant patterns (non-causal features), thus limiting generalization. To address this issue, we propose CausalCLIP, a framework that explicitly disentangles causal from non-causal features and employs targeted filtering guided by causal inference principles to retain only the most transferable and discriminative forensic cues. By modeling the generation process with a structural causal model and enforcing statistical independence through Gumbel-Softmax-based feature masking and Hilbert-Schmidt Independence Criterion (HSIC) constraints, CausalCLIP isolates stable causal features robust to distribution shifts. When tested on unseen generative models from different series, CausalCLIP demonstrates strong generalization ability, achieving improvements of 6.83% in accuracy and 4.06% in average precision over state-of-the-art methods.",
      "pdf_url": "https://arxiv.org/pdf/2512.13285v2",
      "arxiv_url": "http://arxiv.org/abs/2512.13285v2",
      "published": "2025-12-15",
      "categories": [
        "cs.CV"
      ]
    },
    {
      "title": "From Educational Analytics to AI Governance: Transferable Lessons from Complex Systems Interventions",
      "authors": [
        "Hugo Roger Paz"
      ],
      "abstract": "Both student retention in higher education and artificial intelligence governance face a common structural challenge: the application of linear regulatory frameworks to complex adaptive systems. Risk-based approaches dominate both domains, yet systematically fail because they assume stable causal pathways, predictable actor responses, and controllable system boundaries. This paper extracts transferable methodological principles from CAPIRE (Curriculum, Archetypes, Policies, Interventions & Research Environment), an empirically validated framework for educational analytics that treats student dropout as an emergent property of curricular structures, institutional rules, and macroeconomic shocks. Drawing on longitudinal data from engineering programmes and causal inference methods, CAPIRE demonstrates that well-intentioned interventions routinely generate unintended consequences when system complexity is ignored. We argue that five core principles developed within CAPIRE - temporal observation discipline, structural mapping over categorical classification, archetype-based heterogeneity analysis, causal mechanism identification, and simulation-based policy design - transfer directly to the challenge of governing AI systems. The isomorphism is not merely analogical: both domains exhibit non-linearity, emergence, feedback loops, strategic adaptation, and path dependence. We propose Complex Systems AI Governance (CSAIG) as an integrated framework that operationalises these principles for regulatory design, shifting the central question from \"how risky is this AI system?\" to \"how does this intervention reshape system dynamics?\" The contribution is twofold: demonstrating that empirical lessons from one complex systems domain can accelerate governance design in another, and offering a concrete methodological architecture for complexity-aware AI regulation.",
      "pdf_url": "https://arxiv.org/pdf/2512.13260v1",
      "arxiv_url": "http://arxiv.org/abs/2512.13260v1",
      "published": "2025-12-15",
      "categories": [
        "cs.CY"
      ]
    },
    {
      "title": "Clinical transfusion-outcomes research: A practical guide",
      "authors": [
        "Sarah J Valk",
        "Camila Caram-Deelder",
        "Rolf. H. H. Groenwold",
        "Johanna G van der Bom"
      ],
      "abstract": "Clinical transfusion-outcomes research faces unique methodological challenges compared with other areas of clinical research. These challenges arise because patients frequently receive multiple transfusions, each unit originates from a different donor, and the probability of receiving specific blood product characteristics is influenced by external, often uncontrollable, factors. These complexities complicate causal inference in observational studies of transfusion effectiveness and safety. This guide addresses key challenges in observational transfusion research, with a focus on time-varying exposure, time-varying confounding, and treatment-confounder feedback. Using the example of donor sex and pregnancy history in relation to recipient mortality, we illustrate the strengths and limitations of commonly used analytical approaches. We compare restriction-based analyses, time-varying Cox regression, and inverse probability weighted marginal structural models using a large observational dataset of male transfusion recipients. In the applied example, restriction and conventional time-varying approaches suggested an increased mortality risk associated with transfusion of red blood cells from ever-pregnant female donors compared with male-only donors (hazard ratio [HR] 1.22; 95% CI 1.05-1.42 and HR 1.21; 95% CI 1.04-1.41, respectively). In contrast, inverse probability of treatment and censoring weighted analyses, which account for treatment-confounder feedback, showed no evidence of an association (HR 1.01; 95% CI 0.85-1.20). These findings demonstrate how conventional methods can yield biased estimates when complex longitudinal structures are not adequately handled. We provide practical guidance on study design, target trial emulation, and the use of g-methods, including a reproducible tutorial and example dataset, to support valid causal inference in clinical transfusion research.",
      "pdf_url": "https://arxiv.org/pdf/2512.13155v1",
      "arxiv_url": "http://arxiv.org/abs/2512.13155v1",
      "published": "2025-12-15",
      "categories": [
        "stat.ME"
      ]
    },
    {
      "title": "SneakPeek: Future-Guided Instructional Streaming Video Generation",
      "authors": [
        "Cheeun Hong",
        "German Barquero",
        "Fadime Sener",
        "Markos Georgopoulos",
        "Edgar Schönfeld",
        "Stefan Popov",
        "Yuming Du",
        "Oscar Mañas",
        "Albert Pumarola"
      ],
      "abstract": "Instructional video generation is an emerging task that aims to synthesize coherent demonstrations of procedural activities from textual descriptions. Such capability has broad implications for content creation, education, and human-AI interaction, yet existing video diffusion models struggle to maintain temporal consistency and controllability across long sequences of multiple action steps. We introduce a pipeline for future-driven streaming instructional video generation, dubbed SneakPeek, a diffusion-based autoregressive framework designed to generate precise, stepwise instructional videos conditioned on an initial image and structured textual prompts. Our approach introduces three key innovations to enhance consistency and controllability: (1) predictive causal adaptation, where a causal model learns to perform next-frame prediction and anticipate future keyframes; (2) future-guided self-forcing with a dual-region KV caching scheme to address the exposure bias issue at inference time; (3) multi-prompt conditioning, which provides fine-grained and procedural control over multi-step instructions. Together, these components mitigate temporal drift, preserve motion consistency, and enable interactive video generation where future prompt updates dynamically influence ongoing streaming video generation. Experimental results demonstrate that our method produces temporally coherent and semantically faithful instructional videos that accurately follow complex, multi-step task descriptions.",
      "pdf_url": "https://arxiv.org/pdf/2512.13019v1",
      "arxiv_url": "http://arxiv.org/abs/2512.13019v1",
      "published": "2025-12-15",
      "categories": [
        "cs.CV"
      ]
    }
  ]
}