{
  "last_updated": "2025-06-11T00:55:21.322475",
  "papers": [
    {
      "title": "Self Forcing: Bridging the Train-Test Gap in Autoregressive Video Diffusion",
      "authors": [
        "Xun Huang",
        "Zhengqi Li",
        "Guande He",
        "Mingyuan Zhou",
        "Eli Shechtman"
      ],
      "abstract": "We introduce Self Forcing, a novel training paradigm for autoregressive video\ndiffusion models. It addresses the longstanding issue of exposure bias, where\nmodels trained on ground-truth context must generate sequences conditioned on\ntheir own imperfect outputs during inference. Unlike prior methods that denoise\nfuture frames based on ground-truth context frames, Self Forcing conditions\neach frame's generation on previously self-generated outputs by performing\nautoregressive rollout with key-value (KV) caching during training. This\nstrategy enables supervision through a holistic loss at the video level that\ndirectly evaluates the quality of the entire generated sequence, rather than\nrelying solely on traditional frame-wise objectives. To ensure training\nefficiency, we employ a few-step diffusion model along with a stochastic\ngradient truncation strategy, effectively balancing computational cost and\nperformance. We further introduce a rolling KV cache mechanism that enables\nefficient autoregressive video extrapolation. Extensive experiments demonstrate\nthat our approach achieves real-time streaming video generation with sub-second\nlatency on a single GPU, while matching or even surpassing the generation\nquality of significantly slower and non-causal diffusion models. Project\nwebsite: http://self-forcing.github.io/",
      "pdf_url": "http://arxiv.org/pdf/2506.08009v1",
      "arxiv_url": "http://arxiv.org/abs/2506.08009v1",
      "published": "2025-06-09",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "CausalPFN: Amortized Causal Effect Estimation via In-Context Learning",
      "authors": [
        "Vahid Balazadeh",
        "Hamidreza Kamkari",
        "Valentin Thomas",
        "Benson Li",
        "Junwei Ma",
        "Jesse C. Cresswell",
        "Rahul G. Krishnan"
      ],
      "abstract": "Causal effect estimation from observational data is fundamental across\nvarious applications. However, selecting an appropriate estimator from dozens\nof specialized methods demands substantial manual effort and domain expertise.\nWe present CausalPFN, a single transformer that amortizes this workflow:\ntrained once on a large library of simulated data-generating processes that\nsatisfy ignorability, it infers causal effects for new observational datasets\nout-of-the-box. CausalPFN combines ideas from Bayesian causal inference with\nthe large-scale training protocol of prior-fitted networks (PFNs), learning to\nmap raw observations directly to causal effects without any task-specific\nadjustment. Our approach achieves superior average performance on heterogeneous\nand average treatment effect estimation benchmarks (IHDP, Lalonde, ACIC).\nMoreover, it shows competitive performance for real-world policy making on\nuplift modeling tasks. CausalPFN provides calibrated uncertainty estimates to\nsupport reliable decision-making based on Bayesian principles. This\nready-to-use model does not require any further training or tuning and takes a\nstep toward automated causal inference (https://github.com/vdblm/CausalPFN).",
      "pdf_url": "http://arxiv.org/pdf/2506.07918v1",
      "arxiv_url": "http://arxiv.org/abs/2506.07918v1",
      "published": "2025-06-09",
      "categories": [
        "cs.LG",
        "stat.ML"
      ]
    },
    {
      "title": "Learning to Focus: Causal Attention Distillation via Gradient-Guided Token Pruning",
      "authors": [
        "Yiju Guo",
        "Wenkai Yang",
        "Zexu Sun",
        "Ning Ding",
        "Zhiyuan Liu",
        "Yankai Lin"
      ],
      "abstract": "Large language models (LLMs) have demonstrated significant improvements in\ncontextual understanding. However, their ability to attend to truly critical\ninformation during long-context reasoning and generation still falls behind the\npace. Specifically, our preliminary experiments reveal that certain distracting\npatterns can misdirect the model's attention during inference, and removing\nthese patterns substantially improves reasoning accuracy and generation\nquality. We attribute this phenomenon to spurious correlations in the training\ndata, which obstruct the model's capacity to infer authentic causal\ninstruction-response relationships. This phenomenon may induce redundant\nreasoning processes, potentially resulting in significant inference overhead\nand, more critically, the generation of erroneous or suboptimal responses. To\nmitigate this, we introduce a two-stage framework called Learning to Focus\n(LeaF) leveraging intervention-based inference to disentangle confounding\nfactors. In the first stage, LeaF employs gradient-based comparisons with an\nadvanced teacher to automatically identify confounding tokens based on causal\nrelationships in the training corpus. Then, in the second stage, it prunes\nthese tokens during distillation to enact intervention, aligning the student's\nattention with the teacher's focus distribution on truly critical context\ntokens. Experimental results demonstrate that LeaF not only achieves an\nabsolute improvement in various mathematical reasoning and code generation\nbenchmarks but also effectively suppresses attention to confounding tokens\nduring inference, yielding a more interpretable and reliable reasoning model.",
      "pdf_url": "http://arxiv.org/pdf/2506.07851v1",
      "arxiv_url": "http://arxiv.org/abs/2506.07851v1",
      "published": "2025-06-09",
      "categories": [
        "cs.CL"
      ]
    },
    {
      "title": "Individual Treatment Effect: Prediction Intervals and Sharp Bounds",
      "authors": [
        "Zhehao Zhang",
        "Thomas S. Richardson"
      ],
      "abstract": "Individual treatment effect (ITE) is often regarded as the ideal target of\ninference in causal analyses and has been the focus of several recent studies.\nIn this paper, we describe the intrinsic limits regarding what can be learned\nconcerning ITEs given data from large randomized experiments. We consider when\na valid prediction interval for the ITE is informative and when it can be\nbounded away from zero. The joint distribution over potential outcomes is only\npartially identified from a randomized trial. Consequently, to be valid, an ITE\nprediction interval must be valid for all joint distribution consistent with\nthe observed data and hence will in general be wider than that resulting from\nknowledge of this joint distribution. We characterize prediction intervals in\nthe binary treatment and outcome setting, and extend these insights to models\nwith continuous and ordinal outcomes. We derive sharp bounds on the probability\nmass function (pmf) of the individual treatment effect (ITE). Finally, we\ncontrast prediction intervals for the ITE and confidence intervals for the\naverage treatment effect (ATE). This also leads to the consideration of Fisher\nversus Neyman null hypotheses. While confidence intervals for the ATE shrink\nwith increasing sample size due to its status as a population parameter,\nprediction intervals for the ITE generally do not vanish, leading to scenarios\nwhere one may reject the Neyman null yet still find evidence consistent with\nthe Fisher null, highlighting the challenges of individualized decision-making\nunder partial identification.",
      "pdf_url": "http://arxiv.org/pdf/2506.07469v1",
      "arxiv_url": "http://arxiv.org/abs/2506.07469v1",
      "published": "2025-06-09",
      "categories": [
        "stat.ME",
        "econ.EM",
        "math.ST",
        "stat.TH"
      ]
    },
    {
      "title": "Investigating the Relationship Between Physical Activity and Tailored Behavior Change Messaging: Connecting Contextual Bandit with Large Language Models",
      "authors": [
        "Haochen Song",
        "Dominik Hofer",
        "Rania Islambouli",
        "Laura Hawkins",
        "Ananya Bhattacharjee",
        "Meredith Franklin",
        "Joseph Jay Williams"
      ],
      "abstract": "Machine learning approaches, such as contextual multi-armed bandit (cMAB)\nalgorithms, offer a promising strategy to reduce sedentary behavior by\ndelivering personalized interventions to encourage physical activity. However,\ncMAB algorithms typically require large participant samples to learn\neffectively and may overlook key psychological factors that are not explicitly\nencoded in the model. In this study, we propose a hybrid approach that combines\ncMAB for selecting intervention types with large language models (LLMs) to\npersonalize message content. We evaluate four intervention types: behavioral\nself-monitoring, gain-framed, loss-framed, and social comparison, each\ndelivered as a motivational message aimed at increasing motivation for physical\nactivity and daily step count. Message content is further personalized using\ndynamic contextual factors including daily fluctuations in self-efficacy,\nsocial influence, and regulatory focus. Over a seven-day trial, participants\nreceive daily messages assigned by one of four models: cMAB alone, LLM alone,\ncombined cMAB with LLM personalization (cMABxLLM), or equal randomization\n(RCT). Outcomes include daily step count and message acceptance, assessed via\necological momentary assessments (EMAs). We apply a causal inference framework\nto evaluate the effects of each model. Our findings offer new insights into the\ncomplementary roles of LLM-based personalization and cMAB adaptation in\npromoting physical activity through personalized behavioral messaging.",
      "pdf_url": "http://arxiv.org/pdf/2506.07275v1",
      "arxiv_url": "http://arxiv.org/abs/2506.07275v1",
      "published": "2025-06-08",
      "categories": [
        "cs.LG",
        "cs.HC",
        "stat.AP"
      ]
    },
    {
      "title": "Quantile-Optimal Policy Learning under Unmeasured Confounding",
      "authors": [
        "Zhongren Chen",
        "Siyu Chen",
        "Zhengling Qi",
        "Xiaohong Chen",
        "Zhuoran Yang"
      ],
      "abstract": "We study quantile-optimal policy learning where the goal is to find a policy\nwhose reward distribution has the largest $\\alpha$-quantile for some $\\alpha\n\\in (0, 1)$. We focus on the offline setting whose generating process involves\nunobserved confounders. Such a problem suffers from three main challenges: (i)\nnonlinearity of the quantile objective as a functional of the reward\ndistribution, (ii) unobserved confounding issue, and (iii) insufficient\ncoverage of the offline dataset. To address these challenges, we propose a\nsuite of causal-assisted policy learning methods that provably enjoy strong\ntheoretical guarantees under mild conditions. In particular, to address (i) and\n(ii), using causal inference tools such as instrumental variables and negative\ncontrols, we propose to estimate the quantile objectives by solving nonlinear\nfunctional integral equations. Then we adopt a minimax estimation approach with\nnonparametric models to solve these integral equations, and propose to\nconstruct conservative policy estimates that address (iii). The final policy is\nthe one that maximizes these pessimistic estimates. In addition, we propose a\nnovel regularized policy learning method that is more amenable to computation.\nFinally, we prove that the policies learned by these methods are\n$\\tilde{\\mathscr{O}}(n^{-1/2})$ quantile-optimal under a mild coverage\nassumption on the offline dataset. Here, $\\tilde{\\mathscr{O}}(\\cdot)$ omits\npoly-logarithmic factors. To the best of our knowledge, we propose the first\nsample-efficient policy learning algorithms for estimating the quantile-optimal\npolicy when there exist unmeasured confounding.",
      "pdf_url": "http://arxiv.org/pdf/2506.07140v1",
      "arxiv_url": "http://arxiv.org/abs/2506.07140v1",
      "published": "2025-06-08",
      "categories": [
        "stat.ML",
        "cs.LG",
        "econ.EM"
      ]
    },
    {
      "title": "Half-AVAE: Adversarial-Enhanced Factorized and Structured Encoder-Free VAE for Underdetermined Independent Component Analysis",
      "authors": [
        "Yuan-Hao Wei",
        "Yan-Jie Sun"
      ],
      "abstract": "This study advances the Variational Autoencoder (VAE) framework by addressing\nchallenges in Independent Component Analysis (ICA) under both determined and\nunderdetermined conditions, focusing on enhancing the independence and\ninterpretability of latent variables. Traditional VAEs map observed data to\nlatent variables and back via an encoder-decoder architecture, but struggle\nwith underdetermined ICA where the number of latent variables exceeds observed\nsignals. The proposed Half Adversarial VAE (Half-AVAE) builds on the\nencoder-free Half-VAE framework, eliminating explicit inverse mapping to tackle\nunderdetermined scenarios. By integrating adversarial networks and External\nEnhancement (EE) terms, Half-AVAE promotes mutual independence among latent\ndimensions, achieving factorized and interpretable representations. Experiments\nwith synthetic signals demonstrate that Half-AVAE outperforms baseline models,\nincluding GP-AVAE and Half-VAE, in recovering independent components under\nunderdetermined conditions, as evidenced by lower root mean square errors. The\nstudy highlights the flexibility of VAEs in variational inference, showing that\nencoder omission, combined with adversarial training and structured priors,\nenables effective solutions for complex ICA tasks, advancing applications in\ndisentanglement, causal inference, and generative modeling.",
      "pdf_url": "http://arxiv.org/pdf/2506.07011v1",
      "arxiv_url": "http://arxiv.org/abs/2506.07011v1",
      "published": "2025-06-08",
      "categories": [
        "stat.ML",
        "cs.LG",
        "eess.SP"
      ]
    },
    {
      "title": "Causal Graph based Event Reasoning using Semantic Relation Experts",
      "authors": [
        "Mahnaz Koupaee",
        "Xueying Bai",
        "Mudan Chen",
        "Greg Durrett",
        "Nathanael Chambers",
        "Niranjan Balasubramanian"
      ],
      "abstract": "Understanding how events in a scenario causally connect with each other is\nimportant for effectively modeling and reasoning about events. But event\nreasoning remains a difficult challenge, and despite recent advances, Large\nLanguage Models (LLMs) still struggle to accurately identify causal connections\nbetween events. This struggle leads to poor performance on deeper reasoning\ntasks like event forecasting and timeline understanding. To address this\nchallenge, we investigate the generation of causal event graphs (e.g., A\nenables B) as a parallel mechanism to help LLMs explicitly represent causality\nduring inference. This paper evaluates both how to generate correct graphs as\nwell as how graphs can assist reasoning. We propose a collaborative approach to\ncausal graph generation where we use LLMs to simulate experts that focus on\nspecific semantic relations. The experts engage in multiple rounds of\ndiscussions which are then consolidated by a final expert. Then, to demonstrate\nthe utility of causal graphs, we use them on multiple downstream applications,\nand also introduce a new explainable event prediction task that requires a\ncausal chain of events in the explanation. These explanations are more\ninformative and coherent than baseline generations. Finally, our overall\napproach not finetuned on any downstream task, achieves competitive results\nwith state-of-the-art models on both forecasting and next event prediction\ntasks.",
      "pdf_url": "http://arxiv.org/pdf/2506.06910v1",
      "arxiv_url": "http://arxiv.org/abs/2506.06910v1",
      "published": "2025-06-07",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Perturbative QCD reveals the softening of matter in the cores of massive neutron stars",
      "authors": [
        "Oleg Komoltsev"
      ],
      "abstract": "The cores of neutron stars (NSs) contain the densest matter in the universe.\nRapid advancements in neutron-star observations allow unprecedented empirical\naccess to cold, ultra-dense Quantum Chromodynamics (QCD) matter. The\ncombination of these observations with theoretical calculations has revealed\npreviously inaccessible features of the equation of state (EoS) and the QCD\nphase diagram. In this thesis, I demonstrate how perturbative-QCD calculations\nat asymptotically high baryon densities provide robust constraints on the EoS\nat neutron-star densities. The method for constraint propagation is based\nsolely on thermodynamical causality, stability, and consistency of the EoS. By\nconstructing a large ensemble of EoSs using Gaussian processes regression and\nincorporating it into a Bayesian inference of EoS, I demonstrate that the novel\npQCD constraints go beyond those obtained from current astrophysical\nobservations alone, forcing the EoS to soften at the maximum densities of\nstable neutron stars. This softening of the EoS can be interpreted as an\nindication of approximate conformal symmetry restoration, a sign of a\nfirst-order phase transition (FOPT), or potentially both. I show that the\nconformal symmetry restoration is consistent with the hypothesis of quark\nmatter cores inside the most massive NSs. Although current astrophysical data\nand theoretical inputs cannot definitively distinguish between the two\nscenarios, they slightly favor the occurrence of a phase transition of some\nkind - whether a crossover to quark matter or a destabilizing FOPT - in the\ncores of the most massive neutron stars.",
      "pdf_url": "http://arxiv.org/pdf/2506.06465v1",
      "arxiv_url": "http://arxiv.org/abs/2506.06465v1",
      "published": "2025-06-06",
      "categories": [
        "astro-ph.HE",
        "hep-ph",
        "nucl-th"
      ]
    },
    {
      "title": "Preference Learning for AI Alignment: a Causal Perspective",
      "authors": [
        "Katarzyna Kobalczyk",
        "Mihaela van der Schaar"
      ],
      "abstract": "Reward modelling from preference data is a crucial step in aligning large\nlanguage models (LLMs) with human values, requiring robust generalisation to\nnovel prompt-response pairs. In this work, we propose to frame this problem in\na causal paradigm, providing the rich toolbox of causality to identify the\npersistent challenges, such as causal misidentification, preference\nheterogeneity, and confounding due to user-specific factors. Inheriting from\nthe literature of causal inference, we identify key assumptions necessary for\nreliable generalisation and contrast them with common data collection\npractices. We illustrate failure modes of naive reward models and demonstrate\nhow causally-inspired approaches can improve model robustness. Finally, we\noutline desiderata for future research and practices, advocating targeted\ninterventions to address inherent limitations of observational data.",
      "pdf_url": "http://arxiv.org/pdf/2506.05967v1",
      "arxiv_url": "http://arxiv.org/abs/2506.05967v1",
      "published": "2025-06-06",
      "categories": [
        "cs.AI",
        "cs.LG",
        "stat.ML"
      ]
    }
  ]
}