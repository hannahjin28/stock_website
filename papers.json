{
  "last_updated": "2025-06-12T00:54:55.511211",
  "papers": [
    {
      "title": "Paths to Causality: Finding Informative Subgraphs Within Knowledge Graphs for Knowledge-Based Causal Discovery",
      "authors": [
        "Yuni Susanti",
        "Michael FÃ¤rber"
      ],
      "abstract": "Inferring causal relationships between variable pairs is crucial for\nunderstanding multivariate interactions in complex systems. Knowledge-based\ncausal discovery -- which involves inferring causal relationships by reasoning\nover the metadata of variables (e.g., names or textual context) -- offers a\ncompelling alternative to traditional methods that rely on observational data.\nHowever, existing methods using Large Language Models (LLMs) often produce\nunstable and inconsistent results, compromising their reliability for causal\ninference. To address this, we introduce a novel approach that integrates\nKnowledge Graphs (KGs) with LLMs to enhance knowledge-based causal discovery.\nOur approach identifies informative metapath-based subgraphs within KGs and\nfurther refines the selection of these subgraphs using Learning-to-Rank-based\nmodels. The top-ranked subgraphs are then incorporated into zero-shot prompts,\nimproving the effectiveness of LLMs in inferring the causal relationship.\nExtensive experiments on biomedical and open-domain datasets demonstrate that\nour method outperforms most baselines by up to 44.4 points in F1 scores,\nevaluated across diverse LLMs and KGs. Our code and datasets are available on\nGitHub: https://github.com/susantiyuni/path-to-causality",
      "pdf_url": "http://arxiv.org/pdf/2506.08771v1",
      "arxiv_url": "http://arxiv.org/abs/2506.08771v1",
      "published": "2025-06-10",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.IR",
        "cs.LG"
      ]
    },
    {
      "title": "CC-RAG: Structured Multi-Hop Reasoning via Theme-Based Causal Graphs",
      "authors": [
        "Jash Rajesh Parekh",
        "Pengcheng Jiang",
        "Jiawei Han"
      ],
      "abstract": "Understanding cause and effect relationships remains a formidable challenge\nfor Large Language Models (LLMs), particularly in specialized domains where\nreasoning requires more than surface-level correlations. Retrieval-Augmented\nGeneration (RAG) improves factual accuracy, but standard RAG pipelines treat\nevidence as flat context, lacking the structure required to model true causal\ndependencies. We introduce Causal-Chain RAG (CC-RAG), a novel approach that\nintegrates zero-shot triple extraction and theme-aware graph chaining into the\nRAG pipeline, enabling structured multi-hop inference. Given a domain specific\ncorpus, CC-RAG constructs a Directed Acyclic Graph (DAG) of <cause, relation,\neffect> triples and uses forward/backward chaining to guide structured answer\ngeneration. Experiments on two real-world domains: Bitcoin price fluctuations\nand Gaucher disease, show that CC-RAG outperforms standard RAG and zero-shot\nLLMs in chain similarity, information density, and lexical diversity. Both\nLLM-as-a-Judge and human evaluations consistently favor CC-RAG. Our results\ndemonstrate that explicitly modeling causal structure enables LLMs to generate\nmore accurate and interpretable responses, especially in specialized domains\nwhere flat retrieval fails.",
      "pdf_url": "http://arxiv.org/pdf/2506.08364v2",
      "arxiv_url": "http://arxiv.org/abs/2506.08364v2",
      "published": "2025-06-10",
      "categories": [
        "cs.CL"
      ]
    },
    {
      "title": "DEAL: Disentangling Transformer Head Activations for LLM Steering",
      "authors": [
        "Li-Ming Zhan",
        "Bo Liu",
        "Zexin Lu",
        "Chengqiang Xie",
        "Jiannong Cao",
        "Xiao-Ming Wu"
      ],
      "abstract": "Inference-time steering aims to alter the response characteristics of large\nlanguage models (LLMs) without modifying their underlying parameters. A\ncritical step in this process is the identification of internal modules within\nLLMs that are associated with the target behavior. However, current approaches\nto module selection often depend on superficial cues or ad-hoc heuristics,\nwhich can result in suboptimal or unintended outcomes. In this work, we propose\na principled causal-attribution framework for identifying behavior-relevant\nattention heads in transformers. For each head, we train a vector-quantized\nautoencoder (VQ-AE) on its attention activations, partitioning the latent space\ninto behavior-relevant and behavior-irrelevant subspaces, each quantized with a\nshared learnable codebook. We assess the behavioral relevance of each head by\nquantifying the separability of VQ-AE encodings for behavior-aligned versus\nbehavior-violating responses using a binary classification metric. This yields\na behavioral relevance score that reflects each head discriminative capacity\nwith respect to the target behavior, guiding both selection and importance\nweighting. Experiments on seven LLMs from two model families and five\nbehavioral steering datasets demonstrate that our method enables more accurate\ninference-time interventions, achieving superior performance on the\ntruthfulness-steering task. Furthermore, the heads selected by our approach\nexhibit strong zero-shot generalization in cross-domain truthfulness-steering\nscenarios.",
      "pdf_url": "http://arxiv.org/pdf/2506.08359v1",
      "arxiv_url": "http://arxiv.org/abs/2506.08359v1",
      "published": "2025-06-10",
      "categories": [
        "cs.CL"
      ]
    },
    {
      "title": "Operator theoretic measure of causality in linear dynamical systems",
      "authors": [
        "Ankit Srivastava",
        "Louis Cattafesta",
        "Scott Dawson"
      ],
      "abstract": "This paper introduces Linear Operator Causality Analysis (LOCA), a\nphysics-based framework for analyzing causality in linear dynamical systems,\nwith a focus on fluid mechanics. Unlike data-driven statistical methods like\nGranger causality, LOCA uses the matrix exponential from the system's governing\nequations to provide a more rigorous and interpretable measure of causal\ninteractions. Our framework reveals that, under certain conditions, common\ndata-driven measures effectively approximate the squared magnitude of the\nmatrix exponential. LOCA also mitigates common analysis issues, such as\nmisleading inferences from correlated variables. We validate the methodology\nusing a linearized Couette flow, demonstrating its ability to robustly identify\nboth direct and indirect causal relationships between flow structures. This\napproach offers a powerful tool for understanding modal interactions and\nperturbation propagation in complex systems.",
      "pdf_url": "http://arxiv.org/pdf/2506.08118v1",
      "arxiv_url": "http://arxiv.org/abs/2506.08118v1",
      "published": "2025-06-09",
      "categories": [
        "nlin.CD",
        "math-ph",
        "math.MP"
      ]
    },
    {
      "title": "Self Forcing: Bridging the Train-Test Gap in Autoregressive Video Diffusion",
      "authors": [
        "Xun Huang",
        "Zhengqi Li",
        "Guande He",
        "Mingyuan Zhou",
        "Eli Shechtman"
      ],
      "abstract": "We introduce Self Forcing, a novel training paradigm for autoregressive video\ndiffusion models. It addresses the longstanding issue of exposure bias, where\nmodels trained on ground-truth context must generate sequences conditioned on\ntheir own imperfect outputs during inference. Unlike prior methods that denoise\nfuture frames based on ground-truth context frames, Self Forcing conditions\neach frame's generation on previously self-generated outputs by performing\nautoregressive rollout with key-value (KV) caching during training. This\nstrategy enables supervision through a holistic loss at the video level that\ndirectly evaluates the quality of the entire generated sequence, rather than\nrelying solely on traditional frame-wise objectives. To ensure training\nefficiency, we employ a few-step diffusion model along with a stochastic\ngradient truncation strategy, effectively balancing computational cost and\nperformance. We further introduce a rolling KV cache mechanism that enables\nefficient autoregressive video extrapolation. Extensive experiments demonstrate\nthat our approach achieves real-time streaming video generation with sub-second\nlatency on a single GPU, while matching or even surpassing the generation\nquality of significantly slower and non-causal diffusion models. Project\nwebsite: http://self-forcing.github.io/",
      "pdf_url": "http://arxiv.org/pdf/2506.08009v1",
      "arxiv_url": "http://arxiv.org/abs/2506.08009v1",
      "published": "2025-06-09",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "CausalPFN: Amortized Causal Effect Estimation via In-Context Learning",
      "authors": [
        "Vahid Balazadeh",
        "Hamidreza Kamkari",
        "Valentin Thomas",
        "Benson Li",
        "Junwei Ma",
        "Jesse C. Cresswell",
        "Rahul G. Krishnan"
      ],
      "abstract": "Causal effect estimation from observational data is fundamental across\nvarious applications. However, selecting an appropriate estimator from dozens\nof specialized methods demands substantial manual effort and domain expertise.\nWe present CausalPFN, a single transformer that amortizes this workflow:\ntrained once on a large library of simulated data-generating processes that\nsatisfy ignorability, it infers causal effects for new observational datasets\nout-of-the-box. CausalPFN combines ideas from Bayesian causal inference with\nthe large-scale training protocol of prior-fitted networks (PFNs), learning to\nmap raw observations directly to causal effects without any task-specific\nadjustment. Our approach achieves superior average performance on heterogeneous\nand average treatment effect estimation benchmarks (IHDP, Lalonde, ACIC).\nMoreover, it shows competitive performance for real-world policy making on\nuplift modeling tasks. CausalPFN provides calibrated uncertainty estimates to\nsupport reliable decision-making based on Bayesian principles. This\nready-to-use model does not require any further training or tuning and takes a\nstep toward automated causal inference (https://github.com/vdblm/CausalPFN).",
      "pdf_url": "http://arxiv.org/pdf/2506.07918v1",
      "arxiv_url": "http://arxiv.org/abs/2506.07918v1",
      "published": "2025-06-09",
      "categories": [
        "cs.LG",
        "stat.ML"
      ]
    },
    {
      "title": "Learning to Focus: Causal Attention Distillation via Gradient-Guided Token Pruning",
      "authors": [
        "Yiju Guo",
        "Wenkai Yang",
        "Zexu Sun",
        "Ning Ding",
        "Zhiyuan Liu",
        "Yankai Lin"
      ],
      "abstract": "Large language models (LLMs) have demonstrated significant improvements in\ncontextual understanding. However, their ability to attend to truly critical\ninformation during long-context reasoning and generation still falls behind the\npace. Specifically, our preliminary experiments reveal that certain distracting\npatterns can misdirect the model's attention during inference, and removing\nthese patterns substantially improves reasoning accuracy and generation\nquality. We attribute this phenomenon to spurious correlations in the training\ndata, which obstruct the model's capacity to infer authentic causal\ninstruction-response relationships. This phenomenon may induce redundant\nreasoning processes, potentially resulting in significant inference overhead\nand, more critically, the generation of erroneous or suboptimal responses. To\nmitigate this, we introduce a two-stage framework called Learning to Focus\n(LeaF) leveraging intervention-based inference to disentangle confounding\nfactors. In the first stage, LeaF employs gradient-based comparisons with an\nadvanced teacher to automatically identify confounding tokens based on causal\nrelationships in the training corpus. Then, in the second stage, it prunes\nthese tokens during distillation to enact intervention, aligning the student's\nattention with the teacher's focus distribution on truly critical context\ntokens. Experimental results demonstrate that LeaF not only achieves an\nabsolute improvement in various mathematical reasoning and code generation\nbenchmarks but also effectively suppresses attention to confounding tokens\nduring inference, yielding a more interpretable and reliable reasoning model.",
      "pdf_url": "http://arxiv.org/pdf/2506.07851v1",
      "arxiv_url": "http://arxiv.org/abs/2506.07851v1",
      "published": "2025-06-09",
      "categories": [
        "cs.CL"
      ]
    },
    {
      "title": "Individual Treatment Effect: Prediction Intervals and Sharp Bounds",
      "authors": [
        "Zhehao Zhang",
        "Thomas S. Richardson"
      ],
      "abstract": "Individual treatment effect (ITE) is often regarded as the ideal target of\ninference in causal analyses and has been the focus of several recent studies.\nIn this paper, we describe the intrinsic limits regarding what can be learned\nconcerning ITEs given data from large randomized experiments. We consider when\na valid prediction interval for the ITE is informative and when it can be\nbounded away from zero. The joint distribution over potential outcomes is only\npartially identified from a randomized trial. Consequently, to be valid, an ITE\nprediction interval must be valid for all joint distribution consistent with\nthe observed data and hence will in general be wider than that resulting from\nknowledge of this joint distribution. We characterize prediction intervals in\nthe binary treatment and outcome setting, and extend these insights to models\nwith continuous and ordinal outcomes. We derive sharp bounds on the probability\nmass function (pmf) of the individual treatment effect (ITE). Finally, we\ncontrast prediction intervals for the ITE and confidence intervals for the\naverage treatment effect (ATE). This also leads to the consideration of Fisher\nversus Neyman null hypotheses. While confidence intervals for the ATE shrink\nwith increasing sample size due to its status as a population parameter,\nprediction intervals for the ITE generally do not vanish, leading to scenarios\nwhere one may reject the Neyman null yet still find evidence consistent with\nthe Fisher null, highlighting the challenges of individualized decision-making\nunder partial identification.",
      "pdf_url": "http://arxiv.org/pdf/2506.07469v1",
      "arxiv_url": "http://arxiv.org/abs/2506.07469v1",
      "published": "2025-06-09",
      "categories": [
        "stat.ME",
        "econ.EM",
        "math.ST",
        "stat.TH"
      ]
    },
    {
      "title": "Investigating the Relationship Between Physical Activity and Tailored Behavior Change Messaging: Connecting Contextual Bandit with Large Language Models",
      "authors": [
        "Haochen Song",
        "Dominik Hofer",
        "Rania Islambouli",
        "Laura Hawkins",
        "Ananya Bhattacharjee",
        "Meredith Franklin",
        "Joseph Jay Williams"
      ],
      "abstract": "Machine learning approaches, such as contextual multi-armed bandit (cMAB)\nalgorithms, offer a promising strategy to reduce sedentary behavior by\ndelivering personalized interventions to encourage physical activity. However,\ncMAB algorithms typically require large participant samples to learn\neffectively and may overlook key psychological factors that are not explicitly\nencoded in the model. In this study, we propose a hybrid approach that combines\ncMAB for selecting intervention types with large language models (LLMs) to\npersonalize message content. We evaluate four intervention types: behavioral\nself-monitoring, gain-framed, loss-framed, and social comparison, each\ndelivered as a motivational message aimed at increasing motivation for physical\nactivity and daily step count. Message content is further personalized using\ndynamic contextual factors including daily fluctuations in self-efficacy,\nsocial influence, and regulatory focus. Over a seven-day trial, participants\nreceive daily messages assigned by one of four models: cMAB alone, LLM alone,\ncombined cMAB with LLM personalization (cMABxLLM), or equal randomization\n(RCT). Outcomes include daily step count and message acceptance, assessed via\necological momentary assessments (EMAs). We apply a causal inference framework\nto evaluate the effects of each model. Our findings offer new insights into the\ncomplementary roles of LLM-based personalization and cMAB adaptation in\npromoting physical activity through personalized behavioral messaging.",
      "pdf_url": "http://arxiv.org/pdf/2506.07275v1",
      "arxiv_url": "http://arxiv.org/abs/2506.07275v1",
      "published": "2025-06-08",
      "categories": [
        "cs.LG",
        "cs.HC",
        "stat.AP"
      ]
    },
    {
      "title": "Quantile-Optimal Policy Learning under Unmeasured Confounding",
      "authors": [
        "Zhongren Chen",
        "Siyu Chen",
        "Zhengling Qi",
        "Xiaohong Chen",
        "Zhuoran Yang"
      ],
      "abstract": "We study quantile-optimal policy learning where the goal is to find a policy\nwhose reward distribution has the largest $\\alpha$-quantile for some $\\alpha\n\\in (0, 1)$. We focus on the offline setting whose generating process involves\nunobserved confounders. Such a problem suffers from three main challenges: (i)\nnonlinearity of the quantile objective as a functional of the reward\ndistribution, (ii) unobserved confounding issue, and (iii) insufficient\ncoverage of the offline dataset. To address these challenges, we propose a\nsuite of causal-assisted policy learning methods that provably enjoy strong\ntheoretical guarantees under mild conditions. In particular, to address (i) and\n(ii), using causal inference tools such as instrumental variables and negative\ncontrols, we propose to estimate the quantile objectives by solving nonlinear\nfunctional integral equations. Then we adopt a minimax estimation approach with\nnonparametric models to solve these integral equations, and propose to\nconstruct conservative policy estimates that address (iii). The final policy is\nthe one that maximizes these pessimistic estimates. In addition, we propose a\nnovel regularized policy learning method that is more amenable to computation.\nFinally, we prove that the policies learned by these methods are\n$\\tilde{\\mathscr{O}}(n^{-1/2})$ quantile-optimal under a mild coverage\nassumption on the offline dataset. Here, $\\tilde{\\mathscr{O}}(\\cdot)$ omits\npoly-logarithmic factors. To the best of our knowledge, we propose the first\nsample-efficient policy learning algorithms for estimating the quantile-optimal\npolicy when there exist unmeasured confounding.",
      "pdf_url": "http://arxiv.org/pdf/2506.07140v1",
      "arxiv_url": "http://arxiv.org/abs/2506.07140v1",
      "published": "2025-06-08",
      "categories": [
        "stat.ML",
        "cs.LG",
        "econ.EM"
      ]
    }
  ]
}