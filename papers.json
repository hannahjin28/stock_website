{
  "last_updated": "2026-01-14T01:01:01.291956",
  "papers": [
    {
      "title": "The Role of Confounders and Linearity in Ecological Inference: A Reassessment",
      "authors": [
        "Shiro Kuriwaki",
        "Cory McCartan"
      ],
      "abstract": "Estimating conditional means using only the marginal means available from aggregate data is commonly known as the ecological inference problem (EI). We provide a reassessment of EI, including a new formalization of identification conditions and a demonstration of how these conditions fail to hold in common cases. The identification conditions reveal that, similar to causal inference, credible ecological inference requires controlling for confounders. The aggregation process itself creates additional structure to assist in estimation by restricting the conditional expectation function to be linear in the predictor variable. A linear model perspective also clarifies the differences between the EI methods commonly used in the literature, and when they lead to ecological fallacies. We provide an overview of new methodology which builds on both the identification and linearity results to flexibly control for confounders and yield improved ecological inferences. Finally, using datasets for common EI problems in which the ground truth is fortuitously observed, we show that, while covariates can help, all methods are prone to overestimating both racial polarization and nationalized partisan voting.",
      "pdf_url": "https://arxiv.org/pdf/2601.07668v1",
      "arxiv_url": "http://arxiv.org/abs/2601.07668v1",
      "published": "2026-01-12",
      "categories": [
        "stat.AP"
      ]
    },
    {
      "title": "Functional Synthetic Control Methods for Metric Space-Valued Outcomes",
      "authors": [
        "Ryo Okano",
        "Daisuke Kurisu"
      ],
      "abstract": "The synthetic control method (SCM) is a widely used tool for evaluating causal effects of policy changes in panel data settings. Recent studies have extended its framework to accommodate complex outcomes that take values in metric spaces, such as distributions, functions, networks, covariance matrices, and compositional data. However, due to the lack of linear structure in general metric spaces, theoretical guarantees for estimation and inference within these extended frameworks remain underdeveloped. In this study, we propose the functional synthetic control (FSC) method as an extension of the SCM for metric space-valued outcomes. To address challenges arising from the nonlinearlity of metric spaces, we leverage isometric embeddings into Hilbert spaces. Building on this approach, we develop the FSC and augmented FSC estimators for counterfactual outcomes, with the latter being a bias-corrected version of the former. We then derive their finite-sample error bounds to establish theoretical guarantees for estimation, and construct prediction sets based on these estimators to conduct inference on causal effects. We demonstrate the usefulness of the proposed framework through simulation studies and three empirical applications.",
      "pdf_url": "https://arxiv.org/pdf/2601.07539v1",
      "arxiv_url": "http://arxiv.org/abs/2601.07539v1",
      "published": "2026-01-12",
      "categories": [
        "stat.ME"
      ]
    },
    {
      "title": "Minimum Wasserstein distance estimator under covariate shift: closed-form, super-efficiency and irregularity",
      "authors": [
        "Junjun Lang",
        "Qiong Zhang",
        "Yukun Liu"
      ],
      "abstract": "Covariate shift arises when covariate distributions differ between source and target populations while the conditional distribution of the response remains invariant, and it underlies problems in missing data and causal inference. We propose a minimum Wasserstein distance estimation framework for inference under covariate shift that avoids explicit modeling of outcome regressions or importance weights. The resulting W-estimator admits a closed-form expression and is numerically equivalent to the classical 1-nearest neighbor estimator, yielding a new optimal transport interpretation of nearest neighbor methods. We establish root-$n$ asymptotic normality and show that the estimator is not asymptotically linear, leading to super-efficiency relative to the semiparametric efficient estimator under covariate shift in certain regimes, and uniformly in missing data problems. Numerical simulations, along with an analysis of a rainfall dataset, underscore the exceptional performance of our W-estimator.",
      "pdf_url": "https://arxiv.org/pdf/2601.07282v1",
      "arxiv_url": "http://arxiv.org/abs/2601.07282v1",
      "published": "2026-01-12",
      "categories": [
        "stat.ME",
        "stat.ML"
      ]
    },
    {
      "title": "Connections as treatment: causal inference with edge interventions in networks",
      "authors": [
        "Shuli Chen",
        "Jie Hu",
        "Zhichao Jiang"
      ],
      "abstract": "Causal inference has traditionally focused on interventions at the unit level. In many applications, however, the central question concerns the causal effects of connections between units, such as transportation links, social relationships, or collaborative ties. We develop a causal framework for edge interventions in networks, where treatments correspond to the presence or absence of edges. Our framework defines causal estimands under stochastic interventions on the network structure and introduces an inverse probability weighting estimator under an unconfoundedness assumption on edge assignment. We estimate edge probabilities using exponential random graph models, a widely used class of network models. We establish consistency and asymptotic normality of the proposed estimator. Finally, we apply our methodology to China's transportation network to estimate the causal impact of railroad connections on regional economic development.",
      "pdf_url": "https://arxiv.org/pdf/2601.07267v1",
      "arxiv_url": "http://arxiv.org/abs/2601.07267v1",
      "published": "2026-01-12",
      "categories": [
        "stat.ME",
        "stat.AP"
      ]
    },
    {
      "title": "MeepleLM: A Virtual Playtester Simulating Diverse Subjective Experiences",
      "authors": [
        "Zizhen Li",
        "Chuanhao Li",
        "Yibin Wang",
        "Yukang Feng",
        "Jianwen Sun",
        "Jiaxin Ai",
        "Fanrui Zhang",
        "Mingzhu Sun",
        "Yifei Huang",
        "Kaipeng Zhang"
      ],
      "abstract": "Recent advancements have expanded the role of Large Language Models in board games from playing agents to creative co-designers. However, a critical gap remains: current systems lack the capacity to offer constructive critique grounded in the emergent user experience. Bridging this gap is fundamental for harmonizing Human-AI collaboration, as it empowers designers to refine their creations via external perspectives while steering models away from biased or unpredictable outcomes. Automating critique for board games presents two challenges: inferring the latent dynamics connecting rules to gameplay without an explicit engine, and modeling the subjective heterogeneity of diverse player groups. To address these, we curate a dataset of 1,727 structurally corrected rulebooks and 150K reviews selected via quality scoring and facet-aware sampling. We augment this data with Mechanics-Dynamics-Aesthetics (MDA) reasoning to explicitly bridge the causal gap between written rules and player experience. We further distill player personas and introduce MeepleLM, a specialized model that internalizes persona-specific reasoning patterns to accurately simulate the subjective feedback of diverse player archetypes. Experiments demonstrate that MeepleLM significantly outperforms latest commercial models (e.g., GPT-5.1, Gemini3-Pro) in community alignment and critique quality, achieving a 70% preference rate in user studies assessing utility. MeepleLM serves as a reliable virtual playtester for general interactive systems, marking a pivotal step towards audience-aligned, experience-aware Human-AI collaboration.",
      "pdf_url": "https://arxiv.org/pdf/2601.07251v1",
      "arxiv_url": "http://arxiv.org/abs/2601.07251v1",
      "published": "2026-01-12",
      "categories": [
        "cs.HC"
      ]
    },
    {
      "title": "Benchmarking Egocentric Clinical Intent Understanding Capability for Medical Multimodal Large Language Models",
      "authors": [
        "Shaonan Liu",
        "Guo Yu",
        "Xiaoling Luo",
        "Shiyi Zheng",
        "Wenting Chen",
        "Jie Liu",
        "Linlin Shen"
      ],
      "abstract": "Medical Multimodal Large Language Models (Med-MLLMs) require egocentric clinical intent understanding for real-world deployment, yet existing benchmarks fail to evaluate this critical capability. To address these challenges, we introduce MedGaze-Bench, the first benchmark leveraging clinician gaze as a Cognitive Cursor to assess intent understanding across surgery, emergency simulation, and diagnostic interpretation. Our benchmark addresses three fundamental challenges: visual homogeneity of anatomical structures, strict temporal-causal dependencies in clinical workflows, and implicit adherence to safety protocols. We propose a Three-Dimensional Clinical Intent Framework evaluating: (1) Spatial Intent: discriminating precise targets amid visual noise, (2) Temporal Intent: inferring causal rationale through retrospective and prospective reasoning, and (3) Standard Intent: verifying protocol compliance through safety checks. Beyond accuracy metrics, we introduce Trap QA mechanisms to stress-test clinical reliability by penalizing hallucinations and cognitive sycophancy. Experiments reveal current MLLMs struggle with egocentric intent due to over-reliance on global features, leading to fabricated observations and uncritical acceptance of invalid instructions.",
      "pdf_url": "https://arxiv.org/pdf/2601.06750v1",
      "arxiv_url": "http://arxiv.org/abs/2601.06750v1",
      "published": "2026-01-11",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "MedEinst: Benchmarking the Einstellung Effect in Medical LLMs through Counterfactual Differential Diagnosis",
      "authors": [
        "Wenting Chen",
        "Zhongrui Zhu",
        "Guolin Huang",
        "Wenxuan Wang"
      ],
      "abstract": "Despite achieving high accuracy on medical benchmarks, LLMs exhibit the Einstellung Effect in clinical diagnosis--relying on statistical shortcuts rather than patient-specific evidence, causing misdiagnosis in atypical cases. Existing benchmarks fail to detect this critical failure mode. We introduce MedEinst, a counterfactual benchmark with 5,383 paired clinical cases across 49 diseases. Each pair contains a control case and a \"trap\" case with altered discriminative evidence that flips the diagnosis. We measure susceptibility via Bias Trap Rate--probability of misdiagnosing traps despite correctly diagnosing controls. Extensive Evaluation of 17 LLMs shows frontier models achieve high baseline accuracy but severe bias trap rates. Thus, we propose ECR-Agent, aligning LLM reasoning with Evidence-Based Medicine standard via two components: (1) Dynamic Causal Inference (DCI) performs structured reasoning through dual-pathway perception, dynamic causal graph reasoning across three levels (association, intervention, counterfactual), and evidence audit for final diagnosis; (2) Critic-Driven Graph and Memory Evolution (CGME) iteratively refines the system by storing validated reasoning paths in an exemplar base and consolidating disease-specific knowledge into evolving illness graphs. Source code is to be released.",
      "pdf_url": "https://arxiv.org/pdf/2601.06636v1",
      "arxiv_url": "http://arxiv.org/abs/2601.06636v1",
      "published": "2026-01-10",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Long-Term Causal Inference with Many Noisy Proxies",
      "authors": [
        "Apoorva Lal",
        "Guido Imbens",
        "Peter Hull"
      ],
      "abstract": "We propose a method for estimating long-term treatment effects with many short-term proxy outcomes: a central challenge when experimenting on digital platforms. We formalize this challenge as a latent variable problem where observed proxies are noisy measures of a low-dimensional set of unobserved surrogates that mediate treatment effects. Through theoretical analysis and simulations, we demonstrate that regularized regression methods substantially outperform naive proxy selection. We show in particular that the bias of Ridge regression decreases as more proxies are added, with closed-form expressions for the bias-variance tradeoff. We illustrate our method with an empirical application to the California GAIN experiment.",
      "pdf_url": "https://arxiv.org/pdf/2601.06359v1",
      "arxiv_url": "http://arxiv.org/abs/2601.06359v1",
      "published": "2026-01-09",
      "categories": [
        "econ.EM"
      ]
    },
    {
      "title": "VideoAR: Autoregressive Video Generation via Next-Frame & Scale Prediction",
      "authors": [
        "Longbin Ji",
        "Xiaoxiong Liu",
        "Junyuan Shang",
        "Shuohuan Wang",
        "Yu Sun",
        "Hua Wu",
        "Haifeng Wang"
      ],
      "abstract": "Recent advances in video generation have been dominated by diffusion and flow-matching models, which produce high-quality results but remain computationally intensive and difficult to scale. In this work, we introduce VideoAR, the first large-scale Visual Autoregressive (VAR) framework for video generation that combines multi-scale next-frame prediction with autoregressive modeling. VideoAR disentangles spatial and temporal dependencies by integrating intra-frame VAR modeling with causal next-frame prediction, supported by a 3D multi-scale tokenizer that efficiently encodes spatio-temporal dynamics. To improve long-term consistency, we propose Multi-scale Temporal RoPE, Cross-Frame Error Correction, and Random Frame Mask, which collectively mitigate error propagation and stabilize temporal coherence. Our multi-stage pretraining pipeline progressively aligns spatial and temporal learning across increasing resolutions and durations. Empirically, VideoAR achieves new state-of-the-art results among autoregressive models, improving FVD on UCF-101 from 99.5 to 88.6 while reducing inference steps by over 10x, and reaching a VBench score of 81.74-competitive with diffusion-based models an order of magnitude larger. These results demonstrate that VideoAR narrows the performance gap between autoregressive and diffusion paradigms, offering a scalable, efficient, and temporally consistent foundation for future video generation research.",
      "pdf_url": "https://arxiv.org/pdf/2601.05966v1",
      "arxiv_url": "http://arxiv.org/abs/2601.05966v1",
      "published": "2026-01-09",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "A Causal Information-Flow Framework for Unbiased Learning-to-Rank",
      "authors": [
        "Haoming Gong",
        "Qingyao Ai",
        "Zhihao Tao",
        "Yongfeng Zhang"
      ],
      "abstract": "In web search and recommendation systems, user clicks are widely used to train ranking models. However, click data is heavily biased, i.e., users tend to click higher-ranked items (position bias), choose only what was shown to them (selection bias), and trust top results more (trust bias). Without explicitly modeling these biases, the true relevance of ranked items cannot be correctly learned from clicks. Existing Unbiased Learning-to-Rank (ULTR) methods mainly correct position bias and rely on propensity estimation, but they cannot measure remaining bias, provide risk guarantees, or jointly handle multiple bias sources. To overcome these challenges, this paper introduces a novel causal learning-based ranking framework that extends ULTR by combining Structural Causal Models (SCMs) with information-theoretic tools. SCMs specify how clicks are generated and help identify the true relevance signal from click data, while conditional mutual information, measures how much bias leaks into the\n  learned relevance estimates. We use this leakage measure to define a rigorous notion of disentanglement and include it as a regularizer during model training to reduce bias. In addition, we incorporate a causal inference estimator, i.e., doubly robust estimator, to ensure more reliable risk estimation. Experiments on standard Learning-to-Rank benchmarks show that our method consistently reduces measured bias leakage and improves ranking performance, especially in realistic scenarios where multiple biases-such as position and trust bias-interact strongly.",
      "pdf_url": "https://arxiv.org/pdf/2601.05590v1",
      "arxiv_url": "http://arxiv.org/abs/2601.05590v1",
      "published": "2026-01-09",
      "categories": [
        "cs.AI"
      ]
    }
  ]
}