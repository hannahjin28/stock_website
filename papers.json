{
  "last_updated": "2025-11-25T00:53:14.488174",
  "papers": [
    {
      "title": "On treating right-censoring events like treatments",
      "authors": [
        "Lan Wen",
        "Aaron L. Sarvet",
        "Jessica G. Young"
      ],
      "abstract": "In causal inference literature, potential outcomes are often indexed by the \"elimination of all right-censoring events,\" leading to the perception that such a restriction is necessary for defining well-posed causal estimands. In this paper, we clarify that this restriction is not required: a well-defined estimand can be formulated without indexing on the elimination of such events. Achieving this requires a more precise classification of right-censoring events than has historically been considered, as the nature of these events has direct implications for identification of the target estimand. We provide a framework that distinguishes different types of right-censoring events from a causal perspective, and demonstrate how this framework relates to censoring definitions and assumptions in classical survival analysis literature. By bridging these perspectives, we provide a clearer understanding of how to handle right-censoring events and provide guidance for identifying causal estimands when right-censored events are present.",
      "pdf_url": "https://arxiv.org/pdf/2511.17379v1",
      "arxiv_url": "http://arxiv.org/abs/2511.17379v1",
      "published": "2025-11-21",
      "categories": [
        "stat.ME",
        "math.ST",
        "stat.AP"
      ]
    },
    {
      "title": "FireScope: Wildfire Risk Prediction with a Chain-of-Thought Oracle",
      "authors": [
        "Mario Markov",
        "Stefan Maria Ailuro",
        "Luc Van Gool",
        "Konrad Schindler",
        "Danda Pani Paudel"
      ],
      "abstract": "Predicting wildfire risk is a reasoning-intensive spatial problem that requires the integration of visual, climatic, and geographic factors to infer continuous risk maps. Existing methods lack the causal reasoning and multimodal understanding required for reliable generalization. We introduce $\\textbf{FireScope-Bench}$, a large-scale dataset and benchmark that couples Sentinel-2 imagery and climate data with expert-defined risk rasters across the USA, and real wildfire events in Europe for cross-continental evaluation. Building on this dataset, we propose $\\textbf{FireScope}$, a VLM-based reasoning-to-generation framework that learns from both reinforcement learning and visual supervision to predict risk rasters with complementary reasoning traces. When trained in the USA and tested in Europe, $\\textbf{FireScope}$ achieves substantial performance gains, while expert feedback and automated analysis confirm that its reasoning traces are faithful and semantically meaningful. Our findings demonstrate that reasoning can ground raster prediction models, improving both generalization and interpretability. To our knowledge, this is the first framework to (1) demonstrate that language-based reasoning can improve generalization in visual generation, (2) propose a high-resolution wildfire risk model that can be applied across continents, and (3) enable systematic studies of robust cross-continental generalization for multimodal fire risk models. We believe that $\\textbf{FireScope-Bench}$ has the potential to serve as a foundation for advancing reasoning-driven, interpretable and generalizable spatial modeling. Data and source code will be made publicly available.",
      "pdf_url": "https://arxiv.org/pdf/2511.17171v1",
      "arxiv_url": "http://arxiv.org/abs/2511.17171v1",
      "published": "2025-11-21",
      "categories": [
        "cs.CV",
        "cs.LG"
      ]
    },
    {
      "title": "Hallucinate Less by Thinking More: Aspect-Based Causal Abstention for Large Language Models",
      "authors": [
        "Vy Nguyen",
        "Ziqi Xu",
        "Jeffrey Chan",
        "Estrid He",
        "Feng Xia",
        "Xiuzhen Zhang"
      ],
      "abstract": "Large Language Models (LLMs) often produce fluent but factually incorrect responses, a phenomenon known as hallucination. Abstention, where the model chooses not to answer and instead outputs phrases such as \"I don't know\", is a common safeguard. However, existing abstention methods typically rely on post-generation signals, such as generation variations or feedback, which limits their ability to prevent unreliable responses in advance. In this paper, we introduce Aspect-Based Causal Abstention (ABCA), a new framework that enables early abstention by analysing the internal diversity of LLM knowledge through causal inference. This diversity reflects the multifaceted nature of parametric knowledge acquired from various sources, representing diverse aspects such as disciplines, legal contexts, or temporal frames. ABCA estimates causal effects conditioned on these aspects to assess the reliability of knowledge relevant to a given query. Based on these estimates, we enable two types of abstention: Type-1, where aspect effects are inconsistent (knowledge conflict), and Type-2, where aspect effects consistently support abstention (knowledge insufficiency). Experiments on standard benchmarks demonstrate that ABCA improves abstention reliability, achieves state-of-the-art performance, and enhances the interpretability of abstention decisions.",
      "pdf_url": "https://arxiv.org/pdf/2511.17170v1",
      "arxiv_url": "http://arxiv.org/abs/2511.17170v1",
      "published": "2025-11-21",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Possibilistic Instrumental Variable Regression",
      "authors": [
        "Gregor Steiner",
        "Jeremie Houssineau",
        "Mark F. J. Steel"
      ],
      "abstract": "Instrumental variable regression is a common approach for causal inference in the presence of unobserved confounding. However, identifying valid instruments is often difficult in practice. In this paper, we propose a novel method based on possibility theory that performs posterior inference on the treatment effect, conditional on a user-specified set of potential violations of the exogeneity assumption. Our method can provide informative results even when only a single, potentially invalid, instrument is available, offering a natural and principled framework for sensitivity analysis. Simulation experiments and a real-data application indicate strong performance of the proposed approach.",
      "pdf_url": "https://arxiv.org/pdf/2511.16029v1",
      "arxiv_url": "http://arxiv.org/abs/2511.16029v1",
      "published": "2025-11-20",
      "categories": [
        "stat.ME",
        "econ.EM",
        "math.ST"
      ]
    },
    {
      "title": "Bayesian Semiparametric Causal Inference: Targeted Doubly Robust Estimation of Treatment Effects",
      "authors": [
        "Gözde Sert",
        "Abhishek Chakrabortty",
        "Anirban Bhattacharya"
      ],
      "abstract": "We propose a semiparametric Bayesian methodology for estimating the average treatment effect (ATE) within the potential outcomes framework using observational data with high-dimensional nuisance parameters. Our method introduces a Bayesian debiasing procedure that corrects for bias arising from nuisance estimation and employs a targeted modeling strategy based on summary statistics rather than the full data. These summary statistics are identified in a debiased manner, enabling the estimation of nuisance bias via weighted observables and facilitating hierarchical learning of the ATE. By combining debiasing with sample splitting, our approach separates nuisance estimation from inference on the target parameter, reducing sensitivity to nuisance model specification. We establish that, under mild conditions, the marginal posterior for the ATE satisfies a Bernstein-von Mises theorem when both nuisance models are correctly specified and remains consistent and robust when only one is correct, achieving Bayesian double robustness. This ensures asymptotic efficiency and frequentist validity. Extensive simulations confirm the theoretical results, demonstrating accurate point estimation and credible intervals with nominal coverage, even in high-dimensional settings. The proposed framework can also be extended to other causal estimands, and its key principles offer a general foundation for advancing Bayesian semiparametric inference more broadly.",
      "pdf_url": "https://arxiv.org/pdf/2511.15904v1",
      "arxiv_url": "http://arxiv.org/abs/2511.15904v1",
      "published": "2025-11-19",
      "categories": [
        "stat.ME",
        "econ.EM",
        "math.ST",
        "stat.ML"
      ]
    },
    {
      "title": "Cross-Balancing for Data-Informed Design and Efficient Analysis of Observational Studies",
      "authors": [
        "Ying Jin",
        "José Zubizarreta"
      ],
      "abstract": "Causal inference starts with a simple idea: compare groups that differ by treatment, not much else. Traditionally, similar groups are constructed using only observed covariates; however, it remains a long-standing challenge to incorporate available outcome data into the study design while preserving valid inference. In this paper, we study the general problem of covariate adjustment, effect estimation, and statistical inference when balancing features are constructed or selected with the aid of outcome information from the data. We propose cross-balancing, a method that uses sample splitting to separate the error in feature construction from the error in weight estimation. Our framework addresses two cases: one where the features are learned functions and one where they are selected from a potentially high-dimensional dictionary. In both cases, we establish mild and general conditions under which cross-balancing produces consistent, asymptotically normal, and efficient estimators. In the learned-function case, cross-balancing achieves finite-sample bias reduction relative to plug-in-type estimators, and is multiply robust when the learned features converge at slow rates. In the variable-selection case, cross-balancing only requires a product condition on how well the selected variables approximate true functions. We illustrate cross-balancing in extensive simulations and an observational study, showing that careful use of outcome information can substantially improve both estimation and inference while maintaining interpretability.",
      "pdf_url": "https://arxiv.org/pdf/2511.15896v1",
      "arxiv_url": "http://arxiv.org/abs/2511.15896v1",
      "published": "2025-11-19",
      "categories": [
        "stat.ME",
        "math.ST",
        "stat.AP",
        "stat.ML"
      ]
    },
    {
      "title": "Causal Inference on Sequential Treatments via Tensor Completion",
      "authors": [
        "Chenyin Gao",
        "Han Chen",
        "Anru R. Zhang",
        "Shu Yang"
      ],
      "abstract": "Marginal Structural Models (MSMs) are popular for causal inference of sequential treatments in longitudinal observational studies, which however are sensitive to model misspecification. To achieve flexible modeling, we envision the potential outcomes to form a three-dimensional tensor indexed by subject, time, and treatment regime and propose a tensorized history-restricted MSM (HRMSM). The semi-parametric tensor factor model allows us to leverage the underlying low-rank structure of the potential outcomes tensor and exploit the pre-treatment covariate information to recover the counterfactual outcomes. We incorporate the inverse probability of treatment weighting in the loss function for tensor completion to adjust for time-varying confounding. Theoretically, a non-asymptotic upper bound on the Frobenius norm error for the proposed estimator is provided. Empirically, simulation studies show that the proposed tensor completion approach outperforms the parametric HRMSM and existing matrix/tensor completion methods. Finally, we illustrate the practical utility of the proposed approach to study the effect of ventilation on organ dysfunction from the Medical Information Mart for Intensive Care database.",
      "pdf_url": "https://arxiv.org/pdf/2511.15866v1",
      "arxiv_url": "http://arxiv.org/abs/2511.15866v1",
      "published": "2025-11-19",
      "categories": [
        "stat.ME"
      ]
    },
    {
      "title": "Reflexive Evidence-Based Multimodal Learning for Clean Energy Transitions: Causal Insights on Cooking Fuel Access, Urbanization, and Carbon Emissions",
      "authors": [
        "Shan Shan"
      ],
      "abstract": "Achieving Sustainable Development Goal 7 (Affordable and Clean Energy) requires not only technological innovation but also a deeper understanding of the socioeconomic factors influencing energy access and carbon emissions. While these factors are gaining attention, critical questions remain, particularly regarding how to quantify their impacts on energy systems, model their cross-domain interactions, and capture feedback dynamics in the broader context of energy transitions. To address these gaps, this study introduces ClimateAgents, an AI-based framework that combines large language models with domain-specialized agents to support hypothesis generation and scenario exploration. Leveraging 20 years of socioeconomic and emissions data from 265 economies, countries and regions, and 98 indicators drawn from the World Bank database, the framework applies a machine learning based causal inference approach to identify key determinants of carbon emissions in an evidence-based, data driven manner. The analysis highlights three primary drivers: access to clean cooking fuels in rural areas, access to clean cooking fuels in urban areas, and the percentage of population living in urban areas. These findings underscore the critical role of clean cooking technologies and urbanization patterns in shaping emission outcomes. In line with growing calls for evidence-based AI policy, ClimateAgents offers a modular and reflexive learning system that supports the generation of credible and actionable insights for policy. By integrating heterogeneous data modalities, including structured indicators, policy documents, and semantic reasoning, the framework contributes to adaptive policymaking infrastructures that can evolve with complex socio-technical challenges. This approach aims to support a shift from siloed modeling to reflexive, modular systems designed for dynamic, context-aware climate action.",
      "pdf_url": "https://arxiv.org/pdf/2511.15342v1",
      "arxiv_url": "http://arxiv.org/abs/2511.15342v1",
      "published": "2025-11-19",
      "categories": [
        "cs.HC",
        "cs.AI"
      ]
    },
    {
      "title": "Causal Inference in Financial Event Studies",
      "authors": [
        "Paul Goldsmith-Pinkham",
        "Tianshu Lyu"
      ],
      "abstract": "Financial event studies, ubiquitous in finance research, typically use linear factor models with known factors to estimate abnormal returns and identify causal effects of information events. This paper demonstrates that when factor models are misspecified -- an almost certain reality -- traditional event study estimators produce inconsistent estimates of treatment effects. The bias is particularly severe during volatile periods, over long horizons, and when event timing correlates with market conditions. We derive precise conditions for identification and expressions for asymptotic bias. As an alternative, we propose synthetic control methods that construct replicating portfolios from control securities without imposing specific factor structures. Revisiting four empirical applications, we show that some established findings may reflect model misspecification rather than true treatment effects. While traditional methods remain reliable for short-horizon studies with random event timing, our results suggest caution when interpreting long-horizon or volatile-period event studies and highlight the importance of quasi-experimental designs when available.",
      "pdf_url": "https://arxiv.org/pdf/2511.15123v1",
      "arxiv_url": "http://arxiv.org/abs/2511.15123v1",
      "published": "2025-11-19",
      "categories": [
        "econ.EM",
        "econ.GN",
        "q-fin.GN"
      ]
    },
    {
      "title": "Individualized Prediction Bands in Causal Inference with Continuous Treatments",
      "authors": [
        "Max Sampson",
        "Kung-Sik Chan"
      ],
      "abstract": "Individualized treatments are crucial for optimal decision making and treatment allocation, specifically in personalized medicine based on the estimation of an individual's dose-response curve across a continuum of treatment levels, e.g., drug dosage. Current works focus on conditional mean and median estimates, which are useful but do not provide the full picture. We propose viewing causal inference with a continuous treatment as a covariate shift. This allows us to leverage existing weighted conformal prediction methods with both quantile and point estimates to compute individualized uncertainty quantification for dose-response curves. Our method, individualized prediction bands (IPB), is demonstrated via simulations and a real data analysis, which demonstrates the additional medical expenditure caused by continued smoking for selected individuals. The results demonstrate that IPB provides an effective solution to a gap in individual dose-response uncertainty quantification literature.",
      "pdf_url": "https://arxiv.org/pdf/2511.15075v1",
      "arxiv_url": "http://arxiv.org/abs/2511.15075v1",
      "published": "2025-11-19",
      "categories": [
        "stat.ME",
        "stat.AP"
      ]
    }
  ]
}