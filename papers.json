{
  "last_updated": "2026-01-11T01:04:31.198415",
  "papers": [
    {
      "title": "Robust Reasoning as a Symmetry-Protected Topological Phase",
      "authors": [
        "Ilmo Sung"
      ],
      "abstract": "Large language models suffer from \"hallucinations\"-logical inconsistencies induced by semantic noise. We propose that current architectures operate in a \"Metric Phase,\" where causal order is vulnerable to spontaneous symmetry breaking. Here, we identify robust inference as an effective Symmetry-Protected Topological phase, where logical operations are formally isomorphic to non-Abelian anyon braiding, replacing fragile geometric interpolation with robust topological invariants. Empirically, we demonstrate a sharp topological phase transition: while Transformers and RNNs exhibit gapless decay, our Holonomic Network reveals a macroscopic \"mass gap,\" maintaining invariant fidelity below a critical noise threshold. Furthermore, in a variable-binding task on $S_{10}$ ($3.6 \\times 10^6$ states) representing symbolic manipulation, we demonstrate holonomic generalization: the topological model maintains perfect fidelity extrapolating $100\\times$ beyond training ($L=50 \\to 5000$), consistent with a theoretically indefinite causal horizon, whereas Transformers lose logical coherence. Ablation studies indicate this protection emerges strictly from non-Abelian gauge symmetry. This provides strong evidence for a new universality class for logical reasoning, linking causal stability to the topology of the semantic manifold.",
      "pdf_url": "https://arxiv.org/pdf/2601.05240v1",
      "arxiv_url": "http://arxiv.org/abs/2601.05240v1",
      "published": "2026-01-08",
      "categories": [
        "cs.LG",
        "cond-mat.dis-nn",
        "cs.AI",
        "hep-th"
      ]
    },
    {
      "title": "Vision-Language Introspection: Mitigating Overconfident Hallucinations in MLLMs via Interpretable Bi-Causal Steering",
      "authors": [
        "Shuliang Liu",
        "Songbo Yang",
        "Dong Fang",
        "Sihang Jia",
        "Yuqi Tang",
        "Lingfeng Su",
        "Ruoshui Peng",
        "Yibo Yan",
        "Xin Zou",
        "Xuming Hu"
      ],
      "abstract": "Object hallucination critically undermines the reliability of Multimodal Large Language Models, often stemming from a fundamental failure in cognitive introspection, where models blindly trust linguistic priors over specific visual evidence. Existing mitigations remain limited: contrastive decoding approaches operate superficially without rectifying internal semantic misalignments, while current latent steering methods rely on static vectors that lack instance-specific precision. We introduce Vision-Language Introspection (VLI), a training-free inference framework that simulates a metacognitive self-correction process. VLI first performs Attributive Introspection to diagnose hallucination risks via probabilistic conflict detection and localize the causal visual anchors. It then employs Interpretable Bi-Causal Steering to actively modulate the inference process, dynamically isolating visual evidence from background noise while neutralizing blind confidence through adaptive calibration. VLI achieves state-of-the-art performance on advanced models, reducing object hallucination rates by 12.67% on MMHal-Bench and improving accuracy by 5.8% on POPE.",
      "pdf_url": "https://arxiv.org/pdf/2601.05159v1",
      "arxiv_url": "http://arxiv.org/abs/2601.05159v1",
      "published": "2026-01-08",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Revealing the Truth: Calculating True Values in Causal Inference Simulation Studies via Gaussian Quadrature",
      "authors": [
        "Alex Ocampo",
        "Enrico Giudice",
        "Zachary R. McCaw",
        "Tim P. Morris"
      ],
      "abstract": "Simulation studies are used to understand the properties of statistical methods. A key luxury in many simulation studies is knowledge of the true value (i.e. the estimand) being targeted. With this oracle knowledge in-hand, the researcher conducting the simulation study can assess across repeated realizations of the data how well a given method recovers the truth. In causal inference simulation studies, the truth is rarely a simple parameter of the statistical model chosen to generate the data. Instead, the estimand is often an average treatment effect, marginalized over the distribution of confounders and/or mediators. Luckily, these variables are often generated from common distributions such as the normal, uniform, exponential, or gamma. For all these distributions, Gaussian quadratures provide efficient and accurate calculation for integrands with integral kernels that stem from known probability density functions. We demonstrate through four applications how to use Gaussian quadrature to accurately and efficiently compute the true causal estimand. We also compare the pros and cons of Gauss-Hermite quadrature to Monte Carlo integration approaches, which we use as benchmarks. Overall, we demonstrate that the Gaussian quadrature is an accurate tool with negligible computation time, yet is underused for calculating the true causal estimands in simulation studies.",
      "pdf_url": "https://arxiv.org/pdf/2601.05128v1",
      "arxiv_url": "http://arxiv.org/abs/2601.05128v1",
      "published": "2026-01-08",
      "categories": [
        "stat.ME"
      ]
    },
    {
      "title": "Scalable neural pushbroom architectures for real-time denoising of hyperspectral images onboard satellites",
      "authors": [
        "Ziyao Yi",
        "Davide Piccinini",
        "Diego Valsesia",
        "Tiziano Bianchi",
        "Enrico Magli"
      ],
      "abstract": "The next generation of Earth observation satellites will seek to deploy intelligent models directly onboard the payload in order to minimize the latency incurred by the transmission and processing chain of the ground segment, for time-critical applications. Designing neural architectures for onboard execution, particularly for satellite-based hyperspectral imagers, poses novel challenges due to the unique constraints of this environment and imaging system that are largely unexplored by the traditional computer vision literature. In this paper, we show that this setting requires addressing three competing objectives, namely high-quality inference with low complexity, dynamic power scalability and fault tolerance. We focus on the problem of hyperspectral image denoising, which is a critical task to enable effective downstream inference, and highlights the constraints of the onboard processing scenario. We propose a neural network design that addresses the three aforementioned objectives with several novel contributions. In particular, we propose a mixture of denoisers that can be resilient to radiation-induced faults as well as allowing for time-varying power scaling. Moreover, each denoiser employs an innovative architecture where an image is processed line-by-line in a causal way, with a memory of past lines, in order to match the acquisition process of pushbroom hyperspectral sensors and greatly limit memory requirements. We show that the proposed architecture can run in real-time, i.e., process one line in the time it takes to acquire the next one, on low-power hardware and provide competitive denoising quality with respect to significantly more complex state-of-the-art models. We also show that the power scalability and fault tolerance objectives provide a design space with multiple tradeoffs between those properties and denoising quality.",
      "pdf_url": "https://arxiv.org/pdf/2601.05020v1",
      "arxiv_url": "http://arxiv.org/abs/2601.05020v1",
      "published": "2026-01-08",
      "categories": [
        "eess.IV",
        "cs.CV"
      ]
    },
    {
      "title": "Estimating Causal Effects in Gaussian Linear SCMs with Finite Data",
      "authors": [
        "Aurghya Maiti",
        "Prateek Jain"
      ],
      "abstract": "Estimating causal effects from observational data remains a fundamental challenge in causal inference, especially in the presence of latent confounders. This paper focuses on estimating causal effects in Gaussian Linear Structural Causal Models (GL-SCMs), which are widely used due to their analytical tractability. However, parameter estimation in GL-SCMs is often infeasible with finite data, primarily due to overparameterization. To address this, we introduce the class of Centralized Gaussian Linear SCMs (CGL-SCMs), a simplified yet expressive subclass where exogenous variables follow standardized distributions. We show that CGL-SCMs are equally expressive in terms of causal effect identifiability from observational distributions and present a novel EM-based estimation algorithm that can learn CGL-SCM parameters and estimate identifiable causal effects from finite observational samples. Our theoretical analysis is validated through experiments on synthetic data and benchmark causal graphs, demonstrating that the learned models accurately recover causal distributions.",
      "pdf_url": "https://arxiv.org/pdf/2601.04673v1",
      "arxiv_url": "http://arxiv.org/abs/2601.04673v1",
      "published": "2026-01-08",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ]
    },
    {
      "title": "Scaling Behavior Cloning Improves Causal Reasoning: An Open Model for Real-Time Video Game Playing",
      "authors": [
        "Yuguang Yue",
        "Irakli Salia",
        "Samuel Hunt",
        "Chris Green",
        "Wenzhe Shi",
        "Jonathan J Hunt"
      ],
      "abstract": "Behavior cloning is enjoying a resurgence in popularity as scaling both model and data sizes proves to provide a strong starting point for many tasks of interest. In this work, we introduce an open recipe for training a video game playing foundation model designed for inference in realtime on a consumer GPU. We release all data (8300+ hours of high quality human gameplay), training and inference code, and pretrained checkpoints under an open license. We show that our best model is capable of playing a variety of 3D video games at a level competitive with human play. We use this recipe to systematically examine the scaling laws of behavior cloning to understand how the model's performance and causal reasoning varies with model and data scale. We first show in a simple toy problem that, for some types of causal reasoning, increasing both the amount of training data and the depth of the network results in the model learning a more causal policy. We then systematically study how causality varies with the number of parameters (and depth) and training steps in scaled models of up to 1.2 billion parameters, and we find similar scaling results to what we observe in the toy problem.",
      "pdf_url": "https://arxiv.org/pdf/2601.04575v1",
      "arxiv_url": "http://arxiv.org/abs/2601.04575v1",
      "published": "2026-01-08",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Industrial Data-Service-Knowledge Governance: Toward Integrated and Trusted Intelligence for Industry 5.0",
      "authors": [
        "Hailiang Zhao",
        "Ziqi Wang",
        "Daojiang Hu",
        "Zhiwei Ling",
        "Wenzhuo Qian",
        "Jiahui Zhai",
        "Yuhao Yang",
        "Zhipeng Gao",
        "Mingyi Liu",
        "Kai Di",
        "Xinkui Zhao",
        "Zhongjie Wang",
        "Jianwei Yin",
        "MengChu Zhou",
        "Shuiguang Deng"
      ],
      "abstract": "The convergence of artificial intelligence, cyber-physical systems, and cross-enterprise data ecosystems has propelled industrial intelligence to unprecedented scales. Yet, the absence of a unified trust foundation across data, services, and knowledge layers undermines reliability, accountability, and regulatory compliance in real-world deployments. While existing surveys address isolated aspects, such as data governance, service orchestration, and knowledge representation, none provides a holistic, cross-layer perspective on trustworthiness tailored to industrial settings. To bridge this gap, we present \\textsc{Trisk} (TRusted Industrial Data-Service-Knowledge governance), a novel conceptual and taxonomic framework for trustworthy industrial intelligence. Grounded in a five-dimensional trust model (quality, security, privacy, fairness, and explainability), \\textsc{Trisk} unifies 120+ representative studies along three orthogonal axes: governance scope (data, service, and knowledge), architectural paradigm (centralized, federated, or edge-embedded), and enabling technology (knowledge graphs, zero-trust policies, causal inference, etc.). We systematically analyze how trust propagates across digital layers, identify critical gaps in semantic interoperability, runtime policy enforcement, and operational/information technologies alignment, and evaluate the maturity of current industrial implementations. Finally, we articulate a forward-looking research agenda for Industry 5.0, advocating for an integrated governance fabric that embeds verifiable trust semantics into every layer of the industrial intelligence stack. This survey serves as both a foundational reference for researchers and a practical roadmap for engineers to deploy trustworthy AI in complex and multi-stakeholder environments.",
      "pdf_url": "https://arxiv.org/pdf/2601.04569v1",
      "arxiv_url": "http://arxiv.org/abs/2601.04569v1",
      "published": "2026-01-08",
      "categories": [
        "cs.CE"
      ]
    },
    {
      "title": "Personalization of Large Foundation Models for Health Interventions",
      "authors": [
        "Stefan Konigorski",
        "Johannes E. Vedder",
        "Babajide Alamu Owoyele",
        "İbrahim Özkan"
      ],
      "abstract": "Large foundation models (LFMs) transform healthcare AI in prevention, diagnostics, and treatment. However, whether LFMs can provide truly personalized treatment recommendations remains an open question. Recent research has revealed multiple challenges for personalization, including the fundamental generalizability paradox: models achieving high accuracy in one clinical study perform at chance level in others, demonstrating that personalization and external validity exist in tension. This exemplifies broader contradictions in AI-driven healthcare: the privacy-performance paradox, scale-specificity paradox, and the automation-empathy paradox. As another challenge, the degree of causal understanding required for personalized recommendations, as opposed to mere predictive capacities of LFMs, remains an open question. N-of-1 trials -- crossover self-experiments and the gold standard for individual causal inference in personalized medicine -- resolve these tensions by providing within-person causal evidence while preserving privacy through local experimentation. Despite their impressive capabilities, this paper argues that LFMs cannot replace N-of-1 trials. We argue that LFMs and N-of-1 trials are complementary: LFMs excel at rapid hypothesis generation from population patterns using multimodal data, while N-of-1 trials excel at causal validation for a given individual. We propose a hybrid framework that combines the strengths of both to enable personalization and navigate the identified paradoxes: LFMs generate ranked intervention candidates with uncertainty estimates, which trigger subsequent N-of-1 trials. Clarifying the boundary between prediction and causation and explicitly addressing the paradoxical tensions are essential for responsible AI integration in personalized medicine.",
      "pdf_url": "https://arxiv.org/pdf/2601.03482v1",
      "arxiv_url": "http://arxiv.org/abs/2601.03482v1",
      "published": "2026-01-07",
      "categories": [
        "cs.AI",
        "cs.LG",
        "stat.AP"
      ]
    },
    {
      "title": "Improving operating characteristics of clinical trials by augmenting control arm using propensity score-weighted borrowing-by-parts power prior",
      "authors": [
        "Apu Chandra Das",
        "Sakib Salam",
        "Aninda Roy",
        "Rakhi Chowdhury",
        "Antar Chandra Das",
        "Ashim Chandra Das"
      ],
      "abstract": "Borrowing external data can improve estimation efficiency but may introduce bias when populations differ in covariate distributions or outcome variability. A proper balance needs to be maintained between the two datasets to justify the borrowing. We propose a propensity score weighting borrowing-by-parts power prior (PSW-BPP) that integrates causal covariate adjustment through propensity score weighting with a flexible Bayesian borrowing approach to address these challenges in a unified framework. The proposed approach first applies propensity score weighting to align the covariate distribution of the external data with that of the current study, thereby targeting a common estimand and reducing confounding due to population heterogeneity. The weighted external likelihood is then incorporated into a Bayesian model through a borrowing-by-parts power prior, which allows distinct power parameters for the mean and variance components of the likelihood, enabling differential and calibrated information borrowing. Additionally, we adopt the idea of the minimal plausibility index (mPI) to calculate the power parameters. This separate borrowing provides greater robustness to prior-data conflict compared with traditional power prior methods that impose a single borrowing parameter. We study the operating characteristics of PSW-BPP through extensive simulation and a real data example. Simulation studies demonstrate that PSW-BPP yields more efficient and stable estimation than no borrowing and fixed borrowing, particularly under moderate covariate imbalance and outcome heterogeneity. The proposed framework offers a principled and extensible methodological contribution for Bayesian inference with external data in observational and hybrid study designs.",
      "pdf_url": "https://arxiv.org/pdf/2601.03480v1",
      "arxiv_url": "http://arxiv.org/abs/2601.03480v1",
      "published": "2026-01-07",
      "categories": [
        "stat.ME"
      ]
    },
    {
      "title": "On estimands in target trial emulation",
      "authors": [
        "Edoardo Efrem Gervasoni",
        "Liesbet De Bus",
        "Stijn Vansteelandt",
        "Oliver Dukes"
      ],
      "abstract": "The target trial framework enables causal inference from longitudinal observational data by emulating randomized trials initiated at multiple time points. Precision is often improved by pooling information across trials, with standard models typically assuming - among other things - a time-constant treatment effect. However, this obscures interpretation when the true treatment effect varies, which we argue to be likely as a result of relying on noncollapsible estimands. To address these challenges, this paper introduces a model-free strategy for target trial analysis, centered around the choice of the estimand, rather than model specification. This ensures that treatment effects remain clearly interpretable for well-defined populations even under model misspecification. We propose estimands suitable for different study designs, and develop accompanying G-computation and inverse probability weighted estimators. Applications on simulations and real data on antimicrobial de-escalation in an intensive care unit setting demonstrate the greater clarity and reliability of the proposed methodology over traditional techniques.",
      "pdf_url": "https://arxiv.org/pdf/2601.03377v1",
      "arxiv_url": "http://arxiv.org/abs/2601.03377v1",
      "published": "2026-01-06",
      "categories": [
        "stat.ME"
      ]
    }
  ]
}