{
  "last_updated": "2025-08-08T01:00:03.918434",
  "papers": [
    {
      "title": "Causal Reflection with Language Models",
      "authors": [
        "Abi Aryan",
        "Zac Liu"
      ],
      "abstract": "While LLMs exhibit impressive fluency and factual recall, they struggle with\nrobust causal reasoning, often relying on spurious correlations and brittle\npatterns. Similarly, traditional Reinforcement Learning agents also lack causal\nunderstanding, optimizing for rewards without modeling why actions lead to\noutcomes. We introduce Causal Reflection, a framework that explicitly models\ncausality as a dynamic function over state, action, time, and perturbation,\nenabling agents to reason about delayed and nonlinear effects. Additionally, we\ndefine a formal Reflect mechanism that identifies mismatches between predicted\nand observed outcomes and generates causal hypotheses to revise the agent's\ninternal model. In this architecture, LLMs serve not as black-box reasoners,\nbut as structured inference engines translating formal causal outputs into\nnatural language explanations and counterfactuals. Our framework lays the\ntheoretical groundwork for Causal Reflective agents that can adapt,\nself-correct, and communicate causal understanding in evolving environments.",
      "pdf_url": "http://arxiv.org/pdf/2508.04495v1",
      "arxiv_url": "http://arxiv.org/abs/2508.04495v1",
      "published": "2025-08-06",
      "categories": [
        "cs.LG",
        "cs.CL"
      ]
    },
    {
      "title": "Boosting Visual Knowledge-Intensive Training for LVLMs Through Causality-Driven Visual Object Completion",
      "authors": [
        "Qingguo Hu",
        "Ante Wang",
        "Jia Song",
        "Delai Qiu",
        "Qingsong Liu",
        "Jinsong Su"
      ],
      "abstract": "Large Vision-Language Models (LVLMs) have experienced significant\nadvancements in recent years. However, their performance still falls short in\ntasks requiring deep visual perception, such as identifying subtle differences\nbetween images. A potential cause is the scarcity of visual knowledge in\npopular instruction-tuning corpora, resulting in inadequate visual perception\nand reasoning capabilities. To address this challenge, we introduce a\nself-improvement framework grounded in a novel visual knowledge-intensive task,\n\\underline{C}ausality-driven \\underline{V}isual object \\underline{C}ompletion\n(CVC). This task requires LVLMs to infer the masked object in an image based on\nits \\textit{causal} relationships with the other visible information. We first\nobtain rich examples cheaply through our automated instance construction\npipeline, without relying on sophisticated LVLMs (\\textit{e.g.}, GPT-4V) or\nhuman assistance. Then, LVLMs effectively self-improve through trial and error\nlearning using these created instances. Our experiments demonstrate substantial\ngains across four challenging specialized tasks and four widely-used\ncomprehensive benchmarks. Especially on specialized tasks, our method achieves\nan average improvement of 5.4\\% and 4.0\\% compared to the corresponding\nbaselines when utilizing LLaVA-1.5-7B and LLaVA-1.5-13B, respectively. The code\nis available at https://github.com/XMUDeepLIT/CVC.",
      "pdf_url": "http://arxiv.org/pdf/2508.04453v1",
      "arxiv_url": "http://arxiv.org/abs/2508.04453v1",
      "published": "2025-08-06",
      "categories": [
        "cs.CV"
      ]
    },
    {
      "title": "Causal Reward Adjustment: Mitigating Reward Hacking in External Reasoning via Backdoor Correction",
      "authors": [
        "Ruike Song",
        "Zeen Song",
        "Huijie Guo",
        "Wenwen Qiang"
      ],
      "abstract": "External reasoning systems combine language models with process reward models\n(PRMs) to select high-quality reasoning paths for complex tasks such as\nmathematical problem solving. However, these systems are prone to reward\nhacking, where high-scoring but logically incorrect paths are assigned high\nscores by the PRMs, leading to incorrect answers. From a causal inference\nperspective, we attribute this phenomenon primarily to the presence of\nconfounding semantic features. To address it, we propose Causal Reward\nAdjustment (CRA), a method that mitigates reward hacking by estimating the true\nreward of a reasoning path. CRA trains sparse autoencoders on the PRM's\ninternal activations to recover interpretable features, then corrects\nconfounding by using backdoor adjustment. Experiments on math solving datasets\ndemonstrate that CRA mitigates reward hacking and improves final accuracy,\nwithout modifying the policy model or retraining PRM.",
      "pdf_url": "http://arxiv.org/pdf/2508.04216v1",
      "arxiv_url": "http://arxiv.org/abs/2508.04216v1",
      "published": "2025-08-06",
      "categories": [
        "cs.LG"
      ]
    },
    {
      "title": "Robust estimation of causal dose-response relationship using exposure data with dose as an instrumental variable",
      "authors": [
        "Jixian Wang",
        "Zhiwei Zhang",
        "Ram Tiwari"
      ],
      "abstract": "An accurate estimation of the dose-response relationship is important to\ndetermine the optimal dose. For this purpose, a dose finding trial in which\nsubjects are randomized to a few fixed dose levels is the most commonly used\ndesign. Often, the estimation uses response data only, although drug exposure\ndata are often obtained during the trial. The use of exposure data to improve\nthis estimation is difficult, as exposure-response relationships are typically\nsubject to confounding bias even in a randomized trial. We propose a robust\napproach to estimate the dose-response relationship without assuming a true\nexposure-response model, using dose as an instrumental variable. Our approach\ncombines the control variable approach in causal inference with unobserved\nconfounding factors and the ANCOVA adjustment of randomized trials. The\napproach presented uses working models for dose-exposure-response data, but\nthey are robust to model misspecification and remain consistent when the\nworking models are far from correct. The asymptotic properties of the proposed\napproach are also examined. A simulation study is performed to evaluate the\nperformance of the proposed approach. For illustration, the approach is used to\na Car-T trial with randomized doses.",
      "pdf_url": "http://arxiv.org/pdf/2508.04215v1",
      "arxiv_url": "http://arxiv.org/abs/2508.04215v1",
      "published": "2025-08-06",
      "categories": [
        "stat.ME"
      ]
    },
    {
      "title": "Can Large Language Models Adequately Perform Symbolic Reasoning Over Time Series?",
      "authors": [
        "Zewen Liu",
        "Juntong Ni",
        "Xianfeng Tang",
        "Max S. Y. Lau",
        "Wei Jin"
      ],
      "abstract": "Uncovering hidden symbolic laws from time series data, as an aspiration\ndating back to Kepler's discovery of planetary motion, remains a core challenge\nin scientific discovery and artificial intelligence. While Large Language\nModels show promise in structured reasoning tasks, their ability to infer\ninterpretable, context-aligned symbolic structures from time series data is\nstill underexplored. To systematically evaluate this capability, we introduce\nSymbolBench, a comprehensive benchmark designed to assess symbolic reasoning\nover real-world time series across three tasks: multivariate symbolic\nregression, Boolean network inference, and causal discovery. Unlike prior\nefforts limited to simple algebraic equations, SymbolBench spans a diverse set\nof symbolic forms with varying complexity. We further propose a unified\nframework that integrates LLMs with genetic programming to form a closed-loop\nsymbolic reasoning system, where LLMs act both as predictors and evaluators.\nOur empirical results reveal key strengths and limitations of current models,\nhighlighting the importance of combining domain knowledge, context alignment,\nand reasoning structure to improve LLMs in automated scientific discovery.",
      "pdf_url": "http://arxiv.org/pdf/2508.03963v1",
      "arxiv_url": "http://arxiv.org/abs/2508.03963v1",
      "published": "2025-08-05",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "The Regression Discontinuity Design in Medical Science",
      "authors": [
        "Matias D. Cattaneo",
        "Rocio Titiunik"
      ],
      "abstract": "This article provides an introduction to the Regression Discontinuity (RD)\ndesign, and its application to empirical research in the medical sciences.\nWhile the main focus of this article is on causal interpretation, key concepts\nof estimation and inference are also briefly mentioned. A running medical\nempirical example is provided.",
      "pdf_url": "http://arxiv.org/pdf/2508.03878v1",
      "arxiv_url": "http://arxiv.org/abs/2508.03878v1",
      "published": "2025-08-05",
      "categories": [
        "stat.ME",
        "econ.EM",
        "stat.AP"
      ]
    },
    {
      "title": "EmbedGrad: Gradient-Based Prompt Optimization in Embedding Space for Large Language Models",
      "authors": [
        "Xiaoming Hou",
        "Jiquan Zhang",
        "Zibin Lin",
        "DaCheng Tao",
        "Shengli Zhang"
      ],
      "abstract": "Effectively adapting powerful pretrained foundation models to diverse tasks\nremains a key challenge in AI deployment. Current approaches primarily follow\ntwo paradigms:discrete optimization of text prompts through prompt engineering,\nor continuous adaptation via additional trainable parameters. Both exhibit\nlimitations-discrete methods lack refinement precision while parameter-based\ntechniques increase complexity and reduce interpretability. To address these\nconstraints, we propose EmbedGrad, a novel framework that optimizes text prompt\nembeddings through gradient-based refinement. Our approach uniquely decouples\ntraining from deployment:during optimization,labeled examples guide precise\nembedding adjustments while preserving semantic meaning; during inference, only\noptimized embeddings integrate with user queries. This enables fine-grained\ncalibration impossible in text space, such as enhancing the reasoning\ncapability of prompts like please reason step by step. Comprehensive\nevaluations across mathematical reasoning, sentiment analysis, and causal\njudgment tasks demonstrate EmbedGrad's effectiveness:optimizing this reasoning\nprompt for Qwen2.5-Math-1.5B increased accuracy from 14.74\\% to 58.96\\% on\nmathematical problems. Consistent improvements were observed across model\nscales (0.5B-14B) and all tasks, with particularly significant gains for\nsmaller models on complex problems like causal judgment. By bridging prompt\nengineering and parameter efficiency without architectural changes, our work\nestablishes embedding refinement as a powerful new paradigm for task\nadaptation.",
      "pdf_url": "http://arxiv.org/pdf/2508.03533v1",
      "arxiv_url": "http://arxiv.org/abs/2508.03533v1",
      "published": "2025-08-05",
      "categories": [
        "cs.CL"
      ]
    },
    {
      "title": "Causal identification with $Y_0$",
      "authors": [
        "Charles Tapley Hoyt",
        "Craig Bakker",
        "Richard J. Callahan",
        "Joseph Cottam",
        "August George",
        "Benjamin M. Gyori",
        "Haley M. Hummel",
        "Nathaniel Merrill",
        "Sara Mohammad Taheri",
        "Pruthvi Prakash Navada",
        "Marc-Antoine Parent",
        "Adam Rupe",
        "Olga Vitek",
        "Jeremy Zucker"
      ],
      "abstract": "We present the $Y_0$ Python package, which implements causal identification\nalgorithms that apply interventional, counterfactual, and transportability\nqueries to data from (randomized) controlled trials, observational studies, or\nmixtures thereof. $Y_0$ focuses on the qualitative investigation of causation,\nhelping researchers determine whether a causal relationship can be estimated\nfrom available data before attempting to estimate how strong that relationship\nis. Furthermore, $Y_0$ provides guidance on how to transform the causal query\ninto a symbolic estimand that can be non-parametrically estimated from the\navailable data. $Y_0$ provides a domain-specific language for representing\ncausal queries and estimands as symbolic probabilistic expressions, tools for\nrepresenting causal graphical models with unobserved confounders, such as\nacyclic directed mixed graphs (ADMGs), and implementations of numerous\nidentification algorithms from the recent causal inference literature. The\n$Y_0$ source code can be found under the MIT License at\nhttps://github.com/y0-causal-inference/y0 and it can be installed with pip\ninstall y0.",
      "pdf_url": "http://arxiv.org/pdf/2508.03167v1",
      "arxiv_url": "http://arxiv.org/abs/2508.03167v1",
      "published": "2025-08-05",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Bayesian Sensitivity Analyses for Policy Evaluation with Difference-in-Differences under Violations of Parallel Trends",
      "authors": [
        "Seong Woo Han",
        "Nandita Mitra",
        "Gary Hettinger",
        "Arman Oganisian"
      ],
      "abstract": "Violations of the parallel trends assumption pose significant challenges for\ncausal inference in difference-in-differences (DiD) studies, especially in\npolicy evaluations where pre-treatment dynamics and external shocks may bias\nestimates. In this work, we propose a Bayesian DiD framework to allow us to\nestimate the effect of policies when parallel trends is violated. To address\npotential deviations from the parallel trends assumption, we introduce a formal\nsensitivity parameter representing the extent of the violation, specify an\nautoregressive AR(1) prior on this term to robustly model temporal correlation,\nand explore a range of prior specifications - including fixed, fully Bayesian,\nand empirical Bayes (EB) approaches calibrated from pre-treatment data. By\nsystematically comparing posterior treatment effect estimates across prior\nconfigurations when evaluating Philadelphia's sweetened beverage tax using\nBaltimore as a control, we show how Bayesian sensitivity analyses support\nrobust and interpretable policy conclusions under violations of parallel\ntrends.",
      "pdf_url": "http://arxiv.org/pdf/2508.02970v1",
      "arxiv_url": "http://arxiv.org/abs/2508.02970v1",
      "published": "2025-08-05",
      "categories": [
        "stat.ME"
      ]
    },
    {
      "title": "Sensitivity of weighted least squares estimators to omitted variables",
      "authors": [
        "Leonard Wainstein",
        "Chad Hazlett"
      ],
      "abstract": "This paper introduces tools for assessing the sensitivity, to unobserved\nconfounding, of a common estimator of the causal effect of a treatment on an\noutcome that employs weights: the weighted linear regression of the outcome on\nthe treatment and observed covariates. We demonstrate through the omitted\nvariable bias framework that the bias of this estimator is a function of two\nintuitive sensitivity parameters: (i) the proportion of weighted variance in\nthe treatment that unobserved confounding explains given the covariates and\n(ii) the proportion of weighted variance in the outcome that unobserved\nconfounding explains given the covariates and the treatment, i.e., two weighted\npartial $R^2$ values. Following previous work, we define sensitivity statistics\nthat lend themselves well to routine reporting, and derive formal bounds on the\nstrength of the unobserved confounding with (a multiple of) the strength of\nselect dimensions of the covariates, which help the user determine if\nunobserved confounding that would alter one's conclusions is plausible. We also\npropose tools for adjusted inference. A key choice we make is to examine only\nhow the (weighted) outcome model is influenced by unobserved confounding,\nrather than examining how the weights have been biased by omitted confounding.\nOne benefit of this choice is that the resulting tool applies with any weights\n(e.g., inverse-propensity score, matching, or covariate balancing weights).\nAnother benefit is that we can rely on simple omitted variable bias approaches\nthat, for example, impose no distributional assumptions on the data or\nunobserved confounding, and can address bias from misspecification in the\nobserved data. We make these tools available in the weightsense package for the\nR computing language.",
      "pdf_url": "http://arxiv.org/pdf/2508.02954v1",
      "arxiv_url": "http://arxiv.org/abs/2508.02954v1",
      "published": "2025-08-04",
      "categories": [
        "stat.ME"
      ]
    }
  ]
}