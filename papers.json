{
  "last_updated": "2025-06-13T00:55:00.703854",
  "papers": [
    {
      "title": "Multiverse: Your Language Models Secretly Decide How to Parallelize and Merge Generation",
      "authors": [
        "Xinyu Yang",
        "Yuwei An",
        "Hongyi Liu",
        "Tianqi Chen",
        "Beidi Chen"
      ],
      "abstract": "Autoregressive Large Language Models (AR-LLMs) frequently exhibit implicit\nparallelism in sequential generation. Inspired by this, we introduce\nMultiverse, a new generative model that enables natively parallel generation.\nMultiverse internalizes a MapReduce paradigm, generating automatically through\nthree stages: (i) a Map stage for adaptive task decomposition, (ii) a Process\nstage for parallel subtask execution, and (iii) a Reduce stage for lossless\nresult synthesis. Next, we build a real-world Multiverse reasoning model with\nco-design of data, algorithm, and system, enabling rapid and seamless transfer\nfrom frontier AR-LLMs. Starting from sequential reasoning chains, we create\nMultiverse 1K by converting them into structured training data using an\nautomated LLM-assisted pipeline, avoiding costly human annotations.\nAlgorithmically, we design Multiverse Attention to separate parallel reasoning\nsteps while keeping compatibility with causal attention for efficient training.\nSystematically, we implement Multiverse Engine to enable parallel inference. It\nfeatures a dedicated scheduler that dynamically switches between sequential and\nparallel generation, triggered directly by the model. After a 3-hour\nfine-tuning with 1K examples, our Multiverse-32B stands as the only\nopen-sourced non-AR model achieving performance on par with leading AR-LLMs of\nthe same scale, evidenced by AIME24 & 25 scores of 54% and 46%, respectively.\nMoreover, our budget control experiments show that Multiverse-32B exhibits\nsuperior scaling, outperforming AR-LLMs by 1.87% on average using the same\ncontext length. Such scaling further leads to practical efficiency gain,\nachieving up to 2x speedup across varying batch sizes. We have open-sourced the\nentire Multiverse ecosystem, including data, model weights, engine, supporting\ntools, as well as complete data curation prompts and detailed training and\nevaluation recipes.",
      "pdf_url": "http://arxiv.org/pdf/2506.09991v1",
      "arxiv_url": "http://arxiv.org/abs/2506.09991v1",
      "published": "2025-06-11",
      "categories": [
        "cs.LG"
      ]
    },
    {
      "title": "Causal Sufficiency and Necessity Improves Chain-of-Thought Reasoning",
      "authors": [
        "Xiangning Yu",
        "Zhuohan Wang",
        "Linyi Yang",
        "Haoxuan Li",
        "Anjie Liu",
        "Xiao Xue",
        "Jun Wang",
        "Mengyue Yang"
      ],
      "abstract": "Chain-of-Thought (CoT) prompting plays an indispensable role in endowing\nlarge language models (LLMs) with complex reasoning capabilities. However, CoT\ncurrently faces two fundamental challenges: (1) Sufficiency, which ensures that\nthe generated intermediate inference steps comprehensively cover and\nsubstantiate the final conclusion; and (2) Necessity, which identifies the\ninference steps that are truly indispensable for the soundness of the resulting\nanswer. We propose a causal framework that characterizes CoT reasoning through\nthe dual lenses of sufficiency and necessity. Incorporating causal Probability\nof Sufficiency and Necessity allows us not only to determine which steps are\nlogically sufficient or necessary to the prediction outcome, but also to\nquantify their actual influence on the final reasoning outcome under different\nintervention scenarios, thereby enabling the automated addition of missing\nsteps and the pruning of redundant ones. Extensive experimental results on\nvarious mathematical and commonsense reasoning benchmarks confirm substantial\nimprovements in reasoning efficiency and reduced token usage without\nsacrificing accuracy. Our work provides a promising direction for improving LLM\nreasoning performance and cost-effectiveness.",
      "pdf_url": "http://arxiv.org/pdf/2506.09853v1",
      "arxiv_url": "http://arxiv.org/abs/2506.09853v1",
      "published": "2025-06-11",
      "categories": [
        "cs.CL",
        "cs.AI",
        "math.ST",
        "stat.ME",
        "stat.TH"
      ]
    },
    {
      "title": "Benchmarking Debiasing Methods for LLM-based Parameter Estimates",
      "authors": [
        "Nicolas Audinet de Pieuchon",
        "Adel Daoud",
        "Connor T. Jerzak",
        "Moa Johansson",
        "Richard Johansson"
      ],
      "abstract": "Large language models (LLMs) offer an inexpensive yet powerful way to\nannotate text, but are often inconsistent when compared with experts. These\nerrors can bias downstream estimates of population parameters such as\nregression coefficients and causal effects. To mitigate this bias, researchers\nhave developed debiasing methods such as Design-based Supervised Learning (DSL)\nand Prediction-Powered Inference (PPI), which promise valid estimation by\ncombining LLM annotations with a limited number of expensive expert\nannotations. Although these methods produce consistent estimates under\ntheoretical assumptions, it is unknown how they compare in finite samples of\nsizes encountered in applied research. We make two contributions: First, we\nstudy how each method's performance scales with the number of expert\nannotations, highlighting regimes where LLM bias or limited expert labels\nsignificantly affect results. Second, we compare DSL and PPI across a range of\ntasks, finding that although both achieve low bias with large datasets, DSL\noften outperforms PPI on bias reduction and empirical efficiency, but its\nperformance is less consistent across datasets. Our findings indicate that\nthere is a bias-variance tradeoff at the level of debiasing methods, calling\nfor more research on developing metrics for quantifying their efficiency in\nfinite samples.",
      "pdf_url": "http://arxiv.org/pdf/2506.09627v1",
      "arxiv_url": "http://arxiv.org/abs/2506.09627v1",
      "published": "2025-06-11",
      "categories": [
        "cs.CL"
      ]
    },
    {
      "title": "STOAT: Spatial-Temporal Probabilistic Causal Inference Network",
      "authors": [
        "Yang Yang",
        "Du Yin",
        "Hao Xue",
        "Flora Salim"
      ],
      "abstract": "Spatial-temporal causal time series (STC-TS) involve region-specific temporal\nobservations driven by causally relevant covariates and interconnected across\ngeographic or network-based spaces. Existing methods often model spatial and\ntemporal dynamics independently and overlook causality-driven probabilistic\nforecasting, limiting their predictive power. To address this, we propose STOAT\n(Spatial-Temporal Probabilistic Causal Inference Network), a novel framework\nfor probabilistic forecasting in STC-TS. The proposed method extends a causal\ninference approach by incorporating a spatial relation matrix that encodes\ninterregional dependencies (e.g. proximity or connectivity), enabling spatially\ninformed causal effect estimation. The resulting latent series are processed by\ndeep probabilistic models to estimate the parameters of the distributions,\nenabling calibrated uncertainty modeling. We further explore multiple output\ndistributions (e.g., Gaussian, Student's-$t$, Laplace) to capture\nregion-specific variability. Experiments on COVID-19 data across six countries\ndemonstrate that STOAT outperforms state-of-the-art probabilistic forecasting\nmodels (DeepAR, DeepVAR, Deep State Space Model, etc.) in key metrics,\nparticularly in regions with strong spatial dependencies. By bridging causal\ninference and geospatial probabilistic forecasting, STOAT offers a\ngeneralizable framework for complex spatial-temporal tasks, such as epidemic\nmanagement.",
      "pdf_url": "http://arxiv.org/pdf/2506.09544v2",
      "arxiv_url": "http://arxiv.org/abs/2506.09544v2",
      "published": "2025-06-11",
      "categories": [
        "cs.LG"
      ]
    },
    {
      "title": "Mitigating Spurious Correlations in LLMs via Causality-Aware Post-Training",
      "authors": [
        "Shurui Gui",
        "Shuiwang Ji"
      ],
      "abstract": "While large language models (LLMs) have demonstrated remarkable capabilities\nin language modeling, recent studies reveal that they often fail on\nout-of-distribution (OOD) samples due to spurious correlations acquired during\npre-training. Here, we aim to mitigate such spurious correlations through\ncausality-aware post-training (CAPT). By decomposing a biased prediction into\ntwo unbiased steps, known as \\textit{event estimation} and \\textit{event\nintervention}, we reduce LLMs' pre-training biases without incurring additional\nfine-tuning biases, thus enhancing the model's generalization ability.\nExperiments on the formal causal inference benchmark CLadder and the logical\nreasoning dataset PrOntoQA show that 3B-scale language models fine-tuned with\nCAPT can outperform both traditional SFT and larger LLMs on in-distribution\n(ID) and OOD tasks using only 100 ID fine-tuning samples, demonstrating the\neffectiveness and sample efficiency of CAPT.",
      "pdf_url": "http://arxiv.org/pdf/2506.09433v1",
      "arxiv_url": "http://arxiv.org/abs/2506.09433v1",
      "published": "2025-06-11",
      "categories": [
        "cs.LG"
      ]
    },
    {
      "title": "G-Sim: Generative Simulations with Large Language Models and Gradient-Free Calibration",
      "authors": [
        "Samuel Holt",
        "Max Ruiz Luyten",
        "Antonin Berthon",
        "Mihaela van der Schaar"
      ],
      "abstract": "Constructing robust simulators is essential for asking \"what if?\" questions\nand guiding policy in critical domains like healthcare and logistics. However,\nexisting methods often struggle, either failing to generalize beyond historical\ndata or, when using Large Language Models (LLMs), suffering from inaccuracies\nand poor empirical alignment. We introduce G-Sim, a hybrid framework that\nautomates simulator construction by synergizing LLM-driven structural design\nwith rigorous empirical calibration. G-Sim employs an LLM in an iterative loop\nto propose and refine a simulator's core components and causal relationships,\nguided by domain knowledge. This structure is then grounded in reality by\nestimating its parameters using flexible calibration techniques. Specifically,\nG-Sim can leverage methods that are both likelihood-free and gradient-free with\nrespect to the simulator, such as gradient-free optimization for direct\nparameter estimation or simulation-based inference for obtaining a posterior\ndistribution over parameters. This allows it to handle non-differentiable and\nstochastic simulators. By integrating domain priors with empirical evidence,\nG-Sim produces reliable, causally-informed simulators, mitigating\ndata-inefficiency and enabling robust system-level interventions for complex\ndecision-making.",
      "pdf_url": "http://arxiv.org/pdf/2506.09272v1",
      "arxiv_url": "http://arxiv.org/abs/2506.09272v1",
      "published": "2025-06-10",
      "categories": [
        "cs.LG",
        "stat.ML",
        "68T05, 68U20, 62F15",
        "I.2.6; I.6.5; G.3"
      ]
    },
    {
      "title": "Longitudinal weighted and trimmed treatment effects with flip interventions",
      "authors": [
        "Alec McClean",
        "Alexander W. Levis",
        "Nicholas Williams",
        "Ivan Diaz"
      ],
      "abstract": "Weighting and trimming are popular methods for addressing positivity\nviolations in causal inference. While well-studied with single-timepoint data,\nstandard methods do not easily generalize to address non-baseline positivity\nviolations in longitudinal data, and remain vulnerable to such violations. In\nthis paper, we extend weighting and trimming to longitudinal data via\nstochastic ``flip'' interventions, which maintain the treatment status of\nsubjects who would have received the target treatment, and flip others'\ntreatment to the target with probability equal to their weight (e.g., overlap\nweight, trimming indicator). We first show, in single-timepoint data, that flip\ninterventions yield a large class of weighted average treatment effects,\nascribing a novel policy interpretation to these popular weighted estimands.\nWith longitudinal data, we then show that flip interventions provide\ninterpretable weighting or trimming on non-baseline covariates and, crucially,\nyield effects that are identifiable under arbitrary positivity violations.\nMoreover, we demonstrate that flip interventions are policy-relevant since they\ncould be implemented in practice. By contrast, we show that alternative\napproaches for weighting on non-baseline covariates fail to achieve this\nproperty. We derive flexible and efficient estimators based on efficient\ninfluence functions when the weight is a smooth function of the propensity\nscore. Namely, we construct multiply robust-style and sequentially doubly\nrobust-style estimators that achieve root-n consistency and asymptotic\nnormality under nonparametric conditions. Finally, we demonstrate our methods\nthrough an analysis of the effect of union membership on earnings.",
      "pdf_url": "http://arxiv.org/pdf/2506.09188v1",
      "arxiv_url": "http://arxiv.org/abs/2506.09188v1",
      "published": "2025-06-10",
      "categories": [
        "stat.ME"
      ]
    },
    {
      "title": "Revolutionizing Clinical Trials: A Manifesto for AI-Driven Transformation",
      "authors": [
        "Mihaela van der Schaar",
        "Richard Peck",
        "Eoin McKinney",
        "Jim Weatherall",
        "Stuart Bailey",
        "Justine Rochon",
        "Chris Anagnostopoulos",
        "Pierre Marquet",
        "Anthony Wood",
        "Nicky Best",
        "Harry Amad",
        "Julianna Piskorz",
        "Krzysztof Kacprzyk",
        "Rafik Salama",
        "Christina Gunther",
        "Francesca Frau",
        "Antoine Pugeat",
        "Ramon Hernandez"
      ],
      "abstract": "This manifesto represents a collaborative vision forged by leaders in\npharmaceuticals, consulting firms, clinical research, and AI. It outlines a\nroadmap for two AI technologies - causal inference and digital twins - to\ntransform clinical trials, delivering faster, safer, and more personalized\noutcomes for patients. By focusing on actionable integration within existing\nregulatory frameworks, we propose a way forward to revolutionize clinical\nresearch and redefine the gold standard for clinical trials using AI.",
      "pdf_url": "http://arxiv.org/pdf/2506.09102v1",
      "arxiv_url": "http://arxiv.org/abs/2506.09102v1",
      "published": "2025-06-10",
      "categories": [
        "cs.CY",
        "cs.AI"
      ]
    },
    {
      "title": "Paths to Causality: Finding Informative Subgraphs Within Knowledge Graphs for Knowledge-Based Causal Discovery",
      "authors": [
        "Yuni Susanti",
        "Michael Färber"
      ],
      "abstract": "Inferring causal relationships between variable pairs is crucial for\nunderstanding multivariate interactions in complex systems. Knowledge-based\ncausal discovery -- which involves inferring causal relationships by reasoning\nover the metadata of variables (e.g., names or textual context) -- offers a\ncompelling alternative to traditional methods that rely on observational data.\nHowever, existing methods using Large Language Models (LLMs) often produce\nunstable and inconsistent results, compromising their reliability for causal\ninference. To address this, we introduce a novel approach that integrates\nKnowledge Graphs (KGs) with LLMs to enhance knowledge-based causal discovery.\nOur approach identifies informative metapath-based subgraphs within KGs and\nfurther refines the selection of these subgraphs using Learning-to-Rank-based\nmodels. The top-ranked subgraphs are then incorporated into zero-shot prompts,\nimproving the effectiveness of LLMs in inferring the causal relationship.\nExtensive experiments on biomedical and open-domain datasets demonstrate that\nour method outperforms most baselines by up to 44.4 points in F1 scores,\nevaluated across diverse LLMs and KGs. Our code and datasets are available on\nGitHub: https://github.com/susantiyuni/path-to-causality",
      "pdf_url": "http://arxiv.org/pdf/2506.08771v1",
      "arxiv_url": "http://arxiv.org/abs/2506.08771v1",
      "published": "2025-06-10",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.IR",
        "cs.LG"
      ]
    },
    {
      "title": "CC-RAG: Structured Multi-Hop Reasoning via Theme-Based Causal Graphs",
      "authors": [
        "Jash Rajesh Parekh",
        "Pengcheng Jiang",
        "Jiawei Han"
      ],
      "abstract": "Understanding cause and effect relationships remains a formidable challenge\nfor Large Language Models (LLMs), particularly in specialized domains where\nreasoning requires more than surface-level correlations. Retrieval-Augmented\nGeneration (RAG) improves factual accuracy, but standard RAG pipelines treat\nevidence as flat context, lacking the structure required to model true causal\ndependencies. We introduce Causal-Chain RAG (CC-RAG), a novel approach that\nintegrates zero-shot triple extraction and theme-aware graph chaining into the\nRAG pipeline, enabling structured multi-hop inference. Given a domain specific\ncorpus, CC-RAG constructs a Directed Acyclic Graph (DAG) of <cause, relation,\neffect> triples and uses forward/backward chaining to guide structured answer\ngeneration. Experiments on two real-world domains: Bitcoin price fluctuations\nand Gaucher disease, show that CC-RAG outperforms standard RAG and zero-shot\nLLMs in chain similarity, information density, and lexical diversity. Both\nLLM-as-a-Judge and human evaluations consistently favor CC-RAG. Our results\ndemonstrate that explicitly modeling causal structure enables LLMs to generate\nmore accurate and interpretable responses, especially in specialized domains\nwhere flat retrieval fails.",
      "pdf_url": "http://arxiv.org/pdf/2506.08364v2",
      "arxiv_url": "http://arxiv.org/abs/2506.08364v2",
      "published": "2025-06-10",
      "categories": [
        "cs.CL"
      ]
    }
  ]
}