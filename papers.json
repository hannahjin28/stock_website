{
  "last_updated": "2025-03-25T00:50:10.657901",
  "papers": [
    {
      "title": "Calibration Strategies for Robust Causal Estimation: Theoretical and Empirical Insights on Propensity Score Based Estimators",
      "authors": [
        "Jan Rabenseifner",
        "Sven Klaassen",
        "Jannis Kueck",
        "Philipp Bach"
      ],
      "abstract": "The partitioning of data for estimation and calibration critically impacts\nthe performance of propensity score based estimators like inverse probability\nweighting (IPW) and double/debiased machine learning (DML) frameworks. We\nextend recent advances in calibration techniques for propensity score\nestimation, improving the robustness of propensity scores in challenging\nsettings such as limited overlap, small sample sizes, or unbalanced data. Our\ncontributions are twofold: First, we provide a theoretical analysis of the\nproperties of calibrated estimators in the context of DML. To this end, we\nrefine existing calibration frameworks for propensity score models, with a\nparticular emphasis on the role of sample-splitting schemes in ensuring valid\ncausal inference. Second, through extensive simulations, we show that\ncalibration reduces variance of inverse-based propensity score estimators while\nalso mitigating bias in IPW, even in small-sample regimes. Notably, calibration\nimproves stability for flexible learners (e.g., gradient boosting) while\npreserving the doubly robust properties of DML. A key insight is that, even\nwhen methods perform well without calibration, incorporating a calibration step\ndoes not degrade performance, provided that an appropriate sample-splitting\napproach is chosen.",
      "pdf_url": "http://arxiv.org/pdf/2503.17290v1",
      "arxiv_url": "http://arxiv.org/abs/2503.17290v1",
      "published": "2025-03-21",
      "categories": [
        "stat.ML",
        "cs.LG",
        "econ.EM",
        "stat.ME"
      ]
    },
    {
      "title": "Casual Inference via Style Bias Deconfounding for Domain Generalization",
      "authors": [
        "Jiaxi Li",
        "Di Lin",
        "Hao Chen",
        "Hongying Liu",
        "Liang Wan",
        "Wei Feng"
      ],
      "abstract": "Deep neural networks (DNNs) often struggle with out-of-distribution data,\nlimiting their reliability in diverse realworld applications. To address this\nissue, domain generalization methods have been developed to learn\ndomain-invariant features from single or multiple training domains, enabling\ngeneralization to unseen testing domains. However, existing approaches usually\noverlook the impact of style frequency within the training set. This oversight\npredisposes models to capture spurious visual correlations caused by style\nconfounding factors, rather than learning truly causal representations, thereby\nundermining inference reliability. In this work, we introduce Style\nDeconfounding Causal Learning (SDCL), a novel causal inference-based framework\ndesigned to explicitly address style as a confounding factor. Our approaches\nbegins with constructing a structural causal model (SCM) tailored to the domain\ngeneralization problem and applies a backdoor adjustment strategy to account\nfor style influence. Building on this foundation, we design a style-guided\nexpert module (SGEM) to adaptively clusters style distributions during\ntraining, capturing the global confounding style. Additionally, a back-door\ncausal learning module (BDCL) performs causal interventions during feature\nextraction, ensuring fair integration of global confounding styles into sample\npredictions, effectively reducing style bias. The SDCL framework is highly\nversatile and can be seamlessly integrated with state-of-the-art data\naugmentation techniques. Extensive experiments across diverse natural and\nmedical image recognition tasks validate its efficacy, demonstrating superior\nperformance in both multi-domain and the more challenging single-domain\ngeneralization scenarios.",
      "pdf_url": "http://arxiv.org/pdf/2503.16852v1",
      "arxiv_url": "http://arxiv.org/abs/2503.16852v1",
      "published": "2025-03-21",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Feedback-augmented Non-homogeneous Hidden Markov Models for Longitudinal Causal Inference",
      "authors": [
        "Jouni Helske"
      ],
      "abstract": "Hidden Markov models are widely used for modeling sequential data but\ntypically have limited applicability in observational causal inference due to\ntheir strong conditional independence assumptions. I introduce\nfeedback-augmented non-homogeneous hidden Markov model (FAN-HMM), which\nincorporate time-varying covariates and feedback mechanisms from past\nobservations to latent states and future responses. Integrating these models\nwith the structural causal model framework allows flexible causal inference in\nlongitudinal data with time-varying unobserved heterogeneity and multiple\ncausal pathways. I show how, in a common case of categorical response\nvariables, long-term causal effects can be estimated efficiently without the\nneed for simulating counterfactual trajectories. Using simulation experiments,\nI study the performance of FAN-HMM under the common misspecification of the\nnumber of latent states, and finally apply the proposed approach to estimate\nthe effect of the 2013 parental leave reform on fathers' paternal leave uptake\nin Finnish workplaces.",
      "pdf_url": "http://arxiv.org/pdf/2503.16014v1",
      "arxiv_url": "http://arxiv.org/abs/2503.16014v1",
      "published": "2025-03-20",
      "categories": [
        "stat.ME"
      ]
    },
    {
      "title": "Critical review of patient outcome study in head and neck cancer radiotherapy",
      "authors": [
        "Jingyuan Chen",
        "Yunze Yang",
        "Chenbin Liu",
        "Hongying Feng",
        "Jason M. Holmes",
        "Lian Zhang",
        "Steven J. Frank",
        "Charles B. Simone II",
        "Daniel J. Ma",
        "Samir H. Patel",
        "Wei Liu"
      ],
      "abstract": "Rapid technological advances in radiation therapy have significantly improved\ndose delivery and tumor control for head and neck cancers. However,\ntreatment-related toxicities caused by high-dose exposure to critical\nstructures remain a significant clinical challenge, underscoring the need for\naccurate prediction of clinical outcomes-encompassing both tumor control and\nadverse events (AEs). This review critically evaluates the evolution of\ndata-driven approaches in predicting patient outcomes in head and neck cancer\npatients treated with radiation therapy, from traditional dose-volume\nconstraints to cutting-edge artificial intelligence (AI) and causal inference\nframework. The integration of linear energy transfer in patient outcomes study,\nwhich has uncovered critical mechanisms behind unexpected toxicity, was also\nintroduced for proton therapy. Three transformative methodological advances are\nreviewed: radiomics, AI-based algorithms, and causal inference frameworks.\nWhile radiomics has enabled quantitative characterization of medical images, AI\nmodels have demonstrated superior capability than traditional models. However,\nthe field faces significant challenges in translating statistical correlations\nfrom real-world data into interventional clinical insights. We highlight that\nhow causal inference methods can bridge this gap by providing a rigorous\nframework for identifying treatment effects. Looking ahead, we envision that\ncombining these complementary approaches, especially the interventional\nprediction models, will enable more personalized treatment strategies,\nultimately improving both tumor control and quality of life for head and neck\ncancer patients treated with radiation therapy.",
      "pdf_url": "http://arxiv.org/pdf/2503.15691v1",
      "arxiv_url": "http://arxiv.org/abs/2503.15691v1",
      "published": "2025-03-19",
      "categories": [
        "physics.med-ph"
      ]
    },
    {
      "title": "R$^2$: A LLM Based Novel-to-Screenplay Generation Framework with Causal Plot Graphs",
      "authors": [
        "Zefeng Lin",
        "Yi Xiao",
        "Zhiqiang Mo",
        "Qifan Zhang",
        "Jie Wang",
        "Jiayang Chen",
        "Jiajing Zhang",
        "Hui Zhang",
        "Zhengyi Liu",
        "Xianyong Fang",
        "Xiaohua Xu"
      ],
      "abstract": "Automatically adapting novels into screenplays is important for the TV, film,\nor opera industries to promote products with low costs. The strong performances\nof large language models (LLMs) in long-text generation call us to propose a\nLLM based framework Reader-Rewriter (R$^2$) for this task. However, there are\ntwo fundamental challenges here. First, the LLM hallucinations may cause\ninconsistent plot extraction and screenplay generation. Second, the\ncausality-embedded plot lines should be effectively extracted for coherent\nrewriting. Therefore, two corresponding tactics are proposed: 1) A\nhallucination-aware refinement method (HAR) to iteratively discover and\neliminate the affections of hallucinations; and 2) a causal plot-graph\nconstruction method (CPC) based on a greedy cycle-breaking algorithm to\nefficiently construct plot lines with event causalities. Recruiting those\nefficient techniques, R$^2$ utilizes two modules to mimic the human screenplay\nrewriting process: The Reader module adopts a sliding window and CPC to build\nthe causal plot graphs, while the Rewriter module generates first the scene\noutlines based on the graphs and then the screenplays. HAR is integrated into\nboth modules for accurate inferences of LLMs. Experimental results demonstrate\nthe superiority of R$^2$, which substantially outperforms three existing\napproaches (51.3%, 22.6%, and 57.1% absolute increases) in pairwise comparison\nat the overall win rate for GPT-4o.",
      "pdf_url": "http://arxiv.org/pdf/2503.15655v1",
      "arxiv_url": "http://arxiv.org/abs/2503.15655v1",
      "published": "2025-03-19",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "World Models in Artificial Intelligence: Sensing, Learning, and Reasoning Like a Child",
      "authors": [
        "Javier Del Ser",
        "Jesus L. Lobo",
        "Heimo Müller",
        "Andreas Holzinger"
      ],
      "abstract": "World Models help Artificial Intelligence (AI) predict outcomes, reason about\nits environment, and guide decision-making. While widely used in reinforcement\nlearning, they lack the structured, adaptive representations that even young\nchildren intuitively develop. Advancing beyond pattern recognition requires\ndynamic, interpretable frameworks inspired by Piaget's cognitive development\ntheory. We highlight six key research areas -- physics-informed learning,\nneurosymbolic learning, continual learning, causal inference, human-in-the-loop\nAI, and responsible AI -- as essential for enabling true reasoning in AI. By\nintegrating statistical learning with advances in these areas, AI can evolve\nfrom pattern recognition to genuine understanding, adaptation and reasoning\ncapabilities.",
      "pdf_url": "http://arxiv.org/pdf/2503.15168v1",
      "arxiv_url": "http://arxiv.org/abs/2503.15168v1",
      "published": "2025-03-19",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.ET",
        "cs.LG",
        "68T05"
      ]
    },
    {
      "title": "DeCaFlow: A Deconfounding Causal Generative Model",
      "authors": [
        "Alejandro Almodóvar",
        "Adrián Javaloy",
        "Juan Parras",
        "Santiago Zazo",
        "Isabel Valera"
      ],
      "abstract": "Causal generative models (CGMs) have recently emerged as capable approaches\nto simulate the causal mechanisms generating our observations, enabling causal\ninference. Unfortunately, existing approaches either are overly restrictive,\nassuming the absence of hidden confounders, or lack generality, being tailored\nto a particular query and graph. In this work, we introduce DeCaFlow, a CGM\nthat accounts for hidden confounders in a single amortized training process\nusing only observational data and the causal graph. Importantly, DeCaFlow can\nprovably identify all causal queries with a valid adjustment set or\nsufficiently informative proxy variables. Remarkably, for the first time to our\nknowledge, we show that a confounded counterfactual query is identifiable, and\nthus solvable by DeCaFlow, as long as its interventional counterpart is as\nwell. Our empirical results on diverse settings (including the Ecoli70 dataset,\nwith 3 independent hidden confounders, tens of observed variables and hundreds\nof causal queries) show that DeCaFlow outperforms existing approaches, while\ndemonstrating its out-of-the-box flexibility.",
      "pdf_url": "http://arxiv.org/pdf/2503.15114v1",
      "arxiv_url": "http://arxiv.org/abs/2503.15114v1",
      "published": "2025-03-19",
      "categories": [
        "cs.LG"
      ]
    },
    {
      "title": "The Hardness of Validating Observational Studies with Experimental Data",
      "authors": [
        "Jake Fawkes",
        "Michael O'Riordan",
        "Athanasios Vlontzos",
        "Oriol Corcoll",
        "Ciarán Mark Gilligan-Lee"
      ],
      "abstract": "Observational data is often readily available in large quantities, but can\nlead to biased causal effect estimates due to the presence of unobserved\nconfounding. Recent works attempt to remove this bias by supplementing\nobservational data with experimental data, which, when available, is typically\non a smaller scale due to the time and cost involved in running a randomised\ncontrolled trial. In this work, we prove a theorem that places fundamental\nlimits on this ``best of both worlds'' approach. Using the framework of\nimpossible inference, we show that although it is possible to use experimental\ndata to \\emph{falsify} causal effect estimates from observational data, in\ngeneral it is not possible to \\emph{validate} such estimates. Our theorem\nproves that while experimental data can be used to detect bias in observational\nstudies, without additional assumptions on the smoothness of the correction\nfunction, it can not be used to remove it. We provide a practical example of\nsuch an assumption, developing a novel Gaussian Process based approach to\nconstruct intervals which contain the true treatment effect with high\nprobability, both inside and outside of the support of the experimental data.\nWe demonstrate our methodology on both simulated and semi-synthetic datasets\nand make the \\href{https://github.com/Jakefawkes/Obs_and_exp_data}{code\navailable}.",
      "pdf_url": "http://arxiv.org/pdf/2503.14795v1",
      "arxiv_url": "http://arxiv.org/abs/2503.14795v1",
      "published": "2025-03-19",
      "categories": [
        "stat.ML",
        "cs.LG",
        "stat.ME"
      ]
    },
    {
      "title": "Doubly robust identification of treatment effects from multiple environments",
      "authors": [
        "Piersilvio De Bartolomeis",
        "Julia Kostin",
        "Javier Abad",
        "Yixin Wang",
        "Fanny Yang"
      ],
      "abstract": "Practical and ethical constraints often require the use of observational data\nfor causal inference, particularly in medicine and social sciences. Yet,\nobservational datasets are prone to confounding, potentially compromising the\nvalidity of causal conclusions. While it is possible to correct for biases if\nthe underlying causal graph is known, this is rarely a feasible ask in\npractical scenarios. A common strategy is to adjust for all available\ncovariates, yet this approach can yield biased treatment effect estimates,\nespecially when post-treatment or unobserved variables are present. We propose\nRAMEN, an algorithm that produces unbiased treatment effect estimates by\nleveraging the heterogeneity of multiple data sources without the need to know\nor learn the underlying causal graph. Notably, RAMEN achieves doubly robust\nidentification: it can identify the treatment effect whenever the causal\nparents of the treatment or those of the outcome are observed, and the node\nwhose parents are observed satisfies an invariance assumption. Empirical\nevaluations on synthetic and real-world datasets show that our approach\noutperforms existing methods.",
      "pdf_url": "http://arxiv.org/pdf/2503.14459v1",
      "arxiv_url": "http://arxiv.org/abs/2503.14459v1",
      "published": "2025-03-18",
      "categories": [
        "stat.ML",
        "cs.LG",
        "stat.ME"
      ]
    },
    {
      "title": "The covariance of causal effect estimators for binary v-structures",
      "authors": [
        "Jack Kuipers",
        "Giusi Moffa"
      ],
      "abstract": "Previously [Journal of Causal Inference, 10, 90-105 (2022)], we computed the\nvariance of two estimators of causal effects for a v-structure of binary\nvariables. Here we show that a linear combination of these estimators has lower\nvariance than either. Furthermore, we show that this holds also when the\ntreatment variable is block randomised with a predefined number receiving\ntreatment, with analogous results to when it is sampled randomly.",
      "pdf_url": "http://arxiv.org/pdf/2503.14242v1",
      "arxiv_url": "http://arxiv.org/abs/2503.14242v1",
      "published": "2025-03-18",
      "categories": [
        "math.ST",
        "stat.TH"
      ]
    }
  ]
}