{
  "last_updated": "2025-08-14T00:56:16.621237",
  "papers": [
    {
      "title": "Efficient Statistical Estimation for Sequential Adaptive Experiments with Implications for Adaptive Designs",
      "authors": [
        "Wenxin Zhang",
        "Mark van der Laan"
      ],
      "abstract": "Adaptive experimental designs have gained popularity in clinical trials and\nonline experiments. Unlike traditional, fixed experimental designs, adaptive\ndesigns can dynamically adjust treatment randomization probabilities and other\ndesign features in response to data accumulated sequentially during the\nexperiment. These adaptations are useful to achieve diverse objectives,\nincluding reducing uncertainty in the estimation of causal estimands or\nincreasing participants' chances of receiving better treatments during the\nexperiment. At the end of the experiment, it is often desirable to answer\ncausal questions from the observed data. However, the adaptive nature of such\nexperiments and the resulting dependence among observations pose significant\nchallenges to providing valid statistical inference and efficient estimation of\ncausal estimands. Building upon the Targeted Maximum Likelihood Estimator\n(TMLE) framework tailored for adaptive designs (van der Laan, 2008), we\nintroduce a new adaptive-design-likelihood-based TMLE (ADL-TMLE) to estimate a\nvariety of causal estimands from adaptive experiment data. We establish\nasymptotic normality and semiparametric efficiency of ADL-TMLE under relaxed\npositivity and design stabilization assumptions for adaptive experiments.\nMotivated by efficiency results, we further propose a novel adaptive design\naimed at minimizing the variance of estimators based on data generated under\nthat design. Using the average treatment effect as a representative example,\nsimulation studies show that ADL-TMLE demonstrates superior variance-reduction\nperformance across different types of adaptive experiments, and that the\nproposed adaptive design attains lower variance than the standard\nefficiency-oriented adaptive design. Finally, we generalize this estimation and\ndesign framework to broader settings with longitudinal structures.",
      "pdf_url": "http://arxiv.org/pdf/2508.09135v1",
      "arxiv_url": "http://arxiv.org/abs/2508.09135v1",
      "published": "2025-08-12",
      "categories": [
        "stat.ME"
      ]
    },
    {
      "title": "Link Prediction for Event Logs in the Process Industry",
      "authors": [
        "Anastasia Zhukova",
        "Thomas Walton",
        "Christian E. Matt",
        "Bela Gipp"
      ],
      "abstract": "Knowledge management (KM) is vital in the process industry for optimizing\noperations, ensuring safety, and enabling continuous improvement through\neffective use of operational data and past insights. A key challenge in this\ndomain is the fragmented nature of event logs in shift books, where related\nrecords, e.g., entries documenting issues related to equipment or processes and\nthe corresponding solutions, may remain disconnected. This fragmentation\nhinders the recommendation of previous solutions to the users. To address this\nproblem, we investigate record linking (RL) as link prediction, commonly\nstudied in graph-based machine learning, by framing it as a cross-document\ncoreference resolution (CDCR) task enhanced with natural language inference\n(NLI) and semantic text similarity (STS) by shifting it into the causal\ninference (CI). We adapt CDCR, traditionally applied in the news domain, into\nan RL model to operate at the passage level, similar to NLI and STS, while\naccommodating the process industry's specific text formats, which contain\nunstructured text and structured record attributes. Our RL model outperformed\nthe best versions of NLI- and STS-driven baselines by 28% (11.43 points) and\n27% (11.21 points), respectively. Our work demonstrates how domain adaptation\nof the state-of-the-art CDCR models, enhanced with reasoning capabilities, can\nbe effectively tailored to the process industry, improving data quality and\nconnectivity in shift logs.",
      "pdf_url": "http://arxiv.org/pdf/2508.09096v1",
      "arxiv_url": "http://arxiv.org/abs/2508.09096v1",
      "published": "2025-08-12",
      "categories": [
        "cs.CL",
        "cs.IR"
      ]
    },
    {
      "title": "Nonparametric Bayesian Multi-Treatment Mixture Cure Survival Model with Application in Pediatric Oncology",
      "authors": [
        "Peter Chang",
        "John Kairalla",
        "Arkaprava Roy"
      ],
      "abstract": "Heterogeneous treatment effect estimation is critical in oncology,\nparticularly in multi-arm trials with overlapping therapeutic components and\nlong-term survivors. These shared mechanisms pose a central challenge to\nidentifying causal effects in precision medicine. We propose a novel\ncovariate-dependent nonparametric Bayesian multi-treatment cure survival model\nthat jointly accounts for common structures among treatments and cure\nfractions. Through latent link functions, our model leverages sharing among\ntreatments through a flexible modeling approach, enabling individualized\nsurvival inference. We adopt a Bayesian route for inference and implement an\nefficient MCMC algorithm for approximating the posterior. Simulation studies\ndemonstrate the method's robustness and superiority in various specification\nscenarios. Finally, application to the AALL0434 trial reveals clinically\nmeaningful differences in survival across methotrexate-based regimens and their\nassociations with different covariates, underscoring its practical utility for\nlearning treatment effects in real-world pediatric oncology data.",
      "pdf_url": "http://arxiv.org/pdf/2508.08975v2",
      "arxiv_url": "http://arxiv.org/abs/2508.08975v2",
      "published": "2025-08-12",
      "categories": [
        "stat.ME",
        "stat.AP"
      ]
    },
    {
      "title": "Position: Causal Machine Learning Requires Rigorous Synthetic Experiments for Broader Adoption",
      "authors": [
        "Audrey Poinsot",
        "Panayiotis Panayiotou",
        "Alessandro Leite",
        "Nicolas Chesneau",
        "Özgür Şimşek",
        "Marc Schoenauer"
      ],
      "abstract": "Causal machine learning has the potential to revolutionize decision-making by\ncombining the predictive power of machine learning algorithms with the theory\nof causal inference. However, these methods remain underutilized by the broader\nmachine learning community, in part because current empirical evaluations do\nnot permit assessment of their reliability and robustness, undermining their\npractical utility. Specifically, one of the principal criticisms made by the\ncommunity is the extensive use of synthetic experiments. We argue, on the\ncontrary, that synthetic experiments are essential and necessary to precisely\nassess and understand the capabilities of causal machine learning methods. To\nsubstantiate our position, we critically review the current evaluation\npractices, spotlight their shortcomings, and propose a set of principles for\nconducting rigorous empirical analyses with synthetic data. Adopting the\nproposed principles will enable comprehensive evaluations that build trust in\ncausal machine learning methods, driving their broader adoption and impactful\nreal-world use.",
      "pdf_url": "http://arxiv.org/pdf/2508.08883v1",
      "arxiv_url": "http://arxiv.org/abs/2508.08883v1",
      "published": "2025-08-12",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ME",
        "stat.ML"
      ]
    },
    {
      "title": "TiMoE: Time-Aware Mixture of Language Experts",
      "authors": [
        "Robin Faro",
        "Dongyang Fan",
        "Tamar Alphaidze",
        "Martin Jaggi"
      ],
      "abstract": "Large language models (LLMs) are typically trained on fixed snapshots of the\nweb, which means that their knowledge becomes stale and their predictions risk\ntemporal leakage: relying on information that lies in the future relative to a\nquery. We tackle this problem by pre-training from scratch a set of GPT-style\nexperts on disjoint two-year slices of a 2013-2024 corpus and combining them\nthrough TiMoE, a Time-aware Mixture of Language Experts. At inference time,\nTiMoE masks all experts whose training window ends after the query timestamp\nand merges the remaining log-probabilities in a shared space, guaranteeing\nstrict causal validity while retaining the breadth of multi-period knowledge.\nWe also release TSQA, a 10k-question benchmark whose alternatives are\nexplicitly labelled as past, future or irrelevant, allowing fine-grained\nmeasurement of temporal hallucinations. Experiments on eight standard NLP tasks\nplus TSQA show that a co-adapted TiMoE variant matches or exceeds the best\nsingle-period expert and cuts future-knowledge errors by up to 15%. Our results\ndemonstrate that modular, time-segmented pre-training paired with causal\nrouting is a simple yet effective path toward LLMs that stay chronologically\ngrounded without sacrificing general performance much. We open source our code\nat TiMoE (Github): https://github.com/epfml/TiMoE",
      "pdf_url": "http://arxiv.org/pdf/2508.08827v1",
      "arxiv_url": "http://arxiv.org/abs/2508.08827v1",
      "published": "2025-08-12",
      "categories": [
        "cs.CL"
      ]
    },
    {
      "title": "Sensitivity Analysis to Unobserved Confounding with Copula-based Normalizing Flows",
      "authors": [
        "Sourabh Balgi",
        "Marc Braun",
        "Jose M. Peña",
        "Adel Daoud"
      ],
      "abstract": "We propose a novel method for sensitivity analysis to unobserved confounding\nin causal inference. The method builds on a copula-based causal graphical\nnormalizing flow that we term $\\rho$-GNF, where $\\rho \\in [-1,+1]$ is the\nsensitivity parameter. The parameter represents the non-causal association\nbetween exposure and outcome due to unobserved confounding, which is modeled as\na Gaussian copula. In other words, the $\\rho$-GNF enables scholars to estimate\nthe average causal effect (ACE) as a function of $\\rho$, accounting for various\nconfounding strengths. The output of the $\\rho$-GNF is what we term the\n$\\rho_{curve}$, which provides the bounds for the ACE given an interval of\nassumed $\\rho$ values. The $\\rho_{curve}$ also enables scholars to identify the\nconfounding strength required to nullify the ACE. We also propose a Bayesian\nversion of our sensitivity analysis method. Assuming a prior over the\nsensitivity parameter $\\rho$ enables us to derive the posterior distribution\nover the ACE, which enables us to derive credible intervals. Finally,\nleveraging on experiments from simulated and real-world data, we show the\nbenefits of our sensitivity analysis method.",
      "pdf_url": "http://arxiv.org/pdf/2508.08752v1",
      "arxiv_url": "http://arxiv.org/abs/2508.08752v1",
      "published": "2025-08-12",
      "categories": [
        "stat.ME",
        "cs.LG",
        "stat.ML"
      ]
    },
    {
      "title": "Re:Verse -- Can Your VLM Read a Manga?",
      "authors": [
        "Aaditya Baranwal",
        "Madhav Kataria",
        "Naitik Agrawal",
        "Yogesh S Rawat",
        "Shruti Vyas"
      ],
      "abstract": "Current Vision Language Models (VLMs) demonstrate a critical gap between\nsurface-level recognition and deep narrative reasoning when processing\nsequential visual storytelling. Through a comprehensive investigation of manga\nnarrative understanding, we reveal that while recent large multimodal models\nexcel at individual panel interpretation, they systematically fail at temporal\ncausality and cross-panel cohesion, core requirements for coherent story\ncomprehension. We introduce a novel evaluation framework that combines\nfine-grained multimodal annotation, cross-modal embedding analysis, and\nretrieval-augmented assessment to systematically characterize these\nlimitations.\n  Our methodology includes (i) a rigorous annotation protocol linking visual\nelements to narrative structure through aligned light novel text, (ii)\ncomprehensive evaluation across multiple reasoning paradigms, including direct\ninference and retrieval-augmented generation, and (iii) cross-modal similarity\nanalysis revealing fundamental misalignments in current VLMs' joint\nrepresentations. Applying this framework to Re:Zero manga across 11 chapters\nwith 308 annotated panels, we conduct the first systematic study of long-form\nnarrative understanding in VLMs through three core evaluation axes: generative\nstorytelling, contextual dialogue grounding, and temporal reasoning. Our\nfindings demonstrate that current models lack genuine story-level intelligence,\nstruggling particularly with non-linear narratives, character consistency, and\ncausal inference across extended sequences. This work establishes both the\nfoundation and practical methodology for evaluating narrative intelligence,\nwhile providing actionable insights into the capability of deep sequential\nunderstanding of Discrete Visual Narratives beyond basic recognition in\nMultimodal Models.",
      "pdf_url": "http://arxiv.org/pdf/2508.08508v1",
      "arxiv_url": "http://arxiv.org/abs/2508.08508v1",
      "published": "2025-08-11",
      "categories": [
        "cs.CV",
        "cs.CL"
      ]
    },
    {
      "title": "A hierarchical modelling approach for Bayesian Causal Forests on longitudinal data: A Case Study in Multiple Sclerosis Clinical Trials",
      "authors": [
        "Emma Prevot",
        "Dieter A. Häring",
        "Thomas E. Nichols",
        "Chris C. Holmes",
        "Habib Ganjgahi"
      ],
      "abstract": "Long-running clinical trials offer a unique opportunity to study disease\nprogression and treatment response over time, enabling questions about how and\nwhen interventions alter patient trajectories. However, drawing causal\nconclusions in this setting is challenging due to irregular follow-up,\nindividual-level heterogeneity, and time-varying confounding. Bayesian Additive\nRegression Trees (BART) and their extension, Bayesian Causal Forests (BCF),\nhave proven powerful for flexible causal inference in observational data,\nespecially for heterogeneous treatment effects and non-linear outcome surfaces.\nYet, both models assume independence across observations and are fundamentally\nlimited in their ability to model within-individual correlation over time. This\nlimits their use in real-world longitudinal settings where repeated measures\nare the norm. Motivated by the NO.MS dataset, the largest and most\ncomprehensive clinical trial dataset in Multiple Sclerosis (MS), with more than\n35,000 patients and up to 15 years follow-up, we develop BCFLong, a\nhierarchical model that preserves BART's strengths while extending it for\nlongitudinal analysis. Inspired by BCF, we decompose the mean into prognostic\nand treatment effects, modelling the former on Image Quality Metrics (IQMs) to\naccount for scanner effects, and introduce individual-specific random effects,\nincluding intercepts and slope, with a sparsity-inducing horseshoe prior.\nSimulations confirm BCFLong's superior performance and robustness to sparsity,\nsignificantly improving outcome and treatment effect estimation. On NO.MS,\nBCFLong captures clinically meaningful longitudinal patterns in brain volume\nchange, which would have otherwise remained undetected. These findings\nhighlight the importance of adaptively accounting for within-individual\ncorrelations and position BCFLong as a flexible framework for causal inference\nin longitudinal data.",
      "pdf_url": "http://arxiv.org/pdf/2508.08418v1",
      "arxiv_url": "http://arxiv.org/abs/2508.08418v1",
      "published": "2025-08-11",
      "categories": [
        "stat.AP"
      ]
    },
    {
      "title": "Doubly robust pointwise confidence intervals for a monotonic continuous treatment effect curve",
      "authors": [
        "Charles R. Doss"
      ],
      "abstract": "We study nonparametric inference for the causal dose-response (or treatment\neffect) curve when the treatment variable is continuous rather than binary or\ndiscrete. We do this by developing doubly robust confidence intervals for the\ncontinuous treatment effect curve (at a fixed point) under the assumption that\nit is monotonic, based on inverting a likelihood ratio-type test. Monotonicity\nof the treatment effect curve is often a very natural assumption, and this\nassumption removes the need to choose a smoothing or tuning parameter for the\nnonparametrically estimated curve. The likelihood ratio procedure is effective\nbecause it allows us to avoid estimating the curve's unknown bias, which is\nchallenging to do. The test statistic is ``doubly robust'' in that a remainder\nterm is the product of errors for the two so-called nuisance functions that\nnaturally arise (the outcome regression and generalized propensity score\nfunctions), which allows one nuisance to be estimated poorly if the other is\nestimated well. Furthermore, we propose a version of our test or confidence\ninterval that is adaptive to a range of the unknown curve's flatness level. We\npresent versions with and without cross fitting. We illustrate the new methods\nvia simulations and a study of a dataset relating the effect of nurse staffing\nhours on hospital performance.",
      "pdf_url": "http://arxiv.org/pdf/2508.08415v1",
      "arxiv_url": "http://arxiv.org/abs/2508.08415v1",
      "published": "2025-08-11",
      "categories": [
        "stat.ME",
        "math.ST",
        "stat.TH"
      ]
    },
    {
      "title": "Multiple Regression Analysis of Unmeasured Confounding",
      "authors": [
        "Brian Knaeble",
        "R Mitchell Hughes"
      ],
      "abstract": "Whereas confidence intervals are used to assess uncertainty due to unmeasured\nindividuals, confounding intervals can be used to assess uncertainty due to\nunmeasured attributes. Previously, we have introduced a methodology for\ncomputing confounding intervals in a simple regression setting in a paper\ntitled ``Regression Analysis of Unmeasured Confounding.\" Here we extend that\nmethodology for more general application in the context of multiple regression.\nOur multiple regression analysis of unmeasured confounding utilizes subject\nmatter knowledge about coefficients of determination to bound omitted variables\nbias, while taking into account measured covariate data. Our generalized\nmethodology can be used to partially identify causal effects. The methodology\nis demonstrated with example applications, to show how coefficients of\ndetermination, being complementary to randomness, can support sensitivity\nanalysis for causal inference from observational data. The methodology is best\napplied when natural sources of randomness are present and identifiable within\nthe data generating process. Our main contribution is an algorithm that\nsupports our methodology. The purpose of this article is to describe our\nalgorithm in detail. In the paper we provide a link to our GitHub page for\nreaders who would like to access and utilize our algorithm.",
      "pdf_url": "http://arxiv.org/pdf/2508.08412v1",
      "arxiv_url": "http://arxiv.org/abs/2508.08412v1",
      "published": "2025-08-11",
      "categories": [
        "stat.ME",
        "math.OC",
        "62J99, 62H99, 62P10, 62P25"
      ]
    }
  ]
}