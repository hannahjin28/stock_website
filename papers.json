{
  "last_updated": "2025-02-18T00:44:55.423988",
  "papers": [
    {
      "title": "A Mechanistic Framework for Collider Detection in Observational Data",
      "authors": [
        "Soumik Purkayastha",
        "Peter X. -K. Song"
      ],
      "abstract": "Understanding directionality is crucial for identifying causal structures\nfrom observational data. A key challenge lies in detecting collider structures,\nwhere a $V$--structure is formed between a child node $Z$ receiving directed\nedges from parents $X$ and $Y$, denoted by $X \\rightarrow Z \\leftarrow Y$.\nTraditional causal discovery approaches, such as constraint-based and\nscore-based structure learning algorithms, do not provide statistical inference\non estimated pathways and are often sensitive to latent confounding. To\novercome these issues, we introduce methodology to quantify directionality in\ncollider structures using a pair of conditional asymmetry coefficients to\nsimultaneously examine validity of the pathways $Y \\rightarrow Z$ and $X\n\\rightarrow Z$ in the collider structure. These coefficients are based on\nShannon's differential entropy. Leveraging kernel-based conditional density\nestimation and a nonparametric smoothing technique, we utilise our proposed\nmethod to estimate collider structures and provide uncertainty quantification.\n  Simulation studies demonstrate that our method outperforms existing structure\nlearning algorithms in accurately identifying collider structures. We further\napply our approach to investigate the role of blood pressure as a collider in\nepigenetic DNA methylation, uncovering novel insights into the genetic\nregulation of blood pressure. This framework represents a significant\nadvancement in causal structure learning, offering a robust, nonparametric\nmethod for collider detection with practical applications in biostatistics and\nepidemiology.",
      "pdf_url": "http://arxiv.org/pdf/2502.10317v1",
      "arxiv_url": "http://arxiv.org/abs/2502.10317v1",
      "published": "2025-02-14",
      "categories": [
        "stat.ME",
        "math.ST",
        "stat.TH"
      ]
    },
    {
      "title": "A Latent Causal Inference Framework for Ordinal Variables",
      "authors": [
        "Martina Scauda",
        "Jack Kuipers",
        "Giusi Moffa"
      ],
      "abstract": "Ordinal variables, such as on the Likert scale, are common in applied\nresearch. Yet, existing methods for causal inference tend to target nominal or\ncontinuous data. When applied to ordinal data, this fails to account for the\ninherent ordering or imposes well-defined relative magnitudes. Hence, there is\na need for specialised methods to compute interventional effects between\nordinal variables while accounting for their ordinality. One potential\nframework is to presume a latent Gaussian Directed Acyclic Graph (DAG) model:\nthat the ordinal variables originate from marginally discretizing a set of\nGaussian variables whose latent covariance matrix is constrained to satisfy the\nconditional independencies inherent in a DAG. Conditioned on a given latent\ncovariance matrix and discretisation thresholds, we derive a closed-form\nfunction for ordinal causal effects in terms of interventional distributions in\nthe latent space. Our causal estimation combines naturally with algorithms to\nlearn the latent DAG and its parameters, like the Ordinal Structural EM\nalgorithm. Simulations demonstrate the applicability of the proposed approach\nin estimating ordinal causal effects both for known and unknown structures of\nthe latent graph. As an illustration of a real-world use case, the method is\napplied to survey data of 408 patients from a study on the functional\nrelationships between symptoms of obsessive-compulsive disorder and depression.",
      "pdf_url": "http://arxiv.org/pdf/2502.10276v1",
      "arxiv_url": "http://arxiv.org/abs/2502.10276v1",
      "published": "2025-02-14",
      "categories": [
        "stat.ME",
        "stat.ML"
      ]
    },
    {
      "title": "Do Large Language Models Reason Causally Like Us? Even Better?",
      "authors": [
        "Hanna M. Dettki",
        "Brenden M. Lake",
        "Charley M. Wu",
        "Bob Rehder"
      ],
      "abstract": "Causal reasoning is a core component of intelligence. Large language models\n(LLMs) have shown impressive capabilities in generating human-like text,\nraising questions about whether their responses reflect true understanding or\nstatistical patterns. We compared causal reasoning in humans and four LLMs\nusing tasks based on collider graphs, rating the likelihood of a query variable\noccurring given evidence from other variables. We find that LLMs reason\ncausally along a spectrum from human-like to normative inference, with\nalignment shifting based on model, context, and task. Overall, GPT-4o and\nClaude showed the most normative behavior, including \"explaining away\", whereas\nGemini-Pro and GPT-3.5 did not. Although all agents deviated from the expected\nindependence of causes - Claude the least - they exhibited strong associative\nreasoning and predictive inference when assessing the likelihood of the effect\ngiven its causes. These findings underscore the need to assess AI biases as\nthey increasingly assist human decision-making.",
      "pdf_url": "http://arxiv.org/pdf/2502.10215v1",
      "arxiv_url": "http://arxiv.org/abs/2502.10215v1",
      "published": "2025-02-14",
      "categories": [
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Causal Information Prioritization for Efficient Reinforcement Learning",
      "authors": [
        "Hongye Cao",
        "Fan Feng",
        "Tianpei Yang",
        "Jing Huo",
        "Yang Gao"
      ],
      "abstract": "Current Reinforcement Learning (RL) methods often suffer from\nsample-inefficiency, resulting from blind exploration strategies that neglect\ncausal relationships among states, actions, and rewards. Although recent causal\napproaches aim to address this problem, they lack grounded modeling of\nreward-guided causal understanding of states and actions for goal-orientation,\nthus impairing learning efficiency. To tackle this issue, we propose a novel\nmethod named Causal Information Prioritization (CIP) that improves sample\nefficiency by leveraging factored MDPs to infer causal relationships between\ndifferent dimensions of states and actions with respect to rewards, enabling\nthe prioritization of causal information. Specifically, CIP identifies and\nleverages causal relationships between states and rewards to execute\ncounterfactual data augmentation to prioritize high-impact state features under\nthe causal understanding of the environments. Moreover, CIP integrates a\ncausality-aware empowerment learning objective, which significantly enhances\nthe agent's execution of reward-guided actions for more efficient exploration\nin complex environments. To fully assess the effectiveness of CIP, we conduct\nextensive experiments across 39 tasks in 5 diverse continuous control\nenvironments, encompassing both locomotion and manipulation skills learning\nwith pixel-based and sparse reward settings. Experimental results demonstrate\nthat CIP consistently outperforms existing RL methods across a wide range of\nscenarios.",
      "pdf_url": "http://arxiv.org/pdf/2502.10097v1",
      "arxiv_url": "http://arxiv.org/abs/2502.10097v1",
      "published": "2025-02-14",
      "categories": [
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Object-Centric Latent Action Learning",
      "authors": [
        "Albina Klepach",
        "Alexander Nikulin",
        "Ilya Zisman",
        "Denis Tarasov",
        "Alexander Derevyagin",
        "Andrei Polubarov",
        "Nikita Lyubaykin",
        "Vladislav Kurenkov"
      ],
      "abstract": "Leveraging vast amounts of internet video data for Embodied AI is currently\nbottle-necked by the lack of action annotations and the presence of\naction-correlated distractors. We propose a novel object-centric latent action\nlearning approach, based on VideoSaur and LAPO, that employs self-supervised\ndecomposition of scenes into object representations and annotates video data\nwith proxy-action labels. This method effectively disentangles causal\nagent-object interactions from irrelevant background noise and reduces the\nperformance degradation of latent action learning approaches caused by\ndistractors. Our preliminary experiments with the Distracting Control Suite\nshow that latent action pretraining based on object decompositions improve the\nquality of inferred latent actions by x2.7 and efficiency of downstream\nfine-tuning with a small set of labeled actions, increasing return by x2.6 on\naverage.",
      "pdf_url": "http://arxiv.org/pdf/2502.09680v1",
      "arxiv_url": "http://arxiv.org/abs/2502.09680v1",
      "published": "2025-02-13",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Treatment response as a latent variable",
      "authors": [
        "Christopher Tosh",
        "Boyuan Zhang",
        "Wesley Tansey"
      ],
      "abstract": "Scientists often need to analyze the samples in a study that responded to\ntreatment in order to refine their hypotheses and find potential causal drivers\nof response. Natural variation in outcomes makes teasing apart responders from\nnon-responders a statistical inference problem. To handle latent responses, we\nintroduce the causal two-groups (C2G) model, a causal extension of the\nclassical two-groups model. The C2G model posits that treated samples may or\nmay not experience an effect, according to some prior probability. We propose\ntwo empirical Bayes procedures for the causal two-groups model, one under\nsemi-parametric conditions and another under fully nonparametric conditions.\nThe semi-parametric model assumes additive treatment effects and is\nidentifiable from observed data. The nonparametric model is unidentifiable, but\nwe show it can still be used to test for response in each treated sample. We\nshow empirically and theoretically that both methods for selecting responders\ncontrol the false discovery rate at the target level with near-optimal power.\nWe also propose two novel estimands of interest and provide a strategy for\nderiving estimand intervals in the unidentifiable nonparametric model. On a\ncancer immunotherapy dataset, the nonparametric C2G model recovers\nclinically-validated predictive biomarkers of both positive and negative\noutcomes. Code is available at https://github.com/tansey-lab/causal2groups.",
      "pdf_url": "http://arxiv.org/pdf/2502.08776v1",
      "arxiv_url": "http://arxiv.org/abs/2502.08776v1",
      "published": "2025-02-12",
      "categories": [
        "stat.ME",
        "cs.LG",
        "stat.ML"
      ]
    },
    {
      "title": "Difference-in-Differences and Changes-in-Changes with Sample Selection",
      "authors": [
        "Javier Viviens"
      ],
      "abstract": "Sample selection arises endogenously in causal research when the treatment\naffects whether certain units are observed. It is a common pitfall in\nlongitudinal studies, particularly in settings where treatment assignment is\nconfounded. In this paper, I highlight the drawbacks of one of the most popular\nidentification strategies in such settings: Difference-in-Differences (DiD).\nSpecifically, I employ principal stratification analysis to show that the\nconventional ATT estimand may not be well defined, and the DiD estimand cannot\nbe interpreted causally without additional assumptions. To address these\nissues, I develop an identification strategy to partially identify causal\neffects on the subset of units with well-defined and observed outcomes under\nboth treatment regimes. I adapt Lee bounds to the Changes-in-Changes (CiC)\nsetting (Athey & Imbens, 2006), leveraging the time dimension of the data to\nrelax the unconfoundedness assumption in the original trimming strategy of Lee\n(2009). This setting has the DiD identification strategy as a particular case,\nwhich I also implement in the paper. Additionally, I explore how to leverage\nmultiple sources of sample selection to relax the monotonicity assumption in\nLee (2009), which may be of independent interest. Alongside the identification\nstrategy, I present estimators and inference results. I illustrate the\nrelevance of the proposed methodology by analyzing a job training program in\nColombia.",
      "pdf_url": "http://arxiv.org/pdf/2502.08614v1",
      "arxiv_url": "http://arxiv.org/abs/2502.08614v1",
      "published": "2025-02-12",
      "categories": [
        "econ.EM"
      ]
    },
    {
      "title": "Causal Analysis of ASR Errors for Children: Quantifying the Impact of Physiological, Cognitive, and Extrinsic Factors",
      "authors": [
        "Vishwanath Pratap Singh",
        "Md. Sahidullah",
        "Tomi Kinnunen"
      ],
      "abstract": "The increasing use of children's automatic speech recognition (ASR) systems\nhas spurred research efforts to improve the accuracy of models designed for\nchildren's speech in recent years. The current approach utilizes either\nopen-source speech foundation models (SFMs) directly or fine-tuning them with\nchildren's speech data. These SFMs, whether open-source or fine-tuned for\nchildren, often exhibit higher word error rates (WERs) compared to adult\nspeech. However, there is a lack of systemic analysis of the cause of this\ndegraded performance of SFMs. Understanding and addressing the reasons behind\nthis performance disparity is crucial for improving the accuracy of SFMs for\nchildren's speech. Our study addresses this gap by investigating the causes of\naccuracy degradation and the primary contributors to WER in children's speech.\nIn the first part of the study, we conduct a comprehensive benchmarking study\non two self-supervised SFMs (Wav2Vec2.0 and Hubert) and two weakly supervised\nSFMs (Whisper and MMS) across various age groups on two children speech\ncorpora, establishing the raw data for the causal inference analysis in the\nsecond part. In the second part of the study, we analyze the impact of\nphysiological factors (age, gender), cognitive factors (pronunciation ability),\nand external factors (vocabulary difficulty, background noise, and word count)\non SFM accuracy in children's speech using causal inference. The results\nindicate that physiology (age) and particular external factor (number of words\nin audio) have the highest impact on accuracy, followed by background noise and\npronunciation ability. Fine-tuning SFMs on children's speech reduces\nsensitivity to physiological and cognitive factors, while sensitivity to the\nnumber of words in audio persists.\n  Keywords: Children's ASR, Speech Foundational Models, Causal Inference,\nPhysiology, Cognition, Pronunciation",
      "pdf_url": "http://arxiv.org/pdf/2502.08587v1",
      "arxiv_url": "http://arxiv.org/abs/2502.08587v1",
      "published": "2025-02-12",
      "categories": [
        "eess.AS"
      ]
    },
    {
      "title": "Tutorial for Surrogate Endpoint Validation Using Joint modeling and Mediation Analysis",
      "authors": [
        "Quentin Le Coent",
        "Virginie Rondeau",
        "Catherine Legrand"
      ],
      "abstract": "The use of valid surrogate endpoints is an important stake in clinical\nresearch to help reduce both the duration and cost of a clinical trial and\nspeed up the evaluation of interesting treatments. Several methods have been\nproposed in the statistical literature to validate putative surrogate\nendpoints. Two main approaches have been proposed: the meta-analytic approach\nand the mediation analysis approach. The former uses data from meta-analyses to\nderive associations measures between the surrogate and the final endpoint at\nthe individual and trial levels. The latter rather uses the proportion of the\ntreatment effect on the final endpoint through the surrogate as a measure of\nsurrogacy in a causal inference framework. Both approaches have remained\nseparated as the meta-analytic approach does not estimate the treatment effect\non the final endpoint through the surrogate while the mediation analysis\napproach have been limited to single-trial setting. However, these two\napproaches are complementary. In this work we propose an approach that combines\nthe meta-analytic and mediation analysis approaches using joint modeling for\nsurrogate validation. We focus on the cases where the final endpoint is a\ntime-to-event endpoint (such as time-to-death) and the surrogate is either a\ntime-to-event or a longitudinal biomarker. Two new joint models were proposed\ndepending on the nature of the surrogate. These model are implemented in the R\npackage frailtypack. We illustrate the developed approaches in three\napplications on real datasets in oncology.",
      "pdf_url": "http://arxiv.org/pdf/2502.08443v1",
      "arxiv_url": "http://arxiv.org/abs/2502.08443v1",
      "published": "2025-02-12",
      "categories": [
        "stat.ME",
        "stat.AP",
        "stat.CO"
      ]
    },
    {
      "title": "Individualised Treatment Effects Estimation with Composite Treatments and Composite Outcomes",
      "authors": [
        "Vinod Kumar Chauhan",
        "Lei Clifton",
        "Gaurav Nigam",
        "David A. Clifton"
      ],
      "abstract": "Estimating individualised treatment effect (ITE) -- that is the causal effect\nof a set of variables (also called exposures, treatments, actions, policies, or\ninterventions), referred to as \\textit{composite treatments}, on a set of\noutcome variables of interest, referred to as \\textit{composite outcomes}, for\na unit from observational data -- remains a fundamental problem in causal\ninference with applications across disciplines, such as healthcare, economics,\neducation, social science, marketing, and computer science. Previous work in\ncausal machine learning for ITE estimation is limited to simple settings, like\nsingle treatments and single outcomes. This hinders their use in complex\nreal-world scenarios; for example, consider studying the effect of different\nICU interventions, such as beta-blockers and statins for a patient admitted for\nheart surgery, on different outcomes of interest such as atrial fibrillation\nand in-hospital mortality. The limited research into composite treatments and\noutcomes is primarily due to data scarcity for all treatments and outcomes. To\naddress the above challenges, we propose a novel and innovative\nhypernetwork-based approach, called \\emph{H-Learner}, to solve ITE estimation\nunder composite treatments and composite outcomes, which tackles the data\nscarcity issue by dynamically sharing information across treatments and\noutcomes. Our empirical analysis with binary and arbitrary composite treatments\nand outcomes demonstrates the effectiveness of the proposed approach compared\nto existing methods.",
      "pdf_url": "http://arxiv.org/pdf/2502.08282v1",
      "arxiv_url": "http://arxiv.org/abs/2502.08282v1",
      "published": "2025-02-12",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    }
  ]
}