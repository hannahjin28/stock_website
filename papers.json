{
  "last_updated": "2025-07-28T01:02:53.536322",
  "papers": [
    {
      "title": "Deep Learning for Blood-Brain Barrier Permeability Prediction",
      "authors": [
        "Zihan Yang",
        "Haipeng Gong"
      ],
      "abstract": "Predicting whether a molecule can cross the blood-brain barrier (BBB) is a\nkey step in early-stage neuropharmaceutical development, directly influencing\nboth research efficiency and success rates in drug discovery. Traditional\nempirical methods based on physicochemical properties are prone to systematic\nmisjudgements due to their reliance on static rules. Early machine learning\nmodels, although data-driven, often suffer from limited capacity, poor\ngeneralization, and insufficient interpretability. In recent years, artificial\nintelligence (AI) methods have become essential tools for predicting BBB\npermeability and guiding related drug design, owing to their ability to model\nmolecular structures and capture complex biological mechanisms. This article\nsystematically reviews the evolution of this field-from deep neural networks to\ngraph-based structural modeling-highlighting the advantages of multi-task and\nmultimodal learning strategies in identifying mechanism-relevant variables. We\nfurther explore the emerging potential of generative models and causal\ninference methods for integrating permeability prediction with mechanism-aware\ndrug design. BBB modeling is in the transition from static classification\ntoward mechanistic perception and structure-function modeling. This paradigm\nshift provides a methodological foundation and future roadmap for the\nintegration of AI into neuropharmacological development.",
      "pdf_url": "http://arxiv.org/pdf/2507.18557v1",
      "arxiv_url": "http://arxiv.org/abs/2507.18557v1",
      "published": "2025-07-24",
      "categories": [
        "q-bio.QM"
      ]
    },
    {
      "title": "A Two-armed Bandit Framework for A/B Testing",
      "authors": [
        "Jinjuan Wang",
        "Qianglin Wen",
        "Yu Zhang",
        "Xiaodong Yan",
        "Chengchun Shi"
      ],
      "abstract": "A/B testing is widely used in modern technology companies for policy\nevaluation and product deployment, with the goal of comparing the outcomes\nunder a newly-developed policy against a standard control. Various causal\ninference and reinforcement learning methods developed in the literature are\napplicable to A/B testing. This paper introduces a two-armed bandit framework\ndesigned to improve the power of existing approaches. The proposed procedure\nconsists of three main steps: (i) employing doubly robust estimation to\ngenerate pseudo-outcomes, (ii) utilizing a two-armed bandit framework to\nconstruct the test statistic, and (iii) applying a permutation-based method to\ncompute the $p$-value. We demonstrate the efficacy of the proposed method\nthrough asymptotic theories, numerical experiments and real-world data from a\nridesharing company, showing its superior performance in comparison to existing\nmethods.",
      "pdf_url": "http://arxiv.org/pdf/2507.18118v1",
      "arxiv_url": "http://arxiv.org/abs/2507.18118v1",
      "published": "2025-07-24",
      "categories": [
        "stat.ML",
        "cs.LG",
        "stat.AP"
      ]
    },
    {
      "title": "GrAInS: Gradient-based Attribution for Inference-Time Steering of LLMs and VLMs",
      "authors": [
        "Duy Nguyen",
        "Archiki Prasad",
        "Elias Stengel-Eskin",
        "Mohit Bansal"
      ],
      "abstract": "Inference-time steering methods offer a lightweight alternative to\nfine-tuning large language models (LLMs) and vision-language models (VLMs) by\nmodifying internal activations at test time without updating model weights.\nHowever, most existing approaches rely on fixed, global intervention vectors,\noverlook the causal influence of individual input tokens, and fail to leverage\ninformative gradients from the model's logits, particularly in multimodal\nsettings where visual and textual inputs contribute unevenly. To address these\nlimitations, we introduce GrAInS, an inference-time steering approach that\noperates across both language-only and vision-language models and tasks. GrAInS\nuses contrastive, gradient-based attribution via Integrated Gradients to\nidentify the top-k most influential tokens, both positively and negatively\nattributed based on their contribution to preferred versus dispreferred\noutputs. These tokens are then used to construct directional steering vectors\nthat capture semantic shifts from undesirable to desirable behavior. During\ninference, GrAInS adjusts hidden activations at transformer layers guided by\ntoken-level attribution signals, and normalizes activations to preserve\nrepresentational scale. This enables fine-grained, interpretable, and modular\ncontrol over model behavior, without retraining or auxiliary supervision.\nEmpirically, GrAInS consistently outperforms both fine-tuning and existing\nsteering baselines: it achieves a 13.22% accuracy gain on TruthfulQA using\nLlama-3.1-8B, reduces hallucination rates on MMHal-Bench from 0.624 to 0.514\nwith LLaVA-1.6-7B, and improves alignment win rates on SPA-VL by 8.11%, all\nwhile preserving the model's fluency and general capabilities.",
      "pdf_url": "http://arxiv.org/pdf/2507.18043v1",
      "arxiv_url": "http://arxiv.org/abs/2507.18043v1",
      "published": "2025-07-24",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "Debiased maximum-likelihood estimators for hazard ratios under machine-learning adjustment",
      "authors": [
        "Takashi Hayakawa",
        "Satoshi Asai"
      ],
      "abstract": "Previous studies have shown that hazard ratios between treatment groups\nestimated with the Cox model are uninterpretable because the indefinite\nbaseline hazard of the model fails to identify temporal change in the risk set\ncomposition due to treatment assignment and unobserved factors among multiple,\ncontradictory scenarios. To alleviate this problem, especially in studies based\non observational data with uncontrolled dynamic treatment and real-time\nmeasurement of many covariates, we propose abandoning the baseline hazard and\nusing machine learning to explicitly model the change in the risk set with or\nwithout latent variables. For this framework, we clarify the context in which\nhazard ratios can be causally interpreted, and then develop a method based on\nNeyman orthogonality to compute debiased maximum-likelihood estimators of\nhazard ratios. Computing the constructed estimators is more efficient than\ncomputing those based on weighted regression with marginal structural Cox\nmodels. Numerical simulations confirm that the proposed method identifies the\nground truth with minimal bias. These results lay the foundation for developing\na useful, alternative method for causal inference with uncontrolled,\nobservational data in modern epidemiology.",
      "pdf_url": "http://arxiv.org/pdf/2507.17686v1",
      "arxiv_url": "http://arxiv.org/abs/2507.17686v1",
      "published": "2025-07-23",
      "categories": [
        "stat.ML",
        "cs.LG"
      ]
    },
    {
      "title": "See the Forest and the Trees: A Synergistic Reasoning Framework for Knowledge-Based Visual Question Answering",
      "authors": [
        "Junjie Wang",
        "Yunhan Tang",
        "Yijie Wang",
        "Zhihao Yuan",
        "Huan Wang",
        "Yangfan He",
        "Bin Li"
      ],
      "abstract": "Multimodal Large Language Models (MLLMs) have pushed the frontiers of\nKnowledge-Based Visual Question Answering (KBVQA), yet their reasoning is\nfundamentally bottlenecked by a reliance on uni-dimensional evidence. This\n\"seeing only the trees, but not the forest\" approach prevents robust,\nmulti-faceted understanding. Inspired by the principle of seeing both the\nforest and trees, we propose Synergos-VQA, a novel synergistic reasoning\nframework. At its core, Synergos-VQA concurrently generates and fuses three\ncomplementary evidence streams at inference time: (1) Holistic Evidence to\nperceive the entire scene (the \"forest\"), (2) Structural Evidence from a\nprototype-driven module to identify key objects (the \"trees\"), and (3) Causal\nEvidence from a counterfactual probe to ensure the reasoning is robustly\ngrounded. By synergistically fusing this multi-faceted evidence, our framework\nachieves a more comprehensive and reliable reasoning process. Extensive\nexperiments show that Synergos-VQA decisively establishes a new\nstate-of-the-art on three challenging benchmarks, including OK-VQA and A-OKVQA.\nFurthermore, our approach demonstrates strong plug-and-play capabilities,\nsignificantly boosting various open-source MLLMs and proving that superior\nmethodological design can outperform sheer model scale.",
      "pdf_url": "http://arxiv.org/pdf/2507.17659v1",
      "arxiv_url": "http://arxiv.org/abs/2507.17659v1",
      "published": "2025-07-23",
      "categories": [
        "cs.CV"
      ]
    },
    {
      "title": "Doubly robust outlier resistant inference on causal treatment effect",
      "authors": [
        "Joonsung Kang"
      ],
      "abstract": "Outliers can severely distort causal effect estimation in observational\nstudies, yet this issue has received limited attention in the literature. Their\ninfluence is especially pronounced in small sample sizes, where detecting and\nremoving outliers becomes increasingly difficult. Therefore, it is essential to\nestimate treatment effects robustly without excluding these influential data\npoints. To address this, we propose a doubly robust point estimator for the\naverage treatment effect under a contaminated model that includes outliers.\nRobustness in outcome regression is achieved through a robust estimating\nequation, while covariate balancing propensity scores (CBPS) ensure resilience\nin propensity score modeling.\n  To prevent model overfitting due to the inclusion of numerous parameters, we\nincorporate variable selection. All these components are unified under a\npenalized empirical likelihood framework. For confidence interval estimation,\nmost existing approaches rely on asymptotic properties, which may be unreliable\nin finite samples. We derive an optimal finite-sample confidence interval for\nthe average treatment effect using our proposed estimating equation, ensuring\nthat the interval bounds remain unaffected by outliers. Through simulations and\na real-world application involving hypertension data with outliers, we\ndemonstrate that our method consistently outperforms existing approaches in\nboth accuracy and robustness.",
      "pdf_url": "http://arxiv.org/pdf/2507.17439v1",
      "arxiv_url": "http://arxiv.org/abs/2507.17439v1",
      "published": "2025-07-23",
      "categories": [
        "stat.ME",
        "cs.LG"
      ]
    },
    {
      "title": "CAPRI-CT: Causal Analysis and Predictive Reasoning for Image Quality Optimization in Computed Tomography",
      "authors": [
        "Sneha George Gnanakalavathy",
        "Hairil Abdul Razak",
        "Robert Meertens",
        "Jonathan E. Fieldsend",
        "Xujiong Ye",
        "Mohammed M. Abdelsamea"
      ],
      "abstract": "In computed tomography (CT), achieving high image quality while minimizing\nradiation exposure remains a key clinical challenge. This paper presents\nCAPRI-CT, a novel causal-aware deep learning framework for Causal Analysis and\nPredictive Reasoning for Image Quality Optimization in CT imaging. CAPRI-CT\nintegrates image data with acquisition metadata (such as tube voltage, tube\ncurrent, and contrast agent types) to model the underlying causal relationships\nthat influence image quality. An ensemble of Variational Autoencoders (VAEs) is\nemployed to extract meaningful features and generate causal representations\nfrom observational data, including CT images and associated imaging parameters.\nThese input features are fused to predict the Signal-to-Noise Ratio (SNR) and\nsupport counterfactual inference, enabling what-if simulations, such as changes\nin contrast agents (types and concentrations) or scan parameters. CAPRI-CT is\ntrained and validated using an ensemble learning approach, achieving strong\npredictive performance. By facilitating both prediction and interpretability,\nCAPRI-CT provides actionable insights that could help radiologists and\ntechnicians design more efficient CT protocols without repeated physical scans.\nThe source code and dataset are publicly available at\nhttps://github.com/SnehaGeorge22/capri-ct.",
      "pdf_url": "http://arxiv.org/pdf/2507.17420v1",
      "arxiv_url": "http://arxiv.org/abs/2507.17420v1",
      "published": "2025-07-23",
      "categories": [
        "cs.CV"
      ]
    },
    {
      "title": "Causal Mechanism Estimation in Multi-Sensor Systems Across Multiple Domains",
      "authors": [
        "Jingyi Yu",
        "Tim Pychynski",
        "Marco F. Huber"
      ],
      "abstract": "To gain deeper insights into a complex sensor system through the lens of\ncausality, we present common and individual causal mechanism estimation\n(CICME), a novel three-step approach to inferring causal mechanisms from\nheterogeneous data collected across multiple domains. By leveraging the\nprinciple of Causal Transfer Learning (CTL), CICME is able to reliably detect\ndomain-invariant causal mechanisms when provided with sufficient samples. The\nidentified common causal mechanisms are further used to guide the estimation of\nthe remaining causal mechanisms in each domain individually. The performance of\nCICME is evaluated on linear Gaussian models under scenarios inspired from a\nmanufacturing process. Building upon existing continuous optimization-based\ncausal discovery methods, we show that CICME leverages the benefits of applying\ncausal discovery on the pooled data and repeatedly on data from individual\ndomains, and it even outperforms both baseline methods under certain scenarios.",
      "pdf_url": "http://arxiv.org/pdf/2507.17792v2",
      "arxiv_url": "http://arxiv.org/abs/2507.17792v2",
      "published": "2025-07-23",
      "categories": [
        "cs.LG",
        "stat.ML"
      ]
    },
    {
      "title": "Ballot Design and Electoral Outcomes: The Role of Candidate Order and Party Affiliation",
      "authors": [
        "Alessandro Arlotto",
        "Alexandre Belloni",
        "Fei Fang",
        "Saša Pekeč"
      ],
      "abstract": "We use causal inference to study how designing ballots with and without party\ndesignations impacts electoral outcomes when partisan voters rely on\nparty-order cues to infer candidate affiliation in races without designations.\nIf the party orders of candidates in races with and without party designations\ndiffer, these voters might cast their votes incorrectly. We identify a\nquasi-randomized natural experiment with contest-level treatment assignment\npertaining to North Carolina judicial elections and use double machine learning\nto accurately capture the magnitude of such incorrectly cast votes. Using\nprecinct-level election and demographic data, we estimate that 11.8% (95%\nconfidence interval: [4.0%, 19.6%]) of democratic partisan voters and 15.4%\n(95% confidence interval: [7.8%, 23.1%]) of republican partisan voters cast\ntheir votes incorrectly due to the difference in party orders. Our results\nindicate that ballots mixing contests with and without party designations\nmislead many voters, leading to outcomes that do not reflect true voter\npreferences. To accurately capture voter intent, such ballot designs should be\navoided.",
      "pdf_url": "http://arxiv.org/pdf/2507.16722v1",
      "arxiv_url": "http://arxiv.org/abs/2507.16722v1",
      "published": "2025-07-22",
      "categories": [
        "stat.AP"
      ]
    },
    {
      "title": "On Causal Inference for the Survivor Function",
      "authors": [
        "Benjamin R. Baer",
        "Ashkan Ertefaie",
        "Robert L. Strawderman"
      ],
      "abstract": "In this expository paper, we consider the problem of causal inference and\nefficient estimation for the counterfactual survivor function. This problem has\npreviously been considered in the literature in several papers, each relying on\nthe imposition of conditions meant to identify the desired estimand from the\nobserved data. These conditions, generally referred to as either implying or\nsatisfying coarsening at random, are inconsistently imposed across this\nliterature and, in all cases, fail to imply coarsening at random. We establish\nthe first general characterization of coarsening at random, and also sequential\ncoarsening at random, for this estimation problem. Other contributions include\nthe first general characterization of the set of all influence functions for\nthe counterfactual survival probability under sequential coarsening at random,\nand the corresponding nonparametric efficient influence function. These\ncharacterizations are general in that neither impose continuity assumptions on\neither the underlying failure or censoring time distributions. We further show\nhow the latter compares to alternative forms recently derived in the\nliterature, including establishing the pointwise equivalence of the influence\nfunctions for our nonparametric efficient estimator and that recently given in\nWestling et al (2024, Journal of the American Statistical Association).",
      "pdf_url": "http://arxiv.org/pdf/2507.16691v1",
      "arxiv_url": "http://arxiv.org/abs/2507.16691v1",
      "published": "2025-07-22",
      "categories": [
        "stat.ME"
      ]
    }
  ]
}