{
  "last_updated": "2025-07-30T00:59:30.044749",
  "papers": [
    {
      "title": "Personalized Treatment Effect Estimation from Unstructured Data",
      "authors": [
        "Henri Arno",
        "Thomas Demeester"
      ],
      "abstract": "Existing methods for estimating personalized treatment effects typically rely\non structured covariates, limiting their applicability to unstructured data.\nYet, leveraging unstructured data for causal inference has considerable\napplication potential, for instance in healthcare, where clinical notes or\nmedical images are abundant. To this end, we first introduce an approximate\n'plug-in' method trained directly on the neural representations of unstructured\ndata. However, when these fail to capture all confounding information, the\nmethod may be subject to confounding bias. We therefore introduce two\ntheoretically grounded estimators that leverage structured measurements of the\nconfounders during training, but allow estimating personalized treatment\neffects purely from unstructured inputs, while avoiding confounding bias. When\nthese structured measurements are only available for a non-representative\nsubset of the data, these estimators may suffer from sampling bias. To address\nthis, we further introduce a regression-based correction that accounts for the\nnon-uniform sampling, assuming the sampling mechanism is known or can be\nwell-estimated. Our experiments on two benchmark datasets show that the plug-in\nmethod, directly trainable on large unstructured datasets, achieves strong\nempirical performance across all settings, despite its simplicity.",
      "pdf_url": "http://arxiv.org/pdf/2507.20993v1",
      "arxiv_url": "http://arxiv.org/abs/2507.20993v1",
      "published": "2025-07-28",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ]
    },
    {
      "title": "Onboard Hyperspectral Super-Resolution with Deep Pushbroom Neural Network",
      "authors": [
        "Davide Piccinini",
        "Diego Valsesia",
        "Enrico Magli"
      ],
      "abstract": "Hyperspectral imagers on satellites obtain the fine spectral signatures\nessential for distinguishing one material from another at the expense of\nlimited spatial resolution. Enhancing the latter is thus a desirable\npreprocessing step in order to further improve the detection capabilities\noffered by hyperspectral images on downstream tasks. At the same time, there is\na growing interest towards deploying inference methods directly onboard of\nsatellites, which calls for lightweight image super-resolution methods that can\nbe run on the payload in real time. In this paper, we present a novel neural\nnetwork design, called Deep Pushbroom Super-Resolution (DPSR) that matches the\npushbroom acquisition of hyperspectral sensors by processing an image line by\nline in the along-track direction with a causal memory mechanism to exploit\npreviously acquired lines. This design greatly limits memory requirements and\ncomputational complexity, achieving onboard real-time performance, i.e., the\nability to super-resolve a line in the time it takes to acquire the next one,\non low-power hardware. Experiments show that the quality of the super-resolved\nimages is competitive or even outperforms state-of-the-art methods that are\nsignificantly more complex.",
      "pdf_url": "http://arxiv.org/pdf/2507.20765v1",
      "arxiv_url": "http://arxiv.org/abs/2507.20765v1",
      "published": "2025-07-28",
      "categories": [
        "eess.IV",
        "cs.CV"
      ]
    },
    {
      "title": "Causal Inference when Intervention Units and Outcome Units Differ",
      "authors": [
        "Georgia Papadogeorgou",
        "Zhaoyan Song",
        "Guido Imbens",
        "Fabrizia Mealli"
      ],
      "abstract": "We study causal inference in settings characterized by interference with a\nbipartite structure. There are two distinct sets of units: intervention units\nto which an intervention can be applied and outcome units on which the outcome\nof interest can be measured. Outcome units may be affected by interventions on\nsome, but not all, intervention units, as captured by a bipartite graph.\nExamples of this setting can be found in analyses of the impact of pollution\nabatement in plants on health outcomes for individuals, or the effect of\ntransportation network expansions on regional economic activity. We introduce\nand discuss a variety of old and new causal estimands for these bipartite\nsettings. We do not impose restrictions on the functional form of the exposure\nmapping and the potential outcomes, thus allowing for heterogeneity,\nnon-linearity, non-additivity, and potential interactions in treatment effects.\nWe propose unbiased weighting estimators for these estimands from a\ndesign-based perspective, based on the knowledge of the bipartite network under\ngeneral experimental designs. We derive their variance and prove consistency\nfor increasing number of outcome units. Using the Chinese high-speed rail\nconstruction study, analyzed in Borusyak and Hull [2023], we discuss\nnon-trivial positivity violations that depend on the estimands, the adopted\nexperimental design, and the structure of the bipartite graph.",
      "pdf_url": "http://arxiv.org/pdf/2507.20231v1",
      "arxiv_url": "http://arxiv.org/abs/2507.20231v1",
      "published": "2025-07-27",
      "categories": [
        "stat.ME"
      ]
    },
    {
      "title": "Causal Inference for Circular Data",
      "authors": [
        "Kuan-Hsun Wu"
      ],
      "abstract": "In causal inference, a fundamental task is to estimate the effect resulting\nfrom a specific treatment, which is often handled with inverse probability\nweighting. Despite an abundance of attention to the advancement of this task,\nmost articles have focused on linear data rather than circular data, which are\nmeasured in angles. In this article, we extend the causal inference framework\nto accommodate circular data. Specifically, two new treatment effects, average\ndirection treatment effect (ADTE) and average length treatment effect (ALTE),\nare introduced to offer a proper causal explanation for these data. As the\naverage direction and average length describe the location and concentration of\na random sample of circular data, the ADTE and ALTE measure the change in\ndirection and length between two counterfactual outcomes. With inverse\nprobability weighting, we propose estimators that exhibit ideal theoretical\nproperties, which are validated by a simulation study. To illustrate the\npractical utility of our estimator, we analyze the effect of different job\ntypes on dispatchers' sleep patterns using data from Federal Railroad\nAdministration.",
      "pdf_url": "http://arxiv.org/pdf/2507.19889v2",
      "arxiv_url": "http://arxiv.org/abs/2507.19889v2",
      "published": "2025-07-26",
      "categories": [
        "stat.ME",
        "stat.AP"
      ]
    },
    {
      "title": "Adaptive Proximal Causal Inference with Some Invalid Proxies",
      "authors": [
        "Prabrisha Rakshit",
        "Xu Shi",
        "Eric Tchetgen Tchetgen"
      ],
      "abstract": "Proximal causal inference (PCI) is a recently proposed framework to identify\nand estimate the causal effect of an exposure on an outcome in the presence of\nhidden confounders, using observed proxies. Specifically, PCI relies on two\ntypes of proxies: a treatment-inducing confounding proxy, related to the\noutcome only through its association with unmeasured confounders (given\ntreatment and covariates), and an outcome-inducing confounding proxy, related\nto the treatment only through such association (given covariates). These\nproxies must satisfy stringent exclusion restrictions - namely, the treatment\nproxy must not affect the outcome, and the outcome proxy must not be affected\nby the treatment. To improve identification and potentially efficiency,\nmultiple proxies are often used, raising concerns about bias from exclusion\nviolations. To address this, we introduce necessary and sufficient conditions\nfor identifying causal effects in the presence of many proxies, some\npotentially invalid. Under a canonical proximal linear structural equations\nmodel, we propose a LASSO-based median estimator that jointly selects valid\nproxies and estimates the causal effect, with theoretical guarantees.\nRecognizing LASSO's limitations in consistently selecting valid treatment\nproxies, we develop an adaptive LASSO-based estimator with differential\npenalization. We show that it is root-n consistent and yields valid confidence\nintervals when a valid outcome proxy is available. We also extend the approach\nto settings with many potentially invalid outcome proxies. Theoretical results\nare supported by simulations and an application assessing the effect of right\nheart catheterization on 30-day survival in ICU patient.",
      "pdf_url": "http://arxiv.org/pdf/2507.19623v1",
      "arxiv_url": "http://arxiv.org/abs/2507.19623v1",
      "published": "2025-07-25",
      "categories": [
        "stat.ME"
      ]
    },
    {
      "title": "Beyond Bonferroni: Hierarchical Multiple Testing in Empirical Research",
      "authors": [
        "Sebastian Calonico",
        "Sebastian Galiani"
      ],
      "abstract": "Empirical research in the social and medical sciences frequently involves\ntesting multiple hypotheses simultaneously, increasing the risk of false\npositives due to chance. Classical multiple testing procedures, such as the\nBonferroni correction, control the family-wise error rate (FWER) but tend to be\noverly conservative, reducing statistical power. Stepwise alternatives like the\nHolm and Hochberg procedures offer improved power while maintaining error\ncontrol under certain dependence structures. However, these standard approaches\ntypically ignore hierarchical relationships among hypotheses -- structures that\nare common in settings such as clinical trials and program evaluations, where\noutcomes are often logically or causally linked. Hierarchical multiple testing\nprocedures -- including fixed sequence, fallback, and gatekeeping methods --\nexplicitly incorporate these relationships, providing more powerful and\ninterpretable frameworks for inference. This paper reviews key hierarchical\nmethods, compares their statistical properties and practical trade-offs, and\ndiscusses implications for applied empirical research.",
      "pdf_url": "http://arxiv.org/pdf/2507.19610v1",
      "arxiv_url": "http://arxiv.org/abs/2507.19610v1",
      "published": "2025-07-25",
      "categories": [
        "econ.EM"
      ]
    },
    {
      "title": "Inference with weights: Residualization produces short, valid intervals for varying estimands and varying resampling processes",
      "authors": [
        "Erin Hartman",
        "Chad Hazlett",
        "Arisa Sadeghpour"
      ],
      "abstract": "Weighting procedures are used in observational causal inference to adjust for\ncovariate imbalance within the sample. Common practice for inference is to\nestimate robust standard errors from a weighted regression of outcome on\ntreatment. However, it is well known that weighting can inflate variance\nestimates, sometimes significantly, leading to standard errors and confidence\nintervals that are overly conservative. We instead examine and recommend the\nuse of robust standard errors from a weighted regression that additionally\nincludes the balancing covariates and their interactions with treatment. We\nshow that these standard errors are more precise and asymptotically correct for\nweights that achieve exact balance under multiple common resampling frameworks,\nincluding design-based and model-based inference, as well as superpopulation\nsampling with a finite sample correction. Gains to precision can be quite\nsignificant when the balancing weights adjust for prognostic covariates. For\nprocedures that balance only approximately or in expectation, such as inverse\npropensity weighting or approximate balancing weights, our proposed method\nimproves precision by reducing residuals through augmentation with the\nparametric model. We demonstrate our approach through simulation and\nre-analysis of multiple empirical studies.",
      "pdf_url": "http://arxiv.org/pdf/2507.19607v1",
      "arxiv_url": "http://arxiv.org/abs/2507.19607v1",
      "published": "2025-07-25",
      "categories": [
        "stat.ME"
      ]
    },
    {
      "title": "Riesz representers for the rest of us",
      "authors": [
        "Nicholas T. Williams",
        "Oliver J. Hines",
        "Kara E. Rudolph"
      ],
      "abstract": "The application of semiparametric efficient estimators, particularly those\nthat leverage machine learning, is rapidly expanding within epidemiology and\ncausal inference. Much of the recent methodological literature on these\nestimators relies heavily on the Riesz representation theorem and Riesz\nregression. This paper aims to introduce the Riesz representation theorem to an\napplied audience, explaining why and how Riesz regression is becoming widely\nused in the semiparametric estimator statistical literature.",
      "pdf_url": "http://arxiv.org/pdf/2507.19413v1",
      "arxiv_url": "http://arxiv.org/abs/2507.19413v1",
      "published": "2025-07-25",
      "categories": [
        "math.ST",
        "stat.TH"
      ]
    },
    {
      "title": "Probably Approximately Correct Causal Discovery",
      "authors": [
        "Mian Wei",
        "Somesh Jha",
        "David Page"
      ],
      "abstract": "The discovery of causal relationships is a foundational problem in artificial\nintelligence, statistics, epidemiology, economics, and beyond. While elegant\ntheories exist for accurate causal discovery given infinite data, real-world\napplications are inherently resource-constrained. Effective methods for\ninferring causal relationships from observational data must perform well under\nfinite data and time constraints, where \"performing well\" implies achieving\nhigh, though not perfect accuracy. In his seminal paper A Theory of the\nLearnable, Valiant highlighted the importance of resource constraints in\nsupervised machine learning, introducing the concept of Probably Approximately\nCorrect (PAC) learning as an alternative to exact learning. Inspired by\nValiant's work, we propose the Probably Approximately Correct Causal (PACC)\nDiscovery framework, which extends PAC learning principles to the causal field.\nThis framework emphasizes both computational and sample efficiency for\nestablished causal methods such as propensity score techniques and instrumental\nvariable approaches. Furthermore, we show that it can also provide theoretical\nguarantees for other widely used methods, such as the Self-Controlled Case\nSeries (SCCS) method, which had previously lacked such guarantees.",
      "pdf_url": "http://arxiv.org/pdf/2507.18903v1",
      "arxiv_url": "http://arxiv.org/abs/2507.18903v1",
      "published": "2025-07-25",
      "categories": [
        "stat.ML",
        "cs.LG"
      ]
    },
    {
      "title": "Deep Learning for Blood-Brain Barrier Permeability Prediction",
      "authors": [
        "Zihan Yang"
      ],
      "abstract": "Predicting whether a molecule can cross the blood-brain barrier (BBB) is a\nkey step in early-stage neuropharmaceutical development, directly influencing\nboth research efficiency and success rates in drug discovery. Traditional\nempirical methods based on physicochemical properties are prone to systematic\nmisjudgements due to their reliance on static rules. Early machine learning\nmodels, although data-driven, often suffer from limited capacity, poor\ngeneralization, and insufficient interpretability. In recent years, artificial\nintelligence (AI) methods have become essential tools for predicting BBB\npermeability and guiding related drug design, owing to their ability to model\nmolecular structures and capture complex biological mechanisms. This article\nsystematically reviews the evolution of this field-from deep neural networks to\ngraph-based structural modeling-highlighting the advantages of multi-task and\nmultimodal learning strategies in identifying mechanism-relevant variables. We\nfurther explore the emerging potential of generative models and causal\ninference methods for integrating permeability prediction with mechanism-aware\ndrug design. BBB modeling is in the transition from static classification\ntoward mechanistic perception and structure-function modeling. This paradigm\nshift provides a methodological foundation and future roadmap for the\nintegration of AI into neuropharmacological development.",
      "pdf_url": "http://arxiv.org/pdf/2507.18557v2",
      "arxiv_url": "http://arxiv.org/abs/2507.18557v2",
      "published": "2025-07-24",
      "categories": [
        "q-bio.QM"
      ]
    }
  ]
}