{
  "last_updated": "2025-10-04T00:45:55.241211",
  "papers": [
    {
      "title": "LLM4Rec: Large Language Models for Multimodal Generative Recommendation with Causal Debiasing",
      "authors": [
        "Bo Ma",
        "Hang Li",
        "ZeHua Hu",
        "XiaoFan Gui",
        "LuYao Liu",
        "Simon Lau"
      ],
      "abstract": "Contemporary generative recommendation systems face significant challenges in\nhandling multimodal data, eliminating algorithmic biases, and providing\ntransparent decision-making processes. This paper introduces an enhanced\ngenerative recommendation framework that addresses these limitations through\nfive key innovations: multimodal fusion architecture, retrieval-augmented\ngeneration mechanisms, causal inference-based debiasing, explainable\nrecommendation generation, and real-time adaptive learning capabilities. Our\nframework leverages advanced large language models as the backbone while\nincorporating specialized modules for cross-modal understanding, contextual\nknowledge integration, bias mitigation, explanation synthesis, and continuous\nmodel adaptation. Extensive experiments on three benchmark datasets\n(MovieLens-25M, Amazon-Electronics, Yelp-2023) demonstrate consistent\nimprovements in recommendation accuracy, fairness, and diversity compared to\nexisting approaches. The proposed framework achieves up to 2.3% improvement in\nNDCG@10 and 1.4% enhancement in diversity metrics while maintaining\ncomputational efficiency through optimized inference strategies.",
      "pdf_url": "http://arxiv.org/pdf/2510.01622v1",
      "arxiv_url": "http://arxiv.org/abs/2510.01622v1",
      "published": "2025-10-02",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "SLOPE and Designing Robust Studies for Generalization",
      "authors": [
        "Xinran Miao",
        "Jiwei Zhao",
        "Hyunseung Kang"
      ],
      "abstract": "A popular task in generalization is to learn about a new, target population\nbased on data from an existing, source population. This task relies on\nconditional exchangeability, which asserts that differences between the source\nand target populations are fully captured by observable characteristics of the\ntwo populations. Unfortunately, this assumption is often untenable in practice\ndue to unobservable differences between the source and target populations.\nWorse, the assumption cannot be verified with data, warranting the need for\nrobust data collection processes and study designs that are inherently less\nsensitive to violation of the assumption. In this paper, we propose SLOPE\n(Sensitivity of LOcal Perturbations from Exchangeability), a simple, intuitive,\nand novel measure that quantifies the sensitivity to local violation of\nconditional exchangeability. SLOPE combines ideas from sensitivity analysis in\ncausal inference and derivative-based measure of robustness from Hampel (1974).\nAmong other properties, SLOPE can help investigators to choose (a) a robust\nsource or target population or (b) a robust estimand. Also, we show an analytic\nrelationship between SLOPE and influence functions (IFs), which investigators\ncan use to derive SLOPE given an IF. We conclude with a re-analysis of a\nmulti-national randomized experiment and illustrate the role of SLOPE in\ninforming robust study designs for generalization.",
      "pdf_url": "http://arxiv.org/pdf/2510.01577v1",
      "arxiv_url": "http://arxiv.org/abs/2510.01577v1",
      "published": "2025-10-02",
      "categories": [
        "stat.ME"
      ]
    },
    {
      "title": "Large-Scale Bayesian Causal Discovery with Interventional Data",
      "authors": [
        "Seong Woo Han",
        "Daniel Duy Vo",
        "Brielin C. Brown"
      ],
      "abstract": "Inferring the causal relationships among a set of variables in the form of a\ndirected acyclic graph (DAG) is an important but notoriously challenging\nproblem. Recently, advancements in high-throughput genomic perturbation screens\nhave inspired development of methods that leverage interventional data to\nimprove model identification. However, existing methods still suffer poor\nperformance on large-scale tasks and fail to quantify uncertainty. Here, we\npropose Interventional Bayesian Causal Discovery (IBCD), an empirical Bayesian\nframework for causal discovery with interventional data. Our approach models\nthe likelihood of the matrix of total causal effects, which can be approximated\nby a matrix normal distribution, rather than the full data matrix. We place a\nspike-and-slab horseshoe prior on the edges and separately learn data-driven\nweights for scale-free and Erd\\H{o}s-R\\'enyi structures from observational\ndata, treating each edge as a latent variable to enable uncertainty-aware\ninference. Through extensive simulation, we show that IBCD achieves superior\nstructure recovery compared to existing baselines. We apply IBCD to CRISPR\nperturbation (Perturb-seq) data on 521 genes, demonstrating that edge posterior\ninclusion probabilities enable identification of robust graph structures.",
      "pdf_url": "http://arxiv.org/pdf/2510.01562v1",
      "arxiv_url": "http://arxiv.org/abs/2510.01562v1",
      "published": "2025-10-02",
      "categories": [
        "cs.LG"
      ]
    },
    {
      "title": "Executable Counterfactuals: Improving LLMs' Causal Reasoning Through Code",
      "authors": [
        "Aniket Vashishtha",
        "Qirun Dai",
        "Hongyuan Mei",
        "Amit Sharma",
        "Chenhao Tan",
        "Hao Peng"
      ],
      "abstract": "Counterfactual reasoning, a hallmark of intelligence, consists of three\nsteps: inferring latent variables from observations (abduction), constructing\nalternatives (interventions), and predicting their outcomes (prediction). This\nskill is essential for advancing LLMs' causal understanding and expanding their\napplications in high-stakes domains such as scientific research. However,\nexisting efforts in assessing LLM's counterfactual reasoning capabilities tend\nto skip the abduction step, effectively reducing to interventional reasoning\nand leading to overestimation of LLM performance. To address this, we introduce\nexecutable counterfactuals, a novel framework that operationalizes causal\nreasoning through code and math problems. Our framework explicitly requires all\nthree steps of counterfactual reasoning and enables scalable synthetic data\ncreation with varying difficulty, creating a frontier for evaluating and\nimproving LLM's reasoning. Our results reveal substantial drop in accuracy\n(25-40%) from interventional to counterfactual reasoning for SOTA models like\no4-mini and Claude-4-Sonnet. To address this gap, we construct a training set\ncomprising counterfactual code problems having if-else condition and test on\nout-of-domain code structures (e.g. having while-loop); we also test whether a\nmodel trained on code would generalize to counterfactual math word problems.\nWhile supervised finetuning on stronger models' reasoning traces improves\nin-domain performance of Qwen models, it leads to a decrease in accuracy on OOD\ntasks such as counterfactual math problems. In contrast, reinforcement learning\ninduces the core cognitive behaviors and generalizes to new domains, yielding\ngains over the base model on both code (improvement of 1.5x-2x) and math\nproblems. Analysis of the reasoning traces reinforces these findings and\nhighlights the promise of RL for improving LLMs' counterfactual reasoning.",
      "pdf_url": "http://arxiv.org/pdf/2510.01539v1",
      "arxiv_url": "http://arxiv.org/abs/2510.01539v1",
      "published": "2025-10-02",
      "categories": [
        "cs.LG"
      ]
    },
    {
      "title": "TAG-EQA: Text-And-Graph for Event Question Answering via Structured Prompting Strategies",
      "authors": [
        "Maithili Kadam",
        "Francis Ferraro"
      ],
      "abstract": "Large language models (LLMs) excel at general language tasks but often\nstruggle with event-based questions-especially those requiring causal or\ntemporal reasoning. We introduce TAG-EQA (Text-And-Graph for Event Question\nAnswering), a prompting framework that injects causal event graphs into LLM\ninputs by converting structured relations into natural-language statements.\nTAG-EQA spans nine prompting configurations, combining three strategies\n(zero-shot, few-shot, chain-of-thought) with three input modalities (text-only,\ngraph-only, text+graph), enabling a systematic analysis of when and how\nstructured knowledge aids inference. On the TORQUESTRA benchmark, TAG-EQA\nimproves accuracy by 5% on average over text-only baselines, with gains up to\n12% in zero-shot settings and 18% when graph-augmented CoT prompting is\neffective. While performance varies by model and configuration, our findings\nshow that causal graphs can enhance event reasoning in LLMs without\nfine-tuning, offering a flexible way to encode structure in prompt-based QA.",
      "pdf_url": "http://arxiv.org/pdf/2510.01391v1",
      "arxiv_url": "http://arxiv.org/abs/2510.01391v1",
      "published": "2025-10-01",
      "categories": [
        "cs.CL"
      ]
    },
    {
      "title": "The causal structure of galactic astrophysics",
      "authors": [
        "Harry Desmond",
        "Joseph Ramsey"
      ],
      "abstract": "Data-driven astrophysics currently relies on the detection and\ncharacterisation of correlations between objects' properties, which are then\nused to test physical theories that make predictions for them. This process\nfails to utilise information in the data that forms a crucial part of the\ntheories' predictions, namely which variables are directly correlated (as\nopposed to accidentally correlated through others), the directions of these\ndeterminations, and the presence or absence of confounders that correlate\nvariables in the dataset but are themselves absent from it. We propose to\nrecover this information through causal discovery, a well-developed methodology\nfor inferring the causal structure of datasets that is however almost entirely\nunknown to astrophysics. We develop a causal discovery algorithm suitable for\nastrophysical datasets and illustrate it on $\\sim$5$\\times10^5$ low-redshift\ngalaxies from the Nasa Sloan Atlas, demonstrating its ability to distinguish\nphysical mechanisms that are degenerate on the basis of correlations alone.",
      "pdf_url": "http://arxiv.org/pdf/2510.01112v1",
      "arxiv_url": "http://arxiv.org/abs/2510.01112v1",
      "published": "2025-10-01",
      "categories": [
        "astro-ph.GA",
        "astro-ph.CO",
        "cs.LG",
        "stat.AP",
        "stat.ME"
      ]
    },
    {
      "title": "InfVSR: Breaking Length Limits of Generic Video Super-Resolution",
      "authors": [
        "Ziqing Zhang",
        "Kai Liu",
        "Zheng Chen",
        "Xi Li",
        "Yucong Chen",
        "Bingnan Duan",
        "Linghe Kong",
        "Yulun Zhang"
      ],
      "abstract": "Real-world videos often extend over thousands of frames. Existing video\nsuper-resolution (VSR) approaches, however, face two persistent challenges when\nprocessing long sequences: (1) inefficiency due to the heavy cost of multi-step\ndenoising for full-length sequences; and (2) poor scalability hindered by\ntemporal decomposition that causes artifacts and discontinuities. To break\nthese limits, we propose InfVSR, which novelly reformulates VSR as an\nautoregressive-one-step-diffusion paradigm. This enables streaming inference\nwhile fully leveraging pre-trained video diffusion priors. First, we adapt the\npre-trained DiT into a causal structure, maintaining both local and global\ncoherence via rolling KV-cache and joint visual guidance. Second, we distill\nthe diffusion process into a single step efficiently, with patch-wise pixel\nsupervision and cross-chunk distribution matching. Together, these designs\nenable efficient and scalable VSR for unbounded-length videos. To fill the gap\nin long-form video evaluation, we build a new benchmark tailored for extended\nsequences and further introduce semantic-level metrics to comprehensively\nassess temporal consistency. Our method pushes the frontier of long-form VSR,\nachieves state-of-the-art quality with enhanced semantic consistency, and\ndelivers up to 58x speed-up over existing methods such as MGLD-VSR. Code will\nbe available at https://github.com/Kai-Liu001/InfVSR.",
      "pdf_url": "http://arxiv.org/pdf/2510.00948v1",
      "arxiv_url": "http://arxiv.org/abs/2510.00948v1",
      "published": "2025-10-01",
      "categories": [
        "cs.CV"
      ]
    },
    {
      "title": "Gather-Scatter Mamba: Accelerating Propagation with Efficient State Space Model",
      "authors": [
        "Hyun-kyu Ko",
        "Youbin Kim",
        "Jihyeon Park",
        "Dongheok Park",
        "Gyeongjin Kang",
        "Wonjun Cho",
        "Hyung Yi",
        "Eunbyung Park"
      ],
      "abstract": "State Space Models (SSMs)-most notably RNNs-have historically played a\ncentral role in sequential modeling. Although attention mechanisms such as\nTransformers have since dominated due to their ability to model global context,\ntheir quadratic complexity and limited scalability make them less suited for\nlong sequences. Video super-resolution (VSR) methods have traditionally relied\non recurrent architectures to propagate features across frames. However, such\napproaches suffer from well-known issues including vanishing gradients, lack of\nparallelism, and slow inference speed. Recent advances in selective SSMs like\nMamba offer a compelling alternative: by enabling input-dependent state\ntransitions with linear-time complexity, Mamba mitigates these issues while\nmaintaining strong long-range modeling capabilities. Despite this potential,\nMamba alone struggles to capture fine-grained spatial dependencies due to its\ncausal nature and lack of explicit context aggregation. To address this, we\npropose a hybrid architecture that combines shifted window self-attention for\nspatial context aggregation with Mamba-based selective scanning for efficient\ntemporal propagation. Furthermore, we introduce Gather-Scatter Mamba (GSM), an\nalignment-aware mechanism that warps features toward a center anchor frame\nwithin the temporal window before Mamba propagation and scatters them back\nafterward, effectively reducing occlusion artifacts and ensuring effective\nredistribution of aggregated information across all frames. The official\nimplementation is provided at: https://github.com/Ko-Lani/GSMamba.",
      "pdf_url": "http://arxiv.org/pdf/2510.00862v1",
      "arxiv_url": "http://arxiv.org/abs/2510.00862v1",
      "published": "2025-10-01",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "The Economic Impact of DeFi Crime Events on Decentralized Autonomous Organizations (DAOs)",
      "authors": [
        "Stefan Kitzler",
        "Masarah Paquet-Clouston",
        "Bernhard Haslhofer"
      ],
      "abstract": "The Decentralized Finance (DeFi) ecosystem has experienced over \\$10 billion\nin direct losses due to crime events. Beyond these immediate losses, such\nevents often trigger broader market reactions, including price declines,\ntrading activity changes, and reductions in market capitalization.\nDecentralized Autonomous Organizations (DAOs) govern DeFi applications through\ntradable governance assets that function like corporate shares for voting and\ndecision-making. Leveraging DeFi's granular trading data, we conduct an event\nstudy on 22 crime events between 2020 and 2022 to assess their economic impact\non governance asset prices, trading volumes, and market capitalization. Using a\ndynamic difference-in-differences (DiD) framework with counterfactual\ngovernance assets, we aim for causal inference of intraday temporal effects.\nOur results show that 55% of crime events lead to significant negative price\nimpacts, with an average decline of about 14%. Additionally, 68% of crime\nevents lead to increased governance asset trading volume. Based on these\nimpacts, we estimate indirect economic losses of over $1.3 billion in DAO\nmarket capitalization, far exceeding direct victim costs and accounting for 74%\nof total losses. Our study provides valuable insights into how crime events\nshape market dynamics and affect DAOs. Moreover, our methodological approach is\nreproducible and applicable beyond DAOs, offering a framework to assess the\nindirect economic impact on other cryptoassets.",
      "pdf_url": "http://arxiv.org/pdf/2510.00669v1",
      "arxiv_url": "http://arxiv.org/abs/2510.00669v1",
      "published": "2025-10-01",
      "categories": [
        "cs.CE"
      ]
    },
    {
      "title": "Threats to the sustainability of Community Notes on X",
      "authors": [
        "Zahra Arjmandi-Lari",
        "Alexios Mantzarlis",
        "Tom Stafford"
      ],
      "abstract": "Community Notes are emerging as an important option for content moderation.\nThe Community Notes system pioneered by Twitter, now known as X, uses a\nbridging algorithm to identify user-generated context with upvotes across\npolitical divides, supposedly spinning consensual gold from partisan straw. It\nis important to understand the nature of the community behind Community Notes,\nespecially as the feature has now been imitated by several billion-user\nplatforms. We look for signs of stability and disruption in the X Community\nNotes community and interrogate the motivations other than partisan animus\n(Allen, Martel, and Rand 2022) which may be driving users to contribute. We\nconduct a novel analysis of the impact of having a note published, which\nrequires being considered \"helpful\" by the bridging algorithm, utilising a\nregression discontinuity design. This allows stronger causal inference than\nconventional methods used with observational data. Our analysis shows the\npositive effect on future note authoring of having a note published. This\nhighlights the risk of the current system, where the proportion of notes\nconsidered \"helpful\" (and therefore shown to users on X) is low, 10%, and\ndeclining. This analysis has implications for the future of Community Notes on\nX and the extension of this approach to other platforms.",
      "pdf_url": "http://arxiv.org/pdf/2510.00650v1",
      "arxiv_url": "http://arxiv.org/abs/2510.00650v1",
      "published": "2025-10-01",
      "categories": [
        "cs.SI"
      ]
    }
  ]
}