{
  "last_updated": "2025-09-17T00:48:48.179359",
  "papers": [
    {
      "title": "AvatarSync: Rethinking Talking-Head Animation through Autoregressive Perspective",
      "authors": [
        "Yuchen Deng",
        "Xiuyang Wu",
        "Hai-Tao Zheng",
        "Suiyang Zhang",
        "Yi He",
        "Yuxing Han"
      ],
      "abstract": "Existing talking-head animation approaches based on Generative Adversarial\nNetworks (GANs) or diffusion models often suffer from inter-frame flicker,\nidentity drift, and slow inference. These limitations inherent to their video\ngeneration pipelines restrict their suitability for applications. To address\nthis, we introduce AvatarSync, an autoregressive framework on phoneme\nrepresentations that generates realistic and controllable talking-head\nanimations from a single reference image, driven directly text or audio input.\nIn addition, AvatarSync adopts a two-stage generation strategy, decoupling\nsemantic modeling from visual dynamics, which is a deliberate \"Divide and\nConquer\" design. The first stage, Facial Keyframe Generation (FKG), focuses on\nphoneme-level semantic representation by leveraging the many-to-one mapping\nfrom text or audio to phonemes. A Phoneme-to-Visual Mapping is constructed to\nanchor abstract phonemes to character-level units. Combined with a customized\nText-Frame Causal Attention Mask, the keyframes are generated. The second\nstage, inter-frame interpolation, emphasizes temporal coherence and visual\nsmoothness. We introduce a timestamp-aware adaptive strategy based on a\nselective state space model, enabling efficient bidirectional context\nreasoning. To support deployment, we optimize the inference pipeline to reduce\nlatency without compromising visual fidelity. Extensive experiments show that\nAvatarSync outperforms existing talking-head animation methods in visual\nfidelity, temporal consistency, and computational efficiency, providing a\nscalable and controllable solution.",
      "pdf_url": "http://arxiv.org/pdf/2509.12052v1",
      "arxiv_url": "http://arxiv.org/abs/2509.12052v1",
      "published": "2025-09-15",
      "categories": [
        "cs.CV"
      ]
    },
    {
      "title": "A Survey of Reasoning and Agentic Systems in Time Series with Large Language Models",
      "authors": [
        "Ching Chang",
        "Yidan Shi",
        "Defu Cao",
        "Wei Yang",
        "Jeehyun Hwang",
        "Haixin Wang",
        "Jiacheng Pang",
        "Wei Wang",
        "Yan Liu",
        "Wen-Chih Peng",
        "Tien-Fu Chen"
      ],
      "abstract": "Time series reasoning treats time as a first-class axis and incorporates\nintermediate evidence directly into the answer. This survey defines the problem\nand organizes the literature by reasoning topology with three families: direct\nreasoning in one step, linear chain reasoning with explicit intermediates, and\nbranch-structured reasoning that explores, revises, and aggregates. The\ntopology is crossed with the main objectives of the field, including\ntraditional time series analysis, explanation and understanding, causal\ninference and decision making, and time series generation, while a compact tag\nset spans these axes and captures decomposition and verification, ensembling,\ntool use, knowledge access, multimodality, agent loops, and LLM alignment\nregimes. Methods and systems are reviewed across domains, showing what each\ntopology enables and where it breaks down in faithfulness or robustness, along\nwith curated datasets, benchmarks, and resources that support study and\ndeployment (https://github.com/blacksnail789521/Time-Series-Reasoning-Survey).\nEvaluation practices that keep evidence visible and temporally aligned are\nhighlighted, and guidance is distilled on matching topology to uncertainty,\ngrounding with observable artifacts, planning for shift and streaming, and\ntreating cost and latency as design budgets. We emphasize that reasoning\nstructures must balance capacity for grounding and self-correction against\ncomputational cost and reproducibility, while future progress will likely\ndepend on benchmarks that tie reasoning quality to utility and on closed-loop\ntestbeds that trade off cost and risk under shift-aware, streaming, and\nlong-horizon settings. Taken together, these directions mark a shift from\nnarrow accuracy toward reliability at scale, enabling systems that not only\nanalyze but also understand, explain, and act on dynamic worlds with traceable\nevidence and credible outcomes.",
      "pdf_url": "http://arxiv.org/pdf/2509.11575v1",
      "arxiv_url": "http://arxiv.org/abs/2509.11575v1",
      "published": "2025-09-15",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Mendelian Randomization Methods for Causal Inference: Estimands, Identification and Inference",
      "authors": [
        "Minhao Yao",
        "Anqi Wang",
        "Xihao Li",
        "Zhonghua Liu"
      ],
      "abstract": "Mendelian randomization (MR) has become an essential tool for causal\ninference in biomedical and public health research. By using genetic variants\nas instrumental variables, MR helps address unmeasured confounding and reverse\ncausation, offering a quasi-experimental framework to evaluate causal effects\nof modifiable exposures on health outcomes. Despite its promise, MR faces\nsubstantial methodological challenges, including invalid instruments, weak\ninstrument bias, and design complexities across different data structures. In\nthis tutorial review, we provide a comprehensive overview of MR methods for\ncausal inference, emphasizing clarity of causal interpretation, study design\ncomparisons, availability of software tools, and practical guidance for applied\nscientists. We organize the review around causal estimands, ensuring that\nanalyses are anchored to well-defined causal questions. We discuss the problems\nof invalid and weak instruments, comparing available strategies for their\ndetection and correction. We integrate discussions of population-based versus\nfamily-based MR designs, analyses based on individual-level versus\nsummary-level data, and one-sample versus two-sample MR designs, highlighting\ntheir relative advantages and limitations. We also summarize recent\nmethodological advances and software developments that extend MR to settings\nwith many weak or invalid instruments and to modern high-dimensional omics\ndata. Real-data applications, including UK Biobank and Alzheimer's disease\nproteomics studies, illustrate the use of these methods in practice. This\nreview aims to serve as a tutorial-style reference for both methodologists and\napplied scientists.",
      "pdf_url": "http://arxiv.org/pdf/2509.11519v1",
      "arxiv_url": "http://arxiv.org/abs/2509.11519v1",
      "published": "2025-09-15",
      "categories": [
        "stat.ME",
        "stat.AP"
      ]
    },
    {
      "title": "The Honest Truth About Causal Trees: Accuracy Limits for Heterogeneous Treatment Effect Estimation",
      "authors": [
        "Matias D. Cattaneo",
        "Jason M. Klusowski",
        "Ruiqi Rae Yu"
      ],
      "abstract": "Recursive decision trees have emerged as a leading methodology for\nheterogeneous causal treatment effect estimation and inference in experimental\nand observational settings. These procedures are fitted using the celebrated\nCART (Classification And Regression Tree) algorithm [Breiman et al., 1984], or\ncustom variants thereof, and hence are believed to be \"adaptive\" to\nhigh-dimensional data, sparsity, or other specific features of the underlying\ndata generating process. Athey and Imbens [2016] proposed several \"honest\"\ncausal decision tree estimators, which have become the standard in both\nacademia and industry. We study their estimators, and variants thereof, and\nestablish lower bounds on their estimation error. We demonstrate that these\npopular heterogeneous treatment effect estimators cannot achieve a\npolynomial-in-$n$ convergence rate under basic conditions, where $n$ denotes\nthe sample size. Contrary to common belief, honesty does not resolve these\nlimitations and at best delivers negligible logarithmic improvements in sample\nsize or dimension. As a result, these commonly used estimators can exhibit poor\nperformance in practice, and even be inconsistent in some settings. Our\ntheoretical insights are empirically validated through simulations.",
      "pdf_url": "http://arxiv.org/pdf/2509.11381v1",
      "arxiv_url": "http://arxiv.org/abs/2509.11381v1",
      "published": "2025-09-14",
      "categories": [
        "math.ST",
        "econ.EM",
        "stat.ME",
        "stat.ML",
        "stat.TH"
      ]
    },
    {
      "title": "PersonaX: Multimodal Datasets with LLM-Inferred Behavior Traits",
      "authors": [
        "Loka Li",
        "Wong Yu Kang",
        "Minghao Fu",
        "Guangyi Chen",
        "Zhenhao Chen",
        "Gongxu Luo",
        "Yuewen Sun",
        "Salman Khan",
        "Peter Spirtes",
        "Kun Zhang"
      ],
      "abstract": "Understanding human behavior traits is central to applications in\nhuman-computer interaction, computational social science, and personalized AI\nsystems. Such understanding often requires integrating multiple modalities to\ncapture nuanced patterns and relationships. However, existing resources rarely\nprovide datasets that combine behavioral descriptors with complementary\nmodalities such as facial attributes and biographical information. To address\nthis gap, we present PersonaX, a curated collection of multimodal datasets\ndesigned to enable comprehensive analysis of public traits across modalities.\nPersonaX consists of (1) CelebPersona, featuring 9444 public figures from\ndiverse occupations, and (2) AthlePersona, covering 4181 professional athletes\nacross 7 major sports leagues. Each dataset includes behavioral trait\nassessments inferred by three high-performing large language models, alongside\nfacial imagery and structured biographical features. We analyze PersonaX at two\ncomplementary levels. First, we abstract high-level trait scores from text\ndescriptions and apply five statistical independence tests to examine their\nrelationships with other modalities. Second, we introduce a novel causal\nrepresentation learning (CRL) framework tailored to multimodal and\nmulti-measurement data, providing theoretical identifiability guarantees.\nExperiments on both synthetic and real-world data demonstrate the effectiveness\nof our approach. By unifying structured and unstructured analysis, PersonaX\nestablishes a foundation for studying LLM-inferred behavioral traits in\nconjunction with visual and biographical attributes, advancing multimodal trait\nanalysis and causal reasoning.",
      "pdf_url": "http://arxiv.org/pdf/2509.11362v1",
      "arxiv_url": "http://arxiv.org/abs/2509.11362v1",
      "published": "2025-09-14",
      "categories": [
        "cs.LG",
        "cs.CV"
      ]
    },
    {
      "title": "Traffic-MLLM: A Spatio-Temporal MLLM with Retrieval-Augmented Generation for Causal Inference in Traffic",
      "authors": [
        "Waikit Xiu",
        "Qiang Lu",
        "Xiying Li",
        "Chen Hu",
        "Shengbo Sun"
      ],
      "abstract": "As intelligent transportation systems advance, traffic video understanding\nplays an increasingly pivotal role in comprehensive scene perception and causal\nanalysis. Yet, existing approaches face notable challenges in accurately\nmodeling spatiotemporal causality and integrating domain-specific knowledge,\nlimiting their effectiveness in complex scenarios. To address these\nlimitations, we propose Traffic-MLLM, a multimodal large language model\ntailored for fine-grained traffic analysis. Built on the Qwen2.5-VL backbone,\nour model leverages high-quality traffic-specific multimodal datasets and uses\nLow-Rank Adaptation (LoRA) for lightweight fine-tuning, significantly enhancing\nits capacity to model continuous spatiotemporal features in video sequences.\nFurthermore, we introduce an innovative knowledge prompting module fusing\nChain-of-Thought (CoT) reasoning with Retrieval-Augmented Generation (RAG),\nenabling precise injection of detailed traffic regulations and domain knowledge\ninto the inference process. This design markedly boosts the model's logical\nreasoning and knowledge adaptation capabilities. Experimental results on\nTrafficQA and DriveQA benchmarks show Traffic-MLLM achieves state-of-the-art\nperformance, validating its superior ability to process multimodal traffic\ndata. It also exhibits remarkable zero-shot reasoning and cross-scenario\ngeneralization capabilities.",
      "pdf_url": "http://arxiv.org/pdf/2509.11165v1",
      "arxiv_url": "http://arxiv.org/abs/2509.11165v1",
      "published": "2025-09-14",
      "categories": [
        "cs.CV"
      ]
    },
    {
      "title": "A Latent Factor Panel Approach to Spatiotemporal Causal Inference",
      "authors": [
        "Jiaxi Wu",
        "Alexander Franks"
      ],
      "abstract": "Unmeasured confounding can severely bias causal effect estimates from\nspatiotemporal observational data, especially when the confounders do not vary\nsmoothly in time and space. In this work, we develop a method for addressing\nunmeasured confounding in spatiotemporal contexts by building on models from\nthe panel data literature and methods in multivariate causal inference. Our\nmethod is based on a factor confounding assumption, which posits that effects\nof unmeasured confounders on exposures and outcomes can be captured by a shared\nlatent factor model. Factor confounding is sufficient to partially identify\ncausal effects, even when there is interference between units. Additional\nassumptions that limit the degree of spatiotemporal interference, reasonable in\nmost applications, are sufficient to point identify the effects. Simulation\nstudies demonstrate that the proposed approach can substantially reduce omitted\nvariable bias relative to other spatial smoothing and panel data baselines. We\nillustrate our method in a case study of the effect of prenatal PM2.5 exposure\non birth weight in California.",
      "pdf_url": "http://arxiv.org/pdf/2509.10974v1",
      "arxiv_url": "http://arxiv.org/abs/2509.10974v1",
      "published": "2025-09-13",
      "categories": [
        "stat.ME"
      ]
    },
    {
      "title": "Two-Stage Least Squares Instrumental Variable Estimation for Semiparametric Accelerated Failure Time Models with Right-Censored Data",
      "authors": [
        "Zian Zhuang",
        "Hua Zhou",
        "Jin Zhou",
        "Gang Li"
      ],
      "abstract": "Instrumental variable (IV) analysis is widely used in fields such as\neconomics and epidemiology to address unobserved confounding and measurement\nerror when estimating the causal effects of intermediate covariates on\noutcomes. However, extending the commonly used two-stage least squares (TSLS)\napproach to survival settings is nontrivial due to censoring. This paper\nintroduces a novel extension of TSLS to the semiparametric accelerated failure\ntime (AFT) model with right-censored data, supported by rigorous theoretical\njustification. Specifically, we propose an iterative reweighted generalized\nestimating equation (GEE) approach that incorporates Leurgans' synthetic\nvariable method, establish the asymptotic properties of the resulting\nestimator, and derive a consistent variance estimator, enabling valid causal\ninference. Simulation studies are conducted to evaluate the finite-sample\nperformance of the proposed method across different scenarios. The results show\nthat it outperforms the naive unweighted GEE method, a parametric IV approach,\nand a one-stage estimator without IV. The proposed method is also highly\nscalable to large datasets, achieving a 300- to 1500-fold speedup relative to a\nBayesian parametric IV approach in both simulations and the real-data example.\nWe further illustrate the utility of the proposed method through a real-data\napplication using the UK Biobank data.",
      "pdf_url": "http://arxiv.org/pdf/2509.10905v1",
      "arxiv_url": "http://arxiv.org/abs/2509.10905v1",
      "published": "2025-09-13",
      "categories": [
        "stat.ME"
      ]
    },
    {
      "title": "Causal Emergence of Consciousness through Learned Multiscale Neural Dynamics in Mice",
      "authors": [
        "Zhipeng Wang",
        "Yingqi Rong",
        "Kaiwei Liu",
        "Mingzhe Yang",
        "Jiang Zhang",
        "Jing He"
      ],
      "abstract": "Consciousness spans macroscopic experience and microscopic neuronal activity,\nyet linking these scales remains challenging. Prevailing theories, such as\nIntegrated Information Theory, focus on a single scale, overlooking how causal\npower and its dynamics unfold across scales. Progress is constrained by scarce\ncross-scale data and difficulties in quantifying multiscale causality and\ndynamics. Here, we present a machine learning framework that infers multiscale\ncausal variables and their dynamics from near-cellular-resolution calcium\nimaging in the mouse dorsal cortex. At lower levels, variables primarily\naggregate input-driven information, whereas at higher levels they realize\ncausality through metastable or saddle-point dynamics during wakefulness,\ncollapsing into localized, stochastic dynamics under anesthesia. A\none-dimensional top-level conscious variable captures the majority of causal\npower, yet variables across other scales also contribute substantially, giving\nrise to high emergent complexity in the conscious state. Together, these\nfindings provide a multiscale causal framework that links neural activity to\nconscious states.",
      "pdf_url": "http://arxiv.org/pdf/2509.10891v1",
      "arxiv_url": "http://arxiv.org/abs/2509.10891v1",
      "published": "2025-09-13",
      "categories": [
        "q-bio.NC"
      ]
    },
    {
      "title": "Abduct, Act, Predict: Scaffolding Causal Inference for Automated Failure Attribution in Multi-Agent Systems",
      "authors": [
        "Alva West",
        "Yixuan Weng",
        "Minjun Zhu",
        "Zhen Lin",
        "Yue Zhang"
      ],
      "abstract": "Failure attribution in multi-agent systems -- pinpointing the exact step\nwhere a decisive error occurs -- is a critical yet unsolved challenge. Current\nmethods treat this as a pattern recognition task over long conversation logs,\nleading to critically low step-level accuracy (below 17\\%), which renders them\nimpractical for debugging complex systems. Their core weakness is a fundamental\ninability to perform robust counterfactual reasoning: to determine if\ncorrecting a single action would have actually averted the task failure. To\nbridge this counterfactual inference gap, we introduce Abduct-Act-Predict (A2P)\nScaffolding, a novel agent framework that transforms failure attribution from\npattern recognition into a structured causal inference task. A2P explicitly\nguides a large language model through a formal three-step reasoning process\nwithin a single inference pass: (1) Abduction, to infer the hidden root causes\nbehind an agent's actions; (2) Action, to define a minimal corrective\nintervention; and (3) Prediction, to simulate the subsequent trajectory and\nverify if the intervention resolves the failure. This structured approach\nleverages the holistic context of the entire conversation while imposing a\nrigorous causal logic on the model's analysis. Our extensive experiments on the\nWho\\&When benchmark demonstrate its efficacy. On the Algorithm-Generated\ndataset, A2P achieves 47.46\\% step-level accuracy, a 2.85$\\times$ improvement\nover the 16.67\\% of the baseline. On the more complex Hand-Crafted dataset, it\nachieves 29.31\\% step accuracy, a 2.43$\\times$ improvement over the baseline's\n12.07\\%. By reframing the problem through a causal lens, A2P Scaffolding\nprovides a robust, verifiable, and significantly more accurate solution for\nautomated failure attribution.",
      "pdf_url": "http://arxiv.org/pdf/2509.10401v1",
      "arxiv_url": "http://arxiv.org/abs/2509.10401v1",
      "published": "2025-09-12",
      "categories": [
        "cs.AI",
        "cs.CL"
      ]
    }
  ]
}