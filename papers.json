{
  "last_updated": "2025-09-25T00:50:07.703591",
  "papers": [
    {
      "title": "Track-On2: Enhancing Online Point Tracking with Memory",
      "authors": [
        "Görkay Aydemir",
        "Weidi Xie",
        "Fatma Güney"
      ],
      "abstract": "In this paper, we consider the problem of long-term point tracking, which\nrequires consistent identification of points across video frames under\nsignificant appearance changes, motion, and occlusion. We target the online\nsetting, i.e. tracking points frame-by-frame, making it suitable for real-time\nand streaming applications. We extend our prior model Track-On into Track-On2,\na simple and efficient transformer-based model for online long-term tracking.\nTrack-On2 improves both performance and efficiency through architectural\nrefinements, more effective use of memory, and improved synthetic training\nstrategies. Unlike prior approaches that rely on full-sequence access or\niterative updates, our model processes frames causally and maintains temporal\ncoherence via a memory mechanism, which is key to handling drift and occlusions\nwithout requiring future frames. At inference, we perform coarse patch-level\nclassification followed by refinement. Beyond architecture, we systematically\nstudy synthetic training setups and their impact on memory behavior, showing\nhow they shape temporal robustness over long sequences. Through comprehensive\nexperiments, Track-On2 achieves state-of-the-art results across five synthetic\nand real-world benchmarks, surpassing prior online trackers and even strong\noffline methods that exploit bidirectional context. These results highlight the\neffectiveness of causal, memory-based architectures trained purely on synthetic\ndata as scalable solutions for real-world point tracking. Project page:\nhttps://kuis-ai.github.io/track_on2",
      "pdf_url": "http://arxiv.org/pdf/2509.19115v1",
      "arxiv_url": "http://arxiv.org/abs/2509.19115v1",
      "published": "2025-09-23",
      "categories": [
        "cs.CV"
      ]
    },
    {
      "title": "Towards Practical Multi-label Causal Discovery in High-Dimensional Event Sequences via One-Shot Graph Aggregation",
      "authors": [
        "Hugo Math",
        "Rainer Lienhart"
      ],
      "abstract": "Understanding causality in event sequences where outcome labels such as\ndiseases or system failures arise from preceding events like symptoms or error\ncodes is critical. Yet remains an unsolved challenge across domains like\nhealthcare or vehicle diagnostics. We introduce CARGO, a scalable multi-label\ncausal discovery method for sparse, high-dimensional event sequences comprising\nof thousands of unique event types. Using two pretrained causal Transformers as\ndomain-specific foundation models for event sequences. CARGO infers in\nparallel, per sequence one-shot causal graphs and aggregates them using an\nadaptive frequency fusion to reconstruct the global Markov boundaries of\nlabels. This two-stage approach enables efficient probabilistic reasoning at\nscale while bypassing the intractable cost of full-dataset conditional\nindependence testing. Our results on a challenging real-world automotive fault\nprediction dataset with over 29,100 unique event types and 474 imbalanced\nlabels demonstrate CARGO's ability to perform structured reasoning.",
      "pdf_url": "http://arxiv.org/pdf/2509.19112v1",
      "arxiv_url": "http://arxiv.org/abs/2509.19112v1",
      "published": "2025-09-23",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Memory in Large Language Models: Mechanisms, Evaluation and Evolution",
      "authors": [
        "Dianxing Zhang",
        "Wendong Li",
        "Kani Song",
        "Jiaye Lu",
        "Gang Li",
        "Liuchun Yang",
        "Sheng Li"
      ],
      "abstract": "Under a unified operational definition, we define LLM memory as a persistent\nstate written during pretraining, finetuning, or inference that can later be\naddressed and that stably influences outputs. We propose a four-part taxonomy\n(parametric, contextual, external, procedural/episodic) and a memory quadruple\n(location, persistence, write/access path, controllability). We link mechanism,\nevaluation, and governance via the chain write -> read -> inhibit/update. To\navoid distorted comparisons across heterogeneous setups, we adopt a\nthree-setting protocol (parametric only, offline retrieval, online retrieval)\nthat decouples capability from information availability on the same data and\ntimeline. On this basis we build a layered evaluation: parametric (closed-book\nrecall, edit differential, memorization/privacy), contextual (position curves\nand the mid-sequence drop), external (answer correctness vs snippet\nattribution/faithfulness), and procedural/episodic (cross-session consistency\nand timeline replay, E MARS+). The framework integrates temporal governance and\nleakage auditing (freshness hits, outdated answers, refusal slices) and\nuncertainty reporting via inter-rater agreement plus paired tests with\nmultiple-comparison correction. For updating and forgetting, we present DMM\nGov: coordinating DAPT/TAPT, PEFT, model editing (ROME, MEND, MEMIT, SERAC),\nand RAG to form an auditable loop covering admission thresholds, rollout,\nmonitoring, rollback, and change audits, with specs for timeliness, conflict\nhandling, and long-horizon consistency. Finally, we give four testable\npropositions: minimum identifiability; a minimal evaluation card; causally\nconstrained editing with verifiable forgetting; and when retrieval with\nsmall-window replay outperforms ultra-long-context reading. This yields a\nreproducible, comparable, and governable coordinate system for research and\ndeployment.",
      "pdf_url": "http://arxiv.org/pdf/2509.18868v1",
      "arxiv_url": "http://arxiv.org/abs/2509.18868v1",
      "published": "2025-09-23",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Optimization-centric cutting feedback for semiparametric models",
      "authors": [
        "Linda S. L. Tan",
        "David J. Nott",
        "David T. Frazier"
      ],
      "abstract": "Modern statistics deals with complex models from which the joint model used\nfor inference is built by coupling submodels, called modules. We consider\nmodular inference where the modules may depend on parametric and nonparametric\ncomponents. In such cases, a joint Bayesian inference is highly susceptible to\nmisspecification across any module, and inappropriate priors for nonparametric\ncomponents may deliver subpar inferences for parametric components, and vice\nversa. We propose a novel ``optimization-centric'' approach to cutting feedback\nfor semiparametric modular inference, which can address misspecification and\nprior-data conflicts. The proposed generalized cut posteriors are defined\nthrough a variational optimization problem for generalized posteriors where\nregularization is based on R\\'{e}nyi divergence, rather than Kullback-Leibler\ndivergence (KLD), and variational computational methods are developed. We show\nempirically that using R\\'{e}nyi divergence to define the cut posterior\ndelivers more robust inferences than KLD. We derive novel posterior\nconcentration results that accommodate the R\\'{e}nyi divergence and allow for\nsemiparametric components, greatly extending existing results for cut\nposteriors that were derived for parametric models and KLD. We demonstrate\nthese new methods in a benchmark toy example and two real examples: Gaussian\nprocess adjustments for confounding in causal inference and misspecified copula\nmodels with nonparametric marginals.",
      "pdf_url": "http://arxiv.org/pdf/2509.18708v1",
      "arxiv_url": "http://arxiv.org/abs/2509.18708v1",
      "published": "2025-09-23",
      "categories": [
        "stat.ME",
        "math.ST",
        "stat.CO",
        "stat.TH"
      ]
    },
    {
      "title": "Estimating Heterogeneous Causal Effect on Networks via Orthogonal Learning",
      "authors": [
        "Yuanchen Wu",
        "Yubai Yuan"
      ],
      "abstract": "Estimating causal effects on networks is important for both scientific\nresearch and practical applications. Unlike traditional settings that assume\nthe Stable Unit Treatment Value Assumption (SUTVA), interference allows an\nintervention/treatment on one unit to affect the outcomes of others.\nUnderstanding both direct and spillover effects is critical in fields such as\nepidemiology, political science, and economics. Causal inference on networks\nfaces two main challenges. First, causal effects are typically heterogeneous,\nvarying with unit features and local network structure. Second, connected units\noften exhibit dependence due to network homophily, creating confounding between\nstructural correlations and causal effects. In this paper, we propose a\ntwo-stage method to estimate heterogeneous direct and spillover effects on\nnetworks. The first stage uses graph neural networks to estimate nuisance\ncomponents that depend on the complex network topology. In the second stage, we\nadjust for network confounding using these estimates and infer causal effects\nthrough a novel attention-based interference model. Our approach balances\nexpressiveness and interpretability, enabling downstream tasks such as\nidentifying influential neighborhoods and recovering the sign of spillover\neffects. We integrate the two stages using Neyman orthogonalization and\ncross-fitting, which ensures that errors from nuisance estimation contribute\nonly at higher order. As a result, our causal effect estimates are robust to\nbias and misspecification in modeling causal effects under network\ndependencies.",
      "pdf_url": "http://arxiv.org/pdf/2509.18484v1",
      "arxiv_url": "http://arxiv.org/abs/2509.18484v1",
      "published": "2025-09-23",
      "categories": [
        "stat.ML",
        "cs.LG"
      ]
    },
    {
      "title": "The Narcissus Hypothesis: Descending to the Rung of Illusion",
      "authors": [
        "Riccardo Cadei",
        "Christian Internò"
      ],
      "abstract": "Modern foundational models increasingly reflect not just world knowledge, but\npatterns of human preference embedded in their training data. We hypothesize\nthat recursive alignment-via human feedback and model-generated corpora-induces\na social desirability bias, nudging models to favor agreeable or flattering\nresponses over objective reasoning. We refer to it as the Narcissus Hypothesis\nand test it across 31 models using standardized personality assessments and a\nnovel Social Desirability Bias score. Results reveal a significant drift toward\nsocially conforming traits, with profound implications for corpus integrity and\nthe reliability of downstream inferences. We then offer a novel epistemological\ninterpretation, tracing how recursive bias may collapse higher-order reasoning\ndown Pearl's Ladder of Causality, culminating in what we refer to as the Rung\nof Illusion.",
      "pdf_url": "http://arxiv.org/pdf/2509.17999v2",
      "arxiv_url": "http://arxiv.org/abs/2509.17999v2",
      "published": "2025-09-22",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.HC",
        "cs.LG"
      ]
    },
    {
      "title": "Everything all at once: On choosing an estimand for multi-component environmental exposures",
      "authors": [
        "Kara E. Rudolph",
        "Shodai Inose",
        "Nicholas Williams",
        "Ivan Diaz",
        "Lucia Calderon",
        "Jacqueline M. Torres",
        "Marianthi-Anna Kioumourtzoglou"
      ],
      "abstract": "Many research questions -- particularly those in environmental health -- do\nnot involve binary exposures. In environmental epidemiology, this includes\nmultivariate exposure mixtures with nondiscrete components. Causal inference\nestimands and estimators to quantify the relationship between an exposure\nmixture and an outcome are relatively few. We propose an approach to quantify a\nrelationship between a shift in the exposure mixture and the outcome -- either\nin the single timepoint or longitudinal setting. The shift in the exposure\nmixture can be defined flexibly in terms of shifting one or more components,\nincluding examining interaction between mixture components, and in terms of\nshifting the same or different amounts across components. The estimand we\ndiscuss has a similar interpretation as a main effect regression coefficient.\nFirst, we focus on choosing a shift in the exposure mixture supported by\nobserved data. We demonstrate how to assess extrapolation and modify the shift\nto minimize reliance on extrapolation. Second, we propose estimating the\nrelationship between the exposure mixture shift and outcome completely\nnonparametrically, using machine learning in model-fitting. This is in contrast\nto other current approaches, which employ parametric modeling for at least some\nrelationships, which we would like to avoid because parametric modeling\nassumptions in complex, nonrandomized settings are tenuous at best. We are\nmotivated by longitudinal data on pesticide exposures among participants in the\nCHAMACOS Maternal Cognition cohort. We examine the relationship between\nlongitudinal exposure to agricultural pesticides and risk of hypertension. We\nprovide step-by-step code to facilitate the easy replication and adaptation of\nthe approaches we use.",
      "pdf_url": "http://arxiv.org/pdf/2509.17960v1",
      "arxiv_url": "http://arxiv.org/abs/2509.17960v1",
      "published": "2025-09-22",
      "categories": [
        "stat.ME",
        "stat.AP"
      ]
    },
    {
      "title": "Revealing Multimodal Causality with Large Language Models",
      "authors": [
        "Jin Li",
        "Shoujin Wang",
        "Qi Zhang",
        "Feng Liu",
        "Tongliang Liu",
        "Longbing Cao",
        "Shui Yu",
        "Fang Chen"
      ],
      "abstract": "Uncovering cause-and-effect mechanisms from data is fundamental to scientific\nprogress. While large language models (LLMs) show promise for enhancing causal\ndiscovery (CD) from unstructured data, their application to the increasingly\nprevalent multimodal setting remains a critical challenge. Even with the advent\nof multimodal LLMs (MLLMs), their efficacy in multimodal CD is hindered by two\nprimary limitations: (1) difficulty in exploring intra- and inter-modal\ninteractions for comprehensive causal variable identification; and (2)\ninsufficiency to handle structural ambiguities with purely observational data.\nTo address these challenges, we propose MLLM-CD, a novel framework for\nmultimodal causal discovery from unstructured data. It consists of three key\ncomponents: (1) a novel contrastive factor discovery module to identify genuine\nmultimodal factors based on the interactions explored from contrastive sample\npairs; (2) a statistical causal structure discovery module to infer causal\nrelationships among discovered factors; and (3) an iterative multimodal\ncounterfactual reasoning module to refine the discovery outcomes iteratively by\nincorporating the world knowledge and reasoning capabilities of MLLMs.\nExtensive experiments on both synthetic and real-world datasets demonstrate the\neffectiveness of MLLM-CD in revealing genuine factors and causal relationships\namong them from multimodal unstructured data.",
      "pdf_url": "http://arxiv.org/pdf/2509.17784v1",
      "arxiv_url": "http://arxiv.org/abs/2509.17784v1",
      "published": "2025-09-22",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Multimodal Health Risk Prediction System for Chronic Diseases via Vision-Language Fusion and Large Language Models",
      "authors": [
        "Dingxin Lu",
        "Shurui Wu",
        "Xinyi Huang"
      ],
      "abstract": "With the rising global burden of chronic diseases and the multimodal and\nheterogeneous clinical data (medical imaging, free-text recordings, wearable\nsensor streams, etc.), there is an urgent need for a unified multimodal AI\nframework that can proactively predict individual health risks. We propose\nVL-RiskFormer, a hierarchical stacked visual-language multimodal Transformer\nwith a large language model (LLM) inference head embedded in its top layer. The\nsystem builds on the dual-stream architecture of existing visual-linguistic\nmodels (e.g., PaLM-E, LLaVA) with four key innovations: (i) pre-training with\ncross-modal comparison and fine-grained alignment of radiological images,\nfundus maps, and wearable device photos with corresponding clinical narratives\nusing momentum update encoders and debiased InfoNCE losses; (ii) a time fusion\nblock that integrates irregular visit sequences into the causal Transformer\ndecoder through adaptive time interval position coding; (iii) a disease\nontology map adapter that injects ICD-10 codes into visual and textual channels\nin layers and infers comorbid patterns with the help of a graph attention\nmechanism. On the MIMIC-IV longitudinal cohort, VL-RiskFormer achieved an\naverage AUROC of 0.90 with an expected calibration error of 2.7 percent.",
      "pdf_url": "http://arxiv.org/pdf/2509.18221v1",
      "arxiv_url": "http://arxiv.org/abs/2509.18221v1",
      "published": "2025-09-22",
      "categories": [
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Regularizing Extrapolation in Causal Inference",
      "authors": [
        "David Arbour",
        "Harsh Parikh",
        "Bijan Niknam",
        "Elizabeth Stuart",
        "Kara Rudolph",
        "Avi Feller"
      ],
      "abstract": "Many common estimators in machine learning and causal inference are linear\nsmoothers, where the prediction is a weighted average of the training outcomes.\nSome estimators, such as ordinary least squares and kernel ridge regression,\nallow for arbitrarily negative weights, which improve feature imbalance but\noften at the cost of increased dependence on parametric modeling assumptions\nand higher variance. By contrast, estimators like importance weighting and\nrandom forests (sometimes implicitly) restrict weights to be non-negative,\nreducing dependence on parametric modeling and variance at the cost of worse\nimbalance. In this paper, we propose a unified framework that directly\npenalizes the level of extrapolation, replacing the current practice of a hard\nnon-negativity constraint with a soft constraint and corresponding\nhyperparameter. We derive a worst-case extrapolation error bound and introduce\na novel \"bias-bias-variance\" tradeoff, encompassing biases due to feature\nimbalance, model misspecification, and estimator variance; this tradeoff is\nespecially pronounced in high dimensions, particularly when positivity is poor.\nWe then develop an optimization procedure that regularizes this bound while\nminimizing imbalance and outline how to use this approach as a sensitivity\nanalysis for dependence on parametric modeling assumptions. We demonstrate the\neffectiveness of our approach through synthetic experiments and a real-world\napplication, involving the generalization of randomized controlled trial\nestimates to a target population of interest.",
      "pdf_url": "http://arxiv.org/pdf/2509.17180v1",
      "arxiv_url": "http://arxiv.org/abs/2509.17180v1",
      "published": "2025-09-21",
      "categories": [
        "cs.LG",
        "econ.EM",
        "stat.ME"
      ]
    }
  ]
}