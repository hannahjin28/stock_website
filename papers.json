{
  "last_updated": "2025-04-21T00:54:27.799165",
  "papers": [
    {
      "title": "Covariate balancing estimation and model selection for difference-in-differences approach",
      "authors": [
        "Takamichi Baba",
        "Yoshiyuki Ninomiya"
      ],
      "abstract": "In causal inference, remarkable progress has been made in\ndifference-in-differences (DID) approaches to estimate the average effect of\ntreatment on the treated (ATT). Of these, the semiparametric DID (SDID)\napproach incorporates a propensity score analysis into the DID setup. Supposing\nthat the ATT is a function of covariates, we estimate it by weighting the\ninverse of the propensity score. As one method to make the estimation robust to\nthe propensity score modeling, we incorporate covariate balancing. Then, by\nattentively constructing the moment conditions used in the covariate balancing,\nwe show that the proposed estimator has doubly robustness. In addition to the\nestimation, model selection is also addressed. In practice, covariate selection\nis an essential task in statistical analysis, but even in the basic setting of\nthe SDID approach, there are no reasonable information criteria. Therefore, we\nderive a model selection criterion as an asymptotically bias-corrected\nestimator of risk based on the loss function used in the SDID estimation. As a\nresult, we show that a penalty term is derived that is considerably different\nfrom almost twice the number of parameters that often appears in AIC-type\ninformation criteria. Numerical experiments show that the proposed method\nestimates the ATT robustly compared to the method using propensity scores given\nby the maximum likelihood estimation (MLE), and that the proposed criterion\nclearly reduces the risk targeted in the SDID approach compared to the\nintuitive generalization of the existing information criterion. In addition,\nreal data analysis confirms that there is a large difference between the\nresults of the proposed method and the existing method.",
      "pdf_url": "http://arxiv.org/pdf/2504.13057v1",
      "arxiv_url": "http://arxiv.org/abs/2504.13057v1",
      "published": "2025-04-17",
      "categories": [
        "stat.ME",
        "62D20"
      ]
    },
    {
      "title": "Information Gain-Guided Causal Intervention for Autonomous Debiasing Large Language Models",
      "authors": [
        "Zhouhao Sun",
        "Xiao Ding",
        "Li Du",
        "Yunpeng Xu",
        "Yixuan Ma",
        "Yang Zhao",
        "Bing Qin",
        "Ting Liu"
      ],
      "abstract": "Despite significant progress, recent studies indicate that current large\nlanguage models (LLMs) may still capture dataset biases and utilize them during\ninference, leading to the poor generalizability of LLMs. However, due to the\ndiversity of dataset biases and the insufficient nature of bias suppression\nbased on in-context learning, the effectiveness of previous prior\nknowledge-based debiasing methods and in-context learning based automatic\ndebiasing methods is limited. To address these challenges, we explore the\ncombination of causal mechanisms with information theory and propose an\ninformation gain-guided causal intervention debiasing (IGCIDB) framework. This\nframework first utilizes an information gain-guided causal intervention method\nto automatically and autonomously balance the distribution of\ninstruction-tuning dataset. Subsequently, it employs a standard supervised\nfine-tuning process to train LLMs on the debiased dataset. Experimental results\nshow that IGCIDB can effectively debias LLM to improve its generalizability\nacross different tasks.",
      "pdf_url": "http://arxiv.org/pdf/2504.12898v1",
      "arxiv_url": "http://arxiv.org/abs/2504.12898v1",
      "published": "2025-04-17",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Cobra: Efficient Line Art COlorization with BRoAder References",
      "authors": [
        "Junhao Zhuang",
        "Lingen Li",
        "Xuan Ju",
        "Zhaoyang Zhang",
        "Chun Yuan",
        "Ying Shan"
      ],
      "abstract": "The comic production industry requires reference-based line art colorization\nwith high accuracy, efficiency, contextual consistency, and flexible control. A\ncomic page often involves diverse characters, objects, and backgrounds, which\ncomplicates the coloring process. Despite advancements in diffusion models for\nimage generation, their application in line art colorization remains limited,\nfacing challenges related to handling extensive reference images,\ntime-consuming inference, and flexible control. We investigate the necessity of\nextensive contextual image guidance on the quality of line art colorization. To\naddress these challenges, we introduce Cobra, an efficient and versatile method\nthat supports color hints and utilizes over 200 reference images while\nmaintaining low latency. Central to Cobra is a Causal Sparse DiT architecture,\nwhich leverages specially designed positional encodings, causal sparse\nattention, and Key-Value Cache to effectively manage long-context references\nand ensure color identity consistency. Results demonstrate that Cobra achieves\naccurate line art colorization through extensive contextual reference,\nsignificantly enhancing inference speed and interactivity, thereby meeting\ncritical industrial demands. We release our codes and models on our project\npage: https://zhuang2002.github.io/Cobra/.",
      "pdf_url": "http://arxiv.org/pdf/2504.12240v1",
      "arxiv_url": "http://arxiv.org/abs/2504.12240v1",
      "published": "2025-04-16",
      "categories": [
        "cs.CV"
      ]
    },
    {
      "title": "Semiparametric Causal Discovery and Inference with Invalid Instruments",
      "authors": [
        "Jing Zou",
        "Wei Li",
        "Wei Lin"
      ],
      "abstract": "Learning causal relationships among a set of variables, as encoded by a\ndirected acyclic graph, from observational data is complicated by the presence\nof unobserved confounders. Instrumental variables (IVs) are a popular remedy\nfor this issue, but most existing methods either assume the validity of all IVs\nor postulate a specific form of relationship, such as a linear model, between\nthe primary variables and the IVs. To overcome these limitations, we introduce\na partially linear structural equation model for causal discovery and inference\nthat accommodates potentially invalid IVs and allows for general dependence of\nthe primary variables on the IVs. We establish identification under this\nsemiparametric model by constructing surrogate valid IVs, and develop a\nfinite-sample procedure for estimating the causal structures and effects.\nTheoretically, we show that our procedure consistently learns the causal\nstructures, yields asymptotically normal estimates, and effectively controls\nthe false discovery rate in edge recovery. Simulation studies demonstrate the\nsuperiority of our method over existing competitors, and an application to\ninferring gene regulatory networks in Alzheimer's disease illustrates its\nusefulness.",
      "pdf_url": "http://arxiv.org/pdf/2504.12085v1",
      "arxiv_url": "http://arxiv.org/abs/2504.12085v1",
      "published": "2025-04-16",
      "categories": [
        "stat.ME"
      ]
    },
    {
      "title": "Causality-enhanced Decision-Making for Autonomous Mobile Robots in Dynamic Environments",
      "authors": [
        "Luca Castri",
        "Gloria Beraldo",
        "Nicola Bellotto"
      ],
      "abstract": "The growing integration of robots in shared environments -- such as\nwarehouses, shopping centres, and hospitals -- demands a deep understanding of\nthe underlying dynamics and human behaviours, including how, when, and where\nindividuals engage in various activities and interactions. This knowledge goes\nbeyond simple correlation studies and requires a more comprehensive causal\nanalysis. By leveraging causal inference to model cause-and-effect\nrelationships, we can better anticipate critical environmental factors and\nenable autonomous robots to plan and execute tasks more effectively. To this\nend, we propose a novel causality-based decision-making framework that reasons\nover a learned causal model to predict battery usage and human obstructions,\nunderstanding how these factors could influence robot task execution. Such\nreasoning framework assists the robot in deciding when and how to complete a\ngiven task. To achieve this, we developed also PeopleFlow, a new Gazebo-based\nsimulator designed to model context-sensitive human-robot spatial interactions\nin shared workspaces. PeopleFlow features realistic human and robot\ntrajectories influenced by contextual factors such as time, environment layout,\nand robot state, and can simulate a large number of agents. While the simulator\nis general-purpose, in this paper we focus on a warehouse-like environment as a\ncase study, where we conduct an extensive evaluation benchmarking our causal\napproach against a non-causal baseline. Our findings demonstrate the efficacy\nof the proposed solutions, highlighting how causal reasoning enables autonomous\nrobots to operate more efficiently and safely in dynamic environments shared\nwith humans.",
      "pdf_url": "http://arxiv.org/pdf/2504.11901v2",
      "arxiv_url": "http://arxiv.org/abs/2504.11901v2",
      "published": "2025-04-16",
      "categories": [
        "cs.RO",
        "cs.AI"
      ]
    },
    {
      "title": "A cautionary note for plasmode simulation studies in the setting of causal inference",
      "authors": [
        "Pamela A Shaw",
        "Susan Gruber",
        "Brian D. Williamson",
        "Rishi Desai",
        "Susan M. Shortreed",
        "Chloe Krakauer",
        "Jennifer C. Nelson",
        "Mark J. van der Laan"
      ],
      "abstract": "Plasmode simulation has become an important tool for evaluating the operating\ncharacteristics of different statistical methods in complex settings, such as\npharmacoepidemiological studies of treatment effectiveness using electronic\nhealth records (EHR) data. These studies provide insight into how estimator\nperformance is impacted by challenges including rare events, small sample size,\netc., that can indicate which among a set of methods performs best in a\nreal-world dataset. Plasmode simulation combines data resampled from a\nreal-world dataset with synthetic data to generate a known truth for an\nestimand in realistic data. There are different potential plasmode strategies\ncurrently in use. We compare two popular plasmode simulation frameworks. We\nprovide numerical evidence and a theoretical result, which shows that one of\nthese frameworks can cause certain estimators to incorrectly appear overly\nbiased with lower than nominal confidence interval coverage. Detailed\nsimulation studies using both synthetic and real-world EHR data demonstrate\nthat these pitfalls remain at large sample sizes and when analyzing data from a\nrandomized controlled trial. We conclude with guidance for the choice of a\nplasmode simulation approach that maintains good theoretical properties to\nallow a fair evaluation of statistical methods while also maintaining the\ndesired similarity to real data.",
      "pdf_url": "http://arxiv.org/pdf/2504.11740v1",
      "arxiv_url": "http://arxiv.org/abs/2504.11740v1",
      "published": "2025-04-16",
      "categories": [
        "stat.ME"
      ]
    },
    {
      "title": "Reimagining Urban Science: Scaling Causal Inference with Large Language Models",
      "authors": [
        "Yutong Xia",
        "Ao Qu",
        "Yunhan Zheng",
        "Yihong Tang",
        "Dingyi Zhuang",
        "Yuxuan Liang",
        "Cathy Wu",
        "Roger Zimmermann",
        "Jinhua Zhao"
      ],
      "abstract": "Urban causal research is essential for understanding the complex dynamics of\ncities and informing evidence-based policies. However, it is challenged by the\ninefficiency and bias of hypothesis generation, barriers to multimodal data\ncomplexity, and the methodological fragility of causal experimentation. Recent\nadvances in large language models (LLMs) present an opportunity to rethink how\nurban causal analysis is conducted. This Perspective examines current urban\ncausal research by analyzing taxonomies that categorize research topics, data\nsources, and methodological approaches to identify structural gaps. We then\nintroduce an LLM-driven conceptual framework, AutoUrbanCI, composed of four\ndistinct modular agents responsible for hypothesis generation, data\nengineering, experiment design and execution, and results interpretation with\npolicy recommendations. We propose evaluation criteria for rigor and\ntransparency and reflect on implications for human-AI collaboration, equity,\nand accountability. We call for a new research agenda that embraces\nAI-augmented workflows not as replacements for human expertise but as tools to\nbroaden participation, improve reproducibility, and unlock more inclusive forms\nof urban causal reasoning.",
      "pdf_url": "http://arxiv.org/pdf/2504.12345v1",
      "arxiv_url": "http://arxiv.org/abs/2504.12345v1",
      "published": "2025-04-15",
      "categories": [
        "cs.CL",
        "cs.CY",
        "cs.MA"
      ]
    },
    {
      "title": "Looking beyond the next token",
      "authors": [
        "Abitha Thankaraj",
        "Yiding Jiang",
        "J. Zico Kolter",
        "Yonatan Bisk"
      ],
      "abstract": "The structure of causal language model training assumes that each token can\nbe accurately predicted from the previous context. This contrasts with humans'\nnatural writing and reasoning process, where goals are typically known before\nthe exact argument or phrasings. While this mismatch has been well studied in\nthe literature, the working assumption has been that architectural changes are\nneeded to address this mismatch. We argue that rearranging and processing the\ntraining data sequences can allow models to more accurately imitate the true\ndata-generating process, and does not require any other changes to the\narchitecture or training infrastructure. We demonstrate that this technique,\nTrelawney, and the inference algorithms derived from it allow us to improve\nperformance on several key benchmarks that span planning, algorithmic\nreasoning, and story generation tasks. Finally, our method naturally enables\nthe generation of long-term goals at no additional cost. We investigate how\nusing the model's goal-generation capability can further improve planning and\nreasoning. Additionally, we believe Trelawney could potentially open doors to\nnew capabilities beyond the current language modeling paradigm.",
      "pdf_url": "http://arxiv.org/pdf/2504.11336v1",
      "arxiv_url": "http://arxiv.org/abs/2504.11336v1",
      "published": "2025-04-15",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Clinically Interpretable Survival Risk Stratification in Head and Neck Cancer Using Bayesian Networks and Markov Blankets",
      "authors": [
        "Keyur D. Shah",
        "Ibrahim Chamseddine",
        "Xiaohan Yuan",
        "Sibo Tian",
        "Richard Qiu",
        "Jun Zhou",
        "Anees Dhabaan",
        "Hania Al-Hallaq",
        "David S. Yu",
        "Harald Paganetti",
        "Xiaofeng Yang"
      ],
      "abstract": "Purpose: To identify a clinically interpretable subset of survival-relevant\nfeatures in HN cancer using Bayesian Network (BN) and evaluate its prognostic\nand causal utility. Methods and Materials: We used the RADCURE dataset,\nconsisting of 3,346 patients with H&N cancer treated with definitive\n(chemo)radiotherapy. A probabilistic BN was constructed to model dependencies\namong clinical, anatomical, and treatment variables. The Markov Blanket (MB) of\ntwo-year survival (SVy2) was extracted and used to train a logistic regression\nmodel. After excluding incomplete cases, a temporal split yielded a train/test\n(2,174/820) dataset using 2007 as the cutoff year. Model performance was\nassessed using area under the ROC curve (AUC), C-index, and Kaplan-Meier (KM)\nsurvival stratification. Model fit was further evaluated using a log-likelihood\nratio (LLR) test. Causal inference was performed using do-calculus\ninterventions on MB variables. Results: The MB of SVy2 included 6 clinically\nrelevant features: ECOG performance status, T-stage, HPV status, disease site,\nthe primary gross tumor volume (GTVp), and treatment modality. The model\nachieved an AUC of 0.65 and C-index of 0.78 on the test dataset, significantly\nstratifying patients into high- and low-risk groups (log-rank p < 0.01). Model\nfit was further supported by a log-likelihood ratio of 70.32 (p < 0.01).\nSubgroup analyses revealed strong performance in HPV-negative (AUC = 0.69,\nC-index = 0.76), T4 (AUC = 0.69, C-index = 0.80), and large-GTV (AUC = 0.67,\nC-index = 0.75) cohorts, each showing significant KM separation. Causal\nanalysis further supported the positive survival impact of ECOG 0, HPV-positive\nstatus, and chemoradiation. Conclusions: A compact, MB-derived BN model can\nrobustly stratify survival risk in HN cancer. The model enables explainable\nprognostication and supports individualized decision-making across key clinical\nsubgroups.",
      "pdf_url": "http://arxiv.org/pdf/2504.11188v1",
      "arxiv_url": "http://arxiv.org/abs/2504.11188v1",
      "published": "2025-04-15",
      "categories": [
        "physics.med-ph"
      ]
    },
    {
      "title": "On relative universality, regression operator, and conditional independence",
      "authors": [
        "Bing Li",
        "Ben Jones",
        "Andreas Artemiou"
      ],
      "abstract": "The notion of relative universality with respect to a {\\sigma}-field was\nintroduced to establish the unbiasedness and Fisher consistency of an estimator\nin nonlinear sufficient dimension reduction. However, there is a gap in the\nproof of this result in the existing literature. The existing definition of\nrelative universality seems to be too strong for the proof to be valid. In this\nnote we modify the definition of relative universality using the concept of\n\\k{o}-measurability, and rigorously establish the mentioned unbiasedness and\nFisher consistency. The significance of this result is beyond its original\ncontext of sufficient dimension reduction, because relative universality allows\nus to use the regression operator to fully characterize conditional\nindependence, a crucially important statistical relation that sits at the core\nof many areas and methodologies in statistics and machine learning, such as\ndimension reduction, graphical models, probability embedding, causal inference,\nand Bayesian estimation.",
      "pdf_url": "http://arxiv.org/pdf/2504.11044v1",
      "arxiv_url": "http://arxiv.org/abs/2504.11044v1",
      "published": "2025-04-15",
      "categories": [
        "math.ST",
        "stat.ME",
        "stat.ML",
        "stat.TH",
        "62",
        "G.3"
      ]
    }
  ]
}