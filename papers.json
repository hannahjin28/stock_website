{
  "last_updated": "2025-08-26T00:52:46.904714",
  "papers": [
    {
      "title": "Heterogeneous Quantile Treatment Effect Estimation for Longitudinal Data with High-Dimensional Confounding",
      "authors": [
        "Zhixin Qiu",
        "Huichen Zhu",
        "Wenjie Wang",
        "Yanlin Tang"
      ],
      "abstract": "Causal inference plays a fundamental role in various real-world applications.\nHowever, in the motivating non-small cell lung cancer (NSCLC) study, it is\nchallenging to estimate the treatment effect of chemotherapy on circulating\ntumor DNA (ctDNA). First, the heterogeneous treatment effects vary across\npatient subgroups defined by baseline characteristics. Second, there exists a\nbroad set of demographic, clinical and molecular variables act as potential\nconfounders. Third, ctDNA trajectories over time show heavy-tailed non-Gaussian\nbehavior. Finally, repeated measurements within subjects introduce unknown\ncorrelation. Combining convolution-smoothed quantile regression and orthogonal\nrandom forest, we propose a framework to estimate heterogeneous quantile\ntreatment effects in the presence of high-dimensional confounding, which not\nonly captures effect heterogeneity across covariates, but also behaves robustly\nto nuisance parameter estimation error. We establish the theoretical properties\nof the proposed estimator and demonstrate its finite-sample performance through\ncomprehensive simulations. We illustrate its practical utility in the motivated\nNSCLC study.",
      "pdf_url": "http://arxiv.org/pdf/2508.16326v1",
      "arxiv_url": "http://arxiv.org/abs/2508.16326v1",
      "published": "2025-08-22",
      "categories": [
        "stat.ME"
      ]
    },
    {
      "title": "Quasi Instrumental Variable Methods for Stable Hidden Confounding and Binary Outcome",
      "authors": [
        "Zhonghua Liu",
        "Baoluo Sun",
        "Ting Ye",
        "David Richardson",
        "Eric Tchetgen Tchetgen"
      ],
      "abstract": "Instrumental variable (IV) methods are central to causal inference from\nobservational data, particularly when a randomized experiment is not feasible.\nHowever, of the three conventional core IV identification conditions, only one,\nIV relevance, is empirically verifiable; often one or both of the other\nconditions, exclusion restriction and IV independence from unmeasured\nconfounders, are unmet in real-world applications. These challenges are\ncompounded when the outcome is binary, a setting for which robust IV methods\nremain underdeveloped. A fundamental contribution of this paper is the\ndevelopment of a general identification strategy justified under a structural\nequilibrium dynamic generative model of so-called stable confounding and a\nquasi instrumental variable (QIV), i.e. a variable that is only assumed to be\npredictive of the outcome. Such a model implies (a) stability of confounding on\nthe multiplicative scale, and (b) stability of the additive average treatment\neffect among the treated (ATT), across levels of that QIV. The former is all\nthat is necessary to ensure a valid test of the causal null hypothesis;\ntogether those two conditions establish nonparametric identification and\nestimation of the conditional and marginal ATT. To address the statistical\nchallenges posed by the need for boundedness in binary outcomes, we introduce a\ngeneralized odds product re-parametrization of the observed data distribution,\nand we develop both a principled maximum likelihood estimator and a triply\nrobust semiparametric locally efficient estimator, which we evaluate through\nsimulations and an empirical application to the UK Biobank.",
      "pdf_url": "http://arxiv.org/pdf/2508.16096v1",
      "arxiv_url": "http://arxiv.org/abs/2508.16096v1",
      "published": "2025-08-22",
      "categories": [
        "stat.ME"
      ]
    },
    {
      "title": "Transforming Causality: Transformer-Based Temporal Causal Discovery with Prior Knowledge Integration",
      "authors": [
        "Jihua Huang",
        "Yi Yao",
        "Ajay Divakaran"
      ],
      "abstract": "We introduce a novel framework for temporal causal discovery and inference\nthat addresses two key challenges: complex nonlinear dependencies and spurious\ncorrelations. Our approach employs a multi-layer Transformer-based time-series\nforecaster to capture long-range, nonlinear temporal relationships among\nvariables. After training, we extract the underlying causal structure and\nassociated time lags from the forecaster using gradient-based analysis,\nenabling the construction of a causal graph. To mitigate the impact of spurious\ncausal relationships, we introduce a prior knowledge integration mechanism\nbased on attention masking, which consistently enforces user-excluded causal\nlinks across multiple Transformer layers. Extensive experiments show that our\nmethod significantly outperforms other state-of-the-art approaches, achieving a\n12.8% improvement in F1-score for causal discovery and 98.9% accuracy in\nestimating causal lags.",
      "pdf_url": "http://arxiv.org/pdf/2508.15928v1",
      "arxiv_url": "http://arxiv.org/abs/2508.15928v1",
      "published": "2025-08-21",
      "categories": [
        "cs.LG",
        "stat.ML"
      ]
    },
    {
      "title": "Understanding Action Effects through Instrumental Empowerment in Multi-Agent Reinforcement Learning",
      "authors": [
        "Ardian Selmonaj",
        "Miroslav Strupl",
        "Oleg Szehr",
        "Alessandro Antonucci"
      ],
      "abstract": "To reliably deploy Multi-Agent Reinforcement Learning (MARL) systems, it is\ncrucial to understand individual agent behaviors. While prior work typically\nevaluates overall team performance based on explicit reward signals, it is\nunclear how to infer agent contributions in the absence of any value feedback.\nIn this work, we investigate whether meaningful insights into agent behaviors\ncan be extracted solely by analyzing the policy distribution. Inspired by the\nphenomenon that intelligent agents tend to pursue convergent instrumental\nvalues, we introduce Intended Cooperation Values (ICVs), a method based on\ninformation-theoretic Shapley values for quantifying each agent's causal\ninfluence on their co-players' instrumental empowerment. Specifically, ICVs\nmeasure an agent's action effect on its teammates' policies by assessing their\ndecision (un)certainty and preference alignment. By analyzing action effects on\npolicies and value functions across cooperative and competitive MARL tasks, our\nmethod identifies which agent behaviors are beneficial to team success, either\nby fostering deterministic decisions or by preserving flexibility for future\naction choices, while also revealing the extent to which agents adopt similar\nor diverse strategies. Our proposed method offers novel insights into\ncooperation dynamics and enhances explainability in MARL systems.",
      "pdf_url": "http://arxiv.org/pdf/2508.15652v2",
      "arxiv_url": "http://arxiv.org/abs/2508.15652v2",
      "published": "2025-08-21",
      "categories": [
        "cs.AI",
        "cs.IT",
        "cs.LG",
        "cs.MA",
        "math.IT"
      ]
    },
    {
      "title": "CSTEapp: An interactive R-Shiny application of the covariate-specific treatment effect curve for visualizing individualized treatment rule",
      "authors": [
        "Yi Zhou",
        "Yuhao Deng",
        "Yu-Shi Tian",
        "Peng Wu",
        "Wenjie Hu",
        "Haoxiang Wang",
        "Ewout Steyerberg",
        "Xiao-Hua Zhou"
      ],
      "abstract": "In precision medicine, deriving the individualized treatment rule (ITR) is\ncrucial for recommending the optimal treatment based on patients' baseline\ncovariates. The covariate-specific treatment effect (CSTE) curve presents a\ngraphical method to visualize an ITR within a causal inference framework.\nRecent advancements have enhanced the causal interpretation of the CSTE curves\nand provided methods for deriving simultaneous confidence bands for various\nstudy types. To facilitate the implementation of these methods and make ITR\nestimation more accessible, we developed CSTEapp, a web-based application built\non the R Shiny framework. CSTEapp allows users to upload data and create CSTE\ncurves through simple point and click operations, making it the first\napplication for estimating the ITRs. CSTEapp simplifies the analytical process\nby providing interactive graphical user interfaces with dynamic results,\nenabling users to easily report optimal treatments for individual patients\nbased on their covariates information. Currently, CSTEapp is applicable to\nstudies with binary and time-to-event outcomes, and we continually expand its\ncapabilities to accommodate other outcome types as new methods emerge. We\ndemonstrate the utility of CSTEapp using real-world examples and simulation\ndatasets. By making advanced statistical methods more accessible, CSTEapp\nempowers researchers and practitioners across various fields to advance\nprecision medicine and improve patient outcomes.",
      "pdf_url": "http://arxiv.org/pdf/2508.15265v1",
      "arxiv_url": "http://arxiv.org/abs/2508.15265v1",
      "published": "2025-08-21",
      "categories": [
        "stat.CO",
        "stat.AP"
      ]
    },
    {
      "title": "Untangling Sample and Population Level Estimands in Bayesian Causal Inference",
      "authors": [
        "Arman Oganisian"
      ],
      "abstract": "Bayesian inference for causal estimands has been growing in popularity,\nhowever many misconceptions and implementation errors arise from conflating\nsample and population-level estimands. We have anecdotally witnessed these at\nconference talks, in the course of peer review service, and even in published\nand arXiv-ed papers. Our goal here is to elucidate the crucial differences\nbetween sample and population-level quantities when it comes to identification,\nmodeling, Bayesian computation, and interpretation. For example, common\nsample-level estimands require cross-world assumptions for identification,\nwhereas common population-level estimands do not. Similarly, the former\nrequires explicit imputation of counterfactuals from their joint posterior,\nwhereas the latter typically only requires a posterior distribution over\nparameters. We start by defining some examples of both types of estimands, then\ndiscuss the full joint posterior over all unknowns (both missing\ncounterfactuals and population distribution parameters). We continue to outline\nhow inference for different estimands are derived from different marginals of\nthis joint posterior. Because the differences are conceptually subtle but can\nbe practically substantial, we provide an illustration of using synthetic data\nin Stan. We also provide a detailed appendix with derivations and computational\ntips along with a discussion of common implementation errors. The overarching\nmessage here is to always engage in first-principles thinking about which\nmarginal of the joint posterior is of interest in a particular application,\nthen follow the strict logic of Bayes' theorem and probability to avoid common\nimplementation errors.",
      "pdf_url": "http://arxiv.org/pdf/2508.15016v1",
      "arxiv_url": "http://arxiv.org/abs/2508.15016v1",
      "published": "2025-08-20",
      "categories": [
        "stat.ME"
      ]
    },
    {
      "title": "Bounding Causal Effects and Counterfactuals",
      "authors": [
        "Tobias Maringgele"
      ],
      "abstract": "Causal inference often hinges on strong assumptions - such as no unmeasured\nconfounding or perfect compliance - that are rarely satisfied in practice.\nPartial identification offers a principled alternative: instead of relying on\nunverifiable assumptions to estimate causal effects precisely, it derives\nbounds that reflect the uncertainty inherent in the data. Despite its\ntheoretical appeal, partial identification remains underutilized in applied\nwork, in part due to the fragmented nature of existing methods and the lack of\npractical guidance. This thesis addresses these challenges by systematically\ncomparing a diverse set of bounding algorithms across multiple causal\nscenarios. We implement, extend, and unify state-of-the-art methods - including\nsymbolic, optimization-based, and information-theoretic approaches - within a\ncommon evaluation framework. In particular, we propose an extension of a\nrecently introduced entropy-bounded method, making it applicable to\ncounterfactual queries such as the Probability of Necessity and Sufficiency\n(PNS). Our empirical study spans thousands of randomized simulations involving\nboth discrete and continuous data-generating processes. We assess each method\nin terms of bound tightness, computational efficiency, and robustness to\nassumption violations. To support practitioners, we distill our findings into a\npractical decision tree for algorithm selection and train a machine learning\nmodel to predict the best-performing method based on observable data\ncharacteristics.\n  All implementations are released as part of an open-source Python package,\nCausalBoundingEngine, which enables users to apply and compare bounding methods\nthrough a unified interface.",
      "pdf_url": "http://arxiv.org/pdf/2508.13607v1",
      "arxiv_url": "http://arxiv.org/abs/2508.13607v1",
      "published": "2025-08-19",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ME",
        "62A01 (Foundations of statistics), 68T01 (Artificial intelligence,\n  general)",
        "G.3; I.2.6"
      ]
    },
    {
      "title": "Counterfactual Probabilistic Diffusion with Expert Models",
      "authors": [
        "Wenhao Mu",
        "Zhi Cao",
        "Mehmed Uludag",
        "Alexander Rodr√≠guez"
      ],
      "abstract": "Predicting counterfactual distributions in complex dynamical systems is\nessential for scientific modeling and decision-making in domains such as public\nhealth and medicine. However, existing methods often rely on point estimates or\npurely data-driven models, which tend to falter under data scarcity. We propose\na time series diffusion-based framework that incorporates guidance from\nimperfect expert models by extracting high-level signals to serve as structured\npriors for generative modeling. Our method, ODE-Diff, bridges mechanistic and\ndata-driven approaches, enabling more reliable and interpretable causal\ninference. We evaluate ODE-Diff across semi-synthetic COVID-19 simulations,\nsynthetic pharmacological dynamics, and real-world case studies, demonstrating\nthat it consistently outperforms strong baselines in both point prediction and\ndistributional accuracy.",
      "pdf_url": "http://arxiv.org/pdf/2508.13355v1",
      "arxiv_url": "http://arxiv.org/abs/2508.13355v1",
      "published": "2025-08-18",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ME"
      ]
    },
    {
      "title": "Reinforced Context Order Recovery for Adaptive Reasoning and Planning",
      "authors": [
        "Long Ma",
        "Fangwei Zhong",
        "Yizhou Wang"
      ],
      "abstract": "Modern causal language models, followed by rapid developments in discrete\ndiffusion models, can now produce a wide variety of interesting and useful\ncontent. However, these families of models are predominantly trained to output\ntokens with a fixed (left-to-right) or random order, which may deviate from the\nlogical order in which tokens are generated originally. In this paper, we\nobserve that current causal and diffusion models encounter difficulties in\nproblems that require adaptive token generation orders to solve tractably,\nwhich we characterize with the $\\mathcal{V}$-information framework. Motivated\nby this, we propose Reinforced Context Order Recovery (ReCOR), a\nreinforcement-learning-based framework to extract adaptive, data-dependent\ntoken generation orders from text data without annotations. Self-supervised by\ntoken prediction statistics, ReCOR estimates the hardness of predicting every\nunfilled token and adaptively selects the next token during both training and\ninference. Experiments on challenging reasoning and planning datasets\ndemonstrate the superior performance of ReCOR compared with baselines,\nsometimes outperforming oracle models supervised with the ground-truth order.",
      "pdf_url": "http://arxiv.org/pdf/2508.13070v1",
      "arxiv_url": "http://arxiv.org/abs/2508.13070v1",
      "published": "2025-08-18",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Bayesian Double Machine Learning for Causal Inference",
      "authors": [
        "Francis J. DiTraglia",
        "Laura Liu"
      ],
      "abstract": "This paper proposes a simple, novel, and fully-Bayesian approach for causal\ninference in partially linear models with high-dimensional control variables.\nOff-the-shelf machine learning methods can introduce biases in the causal\nparameter known as regularization-induced confounding. To address this, we\npropose a Bayesian Double Machine Learning (BDML) method, which modifies a\nstandard Bayesian multivariate regression model and recovers the causal effect\nof interest from the reduced-form covariance matrix. Our BDML is related to the\nburgeoning frequentist literature on DML while addressing its limitations in\nfinite-sample inference. Moreover, the BDML is based on a fully generative\nprobability model in the DML context, adhering to the likelihood principle. We\nshow that in high dimensional setups the naive estimator implicitly assumes no\nselection on observables--unlike our BDML. The BDML exhibits lower asymptotic\nbias and achieves asymptotic normality and semiparametric efficiency as\nestablished by a Bernstein-von Mises theorem, thereby ensuring robustness to\nmisspecification. In simulations, our BDML achieves lower RMSE, better\nfrequentist coverage, and shorter confidence interval width than alternatives\nfrom the literature, both Bayesian and frequentist.",
      "pdf_url": "http://arxiv.org/pdf/2508.12688v1",
      "arxiv_url": "http://arxiv.org/abs/2508.12688v1",
      "published": "2025-08-18",
      "categories": [
        "econ.EM"
      ]
    }
  ]
}