{
  "last_updated": "2025-11-26T00:54:29.563279",
  "papers": [
    {
      "title": "Nonparametric Instrumental Variable Regression with Observed Covariates",
      "authors": [
        "Zikai Shen",
        "Zonghao Chen",
        "Dimitri Meunier",
        "Ingo Steinwart",
        "Arthur Gretton",
        "Zhu Li"
      ],
      "abstract": "We study the problem of nonparametric instrumental variable regression with observed covariates, which we refer to as NPIV-O. Compared with standard nonparametric instrumental variable regression (NPIV), the additional observed covariates facilitate causal identification and enables heterogeneous causal effect estimation. However, the presence of observed covariates introduces two challenges for its theoretical analysis. First, it induces a partial identity structure, which renders previous NPIV analyses - based on measures of ill-posedness, stability conditions, or link conditions - inapplicable. Second, it imposes anisotropic smoothness on the structural function. To address the first challenge, we introduce a novel Fourier measure of partial smoothing; for the second challenge, we extend the existing kernel 2SLS instrumental variable algorithm with observed covariates, termed KIV-O, to incorporate Gaussian kernel lengthscales adaptive to the anisotropic smoothness. We prove upper $L^2$-learning rates for KIV-O and the first $L^2$-minimax lower learning rates for NPIV-O. Both rates interpolate between known optimal rates of NPIV and nonparametric regression (NPR). Interestingly, we identify a gap between our upper and lower bounds, which arises from the choice of kernel lengthscales tuned to minimize a projected risk. Our theoretical analysis also applies to proximal causal inference, an emerging framework for causal effect estimation that shares the same conditional moment restriction as NPIV-O.",
      "pdf_url": "https://arxiv.org/pdf/2511.19404v1",
      "arxiv_url": "http://arxiv.org/abs/2511.19404v1",
      "published": "2025-11-24",
      "categories": [
        "stat.ML",
        "cs.LG",
        "math.ST"
      ]
    },
    {
      "title": "The Unified Non-Convex Framework for Robust Causal Inference: Overcoming the Gaussian Barrier and Optimization Fragility",
      "authors": [
        "Eichi Uehara"
      ],
      "abstract": "This document proposes a Unified Robust Framework that re-engineers the estimation of the Average Treatment Effect on the Overlap (ATO). It synthesizes gamma-Divergence for outlier robustness, Graduated Non-Convexity (GNC) for global optimization, and a \"Gatekeeper\" mechanism to address the impossibility of higher-order orthogonality in Gaussian regimes.",
      "pdf_url": "https://arxiv.org/pdf/2511.19284v1",
      "arxiv_url": "http://arxiv.org/abs/2511.19284v1",
      "published": "2025-11-24",
      "categories": [
        "stat.ML",
        "cs.LG",
        "stat.ME"
      ]
    },
    {
      "title": "Scalable Bayesian Network Structure Learning Using Tsetlin Machine to Constrain the Search Space",
      "authors": [
        "Kunal Dumbre",
        "Lei Jiao",
        "Ole-Christoffer Granmo"
      ],
      "abstract": "The PC algorithm is a widely used method in causal inference for learning the structure of Bayesian networks. Despite its popularity, the PC algorithm suffers from significant time complexity, particularly as the size of the dataset increases, which limits its applicability in large-scale real-world problems. In this study, we propose a novel approach that utilises the Tsetlin Machine (TM) to construct Bayesian structures more efficiently. Our method leverages the most significant literals extracted from the TM and performs conditional independence (CI) tests on these selected literals instead of the full set of variables, resulting in a considerable reduction in computational time. We implemented our approach and compared it with various state-of-the-art methods. Our evaluation includes categorical datasets from the bnlearn repository, such as Munin1, Hepar2. The findings indicate that the proposed TM-based method not only reduces computational complexity but also maintains competitive accuracy in causal discovery, making it a viable alternative to traditional PC algorithm implementations by offering improved efficiency without compromising performance.",
      "pdf_url": "https://arxiv.org/pdf/2511.19273v1",
      "arxiv_url": "http://arxiv.org/abs/2511.19273v1",
      "published": "2025-11-24",
      "categories": [
        "cs.LG"
      ]
    },
    {
      "title": "CDLM: Consistency Diffusion Language Models For Faster Sampling",
      "authors": [
        "Minseo Kim",
        "Chenfeng Xu",
        "Coleman Hooper",
        "Harman Singh",
        "Ben Athiwaratkun",
        "Ce Zhang",
        "Kurt Keutzer",
        "Amir Gholami"
      ],
      "abstract": "Diffusion Language Models (DLMs) offer a promising parallel generation paradigm but suffer from slow inference due to numerous refinement steps and the inability to use standard KV caching. We introduce CDLM (Consistency Diffusion Language Models), a training-based acceleration method that simultaneously tackles both bottlenecks. CDLM integrates consistency modeling to drastically reduce the number of required sampling steps by enabling multi-token finalization. Furthermore, we enforce a block-wise causal attention mask during fine-tuning, making the model fully compatible with KV caching. Experiments show CDLM achieves 3.6x-14.5x lower latency while maintaining competitive accuracy on math and coding tasks. The full training and evaluation code is available at https://github.com/SqueezeAILab/CDLM.",
      "pdf_url": "https://arxiv.org/pdf/2511.19269v1",
      "arxiv_url": "http://arxiv.org/abs/2511.19269v1",
      "published": "2025-11-24",
      "categories": [
        "cs.LG",
        "cs.CL"
      ]
    },
    {
      "title": "Prior-Free Information Design",
      "authors": [
        "Maxwell Rosenthal"
      ],
      "abstract": "This paper introduces a prior-free framework for information design based on partial identification and applies it to robust causal inference. The decision maker observes the distribution of signals generated by an information structure and ranks alternatives by their worst-case payoff over the state distributions consistent with those signals. We characterize the set of robustly implementable actions and show that each can be implemented by an information structure that withholds at most one dimension of information from the decision maker. In the potential outcomes model, every treatment is implementable via an experiment that is almost fully informative.",
      "pdf_url": "https://arxiv.org/pdf/2511.18647v1",
      "arxiv_url": "http://arxiv.org/abs/2511.18647v1",
      "published": "2025-11-23",
      "categories": [
        "econ.TH",
        "econ.EM"
      ]
    },
    {
      "title": "VCU-Bridge: Hierarchical Visual Connotation Understanding via Semantic Bridging",
      "authors": [
        "Ming Zhong",
        "Yuanlei Wang",
        "Liuzhou Zhang",
        "Arctanx An",
        "Renrui Zhang",
        "Hao Liang",
        "Ming Lu",
        "Ying Shen",
        "Wentao Zhang"
      ],
      "abstract": "While Multimodal Large Language Models (MLLMs) excel on benchmarks, their processing paradigm differs from the human ability to integrate visual information. Unlike humans who naturally bridge details and high-level concepts, models tend to treat these elements in isolation. Prevailing evaluation protocols often decouple low-level perception from high-level reasoning, overlooking their semantic and causal dependencies, which yields non-diagnostic results and obscures performance bottlenecks. We present VCU-Bridge, a framework that operationalizes a human-like hierarchy of visual connotation understanding: multi-level reasoning that advances from foundational perception through semantic bridging to abstract connotation, with an explicit evidence-to-inference trace from concrete cues to abstract conclusions. Building on this framework, we construct HVCU-Bench, a benchmark for hierarchical visual connotation understanding with explicit, level-wise diagnostics. Comprehensive experiments demonstrate a consistent decline in performance as reasoning progresses to higher levels. We further develop a data generation pipeline for instruction tuning guided by Monte Carlo Tree Search (MCTS) and show that strengthening low-level capabilities yields measurable gains at higher levels. Interestingly, it not only improves on HVCU-Bench but also brings benefits on general benchmarks (average +2.53%), especially with substantial gains on MMStar (+7.26%), demonstrating the significance of the hierarchical thinking pattern and its effectiveness in enhancing MLLM capabilities. The project page is at https://vcu-bridge.github.io .",
      "pdf_url": "https://arxiv.org/pdf/2511.18121v1",
      "arxiv_url": "http://arxiv.org/abs/2511.18121v1",
      "published": "2025-11-22",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "On Transportability for Structural Causal Bandits",
      "authors": [
        "Min Woo Park",
        "Sanghack Lee"
      ],
      "abstract": "Intelligent agents equipped with causal knowledge can optimize their action spaces to avoid unnecessary exploration. The structural causal bandit framework provides a graphical characterization for identifying actions that are unable to maximize rewards by leveraging prior knowledge of the underlying causal structure. While such knowledge enables an agent to estimate the expected rewards of certain actions based on others in online interactions, there has been little guidance on how to transfer information inferred from arbitrary combinations of datasets collected under different conditions -- observational or experimental -- and from heterogeneous environments. In this paper, we investigate the structural causal bandit with transportability, where priors from the source environments are fused to enhance learning in the deployment setting. We demonstrate that it is possible to exploit invariances across environments to consistently improve learning. The resulting bandit algorithm achieves a sub-linear regret bound with an explicit dependence on informativeness of prior data, and it may outperform standard bandit approaches that rely solely on online learning.",
      "pdf_url": "https://arxiv.org/pdf/2511.17953v1",
      "arxiv_url": "http://arxiv.org/abs/2511.17953v1",
      "published": "2025-11-22",
      "categories": [
        "cs.LG",
        "stat.ML"
      ]
    },
    {
      "title": "Why Is the Double-Robust Estimator for Causal Inference Not Doubly Robust for Variance Estimation?",
      "authors": [
        "Hao Wu",
        "Lucy Shao",
        "Toni Gui",
        "Tsungchin Wu",
        "Zhuochao Huang",
        "Shengjia Tu",
        "Xin Tu",
        "Jinyuan Liu",
        "Tuo Lin"
      ],
      "abstract": "Doubly robust estimators (DRE) are widely used in causal inference because they yield consistent estimators of average causal effect when at least one of the nuisance models, the propensity for treatment (exposure) or the outcome regression, is correct. However, double robustness does not extend to variance estimation; the influence-function (IF)-based variance estimator is consistent only when both nuisance parameters are correct. This raises concerns about applying DRE in practice, where model misspecification is inevitable. The recent paper by Shook-Sa et al. (2025, Biometrics, 81(2), ujaf054) demonstrated through Monte Carlo simulations that the IF-based variance estimator is biased. However, the paper's findings are empirical. The key question remains: why does the variance estimator fail in double robustness, and under what conditions do alternatives succeed, such as the ones demonstrated in Shook-Sa et al. 2025. In this paper, we develop a formal theory to clarify the efficiency properties of DRE that underlie these empirical findings. We also introduce alternative strategies, including a mixture-based framework underlying the sample-splitting and crossfitting approaches, to achieve valid inference with misspecified nuisance parameters. Our considerations are illustrated with simulation and real study data.",
      "pdf_url": "https://arxiv.org/pdf/2511.17907v1",
      "arxiv_url": "http://arxiv.org/abs/2511.17907v1",
      "published": "2025-11-22",
      "categories": [
        "stat.ME"
      ]
    },
    {
      "title": "On treating right-censoring events like treatments",
      "authors": [
        "Lan Wen",
        "Aaron L. Sarvet",
        "Jessica G. Young"
      ],
      "abstract": "In causal inference literature, potential outcomes are often indexed by the \"elimination of all right-censoring events,\" leading to the perception that such a restriction is necessary for defining well-posed causal estimands. In this paper, we clarify that this restriction is not required: a well-defined estimand can be formulated without indexing on the elimination of such events. Achieving this requires a more precise classification of right-censoring events than has historically been considered, as the nature of these events has direct implications for identification of the target estimand. We provide a framework that distinguishes different types of right-censoring events from a causal perspective, and demonstrate how this framework relates to censoring definitions and assumptions in classical survival analysis literature. By bridging these perspectives, we provide a clearer understanding of how to handle right-censoring events and provide guidance for identifying causal estimands when right-censored events are present.",
      "pdf_url": "https://arxiv.org/pdf/2511.17379v1",
      "arxiv_url": "http://arxiv.org/abs/2511.17379v1",
      "published": "2025-11-21",
      "categories": [
        "stat.ME",
        "math.ST",
        "stat.AP"
      ]
    },
    {
      "title": "FireScope: Wildfire Risk Prediction with a Chain-of-Thought Oracle",
      "authors": [
        "Mario Markov",
        "Stefan Maria Ailuro",
        "Luc Van Gool",
        "Konrad Schindler",
        "Danda Pani Paudel"
      ],
      "abstract": "Predicting wildfire risk is a reasoning-intensive spatial problem that requires the integration of visual, climatic, and geographic factors to infer continuous risk maps. Existing methods lack the causal reasoning and multimodal understanding required for reliable generalization. We introduce $\\textbf{FireScope-Bench}$, a large-scale dataset and benchmark that couples Sentinel-2 imagery and climate data with expert-defined risk rasters across the USA, and real wildfire events in Europe for cross-continental evaluation. Building on this dataset, we propose $\\textbf{FireScope}$, a VLM-based reasoning-to-generation framework that learns from both reinforcement learning and visual supervision to predict risk rasters with complementary reasoning traces. When trained in the USA and tested in Europe, $\\textbf{FireScope}$ achieves substantial performance gains, while expert feedback and automated analysis confirm that its reasoning traces are faithful and semantically meaningful. Our findings demonstrate that reasoning can ground raster prediction models, improving both generalization and interpretability. To our knowledge, this is the first framework to (1) demonstrate that language-based reasoning can improve generalization in visual generation, (2) propose a high-resolution wildfire risk model that can be applied across continents, and (3) enable systematic studies of robust cross-continental generalization for multimodal fire risk models. We believe that $\\textbf{FireScope-Bench}$ has the potential to serve as a foundation for advancing reasoning-driven, interpretable and generalizable spatial modeling. Data and source code will be made publicly available.",
      "pdf_url": "https://arxiv.org/pdf/2511.17171v1",
      "arxiv_url": "http://arxiv.org/abs/2511.17171v1",
      "published": "2025-11-21",
      "categories": [
        "cs.CV",
        "cs.LG"
      ]
    }
  ]
}