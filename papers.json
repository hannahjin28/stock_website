{
  "last_updated": "2025-05-20T00:54:52.162255",
  "papers": [
    {
      "title": "A Generative Framework for Causal Estimation via Importance-Weighted Diffusion Distillation",
      "authors": [
        "Xinran Song",
        "Tianyu Chen",
        "Mingyuan Zhou"
      ],
      "abstract": "Estimating individualized treatment effects from observational data is a\ncentral challenge in causal inference, largely due to covariate imbalance and\nconfounding bias from non-randomized treatment assignment. While inverse\nprobability weighting (IPW) is a well-established solution to this problem, its\nintegration into modern deep learning frameworks remains limited. In this work,\nwe propose Importance-Weighted Diffusion Distillation (IWDD), a novel\ngenerative framework that combines the pretraining of diffusion models with\nimportance-weighted score distillation to enable accurate and fast causal\nestimation-including potential outcome prediction and treatment effect\nestimation. We demonstrate how IPW can be naturally incorporated into the\ndistillation of pretrained diffusion models, and further introduce a\nrandomization-based adjustment that eliminates the need to compute IPW\nexplicitly-thereby simplifying computation and, more importantly, provably\nreducing the variance of gradient estimates. Empirical results show that IWDD\nachieves state-of-the-art out-of-sample prediction performance, with the\nhighest win rates compared to other baselines, significantly improving causal\nestimation and supporting the development of individualized treatment\nstrategies. We will release our PyTorch code for reproducibility and future\nresearch.",
      "pdf_url": "http://arxiv.org/pdf/2505.11444v1",
      "arxiv_url": "http://arxiv.org/abs/2505.11444v1",
      "published": "2025-05-16",
      "categories": [
        "cs.LG",
        "stat.AP",
        "stat.ME",
        "stat.ML"
      ]
    },
    {
      "title": "Bayesian Hierarchical Invariant Prediction",
      "authors": [
        "Francisco Madaleno",
        "Pernille Julie Viuff Sand",
        "Francisco C. Pereira",
        "Sergio Hernan Garrido Mejia"
      ],
      "abstract": "We propose Bayesian Hierarchical Invariant Prediction (BHIP) reframing\nInvariant Causal Prediction (ICP) through the lens of Hierarchical Bayes. We\nleverage the hierarchical structure to explicitly test invariance of causal\nmechanisms under heterogeneous data, resulting in improved computational\nscalability for a larger number of predictors compared to ICP. Moreover, given\nits Bayesian nature BHIP enables the use of prior information. In this paper,\nwe test two sparsity inducing priors: horseshoe and spike-and-slab, both of\nwhich allow us a more reliable identification of causal features. We test BHIP\nin synthetic and real-world data showing its potential as an alternative\ninference method to ICP.",
      "pdf_url": "http://arxiv.org/pdf/2505.11211v1",
      "arxiv_url": "http://arxiv.org/abs/2505.11211v1",
      "published": "2025-05-16",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ME",
        "stat.ML"
      ]
    },
    {
      "title": "A Fast Kernel-based Conditional Independence test with Application to Causal Discovery",
      "authors": [
        "Oliver Schacht",
        "Biwei Huang"
      ],
      "abstract": "Kernel-based conditional independence (KCI) testing is a powerful\nnonparametric method commonly employed in causal discovery tasks. Despite its\nflexibility and statistical reliability, cubic computational complexity limits\nits application to large datasets. To address this computational bottleneck, we\npropose \\textit{FastKCI}, a scalable and parallelizable kernel-based\nconditional independence test that utilizes a mixture-of-experts approach\ninspired by embarrassingly parallel inference techniques for Gaussian\nprocesses. By partitioning the dataset based on a Gaussian mixture model over\nthe conditioning variables, FastKCI conducts local KCI tests in parallel,\naggregating the results using an importance-weighted sampling scheme.\nExperiments on synthetic datasets and benchmarks on real-world production data\nvalidate that FastKCI maintains the statistical power of the original KCI test\nwhile achieving substantial computational speedups. FastKCI thus represents a\npractical and efficient solution for conditional independence testing in causal\ninference on large-scale data.",
      "pdf_url": "http://arxiv.org/pdf/2505.11085v1",
      "arxiv_url": "http://arxiv.org/abs/2505.11085v1",
      "published": "2025-05-16",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ]
    },
    {
      "title": "A Cautionary Tale on Integrating Studies with Disparate Outcome Measures for Causal Inference",
      "authors": [
        "Harsh Parikh",
        "Trang Quynh Nguyen",
        "Elizabeth A. Stuart",
        "Kara E. Rudolph",
        "Caleb H. Miles"
      ],
      "abstract": "Data integration approaches are increasingly used to enhance the efficiency\nand generalizability of studies. However, a key limitation of these methods is\nthe assumption that outcome measures are identical across datasets -- an\nassumption that often does not hold in practice. Consider the following opioid\nuse disorder (OUD) studies: the XBOT trial and the POAT study, both evaluating\nthe effect of medications for OUD on withdrawal symptom severity (not the\nprimary outcome of either trial). While XBOT measures withdrawal severity using\nthe subjective opiate withdrawal scale, POAT uses the clinical opiate\nwithdrawal scale. We analyze this realistic yet challenging setting where\noutcome measures differ across studies and where neither study records both\ntypes of outcomes. Our paper studies whether and when integrating studies with\ndisparate outcome measures leads to efficiency gains. We introduce three sets\nof assumptions -- with varying degrees of strength -- linking both outcome\nmeasures. Our theoretical and empirical results highlight a cautionary tale:\nintegration can improve asymptotic efficiency only under the strongest\nassumption linking the outcomes. However, misspecification of this assumption\nleads to bias. In contrast, a milder assumption may yield finite-sample\nefficiency gains, yet these benefits diminish as sample size increases. We\nillustrate these trade-offs via a case study integrating the XBOT and POAT\ndatasets to estimate the comparative effect of two medications for opioid use\ndisorder on withdrawal symptoms. By systematically varying the assumptions\nlinking the SOW and COW scales, we show potential efficiency gains and the\nrisks of bias. Our findings emphasize the need for careful assumption selection\nwhen fusing datasets with differing outcome measures, offering guidance for\nresearchers navigating this common challenge in modern data integration.",
      "pdf_url": "http://arxiv.org/pdf/2505.11014v1",
      "arxiv_url": "http://arxiv.org/abs/2505.11014v1",
      "published": "2025-05-16",
      "categories": [
        "stat.ME",
        "cs.LG",
        "econ.EM"
      ]
    },
    {
      "title": "Observational causality by states",
      "authors": [
        "Álvaro Martínez-Sánchez",
        "Adrián Lozano-Durán"
      ],
      "abstract": "Causality plays a central role in understanding interactions between\nvariables in complex systems. These systems often exhibit state-dependent\ncausal relationships, where both the strength and direction of causality vary\nwith the value of the interacting variables. In this work, we introduce a\nstate-aware causal inference method that quantifies causality in terms of\ninformation gain about future states. The effectiveness of the proposed\napproach stems from two key features: its ability to characterize causal\ninfluence as a function of system state, and its capacity to distinguish\nbetween redundant and synergistic interactions. The method is validated across\na range of benchmark cases in which the direction and strength of causality\nevolve in a prescribed manner with the state of the system. We further\ndemonstrate the applicability of our approach in two real scenarios: the\ninteraction between motions across scales in a turbulent boundary layer, and\nthe Walker circulation phenomenon in tropical Pacific climate dynamics. Our\nresults show that, without accounting for state-dependent causality as well as\nredundant and synergistic effects, traditional approaches to causal inference\nmay lead to incomplete or misleading conclusions.",
      "pdf_url": "http://arxiv.org/pdf/2505.10878v1",
      "arxiv_url": "http://arxiv.org/abs/2505.10878v1",
      "published": "2025-05-16",
      "categories": [
        "physics.data-an",
        "physics.flu-dyn"
      ]
    },
    {
      "title": "Targeted Learning Estimation of Sampling Variance for Improved Inference",
      "authors": [
        "Yunwen Ji",
        "Mark van der Laan",
        "Alan Hubbard"
      ],
      "abstract": "For robust statistical inference it is crucial to obtain a good estimator of\nthe variance of the proposed estimator of the statistical estimand. A commonly\nused estimator of the variance for an asymptotically linear estimator is the\nsample variance of the estimated influence function. This estimator has been\nshown to be anti-conservative in limited samples or in the presence of\nnear-positivity violations, leading to elevated Type-I error rates and poor\ncoverage. In this paper, capitalizing on earlier attempts at targeted variance\nestimators, we propose a one-step targeted variance estimator for the causal\nrisk ratio (CRR) in scenarios involving treatment, outcome, and baseline\ncovariates. While our primary focus is on the variance of log(CRR), our\nmethodology can be extended to other causal effect parameters. Specifically, we\nfocus on the variance of the IF for the log relative risk (log(CRR)) estimator,\nwhich requires deriving the efficient influence function for the variance of\nthe IF as the basis for constructing the estimator. Several methods are\navailable to develop efficient estimators of asymptotically linear parameters.\nIn this paper, we concentrate on the so-called one-step targeted maximum\nlikelihood estimator, which is a substitution estimator that utilizes a\none-dimensional universal least favorable parametric submodel when updating the\ndistribution. We conduct simulations with different effect sizes, sample sizes\nand levels of positivity to compare the estimator with existing methods in\nterms of coverage and Type-I error. Simulation results demonstrate that,\nespecially with small samples and near-positivity violations, the proposed\nvariance estimator offers improved performance, achieving coverage closer to\nthe nominal level of 0.95 and a lower Type-I error rate.",
      "pdf_url": "http://arxiv.org/pdf/2505.10624v1",
      "arxiv_url": "http://arxiv.org/abs/2505.10624v1",
      "published": "2025-05-15",
      "categories": [
        "stat.ME"
      ]
    },
    {
      "title": "Closure and Complexity of Temporal Causality",
      "authors": [
        "Mishel Carelli",
        "Bernd Finkbeiner",
        "Julian Siber"
      ],
      "abstract": "Temporal causality defines what property causes some observed temporal\nbehavior (the effect) in a given computation, based on a counterfactual\nanalysis of similar computations. In this paper, we study its closure\nproperties and the complexity of computing causes. For the former, we establish\nthat safety, reachability, and recurrence properties are all closed under\ncausal inference: If the effect is from one of these property classes, then the\ncause for this effect is from the same class. We also show that persistence and\nobligation properties are not closed in this way. These results rest on a\ntopological characterization of causes which makes them applicable to a wide\nrange of similarity relations between computations. Finally, our complexity\nanalysis establishes improved upper bounds for computing causes for safety,\nreachability, and recurrence properties. We also present the first lower bounds\nfor all of the classes.",
      "pdf_url": "http://arxiv.org/pdf/2505.10186v1",
      "arxiv_url": "http://arxiv.org/abs/2505.10186v1",
      "published": "2025-05-15",
      "categories": [
        "cs.LO"
      ]
    },
    {
      "title": "Forests for Differences: Robust Causal Inference Beyond Parametric DiD",
      "authors": [
        "Hugo Gobato Souto",
        "Francisco Louzada Neto"
      ],
      "abstract": "This paper introduces the Difference-in-Differences Bayesian Causal Forest\n(DiD-BCF), a novel non-parametric model addressing key challenges in DiD\nestimation, such as staggered adoption and heterogeneous treatment effects.\nDiD-BCF provides a unified framework for estimating Average (ATE),\nGroup-Average (GATE), and Conditional Average Treatment Effects (CATE). A core\ninnovation, its Parallel Trends Assumption (PTA)-based reparameterization,\nenhances estimation accuracy and stability in complex panel data settings.\nExtensive simulations demonstrate DiD-BCF's superior performance over\nestablished benchmarks, particularly under non-linearity, selection biases, and\neffect heterogeneity. Applied to U.S. minimum wage policy, the model uncovers\nsignificant conditional treatment effect heterogeneity related to county\npopulation, insights obscured by traditional methods. DiD-BCF offers a robust\nand versatile tool for more nuanced causal inference in modern DiD\napplications.",
      "pdf_url": "http://arxiv.org/pdf/2505.09706v1",
      "arxiv_url": "http://arxiv.org/abs/2505.09706v1",
      "published": "2025-05-14",
      "categories": [
        "stat.ME",
        "cs.LG",
        "stat.ML"
      ]
    },
    {
      "title": "Language Agents Mirror Human Causal Reasoning Biases. How Can We Help Them Think Like Scientists?",
      "authors": [
        "Anthony GX-Chen",
        "Dongyan Lin",
        "Mandana Samiei",
        "Doina Precup",
        "Blake A. Richards",
        "Rob Fergus",
        "Kenneth Marino"
      ],
      "abstract": "Language model (LM) agents are increasingly used as autonomous\ndecision-makers who need to actively gather information to guide their\ndecisions. A crucial cognitive skill for such agents is the efficient\nexploration and understanding of the causal structure of the world -- key to\nrobust, scientifically grounded reasoning. Yet, it remains unclear whether LMs\npossess this capability or exhibit systematic biases leading to erroneous\nconclusions. In this work, we examine LMs' ability to explore and infer causal\nrelationships, using the well-established \"Blicket Test\" paradigm from\ndevelopmental psychology. We find that LMs reliably infer the common, intuitive\ndisjunctive causal relationships but systematically struggle with the unusual,\nyet equally (or sometimes even more) evidenced conjunctive ones. This\n\"disjunctive bias\" persists across model families, sizes, and prompting\nstrategies, and performance further declines as task complexity increases.\nInterestingly, an analogous bias appears in human adults, suggesting that LMs\nmay have inherited deep-seated reasoning heuristics from their training data.\nTo this end, we quantify similarities between LMs and humans, finding that LMs\nexhibit adult-like inference profiles (but not children-like). Finally, we\npropose a test-time sampling method which explicitly samples and eliminates\nhypotheses about causal relationships from the LM. This scalable approach\nsignificantly reduces the disjunctive bias and moves LMs closer to the goal of\nscientific, causally rigorous reasoning.",
      "pdf_url": "http://arxiv.org/pdf/2505.09614v1",
      "arxiv_url": "http://arxiv.org/abs/2505.09614v1",
      "published": "2025-05-14",
      "categories": [
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Moving towards informative and actionable social media research",
      "authors": [
        "Joseph B. Bak-Coleman",
        "Stephan Lewandowsky",
        "Philipp Lorenz-Spreen",
        "Arvind Narayanan",
        "Amy Orben",
        "Lisa Oswald"
      ],
      "abstract": "Social media is nearly ubiquitous in modern life, and concerns have been\nraised about its putative societal impacts, ranging from undermining mental\nhealth and exacerbating polarization to fomenting violence and disrupting\ndemocracy. Despite extensive research, consensus on these effects remains\nelusive, with observational studies often highlighting concerns while\nrandomized controlled trials (RCTs) yield conflicting or null findings. This\nreview examines how the complexity inherent in social systems can account for\nsuch discrepancies, emphasizing that emergent societal and long-term outcomes\ncannot be readily inferred from individual-level effects. In complex systems,\nsuch as social networks, feedback loops, hysteresis, multi-scale dynamics, and\nnon-linearity limit the utility of approaches for assessing causality that are\notherwise robust in simpler contexts. Revisiting large-scale experiments, we\nexplore how null or conflicting findings may reflect these complexities rather\nthan a true absence of effects. Even in cases where the methods are\nappropriate, assessing the net impacts of social media provides little\nactionable insight given that eliminating social media is not a realistic\noption for whole populations. We argue that progress will require a\ncomplexity-minded approach focused on specific design choices of online\nplatforms that triangulates experimental, observational and theoretical\nmethods.",
      "pdf_url": "http://arxiv.org/pdf/2505.09254v1",
      "arxiv_url": "http://arxiv.org/abs/2505.09254v1",
      "published": "2025-05-14",
      "categories": [
        "cs.SI",
        "nlin.AO"
      ]
    }
  ]
}