{
  "last_updated": "2025-05-28T00:54:13.893047",
  "papers": [
    {
      "title": "An Explainable Diagnostic Framework for Neurodegenerative Dementias via Reinforcement-Optimized LLM Reasoning",
      "authors": [
        "Andrew Zamai",
        "Nathanael Fijalkow",
        "Boris Mansencal",
        "Laurent Simon",
        "Eloi Navet",
        "Pierrick Coupe"
      ],
      "abstract": "The differential diagnosis of neurodegenerative dementias is a challenging\nclinical task, mainly because of the overlap in symptom presentation and the\nsimilarity of patterns observed in structural neuroimaging. To improve\ndiagnostic efficiency and accuracy, deep learning-based methods such as\nConvolutional Neural Networks and Vision Transformers have been proposed for\nthe automatic classification of brain MRIs. However, despite their strong\npredictive performance, these models find limited clinical utility due to their\nopaque decision making. In this work, we propose a framework that integrates\ntwo core components to enhance diagnostic transparency. First, we introduce a\nmodular pipeline for converting 3D T1-weighted brain MRIs into textual\nradiology reports. Second, we explore the potential of modern Large Language\nModels (LLMs) to assist clinicians in the differential diagnosis between\nFrontotemporal dementia subtypes, Alzheimer's disease, and normal aging based\non the generated reports. To bridge the gap between predictive accuracy and\nexplainability, we employ reinforcement learning to incentivize diagnostic\nreasoning in LLMs. Without requiring supervised reasoning traces or\ndistillation from larger models, our approach enables the emergence of\nstructured diagnostic rationales grounded in neuroimaging findings. Unlike\npost-hoc explainability methods that retrospectively justify model decisions,\nour framework generates diagnostic rationales as part of the inference\nprocess-producing causally grounded explanations that inform and guide the\nmodel's decision-making process. In doing so, our framework matches the\ndiagnostic performance of existing deep learning methods while offering\nrationales that support its diagnostic conclusions.",
      "pdf_url": "http://arxiv.org/pdf/2505.19954v1",
      "arxiv_url": "http://arxiv.org/abs/2505.19954v1",
      "published": "2025-05-26",
      "categories": [
        "cs.LG",
        "cs.CL"
      ]
    },
    {
      "title": "Causal Bayesian Networks for Data-driven Safety Analysis of Complex Systems",
      "authors": [
        "Roman Gansch",
        "Lina Putze",
        "Tjark Koopmann",
        "Jan Reich",
        "Christian Neurohr"
      ],
      "abstract": "Ensuring safe operation of safety-critical complex systems interacting with\ntheir environment poses significant challenges, particularly when the system's\nworld model relies on machine learning algorithms to process the perception\ninput. A comprehensive safety argumentation requires knowledge of how faults or\nfunctional insufficiencies propagate through the system and interact with\nexternal factors, to manage their safety impact. While statistical analysis\napproaches can support the safety assessment, associative reasoning alone is\nneither sufficient for the safety argumentation nor for the identification and\ninvestigation of safety measures. A causal understanding of the system and its\ninteraction with the environment is crucial for safeguarding safety-critical\ncomplex systems. It allows to transfer and generalize knowledge, such as\ninsights gained from testing, and facilitates the identification of potential\nimprovements. This work explores using causal Bayesian networks to model the\nsystem's causalities for safety analysis, and proposes measures to assess\ncausal influences based on Pearl's framework of causal inference. We compare\nthe approach of causal Bayesian networks to the well-established fault tree\nanalysis, outlining advantages and limitations. In particular, we examine\nimportance metrics typically employed in fault tree analysis as foundation to\ndiscuss suitable causal metrics. An evaluation is performed on the example of a\nperception system for automated driving. Overall, this work presents an\napproach for causal reasoning in safety analysis that enables the integration\nof data-driven and expert-based knowledge to account for uncertainties arising\nfrom complex systems operating in open environments.",
      "pdf_url": "http://arxiv.org/pdf/2505.19860v1",
      "arxiv_url": "http://arxiv.org/abs/2505.19860v1",
      "published": "2025-05-26",
      "categories": [
        "cs.RO"
      ]
    },
    {
      "title": "Model Agnostic Differentially Private Causal Inference",
      "authors": [
        "Christian Lebeda",
        "Mathieu Even",
        "Aurélien Bellet",
        "Julie Josse"
      ],
      "abstract": "Estimating causal effects from observational data is essential in fields such\nas medicine, economics and social sciences, where privacy concerns are\nparamount. We propose a general, model-agnostic framework for differentially\nprivate estimation of average treatment effects (ATE) that avoids strong\nstructural assumptions on the data-generating process or the models used to\nestimate propensity scores and conditional outcomes. In contrast to prior work,\nwhich enforces differential privacy by directly privatizing these nuisance\ncomponents and results in a privacy cost that scales with model complexity, our\napproach decouples nuisance estimation from privacy protection. This separation\nallows the use of flexible, state-of-the-art black-box models, while\ndifferential privacy is achieved by perturbing only predictions and aggregation\nsteps within a fold-splitting scheme with ensemble techniques. We instantiate\nthe framework for three classical estimators -- the G-formula, inverse\npropensity weighting (IPW), and augmented IPW (AIPW) -- and provide formal\nutility and privacy guarantees. Empirical results show that our methods\nmaintain competitive performance under realistic privacy budgets. We further\nextend our framework to support meta-analysis of multiple private ATE\nestimates. Our results bridge a critical gap between causal inference and\nprivacy-preserving data analysis.",
      "pdf_url": "http://arxiv.org/pdf/2505.19589v2",
      "arxiv_url": "http://arxiv.org/abs/2505.19589v2",
      "published": "2025-05-26",
      "categories": [
        "cs.LG",
        "stat.ML"
      ]
    },
    {
      "title": "Estimating Online Influence Needs Causal Modeling! Counterfactual Analysis of Social Media Engagement",
      "authors": [
        "Lin Tian",
        "Marian-Andrei Rizoiu"
      ],
      "abstract": "Understanding true influence in social media requires distinguishing\ncorrelation from causation--particularly when analyzing misinformation spread.\nWhile existing approaches focus on exposure metrics and network structures,\nthey often fail to capture the causal mechanisms by which external temporal\nsignals trigger engagement. We introduce a novel joint treatment-outcome\nframework that leverages existing sequential models to simultaneously adapt to\nboth policy timing and engagement effects. Our approach adapts causal inference\ntechniques from healthcare to estimate Average Treatment Effects (ATE) within\nthe sequential nature of social media interactions, tackling challenges from\nexternal confounding signals. Through our experiments on real-world\nmisinformation and disinformation datasets, we show that our models outperform\nexisting benchmarks by 15--22% in predicting engagement across diverse\ncounterfactual scenarios, including exposure adjustment, timing shifts, and\nvaried intervention durations. Case studies on 492 social media users show our\ncausal effect measure aligns strongly with the gold standard in influence\nestimation, the expert-based empirical influence.",
      "pdf_url": "http://arxiv.org/pdf/2505.19355v1",
      "arxiv_url": "http://arxiv.org/abs/2505.19355v1",
      "published": "2025-05-25",
      "categories": [
        "cs.CL",
        "cs.SI"
      ]
    },
    {
      "title": "Can Large Language Models Infer Causal Relationships from Real-World Text?",
      "authors": [
        "Ryan Saklad",
        "Aman Chadha",
        "Oleg Pavlov",
        "Raha Moraffah"
      ],
      "abstract": "Understanding and inferring causal relationships from texts is a core aspect\nof human cognition and is essential for advancing large language models (LLMs)\ntowards artificial general intelligence. Existing work primarily focuses on\nsynthetically generated texts which involve simple causal relationships\nexplicitly mentioned in the text. This fails to reflect the complexities of\nreal-world tasks. In this paper, we investigate whether LLMs are capable of\ninferring causal relationships from real-world texts. We develop a benchmark\ndrawn from real-world academic literature which includes diverse texts with\nrespect to length, complexity of relationships (different levels of\nexplicitness, number of events, and causal relationships), and domains and\nsub-domains. To the best of our knowledge, our benchmark is the first-ever\nreal-world dataset for this task. Our experiments on state-of-the-art LLMs\nevaluated on our proposed benchmark demonstrate significant challenges, with\nthe best-performing model achieving an average F1 score of only 0.477. Analysis\nreveals common pitfalls: difficulty with implicitly stated information, in\ndistinguishing relevant causal factors from surrounding contextual details, and\nwith connecting causally relevant information spread across lengthy textual\npassages. By systematically characterizing these deficiencies, our benchmark\noffers targeted insights for further research into advancing LLM causal\nreasoning.",
      "pdf_url": "http://arxiv.org/pdf/2505.18931v1",
      "arxiv_url": "http://arxiv.org/abs/2505.18931v1",
      "published": "2025-05-25",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ]
    },
    {
      "title": "Rethinking Causal Mask Attention for Vision-Language Inference",
      "authors": [
        "Xiaohuan Pei",
        "Tao Huang",
        "YanXiang Ma",
        "Chang Xu"
      ],
      "abstract": "Causal attention has become a foundational mechanism in autoregressive\nvision-language models (VLMs), unifying textual and visual inputs under a\nsingle generative framework. However, existing causal mask-based strategies are\ninherited from large language models (LLMs) where they are tailored for\ntext-only decoding, and their adaptation to vision tokens is insufficiently\naddressed in the prefill stage. Strictly masking future positions for vision\nqueries introduces overly rigid constraints, which hinder the model's ability\nto leverage future context that often contains essential semantic cues for\naccurate inference. In this work, we empirically investigate how different\ncausal masking strategies affect vision-language inference and then propose a\nfamily of future-aware attentions tailored for this setting. We first\nempirically analyze the effect of previewing future tokens for vision queries\nand demonstrate that rigid masking undermines the model's capacity to capture\nuseful contextual semantic representations. Based on these findings, we propose\na lightweight attention family that aggregates future visual context into past\nrepresentations via pooling, effectively preserving the autoregressive\nstructure while enhancing cross-token dependencies. We evaluate a range of\ncausal masks across diverse vision-language inference settings and show that\nselectively compressing future semantic context into past representations\nbenefits the inference.",
      "pdf_url": "http://arxiv.org/pdf/2505.18605v1",
      "arxiv_url": "http://arxiv.org/abs/2505.18605v1",
      "published": "2025-05-24",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Scalable Policy Maximization Under Network Interference",
      "authors": [
        "Aidan Gleich",
        "Eric Laber",
        "Alexander Volfovsky"
      ],
      "abstract": "Many interventions, such as vaccines in clinical trials or coupons in online\nmarketplaces, must be assigned sequentially without full knowledge of their\neffects. Multi-armed bandit algorithms have proven successful in such settings.\nHowever, standard independence assumptions fail when the treatment status of\none individual impacts the outcomes of others, a phenomenon known as\ninterference. We study optimal-policy learning under interference on a dynamic\nnetwork. Existing approaches to this problem require repeated observations of\nthe same fixed network and struggle to scale in sample size beyond as few as\nfifteen connected units -- both limit applications. We show that under common\nassumptions on the structure of interference, rewards become linear. This\nenables us to develop a scalable Thompson sampling algorithm that maximizes\npolicy impact when a new $n$-node network is observed each round. We prove a\nBayesian regret bound that is sublinear in $n$ and the number of rounds.\nSimulation experiments show that our algorithm learns quickly and outperforms\nexisting methods. The results close a key scalability gap between causal\ninference methods for interference and practical bandit algorithms, enabling\npolicy optimization in large-scale networked systems.",
      "pdf_url": "http://arxiv.org/pdf/2505.18118v1",
      "arxiv_url": "http://arxiv.org/abs/2505.18118v1",
      "published": "2025-05-23",
      "categories": [
        "stat.ML",
        "cs.LG"
      ]
    },
    {
      "title": "Structured Thinking Matters: Improving LLMs Generalization in Causal Inference Tasks",
      "authors": [
        "Wentao Sun",
        "João Paulo Nogueira",
        "Alonso Silva"
      ],
      "abstract": "Despite remarkable advances in the field, LLMs remain unreliable in\ndistinguishing causation from correlation. Recent results from the Corr2Cause\ndataset benchmark reveal that state-of-the-art LLMs -- such as GPT-4 (F1 score:\n29.08) -- only marginally outperform random baselines (Random Uniform, F1\nscore: 20.38), indicating limited capacity of generalization. To tackle this\nlimitation, we propose a novel structured approach: rather than directly\nanswering causal queries, we provide the model with the capability to structure\nits thinking by guiding the model to build a structured knowledge graph,\nsystematically encoding the provided correlational premises, to answer the\ncausal queries. This intermediate representation significantly enhances the\nmodel's causal capabilities. Experiments on the test subset of the Corr2Cause\ndataset benchmark with Qwen3-32B model (reasoning model) show substantial gains\nover standard direct prompting methods, improving F1 scores from 32.71 to 48.26\n(over 47.5% relative increase), along with notable improvements in precision\nand recall. These results underscore the effectiveness of providing the model\nwith the capability to structure its thinking and highlight its promising\npotential for broader generalization across diverse causal inference tasks.",
      "pdf_url": "http://arxiv.org/pdf/2505.18034v2",
      "arxiv_url": "http://arxiv.org/abs/2505.18034v2",
      "published": "2025-05-23",
      "categories": [
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Federated Causal Inference from Multi-Site Observational Data via Propensity Score Aggregation",
      "authors": [
        "Khellaf Rémi",
        "Bellet Aurélien",
        "Josse Julie"
      ],
      "abstract": "Causal inference typically assumes centralized access to individual-level\ndata. Yet, in practice, data are often decentralized across multiple sites,\nmaking centralization infeasible due to privacy, logistical, or legal\nconstraints. We address this by estimating the Average Treatment Effect (ATE)\nfrom decentralized observational data using federated learning, which enables\ninference through the exchange of aggregate statistics rather than\nindividual-level data. We propose a novel method to estimate propensity scores\nin a (non-)parametric manner by computing a federated weighted average of local\nscores, using two theoretically grounded weighting schemes -- Membership\nWeights (MW) and Density Ratio Weights (DW) -- that balance communication\nefficiency and model flexibility. These federated scores are then used to\nconstruct two ATE estimators: the Federated Inverse Propensity Weighting\nestimator (Fed-IPW) and its augmented variant (Fed-AIPW). Unlike meta-analysis\nmethods, which fail when any site violates positivity, our approach leverages\nheterogeneity in treatment assignment across sites to improve overlap. We show\nthat Fed-IPW and Fed-AIPW perform well under site-level heterogeneity in sample\nsizes, treatment mechanisms, and covariate distributions, with theoretical\nanalysis and experiments on simulated and real-world data highlighting their\nstrengths and limitations relative to meta-analysis and related methods.",
      "pdf_url": "http://arxiv.org/pdf/2505.17961v1",
      "arxiv_url": "http://arxiv.org/abs/2505.17961v1",
      "published": "2025-05-23",
      "categories": [
        "stat.ME",
        "cs.AI",
        "math.ST",
        "stat.AP",
        "stat.TH"
      ]
    },
    {
      "title": "A Distributionally-Robust Framework for Nuisance in Causal Effect Estimation",
      "authors": [
        "Akira Tanimoto"
      ],
      "abstract": "Causal inference requires evaluating models on balanced distributions between\ntreatment and control groups, while training data often exhibits imbalance due\nto historical decision-making policies. Most conventional statistical methods\naddress this distribution shift through inverse probability weighting (IPW),\nwhich requires estimating propensity scores as an intermediate step. These\nmethods face two key challenges: inaccurate propensity estimation and\ninstability from extreme weights. We decompose the generalization error to\nisolate these issues--propensity ambiguity and statistical instability--and\naddress them through an adversarial loss function. Our approach combines\ndistributionally robust optimization for handling propensity uncertainty with\nweight regularization based on weighted Rademacher complexity. Experiments on\nsynthetic and real-world datasets demonstrate consistent improvements over\nexisting methods.",
      "pdf_url": "http://arxiv.org/pdf/2505.17717v1",
      "arxiv_url": "http://arxiv.org/abs/2505.17717v1",
      "published": "2025-05-23",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG"
      ]
    }
  ]
}