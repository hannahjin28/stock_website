{
  "last_updated": "2025-02-26T00:46:16.453546",
  "papers": [
    {
      "title": "Joint Value Estimation and Bidding in Repeated First-Price Auctions",
      "authors": [
        "Yuxiao Wen",
        "Yanjun Han",
        "Zhengyuan Zhou"
      ],
      "abstract": "We study regret minimization in repeated first-price auctions (FPAs), where a\nbidder observes only the realized outcome after each auction -- win or loss.\nThis setup reflects practical scenarios in online display advertising where the\nactual value of an impression depends on the difference between two potential\noutcomes, such as clicks or conversion rates, when the auction is won versus\nlost. We analyze three outcome models: (1) adversarial outcomes without\nfeatures, (2) linear potential outcomes with features, and (3) linear treatment\neffects in features. For each setting, we propose algorithms that jointly\nestimate private values and optimize bidding strategies, achieving near-optimal\nregret bounds. Notably, our framework enjoys a unique feature that the\ntreatments are also actively chosen, and hence eliminates the need for the\noverlap condition commonly required in causal inference.",
      "pdf_url": "http://arxiv.org/pdf/2502.17292v1",
      "arxiv_url": "http://arxiv.org/abs/2502.17292v1",
      "published": "2025-02-24",
      "categories": [
        "cs.LG",
        "cs.GT",
        "cs.IT",
        "math.IT",
        "stat.ME",
        "stat.ML"
      ]
    },
    {
      "title": "Teleology-Driven Affective Computing: A Causal Framework for Sustained Well-Being",
      "authors": [
        "Bin Yin",
        "Chong-Yi Liu",
        "Liya Fu",
        "Jinkun Zhang"
      ],
      "abstract": "Affective computing has made significant strides in emotion recognition and\ngeneration, yet current approaches mainly focus on short-term pattern\nrecognition and lack a comprehensive framework to guide affective agents toward\nlong-term human well-being. To address this, we propose a teleology-driven\naffective computing framework that unifies major emotion theories (basic\nemotion, appraisal, and constructivist approaches) under the premise that\naffect is an adaptive, goal-directed process that facilitates survival and\ndevelopment. Our framework emphasizes aligning agent responses with both\npersonal/individual and group/collective well-being over extended timescales.\nWe advocate for creating a \"dataverse\" of personal affective events, capturing\nthe interplay between beliefs, goals, actions, and outcomes through real-world\nexperience sampling and immersive virtual reality. By leveraging causal\nmodeling, this \"dataverse\" enables AI systems to infer individuals' unique\naffective concerns and provide tailored interventions for sustained well-being.\nAdditionally, we introduce a meta-reinforcement learning paradigm to train\nagents in simulated environments, allowing them to adapt to evolving affective\nconcerns and balance hierarchical goals - from immediate emotional needs to\nlong-term self-actualization. This framework shifts the focus from statistical\ncorrelations to causal reasoning, enhancing agents' ability to predict and\nrespond proactively to emotional challenges, and offers a foundation for\ndeveloping personalized, ethically aligned affective systems that promote\nmeaningful human-AI interactions and societal well-being.",
      "pdf_url": "http://arxiv.org/pdf/2502.17172v1",
      "arxiv_url": "http://arxiv.org/abs/2502.17172v1",
      "published": "2025-02-24",
      "categories": [
        "cs.HC",
        "cs.AI",
        "H.1.2, J.4",
        "H.1.2; J.4"
      ]
    },
    {
      "title": "A tutorial on optimal dynamic treatment regimes",
      "authors": [
        "Chunyu Wang",
        "Brian DM Tom"
      ],
      "abstract": "A dynamic treatment regime is a sequence of treatment decision rules tailored\nto an individual's evolving status over time. In precision medicine, much focus\nhas been placed on finding an optimal dynamic treatment regime which, if\nfollowed by everyone in the population, would yield the best outcome on\naverage; and extensive investigation has been conducted from both\nmethodological and applications standpoints. The aim of this tutorial is to\nprovide readers who are interested in optimal dynamic treatment regimes with a\nsystematic, detailed but accessible introduction, including the formal\ndefinition and formulation of this topic within the framework of causal\ninference, identification assumptions required to link the causal quantity of\ninterest to the observed data, existing statistical models and estimation\nmethods to learn the optimal regime from data, and application of these methods\nto both simulated and real data.",
      "pdf_url": "http://arxiv.org/pdf/2502.16988v1",
      "arxiv_url": "http://arxiv.org/abs/2502.16988v1",
      "published": "2025-02-24",
      "categories": [
        "stat.OT",
        "stat.AP"
      ]
    },
    {
      "title": "Time Series Domain Adaptation via Latent Invariant Causal Mechanism",
      "authors": [
        "Ruichu Cai",
        "Junxian Huang",
        "Zhenhui Yang",
        "Zijian Li",
        "Emadeldeen Eldele",
        "Min Wu",
        "Fuchun Sun"
      ],
      "abstract": "Time series domain adaptation aims to transfer the complex temporal\ndependence from the labeled source domain to the unlabeled target domain.\nRecent advances leverage the stable causal mechanism over observed variables to\nmodel the domain-invariant temporal dependence. However, modeling precise\ncausal structures in high-dimensional data, such as videos, remains\nchallenging. Additionally, direct causal edges may not exist among observed\nvariables (e.g., pixels). These limitations hinder the applicability of\nexisting approaches to real-world scenarios. To address these challenges, we\nfind that the high-dimension time series data are generated from the\nlow-dimension latent variables, which motivates us to model the causal\nmechanisms of the temporal latent process. Based on this intuition, we propose\na latent causal mechanism identification framework that guarantees the\nuniqueness of the reconstructed latent causal structures. Specifically, we\nfirst identify latent variables by utilizing sufficient changes in historical\ninformation. Moreover, by enforcing the sparsity of the relationships of latent\nvariables, we can achieve identifiable latent causal structures. Built on the\ntheoretical results, we develop the Latent Causality Alignment (LCA) model that\nleverages variational inference, which incorporates an intra-domain latent\nsparsity constraint for latent structure reconstruction and an inter-domain\nlatent sparsity constraint for domain-invariant structure reconstruction.\nExperiment results on eight benchmarks show a general improvement in the\ndomain-adaptive time series classification and forecasting tasks, highlighting\nthe effectiveness of our method in real-world scenarios. Codes are available at\nhttps://github.com/DMIRLAB-Group/LCA.",
      "pdf_url": "http://arxiv.org/pdf/2502.16637v1",
      "arxiv_url": "http://arxiv.org/abs/2502.16637v1",
      "published": "2025-02-23",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ME"
      ]
    },
    {
      "title": "Linear Attention for Efficient Bidirectional Sequence Modeling",
      "authors": [
        "Arshia Afzal",
        "Elias Abad Rocamora",
        "Leyla Naz Candogan",
        "Pol Puigdemont",
        "Francesco Tonin",
        "Yongtao Wu",
        "Mahsa Shoaran",
        "Volkan Cevher"
      ],
      "abstract": "Transformers with linear attention enable fast and parallel training.\nMoreover, they can be formulated as Recurrent Neural Networks (RNNs), for\nefficient linear-time inference. While extensively evaluated in causal sequence\nmodeling, they have yet to be extended to the bidirectional setting. This work\nintroduces the LION framework, establishing new theoretical foundations for\nlinear transformers in bidirectional sequence modeling. LION constructs a\nbidirectional RNN equivalent to full Linear Attention. This extends the\nbenefits of linear transformers: parallel training, and efficient inference,\ninto the bidirectional setting. Using LION, we cast three linear transformers\nto their bidirectional form: LION-LIT, the bidirectional variant corresponding\nto (Katharopoulos et al., 2020); LION-D, extending RetNet (Sun et al., 2023);\nand LION-S, a linear transformer with a stable selective mask inspired by\nselectivity of SSMs (Dao & Gu, 2024). Replacing the attention block with LION\n(-LIT, -D, -S) achieves performance on bidirectional tasks that approaches that\nof Transformers and State-Space Models (SSMs), while delivering significant\nimprovements in training speed. Our implementation is available in\nhttp://github.com/LIONS-EPFL/LION.",
      "pdf_url": "http://arxiv.org/pdf/2502.16249v1",
      "arxiv_url": "http://arxiv.org/abs/2502.16249v1",
      "published": "2025-02-22",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Practical programming research of Linear DML model based on the simplest Python code: From the standpoint of novice researchers",
      "authors": [
        "Shunxin Yao"
      ],
      "abstract": "This paper presents linear DML models for causal inference using the simplest\nPython code on a Jupyter notebook based on an Anaconda platform and compares\nthe performance of different DML models. The results show that current Library\nAPI technology is not yet sufficient to enable novice Python users to build\nqualified and high-quality DML models with the simplest coding approach. Novice\nusers attempting to perform DML causal inference using Python still have to\nimprove their mathematical and computer knowledge to adapt to more flexible DML\nprogramming. Additionally, the issue of mismatched outcome variable dimensions\nis also widespread when building linear DML models in Jupyter notebook.",
      "pdf_url": "http://arxiv.org/pdf/2502.16172v1",
      "arxiv_url": "http://arxiv.org/abs/2502.16172v1",
      "published": "2025-02-22",
      "categories": [
        "cs.SE",
        "cs.LG"
      ]
    },
    {
      "title": "Predicting gene essentiality and drug response from perturbation screens in preclinical cancer models with LEAP: Layered Ensemble of Autoencoders and Predictors",
      "authors": [
        "Barbara Bodinier",
        "Gaetan Dissez",
        "Linus Bleistein",
        "Antonin Dauvin"
      ],
      "abstract": "Preclinical perturbation screens, where the effects of genetic, chemical, or\nenvironmental perturbations are systematically tested on disease models, hold\nsignificant promise for machine learning-enhanced drug discovery due to their\nscale and causal nature. Predictive models can infer perturbation responses for\npreviously untested disease models based on molecular profiles. These in silico\nlabels can expand databases and guide experimental prioritization.\n  However, modelling perturbation-specific effects and generating robust\nprediction performances across diverse biological contexts remain elusive. We\nintroduce LEAP (Layered Ensemble of Autoencoders and Predictors), a novel\nensemble framework to improve robustness and generalization. LEAP leverages\nmultiple DAMAE (Data Augmented Masked Autoencoder) representations and LASSO\nregressors. By combining diverse gene expression representation models learned\nfrom different random initializations, LEAP consistently outperforms\nstate-of-the-art approaches in predicting gene essentiality or drug responses\nin unseen cell lines, tissues and disease models. Notably, our results show\nthat ensembling representation models, rather than prediction models alone,\nyields superior predictive performance.\n  Beyond its performance gains, LEAP is computationally efficient, requires\nminimal hyperparameter tuning and can therefore be readily incorporated into\ndrug discovery pipelines to prioritize promising targets and support\nbiomarker-driven stratification. The code and datasets used in this work are\nmade publicly available.",
      "pdf_url": "http://arxiv.org/pdf/2502.15646v1",
      "arxiv_url": "http://arxiv.org/abs/2502.15646v1",
      "published": "2025-02-21",
      "categories": [
        "cs.LG"
      ]
    },
    {
      "title": "EigenShield: Causal Subspace Filtering via Random Matrix Theory for Adversarially Robust Vision-Language Models",
      "authors": [
        "Nastaran Darabi",
        "Devashri Naik",
        "Sina Tayebati",
        "Dinithi Jayasuriya",
        "Ranganath Krishnan",
        "Amit Ranjan Trivedi"
      ],
      "abstract": "Vision-Language Models (VLMs) inherit adversarial vulnerabilities of Large\nLanguage Models (LLMs), which are further exacerbated by their multimodal\nnature. Existing defenses, including adversarial training, input\ntransformations, and heuristic detection, are computationally expensive,\narchitecture-dependent, and fragile against adaptive attacks. We introduce\nEigenShield, an inference-time defense leveraging Random Matrix Theory to\nquantify adversarial disruptions in high-dimensional VLM representations.\nUnlike prior methods that rely on empirical heuristics, EigenShield employs the\nspiked covariance model to detect structured spectral deviations. Using a\nRobustness-based Nonconformity Score (RbNS) and quantile-based thresholding, it\nseparates causal eigenvectors, which encode semantic information, from\ncorrelational eigenvectors that are susceptible to adversarial artifacts. By\nprojecting embeddings onto the causal subspace, EigenShield filters adversarial\nnoise without modifying model parameters or requiring adversarial training.\nThis architecture-independent, attack-agnostic approach significantly reduces\nthe attack success rate, establishing spectral analysis as a principled\nalternative to conventional defenses. Our results demonstrate that EigenShield\nconsistently outperforms all existing defenses, including adversarial training,\nUNIGUARD, and CIDER.",
      "pdf_url": "http://arxiv.org/pdf/2502.14976v1",
      "arxiv_url": "http://arxiv.org/abs/2502.14976v1",
      "published": "2025-02-20",
      "categories": [
        "cs.LG",
        "cs.CR",
        "cs.CV"
      ]
    },
    {
      "title": "Symmetric observations without symmetric causal explanations",
      "authors": [
        "Christian William",
        "Patrick Remy",
        "Jean-Daniel Bancal",
        "Yu Cai",
        "Nicolas Brunner",
        "Alejandro Pozas-Kerstjens"
      ],
      "abstract": "Inferring causal models from observed correlations is a challenging task,\ncrucial to many areas of science. In order to alleviate the effort, it is\nimportant to know whether symmetries in the observations correspond to\nsymmetries in the underlying realization. Via an explicit example, we answer\nthis question in the negative. We use a tripartite probability distribution\nover binary events that is realized by using three (different) independent\nsources of classical randomness. We prove that even removing the condition that\nthe sources distribute systems described by classical physics, the requirements\nthat i) the sources distribute the same physical systems, ii) these physical\nsystems respect relativistic causality, and iii) the correlations are the\nobserved ones, are incompatible.",
      "pdf_url": "http://arxiv.org/pdf/2502.14950v1",
      "arxiv_url": "http://arxiv.org/abs/2502.14950v1",
      "published": "2025-02-20",
      "categories": [
        "quant-ph",
        "cs.LG",
        "math.ST",
        "stat.ML",
        "stat.TH"
      ]
    },
    {
      "title": "Internal Incoherency Scores for Constraint-based Causal Discovery Algorithms",
      "authors": [
        "Sofia Faltenbacher",
        "Jonas Wahl",
        "Rebecca Herman",
        "Jakob Runge"
      ],
      "abstract": "Causal discovery aims to infer causal graphs from observational or\nexperimental data. Methods such as the popular PC algorithm are based on\nconditional independence testing and utilize enabling assumptions, such as the\nfaithfulness assumption, for their inferences. In practice, these assumptions,\nas well as the functional assumptions inherited from the chosen conditional\nindependence test, are typically taken as a given and not further tested for\ntheir validity on the data. In this work, we propose internal coherency scores\nthat allow testing for assumption violations and finite sample errors, whenever\ndetectable without requiring ground truth or further statistical tests. We\nprovide a complete classification of erroneous results, including a distinction\nbetween detectable and undetectable errors, and prove that the detectable\nerroneous results can be measured by our scores. We illustrate our coherency\nscores on the PC algorithm with simulated and real-world datasets, and envision\nthat testing for internal coherency can become a standard tool in applying\nconstraint-based methods, much like a suite of tests is used to validate the\nassumptions of classical regression analysis.",
      "pdf_url": "http://arxiv.org/pdf/2502.14719v1",
      "arxiv_url": "http://arxiv.org/abs/2502.14719v1",
      "published": "2025-02-20",
      "categories": [
        "stat.ML",
        "cs.LG"
      ]
    }
  ]
}