{
  "last_updated": "2025-09-16T00:48:47.544256",
  "papers": [
    {
      "title": "Abduct, Act, Predict: Scaffolding Causal Inference for Automated Failure Attribution in Multi-Agent Systems",
      "authors": [
        "Alva West",
        "Yixuan Weng",
        "Minjun Zhu",
        "Zhen Lin",
        "Yue Zhang"
      ],
      "abstract": "Failure attribution in multi-agent systems -- pinpointing the exact step\nwhere a decisive error occurs -- is a critical yet unsolved challenge. Current\nmethods treat this as a pattern recognition task over long conversation logs,\nleading to critically low step-level accuracy (below 17\\%), which renders them\nimpractical for debugging complex systems. Their core weakness is a fundamental\ninability to perform robust counterfactual reasoning: to determine if\ncorrecting a single action would have actually averted the task failure. To\nbridge this counterfactual inference gap, we introduce Abduct-Act-Predict (A2P)\nScaffolding, a novel agent framework that transforms failure attribution from\npattern recognition into a structured causal inference task. A2P explicitly\nguides a large language model through a formal three-step reasoning process\nwithin a single inference pass: (1) Abduction, to infer the hidden root causes\nbehind an agent's actions; (2) Action, to define a minimal corrective\nintervention; and (3) Prediction, to simulate the subsequent trajectory and\nverify if the intervention resolves the failure. This structured approach\nleverages the holistic context of the entire conversation while imposing a\nrigorous causal logic on the model's analysis. Our extensive experiments on the\nWho\\&When benchmark demonstrate its efficacy. On the Algorithm-Generated\ndataset, A2P achieves 47.46\\% step-level accuracy, a 2.85$\\times$ improvement\nover the 16.67\\% of the baseline. On the more complex Hand-Crafted dataset, it\nachieves 29.31\\% step accuracy, a 2.43$\\times$ improvement over the baseline's\n12.07\\%. By reframing the problem through a causal lens, A2P Scaffolding\nprovides a robust, verifiable, and significantly more accurate solution for\nautomated failure attribution.",
      "pdf_url": "http://arxiv.org/pdf/2509.10401v1",
      "arxiv_url": "http://arxiv.org/abs/2509.10401v1",
      "published": "2025-09-12",
      "categories": [
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "The Language of Approval: Identifying the Drivers of Positive Feedback Online",
      "authors": [
        "Agam Goyal",
        "Charlotte Lambert",
        "Eshwar Chandrasekharan"
      ],
      "abstract": "Positive feedback via likes and awards is central to online governance, yet\nwhich attributes of users' posts elicit rewards -- and how these vary across\nauthors and communities -- remains unclear. To examine this, we combine\nquasi-experimental causal inference with predictive modeling on 11M posts from\n100 subreddits. We identify linguistic patterns and stylistic attributes\ncausally linked to rewards, controlling for author reputation, timing, and\ncommunity context. For example, overtly complicated language, tentative style,\nand toxicity reduce rewards. We use our set of curated features to train models\nthat can detect highly-upvoted posts with high AUC. Our audit of community\nguidelines highlights a ``policy-practice gap'' -- most rules focus primarily\non civility and formatting requirements, with little emphasis on the attributes\nidentified to drive positive feedback. These results inform the design of\ncommunity guidelines, support interfaces that teach users how to craft\ndesirable contributions, and moderation workflows that emphasize positive\nreinforcement over purely punitive enforcement.",
      "pdf_url": "http://arxiv.org/pdf/2509.10370v1",
      "arxiv_url": "http://arxiv.org/abs/2509.10370v1",
      "published": "2025-09-12",
      "categories": [
        "cs.HC"
      ]
    },
    {
      "title": "Compartmentalised Agentic Reasoning for Clinical NLI",
      "authors": [
        "Maël Jullien",
        "Lei Xu",
        "Marco Valentino",
        "André Freitas"
      ],
      "abstract": "A common assumption holds that scaling data and parameters yields\nincreasingly structured, generalisable internal representations. We interrogate\nthis assumption in clinical natural language inference (NLI) by adopting a\nbenchmark decomposed into four reasoning families, Causal Attribution,\nCompositional Grounding, Epistemic Verification, and Risk State Abstraction,\nand introducing CARENLI, a Compartmentalised Agentic Reasoning for Clinical NLI\nthat separates knowledge access from principled inference. CARENLI routes each\npremise, statement pair to a family specific solver and enforces auditable\nprocedures via a planner, verifier, and refiner.\n  Across four LLMs, CARENLI improves fidelity by up to 42 points, reaching\n98.0% in Causal Attribution and 81.2% in Risk State Abstraction. Verifiers flag\nviolations with near-ceiling reliability, while refiners correct a substantial\nshare of epistemic errors. Remaining failures cluster in routing, identifying\nfamily classification as the main bottleneck. These results show that LLMs\noften retain relevant facts but default to heuristics when inference is\nunderspecified, a dissociation CARENLI makes explicit while offering a\nframework for safer, auditable reasoning.",
      "pdf_url": "http://arxiv.org/pdf/2509.10222v1",
      "arxiv_url": "http://arxiv.org/abs/2509.10222v1",
      "published": "2025-09-12",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Weakening assumptions in the evaluation of treatment effects in longitudinal randomized trials with truncation by death or other intercurrent events",
      "authors": [
        "Georgi Baklicharov",
        "Kelly Van Lancker",
        "Stijn Vansteelandt"
      ],
      "abstract": "Intercurrent events, such as treatment switching, rescue medication, or\ntruncation by death, can complicate the interpretation of intention-to-treat\n(ITT) analyses in randomized clinical trials. Recent advances in causal\ninference address these challenges by targeting alternative estimands, such as\nhypothetical estimands or principal stratum estimands (e.g., survivor average\ncausal effects). However, such approaches often require strong, unverifiable\nassumptions, partly due to limited data on time-varying confounders and the\ndifficulty of adjusting for them. Additionally, strict trial protocols\nfrequently lead to (near) violations of the positivity assumption, resulting in\nlimited information for identifying these estimands.\n  In this paper, we propose a novel approach that sidesteps these difficulties\nby focusing on testing the null hypothesis of no treatment effect in the\npresence of arbitrary intercurrent events, including truncation by death, using\nlongitudinal trial data. Our key idea is to compare treated and untreated\nindividuals, matched on baseline covariates, at the most recent time point\nbefore either experiences an intercurrent event. We refer to such contrasts as\nPairwise Last Observation Time (PLOT) estimands. These estimands can be\nidentified in randomized clinical trials without requiring additional\nstructural assumptions, and even in the presence of the aforementioned\npositivity violations. However, they may still be susceptible to a form of\nresidual selection bias. We show that this bias vanishes under the conditions\ntypically required by alternative methods, and find it to be more generally\nsmall in extensive simulation studies. Building on this, we develop\nasymptotically efficient, model-free tests using data-adaptive estimation of\nnuisance parameters. We evaluate the method's performance via simulation\nstudies.",
      "pdf_url": "http://arxiv.org/pdf/2509.10067v1",
      "arxiv_url": "http://arxiv.org/abs/2509.10067v1",
      "published": "2025-09-12",
      "categories": [
        "stat.ME"
      ]
    },
    {
      "title": "Causal PDE-Control Models: A Structural Framework for Dynamic Portfolio Optimization",
      "authors": [
        "Alejandro Rodriguez Dominguez"
      ],
      "abstract": "Classical portfolio models collapse under structural breaks, while modern\nmachine-learning allocators adapt flexibly but often at the cost of\ntransparency and interpretability. This paper introduces Causal PDE-Control\nModels (CPCMs), a unifying framework that integrates causal inference,\nnonlinear filtering, and forward-backward partial differential equations for\ndynamic portfolio optimization. The framework delivers three theoretical\nadvances: (i) the existence of conditional risk-neutral measures under evolving\ninformation sets; (ii) a projection-divergence duality that quantifies the\nstability cost of departing from the causal driver manifold; and (iii) causal\ncompleteness, establishing that a finite driver span can capture all systematic\npremia. Classical methods such as Markowitz, CAPM, and Black-Litterman appear\nas degenerate cases, while reinforcement learning and deep-hedging policies\nemerge as unconstrained, symmetry-breaking approximations. Empirically, CPCM\nsolvers implemented with physics-informed neural networks achieve higher Sharpe\nratios, lower turnover, and more persistent premia than both econometric and\nmachine-learning benchmarks, using a global equity panel with more than 300\ncandidate drivers. By reframing portfolio optimization around structural\ncausality and PDE control, CPCMs provide a rigorous, interpretable, and\ncomputationally tractable foundation for robust asset allocation under\nnonstationary conditions.",
      "pdf_url": "http://arxiv.org/pdf/2509.09585v1",
      "arxiv_url": "http://arxiv.org/abs/2509.09585v1",
      "published": "2025-09-11",
      "categories": [
        "q-fin.PM",
        "G.1.6; G.1.8; G.1.10; G.3; I.2.6; I.5.3; I.5.4; I.6.5; J.2; J.4; J.6"
      ]
    },
    {
      "title": "World Modeling with Probabilistic Structure Integration",
      "authors": [
        "Klemen Kotar",
        "Wanhee Lee",
        "Rahul Venkatesh",
        "Honglin Chen",
        "Daniel Bear",
        "Jared Watrous",
        "Simon Kim",
        "Khai Loong Aw",
        "Lilian Naing Chen",
        "Stefan Stojanov",
        "Kevin Feigelis",
        "Imran Thobani",
        "Alex Durango",
        "Khaled Jedoui",
        "Atlas Kazemian",
        "Dan Yamins"
      ],
      "abstract": "We present Probabilistic Structure Integration (PSI), a system for learning\nrichly controllable and flexibly promptable world models from data. PSI\nconsists of a three-step cycle. The first step, Probabilistic prediction,\ninvolves building a probabilistic graphical model Psi of the data, in the form\nof a random-access autoregressive sequence model. Psi supports a complete set\nof learned conditional distributions describing the dependence of any variables\nin the data on any other set of variables. In step 2, Structure extraction, we\nshow how to extract underlying low-dimensional properties in the data,\ncorresponding to a diverse set of meaningful \"intermediate structures\", in a\nzero-shot fashion via causal inference on Psi. Step 3, Integration, completes\nthe cycle by converting these structures into new token types that are then\ncontinually mixed back into the training diet as conditioning signals and\nprediction targets. Each such cycle augments the capabilities of Psi, both\nallowing it to model the underlying data better, and creating new control\nhandles -- akin to an LLM-like universal prompting language. We train an\ninstance of Psi on 1.4 trillion tokens of internet video data; we use it to\nperform a variety of useful video prediction and understanding inferences; we\nextract state-of-the-art optical flow, self-supervised depth and object\nsegmentation; and we use these structures to support a full cycle of predictive\nimprovements.",
      "pdf_url": "http://arxiv.org/pdf/2509.09737v1",
      "arxiv_url": "http://arxiv.org/abs/2509.09737v1",
      "published": "2025-09-10",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Teamwork as Linear Interpersonal Dynamics",
      "authors": [
        "Andrew Jun Lee",
        "Grace Qiyuan Miao",
        "Rick Dale",
        "Alexia Galati",
        "Hongjing Lu"
      ],
      "abstract": "Successful teamwork depends on interpersonal dynamics, the ways in which\nindividuals coordinate, influence, and adapt to one another over time. Existing\nmeasures of interpersonal dynamics, such as CRQA, correlation, Granger\ncausality, and transfer entropy, typically capture only a single dimension:\neither the synchrony/coordination or the direction of influence between\nindividuals. What is missing is a psychologically meaningful representation\nthat unifies these dimensions and varies systematically with behavior. We\npropose the context matrix as one such representation. The context matrix is\nthe transition matrix in a linear dynamical system, with entries specifying how\nmuch each individual's current behavior is attributable to their own versus\nevery other group member's past behaviors. Its values can be distilled into\npsychologically interpretable summary features of synchrony and directional\ninfluence. Evidence for the context matrix as psychologically meaningful is\nprovided in two steps. First, we develop a sequential Bayesian model that\ninfers context matrices from timeseries data and show that it accurately\nrecovers them in noisy simulations. Second, applying the model to human\neyetracking data, we show that summary features of the inferred context\nmatrices capture expected task-based differences in interpersonal dynamics (or\nlack thereof), predict task accuracy in psychologically reasonable ways, and\nshow some correspondence with existing measures (CRQA and Granger causality).\nWe conclude by situating the context matrix within a broader agenda for\nmodeling interpersonal dynamics.",
      "pdf_url": "http://arxiv.org/pdf/2509.08811v1",
      "arxiv_url": "http://arxiv.org/abs/2509.08811v1",
      "published": "2025-09-10",
      "categories": [
        "cs.MA"
      ]
    },
    {
      "title": "Doubly robust average treatment effect estimation for survival data",
      "authors": [
        "Byeonghee Lee",
        "Joonsung Kang"
      ],
      "abstract": "Considering censored outcomes in survival analysis can lead to quite complex\nresults in the model setting of causal inference. Causal inference has\nattracted a lot of attention over the past few years, but little research has\nbeen done on survival analysis. Even for the only research conducted, the\nmachine learning method was considered assuming a large sample, which is not\nsuitable in that the actual data are high dimensional low sample size (HDLSS)\nmethod. Therefore, penalty is considered for numerous covariates, and the\nrelationship between these covariates and treatment variables is reflected as a\ncovariate balancing property score (CBPS). It also considers censored results.\nTo this end, we will try to solve the above-mentioned problems by using\npenalized empirical likelihood, which considers both estimating equation and\npenalty. The proposed average treatment effect (ATE) estimator possesses the\noracle property, exhibiting key characteristics such as double robustness for\nunbiasedness, sparsity in model selection, and asymptotic normality.",
      "pdf_url": "http://arxiv.org/pdf/2509.08788v1",
      "arxiv_url": "http://arxiv.org/abs/2509.08788v1",
      "published": "2025-09-10",
      "categories": [
        "stat.ME"
      ]
    },
    {
      "title": "Automatic Failure Attribution and Critical Step Prediction Method for Multi-Agent Systems Based on Causal Inference",
      "authors": [
        "Guoqing Ma",
        "Jia Zhu",
        "Hanghui Guo",
        "Weijie Shi",
        "Jiawei Shen",
        "Jingjiang Liu",
        "Yidan Liang"
      ],
      "abstract": "Multi-agent systems (MAS) are critical for automating complex tasks, yet\ntheir practical deployment is severely hampered by the challenge of failure\nattribution. Current diagnostic tools, which rely on statistical correlations,\nare fundamentally inadequate; on challenging benchmarks like Who\\&When,\nstate-of-the-art methods achieve less than 15\\% accuracy in locating the\nroot-cause step of a failure. To address this critical gap, we introduce the\nfirst failure attribution framework for MAS grounded in multi-granularity\ncausal inference. Our approach makes two key technical contributions: (1) a\nperformance causal inversion principle, which correctly models performance\ndependencies by reversing the data flow in execution logs, combined with\nShapley values to accurately assign agent-level blame; (2) a novel causal\ndiscovery algorithm, CDC-MAS, that robustly identifies critical failure steps\nby tackling the non-stationary nature of MAS interaction data. The framework's\nattribution results directly fuel an automated optimization loop, generating\ntargeted suggestions whose efficacy is validated via counterfactual\nsimulations. Evaluations on the Who\\&When and TRAIL benchmarks demonstrate a\nsignificant leap in performance. Our method achieves up to 36.2\\% step-level\naccuracy. Crucially, the generated optimizations boost overall task success\nrates by an average of 22.4\\%. This work provides a principled and effective\nsolution for debugging complex agent interactions, paving the way for more\nreliable and interpretable multi-agent systems.",
      "pdf_url": "http://arxiv.org/pdf/2509.08682v1",
      "arxiv_url": "http://arxiv.org/abs/2509.08682v1",
      "published": "2025-09-10",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "LD-ViCE: Latent Diffusion Model for Video Counterfactual Explanations",
      "authors": [
        "Payal Varshney",
        "Adriano Lucieri",
        "Christoph Balada",
        "Sheraz Ahmed",
        "Andreas Dengel"
      ],
      "abstract": "Video-based AI systems are increasingly adopted in safety-critical domains\nsuch as autonomous driving and healthcare. However, interpreting their\ndecisions remains challenging due to the inherent spatiotemporal complexity of\nvideo data and the opacity of deep learning models. Existing explanation\ntechniques often suffer from limited temporal coherence, insufficient\nrobustness, and a lack of actionable causal insights. Current counterfactual\nexplanation methods typically do not incorporate guidance from the target\nmodel, reducing semantic fidelity and practical utility. We introduce Latent\nDiffusion for Video Counterfactual Explanations (LD-ViCE), a novel framework\ndesigned to explain the behavior of video-based AI models. Compared to previous\napproaches, LD-ViCE reduces the computational costs of generating explanations\nby operating in latent space using a state-of-the-art diffusion model, while\nproducing realistic and interpretable counterfactuals through an additional\nrefinement step. Our experiments demonstrate the effectiveness of LD-ViCE\nacross three diverse video datasets, including EchoNet-Dynamic (cardiac\nultrasound), FERV39k (facial expression), and Something-Something V2 (action\nrecognition). LD-ViCE outperforms a recent state-of-the-art method, achieving\nan increase in R2 score of up to 68% while reducing inference time by half.\nQualitative analysis confirms that LD-ViCE generates semantically meaningful\nand temporally coherent explanations, offering valuable insights into the\ntarget model behavior. LD-ViCE represents a valuable step toward the\ntrustworthy deployment of AI in safety-critical domains.",
      "pdf_url": "http://arxiv.org/pdf/2509.08422v1",
      "arxiv_url": "http://arxiv.org/abs/2509.08422v1",
      "published": "2025-09-10",
      "categories": [
        "cs.CV",
        "cs.LG"
      ]
    }
  ]
}