{
  "last_updated": "2025-10-29T00:55:11.384011",
  "papers": [
    {
      "title": "Causal Deep Q Network",
      "authors": [
        "Elouanes Khelifi",
        "Amir Saki",
        "Usef Faghihi"
      ],
      "abstract": "Deep Q Networks (DQN) have shown remarkable success in various reinforcement\nlearning tasks. However, their reliance on associative learning often leads to\nthe acquisition of spurious correlations, hindering their problem-solving\ncapabilities. In this paper, we introduce a novel approach to integrate causal\nprinciples into DQNs, leveraging the PEACE (Probabilistic Easy vAriational\nCausal Effect) formula for estimating causal effects. By incorporating causal\nreasoning during training, our proposed framework enhances the DQN's\nunderstanding of the underlying causal structure of the environment, thereby\nmitigating the influence of confounding factors and spurious correlations. We\ndemonstrate that integrating DQNs with causal capabilities significantly\nenhances their problem-solving capabilities without compromising performance.\nExperimental results on standard benchmark environments showcase that our\napproach outperforms conventional DQNs, highlighting the effectiveness of\ncausal reasoning in reinforcement learning. Overall, our work presents a\npromising avenue for advancing the capabilities of deep reinforcement learning\nagents through principled causal inference.",
      "pdf_url": "http://arxiv.org/pdf/2510.23424v1",
      "arxiv_url": "http://arxiv.org/abs/2510.23424v1",
      "published": "2025-10-27",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "The Reasoning Trap: How Enhancing LLM Reasoning Amplifies Tool Hallucination",
      "authors": [
        "Chenlong Yin",
        "Zeyang Sha",
        "Shiwen Cui",
        "Changhua Meng"
      ],
      "abstract": "Enhancing the reasoning capabilities of Large Language Models (LLMs) is a key\nstrategy for building Agents that \"think then act.\" However, recent\nobservations, like OpenAI's o3, suggest a paradox: stronger reasoning often\ncoincides with increased hallucination, yet no prior work has systematically\nexamined whether reasoning enhancement itself causes tool hallucination. To\naddress this gap, we pose the central question: Does strengthening reasoning\nincrease tool hallucination? To answer this, we introduce SimpleToolHalluBench,\na diagnostic benchmark measuring tool hallucination in two failure modes: (i)\nno tool available, and (ii) only distractor tools available. Through controlled\nexperiments, we establish three key findings. First, we demonstrate a causal\nrelationship: progressively enhancing reasoning through RL increases tool\nhallucination proportionally with task performance gains. Second, this effect\ntranscends overfitting - training on non-tool tasks (e.g., mathematics) still\namplifies subsequent tool hallucination. Third, the effect is method-agnostic,\nappearing when reasoning is instilled via supervised fine-tuning and when it is\nmerely elicited at inference by switching from direct answers to step-by-step\nthinking. We also evaluate mitigation strategies including Prompt Engineering\nand Direct Preference Optimization (DPO), revealing a fundamental\nreliability-capability trade-off: reducing hallucination consistently degrades\nutility. Mechanistically, Reasoning RL disproportionately collapses\ntool-reliability-related representations, and hallucinations surface as\namplified divergences concentrated in late-layer residual streams. These\nfindings reveal that current reasoning enhancement methods inherently amplify\ntool hallucination, highlighting the need for new training objectives that\njointly optimize for capability and reliability.",
      "pdf_url": "http://arxiv.org/pdf/2510.22977v1",
      "arxiv_url": "http://arxiv.org/abs/2510.22977v1",
      "published": "2025-10-27",
      "categories": [
        "cs.LG",
        "cs.AI",
        "I.2"
      ]
    },
    {
      "title": "Exploring Structures of Inferential Mechanisms through Simplistic Digital Circuits",
      "authors": [
        "Giovanni Sileno",
        "Jean-Louis Dessalles"
      ],
      "abstract": "Cognitive studies and artificial intelligence have developed distinct models\nfor various inferential mechanisms (categorization, induction, abduction,\ncausal inference, contrast, merge, ...). Yet, both natural and artificial views\non cognition lack apparently a unifying framework. This paper formulates a\nspeculative answer attempting to respond to this gap. To postulate on\nhigher-level activation processes from a material perspective, we consider\ninferential mechanisms informed by symbolic AI modelling techniques, through\nthe simplistic lenses of electronic circuits based on logic gates. We observe\nthat a logic gate view entails a different treatment of implication and\nnegation compared to standard logic and logic programming. Then, by\ncombinatorial exploration, we identify four main forms of dependencies that can\nbe realized by these inferential circuits. Looking at how these forms are\ngenerally used in the context of logic programs, we identify eight common\ninferential patterns, exposing traditionally distinct inferential mechanisms in\nan unifying framework. Finally, following a probabilistic interpretation of\nlogic programs, we unveil inner functional dependencies. The paper concludes\nelaborating in what sense, even if our arguments are mostly informed by\nsymbolic means and digital systems infrastructures, our observations may\npinpoint to more generally applicable structures.",
      "pdf_url": "http://arxiv.org/pdf/2510.22883v1",
      "arxiv_url": "http://arxiv.org/abs/2510.22883v1",
      "published": "2025-10-27",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Unifying regression-based and design-based causal inference in time-series experiments",
      "authors": [
        "Zhexiao Lin",
        "Peng Ding"
      ],
      "abstract": "Time-series experiments, also called switchback experiments or N-of-1 trials,\nplay increasingly important roles in modern applications in medical and\nindustrial areas. Under the potential outcomes framework, recent research has\nstudied time-series experiments from the design-based perspective, relying\nsolely on the randomness in the design to drive the statistical inference.\nFocusing on simpler statistical methods, we examine the design-based properties\nof regression-based methods for estimating treatment effects in time-series\nexperiments. We demonstrate that the treatment effects of interest can be\nconsistently estimated using ordinary least squares with an appropriately\nspecified working model and transformed regressors. Our analysis allows for\nestimating a diverging number of treatment effects simultaneously, and\nestablishes the consistency and asymptotic normality of the regression-based\nestimators. Additionally, we show that asymptotically, the heteroskedasticity\nand autocorrelation consistent variance estimators provide conservative\nestimates of the true, design-based variances. Importantly, although our\napproach relies on regression, our design-based framework allows for\nmisspecification of the regression model.",
      "pdf_url": "http://arxiv.org/pdf/2510.22864v1",
      "arxiv_url": "http://arxiv.org/abs/2510.22864v1",
      "published": "2025-10-26",
      "categories": [
        "stat.ME",
        "econ.EM",
        "math.ST",
        "stat.TH"
      ]
    },
    {
      "title": "A Theory of the Mechanics of Information: Generalization Through Measurement of Uncertainty (Learning is Measuring)",
      "authors": [
        "Christopher J. Hazard",
        "Michael Resnick",
        "Jacob Beel",
        "Jack Xia",
        "Cade Mack",
        "Dominic Glennie",
        "Matthew Fulp",
        "David Maze",
        "Andrew Bassett",
        "Martin Koistinen"
      ],
      "abstract": "Traditional machine learning relies on explicit models and domain\nassumptions, limiting flexibility and interpretability. We introduce a\nmodel-free framework using surprisal (information theoretic uncertainty) to\ndirectly analyze and perform inferences from raw data, eliminating distribution\nmodeling, reducing bias, and enabling efficient updates including direct edits\nand deletion of training data. By quantifying relevance through uncertainty,\nthe approach enables generalizable inference across tasks including generative\ninference, causal discovery, anomaly detection, and time series forecasting. It\nemphasizes traceability, interpretability, and data-driven decision making,\noffering a unified, human-understandable framework for machine learning, and\nachieves at or near state-of-the-art performance across most common machine\nlearning tasks. The mathematical foundations create a ``physics'' of\ninformation, which enable these techniques to apply effectively to a wide\nvariety of complex data types, including missing data. Empirical results\nindicate that this may be a viable alternative path to neural networks with\nregard to scalable machine learning and artificial intelligence that can\nmaintain human understandability of the underlying mechanics.",
      "pdf_url": "http://arxiv.org/pdf/2510.22809v1",
      "arxiv_url": "http://arxiv.org/abs/2510.22809v1",
      "published": "2025-10-26",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.ST",
        "stat.ML",
        "stat.TH"
      ]
    },
    {
      "title": "SteerX: Disentangled Steering for LLM Personalization",
      "authors": [
        "Xiaoyan Zhao",
        "Ming Yan",
        "Yilun Qiu",
        "Haoting Ni",
        "Yang Zhang",
        "Fuli Feng",
        "Hong Cheng",
        "Tat-Seng Chua"
      ],
      "abstract": "Large language models (LLMs) have shown remarkable success in recent years,\nenabling a wide range of applications, including intelligent assistants that\nsupport users' daily life and work. A critical factor in building such\nassistants is personalizing LLMs, as user preferences and needs vary widely.\nActivation steering, which directly leverages directions representing user\npreference in the LLM activation space to adjust its behavior, offers a\ncost-effective way to align the model's outputs with individual users. However,\nexisting methods rely on all historical data to compute the steering vector,\nignoring that not all content reflects true user preferences, which undermines\nthe personalization signal. To address this, we propose SteerX, a disentangled\nsteering method that isolates preference-driven components from\npreference-agnostic components. Grounded in causal inference theory, SteerX\nestimates token-level causal effects to identify preference-driven tokens,\ntransforms these discrete signals into a coherent description, and then\nleverages them to steer personalized LLM generation. By focusing on the truly\npreference-driven information, SteerX produces more accurate activation\nsteering vectors and enhances personalization. Experiments on two\nrepresentative steering backbone methods across real-world datasets demonstrate\nthat SteerX consistently enhances steering vector quality, offering a practical\nsolution for more effective LLM personalization.",
      "pdf_url": "http://arxiv.org/pdf/2510.22256v1",
      "arxiv_url": "http://arxiv.org/abs/2510.22256v1",
      "published": "2025-10-25",
      "categories": [
        "cs.CL"
      ]
    },
    {
      "title": "Generalization or Memorization: Dynamic Decoding for Mode Steering",
      "authors": [
        "Xuanming Zhang"
      ],
      "abstract": "Large Language Models (LLMs) exhibit a troubling duality, capable of both\nremarkable generalization and brittle, verbatim memorization of their training\ndata. This unpredictability undermines their reliability in high-stakes\napplications. In this work, we propose a unified framework to understand,\nidentify, and control these distinct reasoning modes. First, we introduce a\ntheoretical model based on the Information Bottleneck (IB) principle,\nformalizing generalization as the learning of a compressed, task-relevant\nrepresentation and memorization as a failure to compress. Building on this\ntheory, we develop Dynamic Mode Steering (DMS), a novel inference-time\nalgorithm which comprises two components: (1) a lightweight, causally-grounded\nlinear probe that identifies the model's instantaneous reliance on\nmemorization, and (2) a dynamic activation steering mechanism that nudges the\nmodel's computation towards pre-identified generalization circuits. We frame\nDMS as a form of adaptive, self-contrastive decoding. Experiments on reasoning\nand faithfulness tasks demonstrate that DMS significantly improves logical\nconsistency and factual accuracy, thereby offering a principled approach to\nenhancing LLM reliability.",
      "pdf_url": "http://arxiv.org/pdf/2510.22099v1",
      "arxiv_url": "http://arxiv.org/abs/2510.22099v1",
      "published": "2025-10-25",
      "categories": [
        "cs.CL"
      ]
    },
    {
      "title": "Resilient Radio Access Networks: AI and the Unknown Unknowns",
      "authors": [
        "Bho Matthiesen",
        "Armin Dekorsy",
        "Petar Popovski"
      ],
      "abstract": "5G networks offer exceptional reliability and availability, ensuring\nconsistent performance and user satisfaction. Yet they might still fail when\nconfronted with the unexpected. A resilient system is able to adapt to\nreal-world complexity, including operating conditions completely unanticipated\nduring system design. This makes resilience a vital attribute for communication\nsystems that must sustain service in scenarios where models are absent or too\nintricate to provide statistical guarantees. Such considerations indicate that\nartifical intelligence (AI) will play a major role in delivering resilience. In\nthis paper, we examine the challenges of designing AIs for resilient radio\naccess networks, especially with respect to unanticipated and rare disruptions.\nOur theoretical results indicate strong limitations of current statistical\nlearning methods for resilience and suggest connections to online learning and\ncausal inference.",
      "pdf_url": "http://arxiv.org/pdf/2510.21587v1",
      "arxiv_url": "http://arxiv.org/abs/2510.21587v1",
      "published": "2025-10-24",
      "categories": [
        "cs.IT",
        "math.IT"
      ]
    },
    {
      "title": "Bridging Prediction and Attribution: Identifying Forward and Backward Causal Influence Ranges Using Assimilative Causal Inference",
      "authors": [
        "Marios Andreou",
        "Nan Chen"
      ],
      "abstract": "Causal inference identifies cause-and-effect relationships between variables.\nWhile traditional approaches rely on data to reveal causal links, a recently\ndeveloped method, assimilative causal inference (ACI), integrates observations\nwith dynamical models. It utilizes Bayesian data assimilation to trace causes\nback from observed effects by quantifying the reduction in uncertainty. ACI\nadvances the detection of instantaneous causal relationships and the\nintermittent reversal of causal roles over time. Beyond identifying causal\nconnections, an equally important challenge is determining the associated\ncausal influence range (CIR), indicating when causal influences emerged and for\nhow long they persist. In this paper, ACI is employed to develop mathematically\nrigorous formulations of both forward and backward CIRs at each time. The\nforward CIR quantifies the temporal impact of a cause, while the backward CIR\ntraces the onset of triggers for an observed effect, thus characterizing causal\npredictability and attribution of outcomes at each transient phase,\nrespectively. Objective and robust metrics for both CIRs are introduced,\neliminating the need for empirical thresholds. Computationally efficient\napproximation algorithms to compute CIRs are developed, which facilitate the\nuse of closed-form expressions for a broad class of nonlinear dynamical\nsystems. Numerical simulations demonstrate how this forward and backward CIR\nframework provides new possibilities for probing complex dynamical systems. It\nadvances the study of bifurcation-driven and noise-induced tipping points in\nEarth systems, investigates the impact from resolving the interfering variables\nwhen determining the influence ranges, and elucidates atmospheric blocking\nmechanisms in the equatorial region. These results have direct implications for\nscience, policy, and decision-making.",
      "pdf_url": "http://arxiv.org/pdf/2510.21889v1",
      "arxiv_url": "http://arxiv.org/abs/2510.21889v1",
      "published": "2025-10-24",
      "categories": [
        "stat.ML",
        "cs.LG",
        "math.ST",
        "physics.data-an",
        "stat.ME",
        "stat.TH",
        "62F15, 62D20, 62M20, 93E11, 93E14, 60H10"
      ]
    },
    {
      "title": "Handling Missing Responses under Cluster Dependence with Applications to Language Model Evaluation",
      "authors": [
        "Zhenghao Zeng",
        "David Arbour",
        "Avi Feller",
        "Ishita Dasgupta",
        "Atanu R Sinha",
        "Edward H. Kennedy"
      ],
      "abstract": "Human annotations play a crucial role in evaluating the performance of GenAI\nmodels. Two common challenges in practice, however, are missing annotations\n(the response variable of interest) and cluster dependence among human-AI\ninteractions (e.g., questions asked by the same user may be highly correlated).\nReliable inference must address both these issues to achieve unbiased\nestimation and appropriately quantify uncertainty when estimating average\nscores from human annotations. In this paper, we analyze the doubly robust\nestimator, a widely used method in missing data analysis and causal inference,\napplied to this setting and establish novel theoretical properties under\ncluster dependence. We further illustrate our findings through simulations and\na real-world conversation quality dataset. Our theoretical and empirical\nresults underscore the importance of incorporating cluster dependence in\nmissing response problems to perform valid statistical inference.",
      "pdf_url": "http://arxiv.org/pdf/2510.20928v1",
      "arxiv_url": "http://arxiv.org/abs/2510.20928v1",
      "published": "2025-10-23",
      "categories": [
        "stat.ME"
      ]
    }
  ]
}