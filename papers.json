{
  "last_updated": "2025-09-08T00:53:10.468277",
  "papers": [
    {
      "title": "Interpretable Clustering with Adaptive Heterogeneous Causal Structure Learning in Mixed Observational Data",
      "authors": [
        "Wenrui Li",
        "Qinghao Zhang",
        "Xiaowo Wang"
      ],
      "abstract": "Understanding causal heterogeneity is essential for scientific discovery in\ndomains such as biology and medicine. However, existing methods lack causal\nawareness, with insufficient modeling of heterogeneity, confounding, and\nobservational constraints, leading to poor interpretability and difficulty\ndistinguishing true causal heterogeneity from spurious associations. We propose\nan unsupervised framework, HCL (Interpretable Causal Mechanism-Aware Clustering\nwith Adaptive Heterogeneous Causal Structure Learning), that jointly infers\nlatent clusters and their associated causal structures from mixed-type\nobservational data without requiring temporal ordering, environment labels,\ninterventions or other prior knowledge. HCL relaxes the homogeneity and\nsufficiency assumptions by introducing an equivalent representation that\nencodes both structural heterogeneity and confounding. It further develops a\nbi-directional iterative strategy to alternately refine causal clustering and\nstructure learning, along with a self-supervised regularization that balance\ncross-cluster universality and specificity. Together, these components enable\nconvergence toward interpretable, heterogeneous causal patterns. Theoretically,\nwe show identifiability of heterogeneous causal structures under mild\nconditions. Empirically, HCL achieves superior performance in both clustering\nand structure learning tasks, and recovers biologically meaningful mechanisms\nin real-world single-cell perturbation data, demonstrating its utility for\ndiscovering interpretable, mechanism-level causal heterogeneity.",
      "pdf_url": "http://arxiv.org/pdf/2509.04415v1",
      "arxiv_url": "http://arxiv.org/abs/2509.04415v1",
      "published": "2025-09-04",
      "categories": [
        "cs.LG"
      ]
    },
    {
      "title": "OccTENS: 3D Occupancy World Model via Temporal Next-Scale Prediction",
      "authors": [
        "Bu Jin",
        "Songen Gu",
        "Xiaotao Hu",
        "Yupeng Zheng",
        "Xiaoyang Guo",
        "Qian Zhang",
        "Xiaoxiao Long",
        "Wei Yin"
      ],
      "abstract": "In this paper, we propose OccTENS, a generative occupancy world model that\nenables controllable, high-fidelity long-term occupancy generation while\nmaintaining computational efficiency. Different from visual generation, the\noccupancy world model must capture the fine-grained 3D geometry and dynamic\nevolution of the 3D scenes, posing great challenges for the generative models.\nRecent approaches based on autoregression (AR) have demonstrated the potential\nto predict vehicle movement and future occupancy scenes simultaneously from\nhistorical observations, but they typically suffer from \\textbf{inefficiency},\n\\textbf{temporal degradation} in long-term generation and \\textbf{lack of\ncontrollability}. To holistically address these issues, we reformulate the\noccupancy world model as a temporal next-scale prediction (TENS) task, which\ndecomposes the temporal sequence modeling problem into the modeling of spatial\nscale-by-scale generation and temporal scene-by-scene prediction. With a\n\\textbf{TensFormer}, OccTENS can effectively manage the temporal causality and\nspatial relationships of occupancy sequences in a flexible and scalable way. To\nenhance the pose controllability, we further propose a holistic pose\naggregation strategy, which features a unified sequence modeling for occupancy\nand ego-motion. Experiments show that OccTENS outperforms the state-of-the-art\nmethod with both higher occupancy quality and faster inference time.",
      "pdf_url": "http://arxiv.org/pdf/2509.03887v1",
      "arxiv_url": "http://arxiv.org/abs/2509.03887v1",
      "published": "2025-09-04",
      "categories": [
        "cs.CV"
      ]
    },
    {
      "title": "Causality-guided Prompt Learning for Vision-language Models via Visual Granulation",
      "authors": [
        "Mengyu Gao",
        "Qiulei Dong"
      ],
      "abstract": "Prompt learning has recently attracted much attention for adapting\npre-trained vision-language models (e.g., CLIP) to downstream recognition\ntasks. However, most of the existing CLIP-based prompt learning methods only\nshow a limited ability for handling fine-grained datasets. To address this\nissue, we propose a causality-guided text prompt learning method via visual\ngranulation for CLIP, called CaPL, where the explored visual granulation\ntechnique could construct sets of visual granules for the text prompt to\ncapture subtle discrepancies among different fine-grained classes through\ncasual inference. The CaPL method contains the following two modules: (1) An\nattribute disentanglement module is proposed to decompose visual features into\nnon-individualized attributes (shared by some classes) and individualized\nattributes (specific to single classes) using a Brownian Bridge Diffusion\nModel; (2) A granule learning module is proposed to construct visual granules\nby integrating the aforementioned attributes for recognition under two causal\ninference strategies. Thanks to the learned visual granules, more\ndiscriminative text prompt is expected to be learned. Extensive experimental\nresults on 15 datasets demonstrate that our CaPL method significantly\noutperforms the state-of-the-art prompt learning methods, especially on\nfine-grained datasets.",
      "pdf_url": "http://arxiv.org/pdf/2509.03803v1",
      "arxiv_url": "http://arxiv.org/abs/2509.03803v1",
      "published": "2025-09-04",
      "categories": [
        "cs.CV"
      ]
    },
    {
      "title": "The super learner for time-to-event outcomes: A tutorial",
      "authors": [
        "Ruth H. Keogh",
        "Karla Diaz-Ordaz",
        "Nan van Geloven",
        "Jon Michael Gran",
        "Kamaryn T. Tanner"
      ],
      "abstract": "Estimating risks or survival probabilities conditional on individual\ncharacteristics based on censored time-to-event data is a commonly faced task.\nThis may be for the purpose of developing a prediction model or may be part of\na wider estimation procedure, such as in causal inference. A challenge is that\nit is impossible to know at the outset which of a set of candidate models will\nprovide the best predictions. The super learner is a powerful approach for\nfinding the best model or combination of models ('ensemble') among a\npre-specified set of candidate models or 'learners', which can include\nparametric and machine learning models. Super learners for time-to-event\noutcomes have been developed, but the literature is technical and a reader may\nfind it challenging to gather together the full details of how these methods\nwork and can be implemented. In this paper we provide a practical tutorial on\nsuper learner methods for time-to-event outcomes. An overview of the general\nsteps involved in the super learner is given, followed by details of three\nspecific implementations for time-to-event outcomes. We cover discrete-time and\ncontinuous-time versions of the super learner, as described by Polley and van\nder Laan (2011), Westling et al. (2023) and Munch and Gerds (2024). We compare\nthe properties of the methods and provide information on how they can be\nimplemented in R. The methods are illustrated using an open access data set and\nR code is provided.",
      "pdf_url": "http://arxiv.org/pdf/2509.03315v1",
      "arxiv_url": "http://arxiv.org/abs/2509.03315v1",
      "published": "2025-09-03",
      "categories": [
        "stat.ME"
      ]
    },
    {
      "title": "Bayesian Network Propensity Score to Evaluate Treatment Effects in Observational Studies",
      "authors": [
        "Clelia Di Serio",
        "Federica Cugnata",
        "Pier Luigi Conti",
        "Alberto Briganti",
        "Fulvia Mecatti",
        "Paola Vicard",
        "Paola Maria Vittoria Rancoita"
      ],
      "abstract": "This paper focuses on the Bayesian Network Propensity Score (BNPS), a novel\napproach for estimating treatment effects in observational studies\ncharacterized by unknown (and likely unbalanced) designs and complex dependency\nstructures among covariates. Traditional methods, such as logistic regression,\noften impose rigid parametric assumptions that may lead to misspecification\nerrors, compromising causal inference. Recent classical and machine learning\nalternatives, such as boosted CART, random forests, and Stable Balancing\nWeights, seem to be attractive in a predictive perspective, but they typically\nlack asymptotic properties, such as consistency, efficiency, and valid variance\nestimation. In contrast, the recently proposed BNPS to estimate propensity\nscores uses Bayesian Networks to flexibly model conditional dependencies while\npreserving essential statistical properties such as consistency, asymptotic\nnormality and asymptotic efficiency. Combined with the H\\'ajek estimator, BNPS\nenables robust estimation of the Average Treatment Effect (ATE) in scenarios\nwith strong covariate interactions and unknown data-generating mechanisms.\nThrough extensive simulations across fifteen realistic scenarios and varying\nsample sizes, BNPS consistently outperforms benchmark methods in both empirical\nrejection rates and coverage accuracy. Finally, an application to a real-world\ndataset of 7,162 prostate cancer patients from San Raffaele Hospital (Milan,\nItaly) demonstrates BNPS's practical value in assessing the impact of pelvic\nlymph node dissection on hospitalization duration and biochemical recurrence.\nThe findings support BNPS as a statistically robust, interpretable and\ntransparent alternative for causal inference in complex observational settings,\nenhancing the reliability of evidence from real-world biomedical data.",
      "pdf_url": "http://arxiv.org/pdf/2509.03194v1",
      "arxiv_url": "http://arxiv.org/abs/2509.03194v1",
      "published": "2025-09-03",
      "categories": [
        "stat.ME",
        "stat.AP"
      ]
    },
    {
      "title": "Distribution-valued Causal Machine Learning: Implications of Credit on Spending Patterns",
      "authors": [
        "Cheuk Hang Leung",
        "Yijun Li",
        "Qi Wu"
      ],
      "abstract": "Fintech lending has become a central mechanism through which digital\nplatforms stimulate consumption, offering dynamic, personalized credit limits\nthat directly shape the purchasing power of consumers. Although prior research\nshows that higher limits increase average spending, scalar-based outcomes\nobscure the heterogeneous distributional nature of consumer responses. This\npaper addresses this gap by proposing a new causal inference framework that\nestimates how continuous changes in the credit limit affect the entire\ndistribution of consumer spending. We formalize distributional causal effects\nwithin the Wasserstein space and introduce a robust Distributional Double\nMachine Learning estimator, supported by asymptotic theory to ensure\nconsistency and validity. To implement this estimator, we design a deep\nlearning architecture comprising two components: a Neural Functional Regression\nNet to capture complex, nonlinear relationships between treatments, covariates,\nand distributional outcomes, and a Conditional Normalizing Flow Net to estimate\ngeneralized propensity scores under continuous treatment. Numerical experiments\ndemonstrate that the proposed estimator accurately recovers distributional\neffects in a range of data-generating scenarios. Applying our framework to\ntransaction-level data from a major BigTech platform, we find that increased\ncredit limits primarily shift consumers towards higher-value purchases rather\nthan uniformly increasing spending, offering new insights for personalized\nmarketing strategies and digital consumer finance.",
      "pdf_url": "http://arxiv.org/pdf/2509.03063v1",
      "arxiv_url": "http://arxiv.org/abs/2509.03063v1",
      "published": "2025-09-03",
      "categories": [
        "econ.GN",
        "q-fin.EC"
      ]
    },
    {
      "title": "Covariate Adjustment Cannot Hurt: Treatment Effect Estimation under Interference with Low-Order Outcome Interactions",
      "authors": [
        "Xinyi Wang",
        "Shuangning Li"
      ],
      "abstract": "In randomized experiments, covariates are often used to reduce variance and\nimprove the precision of treatment effect estimates. However, in many real\nworld settings, interference between units, where one unit's treatment affects\nanother's outcome, complicates causal inference. This raises a key question:\nhow can covariates be effectively used in the presence of interference?\nAddressing this challenge is nontrivial, as direct covariate adjustment, such\nas through regression, can sometimes increase variance due to dependencies\nacross units. In this paper, we study how to use covariate information to\nreduce the variance of treatment effect estimators under interference. We focus\non the total treatment effect (TTE), defined as the difference in average\noutcomes when all units are treated versus when all are controlled. Our\nanalysis is conducted under the neighborhood interference model and a low order\ninteraction outcome model. Building on the SNIPE estimator from\nCortez-Rodriguez et al. (2023), we propose a covariate adjusted SNIPE estimator\nand show that, under sparsity conditions on the interference network, the\nproposed estimator is asymptotically unbiased and has asymptotic variance no\ngreater than that of the original SNIPE estimator. This parallels the classical\nresult of Lin (2013) under the no interference assumption, where covariate\nadjustment does not worsen estimation precision. Importantly, our variance\nimprovement result does not rely on strong assumptions about the covariates:\nthe covariates may be arbitrarily dependent, affect outcomes across units, and\ndepend on the interference network itself.",
      "pdf_url": "http://arxiv.org/pdf/2509.03050v1",
      "arxiv_url": "http://arxiv.org/abs/2509.03050v1",
      "published": "2025-09-03",
      "categories": [
        "stat.ME"
      ]
    },
    {
      "title": "Improving Generative Methods for Causal Evaluation via Simulation-Based Inference",
      "authors": [
        "Pracheta Amaranath",
        "Vinitra Muralikrishnan",
        "Amit Sharma",
        "David D. Jensen"
      ],
      "abstract": "Generating synthetic datasets that accurately reflect real-world\nobservational data is critical for evaluating causal estimators, but remains a\nchallenging task. Existing generative methods offer a solution by producing\nsynthetic datasets anchored in the observed data (source data) while allowing\nvariation in key parameters such as the treatment effect and amount of\nconfounding bias. However, existing methods typically require users to provide\npoint estimates of such parameters (rather than distributions) and fixed\nestimates (rather than estimates that can be improved with reference to the\nsource data). This denies users the ability to express uncertainty over\nparameter values and removes the potential for posterior inference, potentially\nleading to unreliable estimator comparisons. We introduce simulation-based\ninference for causal evaluation (SBICE), a framework that models generative\nparameters as uncertain and infers their posterior distribution given a source\ndataset. Leveraging techniques in simulation-based inference, SBICE identifies\nparameter configurations that produce synthetic datasets closely aligned with\nthe source data distribution. Empirical results demonstrate that SBICE improves\nthe reliability of estimator evaluations by generating more realistic datasets,\nwhich supports a robust and data-consistent approach to causal benchmarking\nunder uncertainty.",
      "pdf_url": "http://arxiv.org/pdf/2509.02892v1",
      "arxiv_url": "http://arxiv.org/abs/2509.02892v1",
      "published": "2025-09-02",
      "categories": [
        "cs.LG",
        "stat.ME"
      ]
    },
    {
      "title": "Causal Spatial Quantile Regression",
      "authors": [
        "Yan Gong",
        "Reetam Majumder",
        "Brian J. Reich",
        "RaphaÃ«l Huser"
      ],
      "abstract": "Treatment effects in a wide range of economic, environmental, and\nepidemiological applications often vary across space, and understanding the\nheterogeneity of causal effects across space and outcome quantiles is a\ncritical challenge in spatial causal inference. To effectively capture spatial\nheterogeneity in distributional treatment effects, we propose a novel\nsemiparametric neural network-based causal framework leveraging deep spatial\nquantile regression and then construct a plug-in estimator for spatial quantile\ntreatment effects (SQTE). This framework incorporates an efficient adjustment\nprocedure to mitigate the impact of spatial hidden confounders. Extensive\nsimulations across various scenarios demonstrate that our methodology can\naccurately estimate SQTE, even with the presence of spatial hidden confounders.\nAdditionally, the spatial confounding adjustment procedure effectively reduces\nneighborhood spatial patterns in the residuals. We apply this method to assess\nthe spatially varying quantile treatment effects of maternal smoking on newborn\nbirth weight in North Carolina, United States. Our findings consistently show\nnegative effects across all birth weight quantiles, with particularly severe\nimpacts observed in the lower quantile regions.",
      "pdf_url": "http://arxiv.org/pdf/2509.02294v1",
      "arxiv_url": "http://arxiv.org/abs/2509.02294v1",
      "published": "2025-09-02",
      "categories": [
        "stat.ME"
      ]
    },
    {
      "title": "Interpretational errors with instrumental variables",
      "authors": [
        "Luca Locher",
        "Mats J. Stensrud",
        "Aaron L. Sarvet"
      ],
      "abstract": "Instrumental variables (IV) are often used to identify causal effects in\nobservational settings and experiments subject to non-compliance. Under\ncanonical assumptions, IVs allow us to identify a so-called local average\ntreatment effect (LATE). The use of IVs is often accompanied by a pragmatic\ndecision to abandon the identification of the causal parameter that corresponds\nto the original research question and target the LATE instead. This pragmatic\ndecision presents a potential source of error: an investigator mistakenly\ninterprets findings as if they had made inference on their original causal\nparameter of interest. We conducted a systematic review and meta-analysis of\npatterns of pragmatism and interpretational errors in the applied IV literature\npublished in leading journals of economics, political science, epidemiology,\nand clinical medicine (n = 309 unique studies). We found that a large fraction\nof studies targeted the LATE, although specific interest in this parameter was\nrare. Of these studies, 61% contained claims that mistakenly suggested that\nanother parameter was targeted -- one whose value likely differs, and could\neven have the opposite sign, from the parameter actually estimated. Our\nfindings suggest that the validity of conclusions drawn from IV applications is\noften compromised by interpretational errors.",
      "pdf_url": "http://arxiv.org/pdf/2509.02045v1",
      "arxiv_url": "http://arxiv.org/abs/2509.02045v1",
      "published": "2025-09-02",
      "categories": [
        "econ.EM",
        "stat.AP"
      ]
    }
  ]
}