{
  "last_updated": "2025-05-05T00:55:26.953512",
  "papers": [
    {
      "title": "On the Mechanistic Interpretability of Neural Networks for Causality in Bio-statistics",
      "authors": [
        "Jean-Baptiste A. Conan"
      ],
      "abstract": "Interpretable insights from predictive models remain critical in\nbio-statistics, particularly when assessing causality, where classical\nstatistical and machine learning methods often provide inherent clarity. While\nNeural Networks (NNs) offer powerful capabilities for modeling complex\nbiological data, their traditional \"black-box\" nature presents challenges for\nvalidation and trust in high-stakes health applications. Recent advances in\nMechanistic Interpretability (MI) aim to decipher the internal computations\nlearned by these networks. This work investigates the application of MI\ntechniques to NNs within the context of causal inference for bio-statistics.\n  We demonstrate that MI tools can be leveraged to: (1) probe and validate the\ninternal representations learned by NNs, such as those estimating nuisance\nfunctions in frameworks like Targeted Minimum Loss-based Estimation (TMLE); (2)\ndiscover and visualize the distinct computational pathways employed by the\nnetwork to process different types of inputs, potentially revealing how\nconfounders and treatments are handled; and (3) provide methodologies for\ncomparing the learned mechanisms and extracted insights across statistical,\nmachine learning, and NN models, fostering a deeper understanding of their\nrespective strengths and weaknesses for causal bio-statistical analysis.",
      "pdf_url": "http://arxiv.org/pdf/2505.00555v1",
      "arxiv_url": "http://arxiv.org/abs/2505.00555v1",
      "published": "2025-05-01",
      "categories": [
        "stat.AP",
        "cs.AI"
      ]
    },
    {
      "title": "Geodesic Synthetic Control Methods for Random Objects and Functional Data",
      "authors": [
        "Daisuke Kurisu",
        "Yidong Zhou",
        "Taisuke Otsu",
        "Hans-Georg Müller"
      ],
      "abstract": "We introduce a geodesic synthetic control method for causal inference that\nextends existing synthetic control methods to scenarios where outcomes are\nelements in a geodesic metric space rather than scalars. Examples of such\noutcomes include distributions, compositions, networks, trees and functional\ndata, among other data types that can be viewed as elements of a geodesic\nmetric space given a suitable metric. We extend this further to geodesic\nsynthetic difference-in-differences that builds on the established synthetic\ndifference-in-differences for Euclidean outcomes. This estimator generalizes\nboth the geodesic synthetic control method and a previously proposed geodesic\ndifference-in-differences method and exhibits a double robustness property. The\nproposed geodesic synthetic control method is illustrated through comprehensive\nsimulation studies and applications to the employment composition changes\nfollowing the 2011 Great East Japan Earthquake, and the impact of abortion\nliberalization policy on fertility patterns in East Germany. We illustrate the\nproposed geodesic synthetic difference-in-differences by studying the\nconsequences of the Soviet Union's collapse on age-at-death distributions for\nmales and females.",
      "pdf_url": "http://arxiv.org/pdf/2505.00331v1",
      "arxiv_url": "http://arxiv.org/abs/2505.00331v1",
      "published": "2025-05-01",
      "categories": [
        "stat.ME",
        "62D20, 62R20"
      ]
    },
    {
      "title": "A Unifying Framework for Robust and Efficient Inference with Unstructured Data",
      "authors": [
        "Jacob Carlson",
        "Melissa Dell"
      ],
      "abstract": "This paper presents a general framework for conducting efficient and robust\ninference on parameters derived from unstructured data, which include text,\nimages, audio, and video. Economists have long incorporated data extracted from\ntexts and images into their analyses, a practice that has accelerated with\nadvancements in deep neural networks. However, neural networks do not\ngenerically produce unbiased predictions, potentially propagating bias to\nestimators that use their outputs. To address this challenge, we reframe\ninference with unstructured data as a missing structured data problem, where\nstructured data are imputed from unstructured inputs using deep neural\nnetworks. This perspective allows us to apply classic results from\nsemiparametric inference, yielding valid, efficient, and robust estimators\nbased on unstructured data. We formalize this approach with MARS (Missing At\nRandom Structured Data), a unifying framework that integrates and extends\nexisting methods for debiased inference using machine learning predictions,\nlinking them to a variety of older, familiar problems such as causal inference.\nWe develop robust and efficient estimators for both descriptive and causal\nestimands and address challenges such as inference using aggregated and\ntransformed predictions from unstructured data. Importantly, MARS applies to\ncommon empirical settings that have received limited attention in the existing\nliterature. Finally, we reanalyze prominent studies that use unstructured data,\ndemonstrating the practical value of MARS.",
      "pdf_url": "http://arxiv.org/pdf/2505.00282v1",
      "arxiv_url": "http://arxiv.org/abs/2505.00282v1",
      "published": "2025-05-01",
      "categories": [
        "econ.EM",
        "cs.LG"
      ]
    },
    {
      "title": "Inference for max-linear Bayesian networks with noise",
      "authors": [
        "Mark Adams",
        "Kamillo Ferry",
        "Ruriko Yoshida"
      ],
      "abstract": "Max-Linear Bayesian Networks (MLBNs) provide a powerful framework for causal\ninference in extreme-value settings; we consider MLBNs with noise parameters\nwith a given topology in terms of the max-plus algebra by taking its logarithm.\nThen, we show that an estimator of a parameter for each edge in a directed\nacyclic graph (DAG) is distributed normally. We end this paper with\ncomputational experiments with the expectation and maximization (EM) algorithm\nand quadratic optimization.",
      "pdf_url": "http://arxiv.org/pdf/2505.00229v1",
      "arxiv_url": "http://arxiv.org/abs/2505.00229v1",
      "published": "2025-05-01",
      "categories": [
        "stat.ML",
        "cs.LG",
        "math.OC",
        "math.ST",
        "stat.TH",
        "14T90, 62A09, 62H30, 90C20, 90C90"
      ]
    },
    {
      "title": "Doubly robust augmented weighting estimators for the analysis of externally controlled single-arm trials and unanchored indirect treatment comparisons",
      "authors": [
        "Harlan Campbell",
        "Antonio Remiro-Azócar"
      ],
      "abstract": "Externally controlled single-arm trials are critical to assess treatment\nefficacy across therapeutic indications for which randomized controlled trials\nare not feasible. A closely-related research design, the unanchored indirect\ntreatment comparison, is often required for disconnected treatment networks in\nhealth technology assessment. We present a unified causal inference framework\nfor both research designs. We develop a novel estimator that augments a popular\nweighting approach based on entropy balancing -- matching-adjusted indirect\ncomparison (MAIC) -- by fitting a model for the conditional outcome\nexpectation. The predictions of the outcome model are combined with the entropy\nbalancing MAIC weights. While the standard MAIC estimator is singly robust\nwhere the outcome model is non-linear, our augmented MAIC approach is doubly\nrobust, providing increased robustness against model misspecification. This is\ndemonstrated in a simulation study with binary outcomes and a logistic outcome\nmodel, where the augmented estimator demonstrates its doubly robust property,\nwhile exhibiting higher precision than all non-augmented weighting estimators\nand near-identical precision to G-computation. We describe the extension of our\nestimator to the setting with unavailable individual participant data for the\nexternal control, illustrating it through an applied example. Our findings\nreinforce the understanding that entropy balancing-based approaches have\ndesirable properties compared to standard ``modeling'' approaches to weighting,\nbut should be augmented to improve protection against bias and guarantee double\nrobustness.",
      "pdf_url": "http://arxiv.org/pdf/2505.00113v1",
      "arxiv_url": "http://arxiv.org/abs/2505.00113v1",
      "published": "2025-04-30",
      "categories": [
        "stat.ME"
      ]
    },
    {
      "title": "Assessing Racial Disparities in Healthcare Expenditures Using Causal Path-Specific Effects",
      "authors": [
        "Xiaxian Ou",
        "Xinwei He",
        "David Benkeser",
        "Razieh Nabi"
      ],
      "abstract": "Racial disparities in healthcare expenditures are well-documented, yet the\nunderlying drivers remain complex and require further investigation. This study\nemploys causal and counterfactual path-specific effects to quantify how various\nfactors, including socioeconomic status, insurance access, health behaviors,\nand health status, mediate these disparities. Using data from the Medical\nExpenditures Panel Survey, we estimate how expenditures would differ under\ncounterfactual scenarios in which the values of specific mediators were aligned\nacross racial groups along selected causal pathways. A key challenge in this\nanalysis is ensuring robustness against model misspecification while addressing\nthe zero-inflation and right-skewness of healthcare expenditures. For reliable\ninference, we derive asymptotically linear estimators by integrating influence\nfunction-based techniques with flexible machine learning methods, including\nsuper learners and a two-part model tailored to the zero-inflated, right-skewed\nnature of healthcare expenditures.",
      "pdf_url": "http://arxiv.org/pdf/2504.21688v1",
      "arxiv_url": "http://arxiv.org/abs/2504.21688v1",
      "published": "2025-04-30",
      "categories": [
        "stat.AP",
        "stat.ME",
        "stat.ML"
      ]
    },
    {
      "title": "Convergence rate for Nearest Neighbour matching: geometry of the domain and higher-order regularity",
      "authors": [
        "Simon Viel",
        "Lionel Truquet",
        "Ikko Yamane"
      ],
      "abstract": "Estimating some mathematical expectations from partially observed data and in\nparticular missing outcomes is a central problem encountered in numerous fields\nsuch as transfer learning, counterfactual analysis or causal inference.\nMatching estimators, estimators based on k-nearest neighbours, are widely used\nin this context. It is known that the variance of such estimators can converge\nto zero at a parametric rate, but their bias can have a slower rate when the\ndimension of the covariates is larger than 2. This makes analysis of this bias\nparticularly important. In this paper, we provide higher order properties of\nthe bias. In contrast to the existing literature related to this problem, we do\nnot assume that the support of the target distribution of the covariates is\nstrictly included in that of the source, and we analyse two geometric\nconditions on the support that avoid such boundary bias problems. We show that\nthese conditions are much more general than the usual convex support\nassumption, leading to an improvement of existing results. Furthermore, we show\nthat the matching estimator studied by Abadie and Imbens (2006) for the average\ntreatment effect can be asymptotically efficient when the dimension of the\ncovariates is less than 4, a result only known in dimension 1.",
      "pdf_url": "http://arxiv.org/pdf/2504.21633v1",
      "arxiv_url": "http://arxiv.org/abs/2504.21633v1",
      "published": "2025-04-30",
      "categories": [
        "math.ST",
        "stat.TH"
      ]
    },
    {
      "title": "Powerful randomization tests for subgroup analysis",
      "authors": [
        "Yao Zhang",
        "Zijun Gao"
      ],
      "abstract": "Randomization tests are widely used to generate valid $p$-values for testing\nsharp null hypotheses in finite-population causal inference. This article\nextends their application to subgroup analysis. We show that directly testing\nsubgroup null hypotheses may lack power due to small subgroup sizes.\nIncorporating an estimator of the conditional average treatment effect (CATE)\ncan substantially improve power but requires splitting the treatment variables\nbetween estimation and testing to preserve finite-sample validity. To this end,\nwe propose BaR-learner, a Bayesian extension of the popular method R-learner\nfor CATE estimation. BaR-learner imputes the treatment variables reserved for\nrandomization tests, reducing information loss due to sample-splitting.\nFurthermore, we show that the treatment variables most informative for training\nBaR-learner are different from those most valuable for increasing test power.\nMotivated by this insight, we introduce AdaSplit, a sample-splitting procedure\nthat adaptively allocates units between estimation and testing. Simulation\nstudies demonstrate that our method yields more powerful randomization tests\nthan baselines that omit CATE estimation or rely on random sample-splitting. We\nalso apply our method to a blood pressure intervention trial, identifying\npatient subgroups with significant treatment effects.",
      "pdf_url": "http://arxiv.org/pdf/2504.21572v1",
      "arxiv_url": "http://arxiv.org/abs/2504.21572v1",
      "published": "2025-04-30",
      "categories": [
        "stat.ME"
      ]
    },
    {
      "title": "Artificial Intelligence for Personalized Prediction of Alzheimer's Disease Progression: A Survey of Methods, Data Challenges, and Future Directions",
      "authors": [
        "Gulsah Hancerliogullari Koksalmis",
        "Bulent Soykan",
        "Laura J. Brattain",
        "Hsin-Hsiung Huang"
      ],
      "abstract": "Alzheimer's Disease (AD) is marked by significant inter-individual\nvariability in its progression, complicating accurate prognosis and\npersonalized care planning. This heterogeneity underscores the critical need\nfor predictive models capable of forecasting patient-specific disease\ntrajectories. Artificial Intelligence (AI) offers powerful tools to address\nthis challenge by analyzing complex, multi-modal, and longitudinal patient\ndata. This paper provides a comprehensive survey of AI methodologies applied to\npersonalized AD progression prediction. We review key approaches including\nstate-space models for capturing temporal dynamics, deep learning techniques\nlike Recurrent Neural Networks for sequence modeling, Graph Neural Networks\n(GNNs) for leveraging network structures, and the emerging concept of AI-driven\ndigital twins for individualized simulation. Recognizing that data limitations\noften impede progress, we examine common challenges such as high\ndimensionality, missing data, and dataset imbalance. We further discuss\nAI-driven mitigation strategies, with a specific focus on synthetic data\ngeneration using Variational Autoencoders (VAEs) and Generative Adversarial\nNetworks (GANs) to augment and balance datasets. The survey synthesizes the\nstrengths and limitations of current approaches, emphasizing the trend towards\nmultimodal integration and the persistent need for model interpretability and\ngeneralizability. Finally, we identify critical open challenges, including\nrobust external validation, clinical integration, and ethical considerations,\nand outline promising future research directions such as hybrid models, causal\ninference, and federated learning. This review aims to consolidate current\nknowledge and guide future efforts in developing clinically relevant AI tools\nfor personalized AD prognostication.",
      "pdf_url": "http://arxiv.org/pdf/2504.21189v1",
      "arxiv_url": "http://arxiv.org/abs/2504.21189v1",
      "published": "2025-04-29",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.ET"
      ]
    },
    {
      "title": "A Hamiltonian Higher-Order Elasticity Framework for Dynamic Diagnostics(2HOED)",
      "authors": [
        "Ngueuleweu Tiwang Gildas"
      ],
      "abstract": "Machine learning detects patterns, block chain guarantees trust and\nimmutability, and modern causal inference identifies directional linkages, yet\nnone alone exposes the full energetic anatomy of complex systems; the\nHamiltonian Higher Order Elasticity Dynamics(2HOED) framework bridges these\ngaps. Grounded in classical mechanics but extended to Economics order\nelasticity terms, 2HOED represents economic, social, and physical systems as\nenergy-based Hamiltonians whose position, velocity, acceleration, and jerk of\nelasticity jointly determine systemic power, Inertia, policy sensitivity, and\nmarginal responses. Because the formalism is scaling free and coordinate\nagnostic, it transfers seamlessly from financial markets to climate science,\nfrom supply chain logistics to epidemiology, thus any discipline in which\nadaptation and shocks coexist. By embedding standard econometric variables\ninside a Hamiltonian, 2HOED enriches conventional economic analysis with\nrigorous diagnostics of resilience, tipping points, and feedback loops,\nrevealing failure modes invisible to linear models. Wavelet spectra, phase\nspace attractors, and topological persistence diagrams derived from 2HOED\nexpose multistage policy leverage that machine learning detects only\nempirically and block chain secures only after the fact. For economists,\nphysicians and other scientists, the method opens a new causal energetic\nchannel linking biological or mechanical elasticity to macro level outcomes.\nPortable, interpretable, and computationally light, 2HOED turns data streams\ninto dynamical energy maps, empowering decision makers to anticipate crises,\ndesign adaptive policies, and engineer robust systems delivering the predictive\npunch of AI with the explanatory clarity of physics.",
      "pdf_url": "http://arxiv.org/pdf/2504.21062v1",
      "arxiv_url": "http://arxiv.org/abs/2504.21062v1",
      "published": "2025-04-29",
      "categories": [
        "cs.LG",
        "econ.GN",
        "q-fin.EC"
      ]
    }
  ]
}