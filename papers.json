{
  "last_updated": "2025-12-19T00:57:17.260237",
  "papers": [
    {
      "title": "End-to-End Training for Autoregressive Video Diffusion via Self-Resampling",
      "authors": [
        "Yuwei Guo",
        "Ceyuan Yang",
        "Hao He",
        "Yang Zhao",
        "Meng Wei",
        "Zhenheng Yang",
        "Weilin Huang",
        "Dahua Lin"
      ],
      "abstract": "Autoregressive video diffusion models hold promise for world simulation but are vulnerable to exposure bias arising from the train-test mismatch. While recent works address this via post-training, they typically rely on a bidirectional teacher model or online discriminator. To achieve an end-to-end solution, we introduce Resampling Forcing, a teacher-free framework that enables training autoregressive video models from scratch and at scale. Central to our approach is a self-resampling scheme that simulates inference-time model errors on history frames during training. Conditioned on these degraded histories, a sparse causal mask enforces temporal causality while enabling parallel training with frame-level diffusion loss. To facilitate efficient long-horizon generation, we further introduce history routing, a parameter-free mechanism that dynamically retrieves the top-k most relevant history frames for each query. Experiments demonstrate that our approach achieves performance comparable to distillation-based baselines while exhibiting superior temporal consistency on longer videos owing to native-length training.",
      "pdf_url": "https://arxiv.org/pdf/2512.15702v1",
      "arxiv_url": "http://arxiv.org/abs/2512.15702v1",
      "published": "2025-12-17",
      "categories": [
        "cs.CV"
      ]
    },
    {
      "title": "mimic-video: Video-Action Models for Generalizable Robot Control Beyond VLAs",
      "authors": [
        "Jonas Pai",
        "Liam Achenbach",
        "Victoriano Montesinos",
        "Benedek Forrai",
        "Oier Mees",
        "Elvis Nava"
      ],
      "abstract": "Prevailing Vision-Language-Action Models (VLAs) for robotic manipulation are built upon vision-language backbones pretrained on large-scale, but disconnected static web data. As a result, despite improved semantic generalization, the policy must implicitly infer complex physical dynamics and temporal dependencies solely from robot trajectories. This reliance creates an unsustainable data burden, necessitating continuous, large-scale expert data collection to compensate for the lack of innate physical understanding. We contend that while vision-language pretraining effectively captures semantic priors, it remains blind to physical causality. A more effective paradigm leverages video to jointly capture semantics and visual dynamics during pretraining, thereby isolating the remaining task of low-level control. To this end, we introduce \\model, a novel Video-Action Model (VAM) that pairs a pretrained Internet-scale video model with a flow matching-based action decoder conditioned on its latent representations. The decoder serves as an Inverse Dynamics Model (IDM), generating low-level robot actions from the latent representation of video-space action plans. Our extensive evaluation shows that our approach achieves state-of-the-art performance on simulated and real-world robotic manipulation tasks, improving sample efficiency by 10x and convergence speed by 2x compared to traditional VLA architectures.",
      "pdf_url": "https://arxiv.org/pdf/2512.15692v1",
      "arxiv_url": "http://arxiv.org/abs/2512.15692v1",
      "published": "2025-12-17",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ]
    },
    {
      "title": "How Do Semantically Equivalent Code Transformations Impact Membership Inference on LLMs for Code?",
      "authors": [
        "Hua Yang",
        "Alejandro Velasco",
        "Thanh Le-Cong",
        "Md Nazmul Haque",
        "Bowen Xu",
        "Denys Poshyvanyk"
      ],
      "abstract": "The success of large language models for code relies on vast amounts of code data, including public open-source repositories, such as GitHub, and private, confidential code from companies. This raises concerns about intellectual property compliance and the potential unauthorized use of license-restricted code. While membership inference (MI) techniques have been proposed to detect such unauthorized usage, their effectiveness can be undermined by semantically equivalent code transformation techniques, which modify code syntax while preserving semantic.\n  In this work, we systematically investigate whether semantically equivalent code transformation rules might be leveraged to evade MI detection. The results reveal that model accuracy drops by only 1.5% in the worst case for each rule, demonstrating that transformed datasets can effectively serve as substitutes for fine-tuning. Additionally, we find that one of the rules (RenameVariable) reduces MI success by 10.19%, highlighting its potential to obscure the presence of restricted code. To validate these findings, we conduct a causal analysis confirming that variable renaming has the strongest causal effect in disrupting MI detection. Notably, we find that combining multiple transformations does not further reduce MI effectiveness. Our results expose a critical loophole in license compliance enforcement for training large language models for code, showing that MI detection can be substantially weakened by transformation-based obfuscation techniques.",
      "pdf_url": "https://arxiv.org/pdf/2512.15468v1",
      "arxiv_url": "http://arxiv.org/abs/2512.15468v1",
      "published": "2025-12-17",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CR"
      ]
    },
    {
      "title": "Non-parametric Causal Inference in Dynamic Thresholding Designs",
      "authors": [
        "Aditya Ghosh",
        "Stefan Wager"
      ],
      "abstract": "Consider a setting where we regularly monitor patients' fasting blood sugar, and declare them to have prediabetes (and encourage preventative care) if this number crosses a pre-specified threshold. The sharp, threshold-based treatment policy suggests that we should be able to estimate the long-term benefit of this preventative care by comparing the health trajectories of patients with blood sugar measurements right above and below the threshold. A naive regression-discontinuity analysis, however, is not applicable here, as it ignores the temporal dynamics of the problem where, e.g., a patient just below the threshold on one visit may become prediabetic (and receive treatment) following their next visit. Here, we study thresholding designs in general dynamic systems, and show that simple reduced-form characterizations remain available for a relevant causal target, namely a dynamic marginal policy effect at the treatment threshold. We develop a local-linear-regression approach for estimation and inference of this estimand, and demonstrate promise of our approach in numerical experiments.",
      "pdf_url": "https://arxiv.org/pdf/2512.15244v1",
      "arxiv_url": "http://arxiv.org/abs/2512.15244v1",
      "published": "2025-12-17",
      "categories": [
        "stat.ME",
        "econ.EM"
      ]
    },
    {
      "title": "Fast and Accurate Causal Parallel Decoding using Jacobi Forcing",
      "authors": [
        "Lanxiang Hu",
        "Siqi Kou",
        "Yichao Fu",
        "Samyam Rajbhandari",
        "Tajana Rosing",
        "Yuxiong He",
        "Zhijie Deng",
        "Hao Zhang"
      ],
      "abstract": "Multi-token generation has emerged as a promising paradigm for accelerating transformer-based large model inference. Recent efforts primarily explore diffusion Large Language Models (dLLMs) for parallel decoding to reduce inference latency. To achieve AR-level generation quality, many techniques adapt AR models into dLLMs to enable parallel decoding. However, they suffer from limited speedup compared to AR models due to a pretrain-to-posttrain mismatch. Specifically, the masked data distribution in post-training deviates significantly from the real-world data distribution seen during pretraining, and dLLMs rely on bidirectional attention, which conflicts with the causal prior learned during pretraining and hinders the integration of exact KV cache reuse. To address this, we introduce Jacobi Forcing, a progressive distillation paradigm where models are trained on their own generated parallel decoding trajectories, smoothly shifting AR models into efficient parallel decoders while preserving their pretrained causal inference property. The models trained under this paradigm, Jacobi Forcing Model, achieves 3.8x wall-clock speedup on coding and math benchmarks with minimal loss in performance. Based on Jacobi Forcing Models' trajectory characteristics, we introduce multi-block decoding with rejection recycling, which enables up to 4.5x higher token acceptance count per iteration and nearly 4.0x wall-clock speedup, effectively trading additional compute for lower inference latency. Our code is available at https://github.com/hao-ai-lab/JacobiForcing.",
      "pdf_url": "https://arxiv.org/pdf/2512.14681v1",
      "arxiv_url": "http://arxiv.org/abs/2512.14681v1",
      "published": "2025-12-16",
      "categories": [
        "cs.CL"
      ]
    },
    {
      "title": "PrivATE: Differentially Private Average Treatment Effect Estimation for Observational Data",
      "authors": [
        "Quan Yuan",
        "Xiaochen Li",
        "Linkang Du",
        "Min Chen",
        "Mingyang Sun",
        "Yunjun Gao",
        "Shibo He",
        "Jiming Chen",
        "Zhikun Zhang"
      ],
      "abstract": "Causal inference plays a crucial role in scientific research across multiple disciplines. Estimating causal effects, particularly the average treatment effect (ATE), from observational data has garnered significant attention. However, computing the ATE from real-world observational data poses substantial privacy risks to users. Differential privacy, which offers strict theoretical guarantees, has emerged as a standard approach for privacy-preserving data analysis. However, existing differentially private ATE estimation works rely on specific assumptions, provide limited privacy protection, or fail to offer comprehensive information protection.\n  To this end, we introduce PrivATE, a practical ATE estimation framework that ensures differential privacy. In fact, various scenarios require varying levels of privacy protection. For example, only test scores are generally sensitive information in education evaluation, while all types of medical record data are usually private. To accommodate different privacy requirements, we design two levels (i.e., label-level and sample-level) of privacy protection in PrivATE. By deriving an adaptive matching limit, PrivATE effectively balances noise-induced error and matching error, leading to a more accurate estimate of ATE. Our evaluation validates the effectiveness of PrivATE. PrivATE outperforms the baselines on all datasets and privacy budgets.",
      "pdf_url": "https://arxiv.org/pdf/2512.14557v1",
      "arxiv_url": "http://arxiv.org/abs/2512.14557v1",
      "published": "2025-12-16",
      "categories": [
        "cs.CR"
      ]
    },
    {
      "title": "Causal Secondary Analysis of Linked Data in the Presence of Mismatch Error",
      "authors": [
        "Martin Slawski"
      ],
      "abstract": "The increased prevalence of observational data and the need to integrate information from multiple sources are critical challenges in contemporary data analysis. Record linkage is a widely used tool for combining datasets in the absence of unique identifiers. The presence of linkage errors such as mismatched records, however, often hampers the analysis of data sets obtained in this way. This issue is more difficult to address in secondary analysis settings, where linkage and subsequent analysis are performed separately, and analysts have limited information about linkage quality. In this paper, we investigate the estimation of average treatment effects in the conventional potential outcome-based causal inference framework under linkage uncertainty. To mitigate the bias that would be incurred with naive analyses, we propose an approach based on estimating equations that treats the unknown match status indicators as missing data. Leveraging a variant of the Expectation-Maximization algorithm, these indicators are imputed based on a corresponding two-component mixture model. The approach is amenable to asymptotic inference. Simulation studies and a case study highlight the importance of accounting for linkage uncertainty and demonstrate the effectiveness of the proposed approach.",
      "pdf_url": "https://arxiv.org/pdf/2512.14492v1",
      "arxiv_url": "http://arxiv.org/abs/2512.14492v1",
      "published": "2025-12-16",
      "categories": [
        "stat.ME"
      ]
    },
    {
      "title": "Causal Structure Learning for Dynamical Systems with Theoretical Score Analysis",
      "authors": [
        "Nicholas Tagliapietra",
        "Katharina Ensinger",
        "Christoph Zimmer",
        "Osman Mian"
      ],
      "abstract": "Real world systems evolve in continuous-time according to their underlying causal relationships, yet their dynamics are often unknown. Existing approaches to learning such dynamics typically either discretize time -- leading to poor performance on irregularly sampled data -- or ignore the underlying causality. We propose CaDyT, a novel method for causal discovery on dynamical systems addressing both these challenges. In contrast to state-of-the-art causal discovery methods that model the problem using discrete-time Dynamic Bayesian networks, our formulation is grounded in Difference-based causal models, which allow milder assumptions for modeling the continuous nature of the system. CaDyT leverages exact Gaussian Process inference for modeling the continuous-time dynamics which is more aligned with the underlying dynamical process. We propose a practical instantiation that identifies the causal structure via a greedy search guided by the Algorithmic Markov Condition and Minimum Description Length principle. Our experiments show that CaDyT outperforms state-of-the-art methods on both regularly and irregularly-sampled data, discovering causal networks closer to the true underlying dynamics.",
      "pdf_url": "https://arxiv.org/pdf/2512.14361v1",
      "arxiv_url": "http://arxiv.org/abs/2512.14361v1",
      "published": "2025-12-16",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.DS"
      ]
    },
    {
      "title": "Scaling Causal Mediation for Complex Systems: A Framework for Root Cause Analysis",
      "authors": [
        "Alessandro Casadei",
        "Sreyoshi Bhaduri",
        "Rohit Malshe",
        "Pavan Mullapudi",
        "Raj Ratan",
        "Ankush Pole",
        "Arkajit Rakshit"
      ],
      "abstract": "Modern operational systems ranging from logistics and cloud infrastructure to industrial IoT, are governed by complex, interdependent processes. Understanding how interventions propagate through such systems requires causal inference methods that go beyond direct effects to quantify mediated pathways. Traditional mediation analysis, while effective in simple settings, fails to scale to the high-dimensional directed acyclic graphs (DAGs) encountered in practice, particularly when multiple treatments and mediators interact. In this paper, we propose a scalable mediation analysis framework tailored for large causal DAGs involving multiple treatments and mediators. Our approach systematically decomposes total effects into interpretable direct and indirect components. We demonstrate its practical utility through applied case studies in fulfillment center logistics, where complex dependencies and non-controllable factors often obscure root causes.",
      "pdf_url": "https://arxiv.org/pdf/2512.14764v1",
      "arxiv_url": "http://arxiv.org/abs/2512.14764v1",
      "published": "2025-12-16",
      "categories": [
        "stat.ME",
        "cs.AI",
        "econ.EM"
      ]
    },
    {
      "title": "A comparative overview of win ratio and joint frailty models for recurrent event endpoints with applications in oncology and cardiology",
      "authors": [
        "Adrien Oru√©",
        "Derek Dinart",
        "Laurent Billot",
        "Carine Bellera",
        "Virginie Rondeau"
      ],
      "abstract": "Composite endpoints that combine recurrent non-fatal events with a terminal event are increasingly used in randomized clinical trials, yet conventional time-to-first event analyses may obscure clinically relevant information. We compared two statistical frameworks tailored to such endpoints: the joint frailty model (JFM) and the last-event assisted recurrent-event win ratio (LWR). The JFM specifies proportional hazards for the recurrent and terminal events linked through a shared frailty, yielding covariate-adjusted, component-specific hazard ratios that account for informative recurrences and dependence with death. The LWR is a nonparametric, prioritized pairwise comparison that incorporates all observed events over follow-up and summarizes a population-level benefit of treatment while respecting a pre-specified hierarchy between death and recurrences. We first assessed the performance of the methods using simulations that varied both the gamma-frailty variance and the event rates. Next, we investigated these two frameworks using practical clinical applications, to assess the performance of the methods and to estimate the sample size required to achieve adequate power. These two approaches delivered complementary insights. The JFM provided component-specific estimates, while the LWR led to a summary measure of treatment effect with direction. Power was systematically improved with JFM, which thus appeared as the most reliable approach for inference and sample size estimation. Methodological extensions of the LWR to appropriately handle censoring and to formalize causal estimands remain a promising direction for future research.",
      "pdf_url": "https://arxiv.org/pdf/2512.13629v1",
      "arxiv_url": "http://arxiv.org/abs/2512.13629v1",
      "published": "2025-12-15",
      "categories": [
        "stat.ME"
      ]
    }
  ]
}