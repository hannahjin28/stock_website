{
  "last_updated": "2025-07-21T01:01:51.679908",
  "papers": [
    {
      "title": "Resurrect Mask AutoRegressive Modeling for Efficient and Scalable Image Generation",
      "authors": [
        "Yi Xin",
        "Le Zhuo",
        "Qi Qin",
        "Siqi Luo",
        "Yuewen Cao",
        "Bin Fu",
        "Yangfan He",
        "Hongsheng Li",
        "Guangtao Zhai",
        "Xiaohong Liu",
        "Peng Gao"
      ],
      "abstract": "AutoRegressive (AR) models have made notable progress in image generation,\nwith Masked AutoRegressive (MAR) models gaining attention for their efficient\nparallel decoding. However, MAR models have traditionally underperformed when\ncompared to standard AR models. This study refines the MAR architecture to\nimprove image generation quality. We begin by evaluating various image\ntokenizers to identify the most effective one. Subsequently, we introduce an\nimproved Bidirectional LLaMA architecture by replacing causal attention with\nbidirectional attention and incorporating 2D RoPE, which together form our\nadvanced model, MaskGIL. Scaled from 111M to 1.4B parameters, MaskGIL achieves\na FID score of 3.71, matching state-of-the-art AR models in the ImageNet\n256x256 benchmark, while requiring only 8 inference steps compared to the 256\nsteps of AR models. Furthermore, we develop a text-driven MaskGIL model with\n775M parameters for generating images from text at various resolutions. Beyond\nimage generation, MaskGIL extends to accelerate AR-based generation and enable\nreal-time speech-to-image conversion. Our codes and models are available at\nhttps://github.com/synbol/MaskGIL.",
      "pdf_url": "http://arxiv.org/pdf/2507.13032v1",
      "arxiv_url": "http://arxiv.org/abs/2507.13032v1",
      "published": "2025-07-17",
      "categories": [
        "cs.CV"
      ]
    },
    {
      "title": "Modeling Open-World Cognition as On-Demand Synthesis of Probabilistic Models",
      "authors": [
        "Lionel Wong",
        "Katherine M. Collins",
        "Lance Ying",
        "Cedegao E. Zhang",
        "Adrian Weller",
        "Tobias Gerstenberg",
        "Timothy O'Donnell",
        "Alexander K. Lew",
        "Jacob D. Andreas",
        "Joshua B. Tenenbaum",
        "Tyler Brooke-Wilson"
      ],
      "abstract": "When faced with novel situations, people are able to marshal relevant\nconsiderations from a wide range of background knowledge and put these to use\nin inferences and predictions. What permits us to draw in globally relevant\ninformation and reason over it coherently? Here, we explore the hypothesis that\npeople use a combination of distributed and symbolic representations to\nconstruct bespoke mental models tailored to novel situations. We propose a\ncomputational implementation of this idea -- a ``Model Synthesis Architecture''\n(MSA) -- using language models to implement global relevance-based retrieval\nand model synthesis and probabilistic programs to implement bespoke, coherent\nworld models. We evaluate our MSA as a model of human judgments on a novel\nreasoning dataset. The dataset -- built around a `Model Olympics` domain of\nsports vignettes -- tests models' capacity for human-like, open-ended reasoning\nby requiring (i) judgments about novel causal structures described in language;\n(ii) drawing on large bodies of background knowledge; and (iii) doing both in\nlight of observations that introduce arbitrary novel variables. Our MSA\napproach captures human judgments better than language model-only baselines,\nunder both direct and chain-of-thought generations from the LM that supports\nmodel synthesis. These results suggest that MSAs can be implemented in a way\nthat mirrors people's ability to deliver locally coherent reasoning over\nglobally relevant variables, offering a path to understanding and replicating\nhuman reasoning in open-ended domains.",
      "pdf_url": "http://arxiv.org/pdf/2507.12547v2",
      "arxiv_url": "http://arxiv.org/abs/2507.12547v2",
      "published": "2025-07-16",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.PL"
      ]
    },
    {
      "title": "Targeted Deep Architectures: A TMLE-Based Framework for Robust Causal Inference in Neural Networks",
      "authors": [
        "Yi Li",
        "David Mccoy",
        "Nolan Gunter",
        "Kaitlyn Lee",
        "Alejandro Schuler",
        "Mark van der Laan"
      ],
      "abstract": "Modern deep neural networks are powerful predictive tools yet often lack\nvalid inference for causal parameters, such as treatment effects or entire\nsurvival curves. While frameworks like Double Machine Learning (DML) and\nTargeted Maximum Likelihood Estimation (TMLE) can debias machine-learning fits,\nexisting neural implementations either rely on \"targeted losses\" that do not\nguarantee solving the efficient influence function equation or computationally\nexpensive post-hoc \"fluctuations\" for multi-parameter settings. We propose\nTargeted Deep Architectures (TDA), a new framework that embeds TMLE directly\ninto the network's parameter space with no restrictions on the backbone\narchitecture. Specifically, TDA partitions model parameters - freezing all but\na small \"targeting\" subset - and iteratively updates them along a targeting\ngradient, derived from projecting the influence functions onto the span of the\ngradients of the loss with respect to weights. This procedure yields plug-in\nestimates that remove first-order bias and produce asymptotically valid\nconfidence intervals. Crucially, TDA easily extends to multi-dimensional causal\nestimands (e.g., entire survival curves) by merging separate targeting\ngradients into a single universal targeting update. Theoretically, TDA inherits\nclassical TMLE properties, including double robustness and semiparametric\nefficiency. Empirically, on the benchmark IHDP dataset (average treatment\neffects) and simulated survival data with informative censoring, TDA reduces\nbias and improves coverage relative to both standard neural-network estimators\nand prior post-hoc approaches. In doing so, TDA establishes a direct, scalable\npathway toward rigorous causal inference within modern deep architectures for\ncomplex multi-parameter targets.",
      "pdf_url": "http://arxiv.org/pdf/2507.12435v1",
      "arxiv_url": "http://arxiv.org/abs/2507.12435v1",
      "published": "2025-07-16",
      "categories": [
        "cs.LG"
      ]
    },
    {
      "title": "Robust Causal Discovery in Real-World Time Series with Power-Laws",
      "authors": [
        "Matteo Tusoni",
        "Giuseppe Masi",
        "Andrea Coletta",
        "Aldo Glielmo",
        "Viviana Arrigoni",
        "Novella Bartolini"
      ],
      "abstract": "Exploring causal relationships in stochastic time series is a challenging yet\ncrucial task with a vast range of applications, including finance, economics,\nneuroscience, and climate science. Many algorithms for Causal Discovery (CD)\nhave been proposed, but they often exhibit a high sensitivity to noise,\nresulting in misleading causal inferences when applied to real data. In this\npaper, we observe that the frequency spectra of typical real-world time series\nfollow a power-law distribution, notably due to an inherent self-organizing\nbehavior. Leveraging this insight, we build a robust CD method based on the\nextraction of power -law spectral features that amplify genuine causal signals.\nOur method consistently outperforms state-of-the-art alternatives on both\nsynthetic benchmarks and real-world datasets with known causal structures,\ndemonstrating its robustness and practical relevance.",
      "pdf_url": "http://arxiv.org/pdf/2507.12257v1",
      "arxiv_url": "http://arxiv.org/abs/2507.12257v1",
      "published": "2025-07-16",
      "categories": [
        "cs.LG",
        "physics.data-an",
        "stat.ML",
        "stat.OT"
      ]
    },
    {
      "title": "PRISM: Distributed Inference for Foundation Models at Edge",
      "authors": [
        "Muhammad Azlan Qazi",
        "Alexandros Iosifidis",
        "Qi Zhang"
      ],
      "abstract": "Foundation models (FMs) have achieved remarkable success across a wide range\nof applications, from image classification to natural langurage processing, but\npose significant challenges for deployment at edge. This has sparked growing\ninterest in developing practical and efficient strategies for bringing\nfoundation models to edge environments. In this work, we propose PRISM, a\ncommunication-efficient and compute-aware strategy for distributed Transformer\ninference on edge devices. Our method leverages a Segment Means representation\nto approximate intermediate output features, drastically reducing inter-device\ncommunication. Additionally, we restructure the self-attention mechanism to\neliminate redundant computations caused by per-device Key/Value calculation in\nposition-wise partitioning and design a partition-aware causal masking scheme\ntailored for autoregressive models. We evaluate PRISM on ViT, BERT, and GPT-2\nacross diverse datasets, namely CIFAR-10, CIFAR-100, ImageNet-1k, GLUE, and\nCBT. Our results demonstrate substantial reductions in communication overhead\n(up to 99.2% for BERT at compression rate CR = 128) and per-device computation\n(51.24% for BERT at the same setting), with only minor accuracy degradation.\nThis method offers a scalable and practical solution for deploying foundation\nmodels in distributed resource-constrained environments.",
      "pdf_url": "http://arxiv.org/pdf/2507.12145v1",
      "arxiv_url": "http://arxiv.org/abs/2507.12145v1",
      "published": "2025-07-16",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ]
    },
    {
      "title": "Inference on Optimal Policy Values and Other Irregular Functionals via Smoothing",
      "authors": [
        "Justin Whitehouse",
        "Morgane Austern",
        "Vasilis Syrgkanis"
      ],
      "abstract": "Constructing confidence intervals for the value of an optimal treatment\npolicy is an important problem in causal inference. Insight into the optimal\npolicy value can guide the development of reward-maximizing, individualized\ntreatment regimes. However, because the functional that defines the optimal\nvalue is non-differentiable, standard semi-parametric approaches for performing\ninference fail to be directly applicable. Existing approaches for handling this\nnon-differentiability fall roughly into two camps. In one camp are estimators\nbased on constructing smooth approximations of the optimal value. These\napproaches are computationally lightweight, but typically place unrealistic\nparametric assumptions on outcome regressions. In another camp are approaches\nthat directly de-bias the non-smooth objective. These approaches don't place\nparametric assumptions on nuisance functions, but they either require the\ncomputation of intractably-many nuisance estimates, assume unrealistic\n$L^\\infty$ nuisance convergence rates, or make strong margin assumptions that\nprohibit non-response to a treatment. In this paper, we revisit the problem of\nconstructing smooth approximations of non-differentiable functionals. By\ncarefully controlling first-order bias and second-order remainders, we show\nthat a softmax smoothing-based estimator can be used to estimate parameters\nthat are specified as a maximum of scores involving nuisance components. In\nparticular, this includes the value of the optimal treatment policy as a\nspecial case. Our estimator obtains $\\sqrt{n}$ convergence rates, avoids\nparametric restrictions/unrealistic margin assumptions, and is often\nstatistically efficient.",
      "pdf_url": "http://arxiv.org/pdf/2507.11780v1",
      "arxiv_url": "http://arxiv.org/abs/2507.11780v1",
      "published": "2025-07-15",
      "categories": [
        "econ.EM",
        "cs.LG",
        "math.ST",
        "stat.ME",
        "stat.TH"
      ]
    },
    {
      "title": "Constructing targeted minimum loss/maximum likelihood estimators: a simple illustration to build intuition",
      "authors": [
        "Rachael K. Ross",
        "Lina M. Montoya",
        "Dana E. Goin",
        "Ivan Diaz",
        "Audrey Renson"
      ],
      "abstract": "Use of machine learning to estimate nuisance functions (e.g. outcomes models,\npropensity score models) in estimators used in causal inference is increasingly\ncommon, as it can mitigate bias due to model misspecification. However, it can\nbe challenging to achieve valid inference (e.g., estimate valid confidence\nintervals). The efficient influence function (EIF) provides a recipe to go from\na statistical estimand relevant to our causal question, to an estimator that\ncan validly incorporate machine learning. Our companion paper, Renson et al.\n2025 (arXiv:2502.05363), provides a thorough but approachable description of\nthe EIF, along with a guide through the steps to go from a unique statistical\nestimand to development of one type of EIF-based estimator, the so-called\none-step estimator. Another commonly used estimator based on the EIF is the\ntargeted maximum likelihood/minimum loss estimator (TMLE). Construction of\nTMLEs is well-discussed in the statistical literature, but there remains a gap\nin translation to a more applied audience. In this letter, which supplements\nRenson et al., we provide a more accessible illustration of how to construct a\nTMLE.",
      "pdf_url": "http://arxiv.org/pdf/2507.11680v1",
      "arxiv_url": "http://arxiv.org/abs/2507.11680v1",
      "published": "2025-07-15",
      "categories": [
        "stat.ME"
      ]
    },
    {
      "title": "Streaming 4D Visual Geometry Transformer",
      "authors": [
        "Dong Zhuo",
        "Wenzhao Zheng",
        "Jiahe Guo",
        "Yuqi Wu",
        "Jie Zhou",
        "Jiwen Lu"
      ],
      "abstract": "Perceiving and reconstructing 4D spatial-temporal geometry from videos is a\nfundamental yet challenging computer vision task. To facilitate interactive and\nreal-time applications, we propose a streaming 4D visual geometry transformer\nthat shares a similar philosophy with autoregressive large language models. We\nexplore a simple and efficient design and employ a causal transformer\narchitecture to process the input sequence in an online manner. We use temporal\ncausal attention and cache the historical keys and values as implicit memory to\nenable efficient streaming long-term 4D reconstruction. This design can handle\nreal-time 4D reconstruction by incrementally integrating historical information\nwhile maintaining high-quality spatial consistency. For efficient training, we\npropose to distill knowledge from the dense bidirectional visual geometry\ngrounded transformer (VGGT) to our causal model. For inference, our model\nsupports the migration of optimized efficient attention operator (e.g.,\nFlashAttention) from the field of large language models. Extensive experiments\non various 4D geometry perception benchmarks demonstrate that our model\nincreases the inference speed in online scenarios while maintaining competitive\nperformance, paving the way for scalable and interactive 4D vision systems.\nCode is available at: https://github.com/wzzheng/StreamVGGT.",
      "pdf_url": "http://arxiv.org/pdf/2507.11539v1",
      "arxiv_url": "http://arxiv.org/abs/2507.11539v1",
      "published": "2025-07-15",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Gradient Regularization-based Neural Granger Causality",
      "authors": [
        "Meiliang Liu",
        "Huiwen Dong",
        "Xiaoxiao Yang",
        "Yunfang Xu",
        "Zijin Li",
        "Zhengye Si",
        "Xinyue Yang",
        "Zhiwen Zhao"
      ],
      "abstract": "With the advancement of deep learning technologies, various neural\nnetwork-based Granger causality models have been proposed. Although these\nmodels have demonstrated notable improvements, several limitations remain. Most\nexisting approaches adopt the component-wise architecture, necessitating the\nconstruction of a separate model for each time series, which results in\nsubstantial computational costs. In addition, imposing the sparsity-inducing\npenalty on the first-layer weights of the neural network to extract causal\nrelationships weakens the model's ability to capture complex interactions. To\naddress these limitations, we propose Gradient Regularization-based Neural\nGranger Causality (GRNGC), which requires only one time series prediction model\nand applies $L_{1}$ regularization to the gradient between model's input and\noutput to infer Granger causality. Moreover, GRNGC is not tied to a specific\ntime series forecasting model and can be implemented with diverse architectures\nsuch as KAN, MLP, and LSTM, offering enhanced flexibility. Numerical\nsimulations on DREAM, Lorenz-96, fMRI BOLD, and CausalTime show that GRNGC\noutperforms existing baselines and significantly reduces computational\noverhead. Meanwhile, experiments on real-world DNA, Yeast, HeLa, and bladder\nurothelial carcinoma datasets further validate the model's effectiveness in\nreconstructing gene regulatory networks.",
      "pdf_url": "http://arxiv.org/pdf/2507.11178v1",
      "arxiv_url": "http://arxiv.org/abs/2507.11178v1",
      "published": "2025-07-15",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Uncovering Causal Relation Shifts in Event Sequences under Out-of-Domain Interventions",
      "authors": [
        "Kazi Tasnim Zinat",
        "Yun Zhou",
        "Xiang Lyu",
        "Yawei Wang",
        "Zhicheng Liu",
        "Panpan Xu"
      ],
      "abstract": "Inferring causal relationships between event pairs in a temporal sequence is\napplicable in many domains such as healthcare, manufacturing, and\ntransportation. Most existing work on causal inference primarily focuses on\nevent types within the designated domain, without considering the impact of\nexogenous out-of-domain interventions. In real-world settings, these\nout-of-domain interventions can significantly alter causal dynamics. To address\nthis gap, we propose a new causal framework to define average treatment effect\n(ATE), beyond independent and identically distributed (i.i.d.) data in classic\nRubin's causal framework, to capture the causal relation shift between events\nof temporal process under out-of-domain intervention. We design an unbiased ATE\nestimator, and devise a Transformer-based neural network model to handle both\nlong-range temporal dependencies and local patterns while integrating\nout-of-domain intervention information into process modeling. Extensive\nexperiments on both simulated and real-world datasets demonstrate that our\nmethod outperforms baselines in ATE estimation and goodness-of-fit under\nout-of-domain-augmented point processes.",
      "pdf_url": "http://arxiv.org/pdf/2507.10809v1",
      "arxiv_url": "http://arxiv.org/abs/2507.10809v1",
      "published": "2025-07-14",
      "categories": [
        "cs.LG",
        "stat.ML"
      ]
    }
  ]
}