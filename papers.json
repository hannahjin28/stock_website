{
  "last_updated": "2025-08-29T00:51:05.751205",
  "papers": [
    {
      "title": "Ego-centric Predictive Model Conditioned on Hand Trajectories",
      "authors": [
        "Binjie Zhang",
        "Mike Zheng Shou"
      ],
      "abstract": "In egocentric scenarios, anticipating both the next action and its visual\noutcome is essential for understanding human-object interactions and for\nenabling robotic planning. However, existing paradigms fall short of jointly\nmodeling these aspects. Vision-Language-Action (VLA) models focus on action\nprediction but lack explicit modeling of how actions influence the visual\nscene, while video prediction models generate future frames without\nconditioning on specific actions, often resulting in implausible or\ncontextually inconsistent outcomes. To bridge this gap, we propose a unified\ntwo-stage predictive framework that jointly models action and visual future in\negocentric scenarios, conditioned on hand trajectories. In the first stage, we\nperform consecutive state modeling to process heterogeneous inputs (visual\nobservations, language, and action history) and explicitly predict future hand\ntrajectories. In the second stage, we introduce causal cross-attention to fuse\nmulti-modal cues, leveraging inferred action signals to guide an image-based\nLatent Diffusion Model (LDM) for frame-by-frame future video generation. Our\napproach is the first unified model designed to handle both egocentric human\nactivity understanding and robotic manipulation tasks, providing explicit\npredictions of both upcoming actions and their visual consequences. Extensive\nexperiments on Ego4D, BridgeData, and RLBench demonstrate that our method\noutperforms state-of-the-art baselines in both action prediction and future\nvideo synthesis.",
      "pdf_url": "http://arxiv.org/pdf/2508.19852v2",
      "arxiv_url": "http://arxiv.org/abs/2508.19852v2",
      "published": "2025-08-27",
      "categories": [
        "cs.CV"
      ]
    },
    {
      "title": "Understanding Spatial Regression Models from a Weighting Perspective in an Observational Study of Superfund Remediation",
      "authors": [
        "Sophie M. Woodward",
        "Francesca Dominici",
        "Jose R. Zubizarreta"
      ],
      "abstract": "Superfund sites are locations in the United States with high levels of\nenvironmental toxicants, often resulting from industrial activity or improper\nwaste management. Given mounting evidence linking prenatal environmental\nexposures to adverse birth outcomes, estimating the impact of Superfund\nremediation is of substantial policy relevance. A widespread approach is to fit\na spatial regression, i.e., a linear regression of the outcome (e.g., birth\nweight) on binary treatment (e.g., indicator for Superfund site remediation)\nand covariates, along with a spatially structured error term to account for\nunmeasured spatial confounding. Despite this common practice, it remains\nunclear to what extent spatial regression models account for unmeasured spatial\nconfounding in finite samples and whether such adjustments can be reformulated\nwithin a design-based framework for causal inference. To fill this knowledge\ngap, we introduce a weighting framework that encompasses three canonical types\nof spatial regression models: random effects, conditional autoregressive, and\nGaussian process models. This framework yields new insights into how spatial\nregression models build causal contrasts between treated and control units.\nSpecifically, we show that: 1) the spatially autocorrelated error term produces\napproximate balance on a hidden set of covariates, thereby adjusting for a\nspecific class of unmeasured confounders; and 2) the error covariance structure\ncan be equivalently expressed as regressors in a linear model. We also\nintroduce a new average treatment effect estimator that simultaneously accounts\nfor multiple forms of unmeasured spatial confounding, as well as diagnostics\nthat enhance interpretability. In a study of Superfund remediation, our\napproach illuminates the role of design-based adjustment for confounding and\nprovides guidance for evaluating environmental interventions in spatial\nsettings.",
      "pdf_url": "http://arxiv.org/pdf/2508.19572v1",
      "arxiv_url": "http://arxiv.org/abs/2508.19572v1",
      "published": "2025-08-27",
      "categories": [
        "stat.ME",
        "stat.AP"
      ]
    },
    {
      "title": "Counterfactual Reward Model Training for Bias Mitigation in Multimodal Reinforcement Learning",
      "authors": [
        "Sheryl Mathew",
        "N Harshit"
      ],
      "abstract": "In reinforcement learning with human feedback (RLHF), reward models can\nefficiently learn and amplify latent biases within multimodal datasets, which\ncan lead to imperfect policy optimization through flawed reward signals and\ndecreased fairness. Bias mitigation studies have often applied passive\nconstraints, which can fail under causal confounding. Here, we present a\ncounterfactual reward model that introduces causal inference with multimodal\nrepresentation learning to provide an unsupervised, bias-resilient reward\nsignal. The heart of our contribution is the Counterfactual Trust Score, an\naggregated score consisting of four components: (1) counterfactual shifts that\ndecompose political framing bias from topical bias; (2) reconstruction\nuncertainty during counterfactual perturbations; (3) demonstrable violations of\nfairness rules for each protected attribute; and (4) temporal reward shifts\naligned with dynamic trust measures. We evaluated the framework on a multimodal\nfake versus true news dataset, which exhibits framing bias, class imbalance,\nand distributional drift. Following methodologies similar to unsupervised drift\ndetection from representation-based distances [1] and temporal robustness\nbenchmarking in language models [2], we also inject synthetic bias across\nsequential batches to test robustness. The resulting system achieved an\naccuracy of 89.12% in fake news detection, outperforming the baseline reward\nmodels. More importantly, it reduced spurious correlations and unfair\nreinforcement signals. This pipeline outlines a robust and interpretable\napproach to fairness-aware RLHF, offering tunable bias reduction thresholds and\nincreasing reliability in dynamic real-time policy making.",
      "pdf_url": "http://arxiv.org/pdf/2508.19567v1",
      "arxiv_url": "http://arxiv.org/abs/2508.19567v1",
      "published": "2025-08-27",
      "categories": [
        "cs.LG"
      ]
    },
    {
      "title": "When Routers, Switches and Interconnects Compute: A processing-in-interconnect Paradigm for Scalable Neuromorphic AI",
      "authors": [
        "Madhuvanthi Srivatsav R",
        "Chiranjib Bhattacharyya",
        "Shantanu Chakrabartty",
        "Chetan Singh Thakur"
      ],
      "abstract": "Routing, switching, and the interconnect fabric are essential for large-scale\nneuromorphic computing. While this fabric only plays a supporting role in the\nprocess of computing, for large AI workloads it ultimately determines energy\nconsumption and speed. In this paper, we address this bottleneck by asking: (a)\nWhat computing paradigms are inherent in existing routing, switching, and\ninterconnect systems, and how can they be used to implement a\nprocessing-in-Interconnect (\\pi^2) computing paradigm? and (b) leveraging\ncurrent and future interconnect trends, how will a \\pi^2 system's performance\nscale compared to other neuromorphic architectures? For (a), we show that\noperations required for typical AI workloads can be mapped onto delays,\ncausality, time-outs, packet drop, and broadcast operations -- primitives\nalready implemented in packet-switching and packet-routing hardware. We show\nthat existing buffering and traffic-shaping embedded algorithms can be\nleveraged to implement neuron models and synaptic operations. Additionally, a\nknowledge-distillation framework can train and cross-map well-established\nneural network topologies onto $\\pi^2$ without degrading generalization\nperformance. For (b), analytical modeling shows that, unlike other neuromorphic\nplatforms, the energy scaling of $\\pi^2$ improves with interconnect bandwidth\nand energy efficiency. We predict that by leveraging trends in interconnect\ntechnology, a \\pi^2 architecture can be more easily scaled to execute\nbrain-scale AI inference workloads with power consumption levels in the range\nof hundreds of watts.",
      "pdf_url": "http://arxiv.org/pdf/2508.19548v1",
      "arxiv_url": "http://arxiv.org/abs/2508.19548v1",
      "published": "2025-08-27",
      "categories": [
        "cs.NE",
        "cs.AR",
        "cs.NI"
      ]
    },
    {
      "title": "Quantum Entanglement as Super-Confounding: From Bell's Theorem to Robust Machine Learning",
      "authors": [
        "Pilsung Kang"
      ],
      "abstract": "Bell's theorem reveals a profound conflict between quantum mechanics and\nlocal realism, a conflict we reinterpret through the modern lens of causal\ninference. We propose and computationally validate a framework where quantum\nentanglement acts as a \"super-confounding\" resource, generating correlations\nthat violate the classical causal bounds set by Bell's inequalities. This work\nmakes three key contributions: First, we establish a physical hierarchy of\nconfounding (Quantum > Classical) and introduce Confounding Strength (CS) to\nquantify this effect. Second, we provide a circuit-based implementation of the\nquantum $\\mathcal{DO}$-calculus to distinguish causality from spurious\ncorrelation. Finally, we apply this calculus to a quantum machine learning\nproblem, where causal feature selection yields a statistically significant\n11.3% average absolute improvement in model robustness. Our framework bridges\nquantum foundations and causal AI, offering a new, practical perspective on\nquantum correlations.",
      "pdf_url": "http://arxiv.org/pdf/2508.19327v1",
      "arxiv_url": "http://arxiv.org/abs/2508.19327v1",
      "published": "2025-08-26",
      "categories": [
        "quant-ph",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "STARec: An Efficient Agent Framework for Recommender Systems via Autonomous Deliberate Reasoning",
      "authors": [
        "Chenghao Wu",
        "Ruiyang Ren",
        "Junjie Zhang",
        "Ruirui Wang",
        "Zhongrui Ma",
        "Qi Ye",
        "Wayne Xin Zhao"
      ],
      "abstract": "While modern recommender systems are instrumental in navigating information\nabundance, they remain fundamentally limited by static user modeling and\nreactive decision-making paradigms. Current large language model (LLM)-based\nagents inherit these shortcomings through their overreliance on heuristic\npattern matching, yielding recommendations prone to shallow correlation bias,\nlimited causal inference, and brittleness in sparse-data scenarios. We\nintroduce STARec, a slow-thinking augmented agent framework that endows\nrecommender systems with autonomous deliberative reasoning capabilities. Each\nuser is modeled as an agent with parallel cognitions: fast response for\nimmediate interactions and slow reasoning that performs chain-of-thought\nrationales. To cultivate intrinsic slow thinking, we develop anchored\nreinforcement training - a two-stage paradigm combining structured knowledge\ndistillation from advanced reasoning models with preference-aligned reward\nshaping. This hybrid approach scaffolds agents in acquiring foundational\ncapabilities (preference summarization, rationale generation) while enabling\ndynamic policy adaptation through simulated feedback loops. Experiments on\nMovieLens 1M and Amazon CDs benchmarks demonstrate that STARec achieves\nsubstantial performance gains compared with state-of-the-art baselines, despite\nusing only 0.4% of the full training data.",
      "pdf_url": "http://arxiv.org/pdf/2508.18812v1",
      "arxiv_url": "http://arxiv.org/abs/2508.18812v1",
      "published": "2025-08-26",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Mapping beyond diseases: Controlled variable selection for secondary phenotypes using tilted knockoffs",
      "authors": [
        "Qian Zhao",
        "Susan Service",
        "Carrie E. Bearden",
        "Carlos Lopez-Jaramillo",
        "Freimer Nelson",
        "Chiara Sabatti"
      ],
      "abstract": "Researchers in biomedical studies often work with samples that are not\nselected uniformly at random from the population of interest, a major example\nbeing a case-control study. While these designs are motivated by specific\nscientific questions, it is often of interest to use the data collected to\npursue secondary lines of investigations. In these cases, ignoring the fact\nthat observations are not sampled uniformly at random can lead to spurious\nresults. For example, in a case-control study, one might identify a spurious\nassociation between an exposure and a secondary phenotype when both affect the\ncase-control status. This phenomenon is known as collider bias in the causal\ninference literature. While tests of independence under biased sampling are\navailable, these methods typically do not apply when the number of variables is\nlarge.\n  Here, we are interested in using the biased sample to select important\nexposures among a multitude of possible variables with replicability\nguarantees. While the model-X knockoff framework has been developed to test\nconditional independence hypotheses with False Discovery Rate (FDR) control, we\nshow that its naive application fails to control FDR in the presence of biased\nsampling. We show how tilting the population distribution with the selection\nprobability and constructing knockoff variables according to this tilted\ndistribution instead leads to selection with FDR control. We study the FDR and\npower of the tilted knockoff method using simulated examples, and apply it to\nidentify genetic underpinning of endophenotypes in a case-control study.",
      "pdf_url": "http://arxiv.org/pdf/2508.18548v1",
      "arxiv_url": "http://arxiv.org/abs/2508.18548v1",
      "published": "2025-08-25",
      "categories": [
        "stat.ME",
        "stat.AP"
      ]
    },
    {
      "title": "Estimating the average treatment effect in cluster-randomized trials with misclassified outcomes and non-random validation subsets",
      "authors": [
        "Dane Isenberg",
        "Nandita Mitra",
        "Steven C. Marcus",
        "Rinad S. Beidas",
        "Kristin A. Linn"
      ],
      "abstract": "Randomized trials are viewed as the benchmark for assessing causal effects of\ntreatments on outcomes of interest. Nonetheless, challenges such as measurement\nerror can undermine the standard causal assumptions for randomized trials. In\nASPIRE, a cluster-randomized trial, pediatric primary care clinics were\nassigned to one of two treatments aimed at promoting clinician delivery of a\nsecure firearm program to parents during well-child visits. A key outcome of\ninterest is thus parent receipt of the program at each visit. Clinicians\ndocumented program delivery in patients' electronic health records for all\nvisits, but their reporting is a proxy measure for the parent receipt outcome.\nParents were also surveyed to report directly on program receipt after their\nchild's visit; however, only a small subset of them completed the survey. Here,\nwe develop a causal inference framework for a binary outcome that is subject to\nmisclassification through silver-standard measures (clinician reports), but\ngold-standard measures (parent reports) are only available for a non-random\ninternal validation subset. We propose a method for identifying the average\ntreatment effect (ATE) that addresses the risk of bias due to misclassification\nand non-random validation selection, even when the outcome (parent receipt) may\ndirectly impact selection propensity (survey responsiveness). We show that ATE\nestimation relies on specifying the relationship between the gold- and\nsilver-standard outcome measures in the validation subset, which may depend on\ntreatment and covariates. Additionally, the clustered design is reflected in\nour causal assumptions and in our cluster-robust approach to estimation of the\nATE. Simulation studies demonstrate acceptable finite-sample operating\ncharacteristics of our ATE estimator, supporting its application to ASPIRE.",
      "pdf_url": "http://arxiv.org/pdf/2508.18137v2",
      "arxiv_url": "http://arxiv.org/abs/2508.18137v2",
      "published": "2025-08-25",
      "categories": [
        "stat.ME"
      ]
    },
    {
      "title": "CausalSent: Interpretable Sentiment Classification with RieszNet",
      "authors": [
        "Daniel Frees",
        "Martin Pollack"
      ],
      "abstract": "Despite the overwhelming performance improvements offered by recent natural\nlanguage processing (NLP) models, the decisions made by these models are\nlargely a black box. Towards closing this gap, the field of causal NLP combines\ncausal inference literature with modern NLP models to elucidate causal effects\nof text features. We replicate and extend Bansal et al's work on regularizing\ntext classifiers to adhere to estimated effects, focusing instead on model\ninterpretability. Specifically, we focus on developing a two-headed\nRieszNet-based neural network architecture which achieves better treatment\neffect estimation accuracy. Our framework, CausalSent, accurately predicts\ntreatment effects in semi-synthetic IMDB movie reviews, reducing MAE of effect\nestimates by 2-3x compared to Bansal et al's MAE on synthetic Civil Comments\ndata. With an ensemble of validated models, we perform an observational case\nstudy on the causal effect of the word \"love\" in IMDB movie reviews, finding\nthat the presence of the word \"love\" causes a +2.9% increase in the probability\nof a positive sentiment.",
      "pdf_url": "http://arxiv.org/pdf/2508.17576v2",
      "arxiv_url": "http://arxiv.org/abs/2508.17576v2",
      "published": "2025-08-25",
      "categories": [
        "cs.CL",
        "cs.LG",
        "68T50"
      ]
    },
    {
      "title": "Visual Analytics for Causal Reasoning from Real-World Health Data",
      "authors": [
        "Arran Zeyu Wang",
        "David Borland",
        "David Gotz"
      ],
      "abstract": "The increasing capture and analysis of large-scale longitudinal health data\noffer opportunities to improve healthcare and advance medical understanding.\nHowever, a critical gap exists between (a) -- the observation of patterns and\ncorrelations, versus (b) -- the understanding of true causal mechanisms that\ndrive outcomes. An accurate understanding of the underlying mechanisms that\ncause various changes in medical status is crucial for decision-makers across\nvarious healthcare domains and roles, yet inferring causality from real-world\nobservational data is difficult for both methodological and practical\nchallenges. This Grand Challenge advocates increased Visual Analytics (VA)\nresearch on this topic to empower people with the tool for sound causal\nreasoning from health data. We note this is complicated by the complex nature\nof medical data -- the volume, variety, sparsity, and temporality of health\ndata streams make the use of causal inference algorithms difficult. Combined\nwith challenges imposed by the realities of health-focused settings, including\ntime constraints and traditional medical work practices, existing causal\nreasoning approaches are valuable but insufficient. We argue that advances in\nresearch can lead to new VA tools that augment human expertise with intuitive\nand robust causal inference capabilities, which can help realize a new paradigm\nof data-driven, causality-aware healthcare practices that improve human health\noutcomes.",
      "pdf_url": "http://arxiv.org/pdf/2508.17474v1",
      "arxiv_url": "http://arxiv.org/abs/2508.17474v1",
      "published": "2025-08-24",
      "categories": [
        "cs.HC"
      ]
    }
  ]
}