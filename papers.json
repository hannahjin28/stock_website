{
  "last_updated": "2025-07-15T00:59:07.091147",
  "papers": [
    {
      "title": "Lumos-1: On Autoregressive Video Generation from a Unified Model Perspective",
      "authors": [
        "Hangjie Yuan",
        "Weihua Chen",
        "Jun Cen",
        "Hu Yu",
        "Jingyun Liang",
        "Shuning Chang",
        "Zhihui Lin",
        "Tao Feng",
        "Pengwei Liu",
        "Jiazheng Xing",
        "Hao Luo",
        "Jiasheng Tang",
        "Fan Wang",
        "Yi Yang"
      ],
      "abstract": "Autoregressive large language models (LLMs) have unified a vast range of\nlanguage tasks, inspiring preliminary efforts in autoregressive video\ngeneration. Existing autoregressive video generators either diverge from\nstandard LLM architectures, depend on bulky external text encoders, or incur\nprohibitive latency due to next-token decoding. In this paper, we introduce\nLumos-1, an autoregressive video generator that retains the LLM architecture\nwith minimal architectural modifications. To inject spatiotemporal correlations\nin LLMs, we identify the efficacy of incorporating 3D RoPE and diagnose its\nimbalanced frequency spectrum ranges. Therefore, we propose MM-RoPE, a RoPE\nscheme that preserves the original textual RoPE while providing comprehensive\nfrequency spectra and scaled 3D positions for modeling multimodal\nspatiotemporal data. Moreover, Lumos-1 resorts to a token dependency strategy\nthat obeys intra-frame bidirectionality and inter-frame temporal causality.\nBased on this dependency strategy, we identify the issue of frame-wise loss\nimbalance caused by spatial information redundancy and solve it by proposing\nAutoregressive Discrete Diffusion Forcing (AR-DF). AR-DF introduces temporal\ntube masking during training with a compatible inference-time masking policy to\navoid quality degradation. By using memory-efficient training techniques, we\npre-train Lumos-1 on only 48 GPUs, achieving performance comparable to EMU3 on\nGenEval, COSMOS-Video2World on VBench-I2V, and OpenSoraPlan on VBench-T2V. Code\nand models are available at https://github.com/alibaba-damo-academy/Lumos.",
      "pdf_url": "http://arxiv.org/pdf/2507.08801v1",
      "arxiv_url": "http://arxiv.org/abs/2507.08801v1",
      "published": "2025-07-11",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.MM"
      ]
    },
    {
      "title": "Propensity score with factor loadings: the effect of the Paris Agreement",
      "authors": [
        "Angelo Forino",
        "Andrea Mercatanti",
        "Giacomo Morelli"
      ],
      "abstract": "Factor models for longitudinal data, where policy adoption is unconfounded\nwith respect to a low-dimensional set of latent factor loadings, have become\nincreasingly popular for causal inference. Most existing approaches, however,\nrely on a causal finite-sample approach or computationally intensive methods,\nlimiting their applicability and external validity. In this paper, we propose a\nnovel causal inference method for panel data based on inverse propensity score\nweighting where the propensity score is a function of latent factor loadings\nwithin a framework of causal inference from super-population. The approach\nrelaxes the traditional restrictive assumptions of causal panel methods, while\noffering advantages in terms of causal interpretability, policy relevance, and\ncomputational efficiency. Under standard assumptions, we outline a three-step\nestimation procedure for the ATT and derive its large-sample properties using\nMestimation theory. We apply the method to assess the causal effect of the\nParis Agreement, a policy aimed at fostering the transition to a low-carbon\neconomy, on European stock returns. Our empirical results suggest a\nstatistically significant and negative short-run effect on the stock returns of\nfirms that issued green bonds.",
      "pdf_url": "http://arxiv.org/pdf/2507.08764v1",
      "arxiv_url": "http://arxiv.org/abs/2507.08764v1",
      "published": "2025-07-11",
      "categories": [
        "econ.EM",
        "stat.AP"
      ]
    },
    {
      "title": "InsightBuild: LLM-Powered Causal Reasoning in Smart Building Systems",
      "authors": [
        "Pinaki Prasad Guha Neogi",
        "Ahmad Mohammadshirazi",
        "Rajiv Ramnath"
      ],
      "abstract": "Smart buildings generate vast streams of sensor and control data, but\nfacility managers often lack clear explanations for anomalous energy usage. We\npropose InsightBuild, a two-stage framework that integrates causality analysis\nwith a fine-tuned large language model (LLM) to provide human-readable, causal\nexplanations of energy consumption patterns. First, a lightweight causal\ninference module applies Granger causality tests and structural causal\ndiscovery on building telemetry (e.g., temperature, HVAC settings, occupancy)\ndrawn from Google Smart Buildings and Berkeley Office datasets. Next, an LLM,\nfine-tuned on aligned pairs of sensor-level causes and textual explanations,\nreceives as input the detected causal relations and generates concise,\nactionable explanations. We evaluate InsightBuild on two real-world datasets\n(Google: 2017-2022; Berkeley: 2018-2020), using expert-annotated ground-truth\ncauses for a held-out set of anomalies. Our results demonstrate that combining\nexplicit causal discovery with LLM-based natural language generation yields\nclear, precise explanations that assist facility managers in diagnosing and\nmitigating energy inefficiencies.",
      "pdf_url": "http://arxiv.org/pdf/2507.08235v1",
      "arxiv_url": "http://arxiv.org/abs/2507.08235v1",
      "published": "2025-07-11",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Efficient Causal Discovery for Autoregressive Time Series",
      "authors": [
        "Mohammad Fesanghary",
        "Achintya Gopal"
      ],
      "abstract": "In this study, we present a novel constraint-based algorithm for causal\nstructure learning specifically designed for nonlinear autoregressive time\nseries. Our algorithm significantly reduces computational complexity compared\nto existing methods, making it more efficient and scalable to larger problems.\nWe rigorously evaluate its performance on synthetic datasets, demonstrating\nthat our algorithm not only outperforms current techniques, but also excels in\nscenarios with limited data availability. These results highlight its potential\nfor practical applications in fields requiring efficient and accurate causal\ninference from nonlinear time series data.",
      "pdf_url": "http://arxiv.org/pdf/2507.07898v1",
      "arxiv_url": "http://arxiv.org/abs/2507.07898v1",
      "published": "2025-07-10",
      "categories": [
        "cs.LG",
        "stat.AP"
      ]
    },
    {
      "title": "Efficient and Scalable Estimation of Distributional Treatment Effects with Multi-Task Neural Networks",
      "authors": [
        "Tomu Hirata",
        "Undral Byambadalai",
        "Tatsushi Oka",
        "Shota Yasui",
        "Shingo Uto"
      ],
      "abstract": "We propose a novel multi-task neural network approach for estimating\ndistributional treatment effects (DTE) in randomized experiments. While DTE\nprovides more granular insights into the experiment outcomes over conventional\nmethods focusing on the Average Treatment Effect (ATE), estimating it with\nregression adjustment methods presents significant challenges. Specifically,\nprecision in the distribution tails suffers due to data imbalance, and\ncomputational inefficiencies arise from the need to solve numerous regression\nproblems, particularly in large-scale datasets commonly encountered in\nindustry. To address these limitations, our method leverages multi-task neural\nnetworks to estimate conditional outcome distributions while incorporating\nmonotonic shape constraints and multi-threshold label learning to enhance\naccuracy. To demonstrate the practical effectiveness of our proposed method, we\napply our method to both simulated and real-world datasets, including a\nrandomized field experiment aimed at reducing water consumption in the US and a\nlarge-scale A/B test from a leading streaming platform in Japan. The\nexperimental results consistently demonstrate superior performance across\nvarious datasets, establishing our method as a robust and practical solution\nfor modern causal inference applications requiring a detailed understanding of\ntreatment effect heterogeneity.",
      "pdf_url": "http://arxiv.org/pdf/2507.07738v1",
      "arxiv_url": "http://arxiv.org/abs/2507.07738v1",
      "published": "2025-07-10",
      "categories": [
        "cs.LG",
        "econ.EM"
      ]
    },
    {
      "title": "Sparse Causal Discovery with Generative Intervention for Unsupervised Graph Domain Adaptation",
      "authors": [
        "Junyu Luo",
        "Yuhao Tang",
        "Yiwei Fu",
        "Xiao Luo",
        "Zhizhuo Kou",
        "Zhiping Xiao",
        "Wei Ju",
        "Wentao Zhang",
        "Ming Zhang"
      ],
      "abstract": "Unsupervised Graph Domain Adaptation (UGDA) leverages labeled source domain\ngraphs to achieve effective performance in unlabeled target domains despite\ndistribution shifts. However, existing methods often yield suboptimal results\ndue to the entanglement of causal-spurious features and the failure of global\nalignment strategies. We propose SLOGAN (Sparse Causal Discovery with\nGenerative Intervention), a novel approach that achieves stable graph\nrepresentation transfer through sparse causal modeling and dynamic intervention\nmechanisms. Specifically, SLOGAN first constructs a sparse causal graph\nstructure, leveraging mutual information bottleneck constraints to disentangle\nsparse, stable causal features while compressing domain-dependent spurious\ncorrelations through variational inference. To address residual spurious\ncorrelations, we innovatively design a generative intervention mechanism that\nbreaks local spurious couplings through cross-domain feature recombination\nwhile maintaining causal feature semantic consistency via covariance\nconstraints. Furthermore, to mitigate error accumulation in target domain\npseudo-labels, we introduce a category-adaptive dynamic calibration strategy,\nensuring stable discriminative learning. Extensive experiments on multiple\nreal-world datasets demonstrate that SLOGAN significantly outperforms existing\nbaselines.",
      "pdf_url": "http://arxiv.org/pdf/2507.07621v1",
      "arxiv_url": "http://arxiv.org/abs/2507.07621v1",
      "published": "2025-07-10",
      "categories": [
        "cs.LG"
      ]
    },
    {
      "title": "Goal-Oriented Sequential Bayesian Experimental Design for Causal Learning",
      "authors": [
        "Zheyu Zhang",
        "Jiayuan Dong",
        "Jie Liu",
        "Xun Huan"
      ],
      "abstract": "We present GO-CBED, a goal-oriented Bayesian framework for sequential causal\nexperimental design. Unlike conventional approaches that select interventions\naimed at inferring the full causal model, GO-CBED directly maximizes the\nexpected information gain (EIG) on user-specified causal quantities of\ninterest, enabling more targeted and efficient experimentation. The framework\nis both non-myopic, optimizing over entire intervention sequences, and\ngoal-oriented, targeting only model aspects relevant to the causal query. To\naddress the intractability of exact EIG computation, we introduce a variational\nlower bound estimator, optimized jointly through a transformer-based policy\nnetwork and normalizing flow-based variational posteriors. The resulting policy\nenables real-time decision-making via an amortized network. We demonstrate that\nGO-CBED consistently outperforms existing baselines across various causal\nreasoning and discovery tasks-including synthetic structural causal models and\nsemi-synthetic gene regulatory networks-particularly in settings with limited\nexperimental budgets and complex causal mechanisms. Our results highlight the\nbenefits of aligning experimental design objectives with specific research\ngoals and of forward-looking sequential planning.",
      "pdf_url": "http://arxiv.org/pdf/2507.07359v1",
      "arxiv_url": "http://arxiv.org/abs/2507.07359v1",
      "published": "2025-07-10",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ME",
        "stat.ML"
      ]
    },
    {
      "title": "Beyond the ATE: Interpretable Modelling of Treatment Effects over Dose and Time",
      "authors": [
        "Julianna Piskorz",
        "Krzysztof Kacprzyk",
        "Mihaela van der Schaar"
      ],
      "abstract": "The Average Treatment Effect (ATE) is a foundational metric in causal\ninference, widely used to assess intervention efficacy in randomized controlled\ntrials (RCTs). However, in many applications -- particularly in healthcare --\nthis static summary fails to capture the nuanced dynamics of treatment effects\nthat vary with both dose and time. We propose a framework for modelling\ntreatment effect trajectories as smooth surfaces over dose and time, enabling\nthe extraction of clinically actionable insights such as onset time, peak\neffect, and duration of benefit. To ensure interpretability, robustness, and\nverifiability -- key requirements in high-stakes domains -- we adapt\nSemanticODE, a recent framework for interpretable trajectory modelling, to the\ncausal setting where treatment effects are never directly observed. Our\napproach decouples the estimation of trajectory shape from the specification of\nclinically relevant properties (e.g., maxima, inflection points), supporting\ndomain-informed priors, post-hoc editing, and transparent analysis. We show\nthat our method yields accurate, interpretable, and editable models of\ntreatment dynamics, facilitating both rigorous causal analysis and practical\ndecision-making.",
      "pdf_url": "http://arxiv.org/pdf/2507.07271v1",
      "arxiv_url": "http://arxiv.org/abs/2507.07271v1",
      "published": "2025-07-09",
      "categories": [
        "cs.LG"
      ]
    },
    {
      "title": "Debiased Semiparametric Efficient Changes-in-Changes Estimation",
      "authors": [
        "Jinghao Sun",
        "Eric J. Tchetgen Tchetgen"
      ],
      "abstract": "We introduce a novel extension of the influential changes-in-changes (CiC)\nframework [Athey and Imbens, 2006] to estimate the average treatment effect on\nthe treated (ATT) and distributional causal estimands in panel data settings\nwith unmeasured confounding. While CiC relaxes the parallel trends assumption\ninherent in difference-in-differences (DiD), existing approaches typically\naccommodate only a single scalar unobserved confounder and rely on monotonicity\nassumptions between the confounder and the outcome. Moreover, current\nformulations lack inference procedures and theoretical guarantees that\naccommodate continuous covariates. Motivated by the intricate nature of\nconfounding in empirical applications and the need to incorporate continuous\ncovariates in a principled manner, we make two key contributions in this\ntechnical report. First, we establish nonparametric identification under a\nnovel set of assumptions that permit high-dimensional unmeasured confounders\nand non-monotonic relationships between confounders and outcomes. Second, we\nconstruct efficient estimators that are Neyman orthogonal to\ninfinite-dimensional nuisance parameters, facilitating valid inference even in\nthe presence of high-dimensional continuous or discrete covariates and flexible\nmachine learning-based nuisance estimation.",
      "pdf_url": "http://arxiv.org/pdf/2507.07228v1",
      "arxiv_url": "http://arxiv.org/abs/2507.07228v1",
      "published": "2025-07-09",
      "categories": [
        "stat.ME",
        "econ.EM"
      ]
    },
    {
      "title": "Deep Disentangled Representation Network for Treatment Effect Estimation",
      "authors": [
        "Hui Meng",
        "Keping Yang",
        "Xuyu Peng",
        "Bo Zheng"
      ],
      "abstract": "Estimating individual-level treatment effect from observational data is a\nfundamental problem in causal inference and has attracted increasing attention\nin the fields of education, healthcare, and public policy.In this work, we\nconcentrate on the study of disentangled representation methods that have shown\npromising outcomes by decomposing observed covariates into instrumental,\nconfounding, and adjustment factors. However, most of the previous work has\nprimarily revolved around generative models or hard decomposition methods for\ncovariates, which often struggle to guarantee the attainment of precisely\ndisentangled factors. In order to effectively model different causal\nrelationships, we propose a novel treatment effect estimation algorithm that\nincorporates a mixture of experts with multi-head attention and a linear\northogonal regularizer to softly decompose the pre-treatment variables, and\nsimultaneously eliminates selection bias via importance sampling re-weighting\ntechniques. We conduct extensive experiments on both public semi-synthetic and\nreal-world production datasets. The experimental results clearly demonstrate\nthat our algorithm outperforms the state-of-the-art methods focused on\nindividual treatment effects.",
      "pdf_url": "http://arxiv.org/pdf/2507.06650v1",
      "arxiv_url": "http://arxiv.org/abs/2507.06650v1",
      "published": "2025-07-09",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    }
  ]
}