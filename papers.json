{
  "last_updated": "2026-02-12T01:13:36.316029",
  "papers": [
    {
      "title": "Doubly Robust Estimation of Desirability of Outcome Ranking (DOOR) Probability with Application to MDRO Studies",
      "authors": [
        "Shiyu Shu",
        "Toshimitsu Hamasaki",
        "Scott Evans",
        "Lauren Komarow",
        "David van Duin",
        "Guoqing Diao"
      ],
      "abstract": "In observational studies, adjusting for confounders is required if a treatment comparison is planned. A crude comparison of the primary endpoint without covariate adjustment will suffer from biases, and the addition of regression models could improve precision by incorporating imbalanced covariates and thus help make correct inference. Desirability of outcome ranking (DOOR) is a patient-centric benefit-risk evaluation methodology designed for randomized clinical trials. Still, robust covariate adjustment methods could further expand the compatibility of this method in observational studies. In DOOR analysis, each participant's outcome is ranked based on pre-specified clinical criteria, where the most desirable rank represents a good outcome with no side effects and the least desirable rank is the worst possible clinical outcome. We develop a causal framework for estimating the population-level DOOR probability, via the inverse probability of treatment weighting method, G-Computation method, and a Doubly Robust method that combines both. The performance of the proposed methodologies is examined through simulations. We also perform a causal analysis of the Multi-Drug Resistant Organism (MDRO) network within the Antibacterial Resistant Leadership Group (ARLG), comparing the benefit:risk between Mono-drug therapy and Combination-drug therapy.",
      "pdf_url": "https://arxiv.org/pdf/2602.10012v1",
      "arxiv_url": "http://arxiv.org/abs/2602.10012v1",
      "published": "2026-02-10",
      "categories": [
        "stat.ME"
      ]
    },
    {
      "title": "Hand2World: Autoregressive Egocentric Interaction Generation via Free-Space Hand Gestures",
      "authors": [
        "Yuxi Wang",
        "Wenqi Ouyang",
        "Tianyi Wei",
        "Yi Dong",
        "Zhiqi Shen",
        "Xingang Pan"
      ],
      "abstract": "Egocentric interactive world models are essential for augmented reality and embodied AI, where visual generation must respond to user input with low latency, geometric consistency, and long-term stability. We study egocentric interaction generation from a single scene image under free-space hand gestures, aiming to synthesize photorealistic videos in which hands enter the scene, interact with objects, and induce plausible world dynamics under head motion. This setting introduces fundamental challenges, including distribution shift between free-space gestures and contact-heavy training data, ambiguity between hand motion and camera motion in monocular views, and the need for arbitrary-length video generation. We present Hand2World, a unified autoregressive framework that addresses these challenges through occlusion-invariant hand conditioning based on projected 3D hand meshes, allowing visibility and occlusion to be inferred from scene context rather than encoded in the control signal. To stabilize egocentric viewpoint changes, we inject explicit camera geometry via per-pixel Plücker-ray embeddings, disentangling camera motion from hand motion and preventing background drift. We further develop a fully automated monocular annotation pipeline and distill a bidirectional diffusion model into a causal generator, enabling arbitrary-length synthesis. Experiments on three egocentric interaction benchmarks show substantial improvements in perceptual quality and 3D consistency while supporting camera control and long-horizon interactive generation.",
      "pdf_url": "https://arxiv.org/pdf/2602.09600v1",
      "arxiv_url": "http://arxiv.org/abs/2602.09600v1",
      "published": "2026-02-10",
      "categories": [
        "cs.CV"
      ]
    },
    {
      "title": "Estimating causal effects of functional treatments with modified functional treatment policies",
      "authors": [
        "Ziren Jiang",
        "Erjia Cui",
        "Jared D. Huling"
      ],
      "abstract": "Functional data are increasingly prevalent in biomedical research. While functional data analysis has been established for decades, causal inference with functional treatments remains largely unexplored. Existing methods typically focus on estimating the causal average dose response functional (ADRF), which requires strong positivity assumptions and offers limited interpretability. In this work, we target a new causal estimand, the modified functional treatment policy (MFTP), which focuses on estimating the average potential outcome when each individual slightly modifies their treatment trajectory from the observed one. A major challenge for this new estimand is the need to define an average over an infinite-dimensional object with no density. By proposing a novel definition of the population average over a functional variable using a functional principal component analysis (FPCA) decomposition, we establish the causal identifiability of the MFTP estimand. We further derive outcome regression, inverse probability weighting, and doubly robust estimators for the MFTP, and provide theoretical guarantees under mild regularity conditions. The proposed estimators are validated through extensive simulation studies. Applying our MFTP framework to the National Health and Nutrition Examination Survey (NHANES) accelerometer data, we estimate the causal effects of reducing disruptive nighttime activity and low-activity duration on all-cause mortality.",
      "pdf_url": "https://arxiv.org/pdf/2602.09145v1",
      "arxiv_url": "http://arxiv.org/abs/2602.09145v1",
      "published": "2026-02-09",
      "categories": [
        "stat.ME"
      ]
    },
    {
      "title": "State policy heterogeneity analyses: considerations and proposals",
      "authors": [
        "Max Rubinstein",
        "Megan S. Schuler",
        "Elizabeth A. Stuart",
        "Bradley D. Stein",
        "Max Griswold",
        "Elizabeth M. Stone",
        "Beth Ann Griffin"
      ],
      "abstract": "State-level policy studies often conduct heterogeneity analyses that quantify how treatment effects vary across state characteristics. These analyses may be used to inform state-specific policy decisions, or to infer how the effect of a policy changes in combination with other state characteristics. However, in state-level settings with varied contexts and policy landscapes, multiple versions of similar policies, and differential policy implementation, the causal quantities targeted by these analyses may not align with the inferential goals. This paper clarifies these issues by distinguishing several causal estimands relevant to heterogeneity analyses in state-policy settings, including state-specific treatment effects (ITE), conditional average treatment effects (CATE), and controlled direct effects (CDE). We argue that the CATE is often the easiest to identify and estimate, but may not be the most policy relevant target of inference. Moreover, the widespread practice of coarsening distinct policies or implementations into a single indicator further complicates the interpretation of these analyses. Motivated by these limitations, we propose bounding ITEs as an alternative inferential goal, yielding ranges for each state's policy effect under explicit assumptions that quantify deviations from the ideal identifying conditions. These bounds target a well-defined and policy-relevant quantity, the effect for specific states. We develop this approach within a difference-in-differences framework and discuss how sensitivity parameters may be informed using pre-treatment data. Through simulations we demonstrate that bounding state-specific effects can more reliably determine the sign of the ITEs than CATE estimates. We then illustrate this method to examine the effect of the Affordable Care Act Medicaid expansion on high-volume buprenorphine prescribing.",
      "pdf_url": "https://arxiv.org/pdf/2602.08643v1",
      "arxiv_url": "http://arxiv.org/abs/2602.08643v1",
      "published": "2026-02-09",
      "categories": [
        "stat.ME",
        "stat.AP"
      ]
    },
    {
      "title": "CauScale: Neural Causal Discovery at Scale",
      "authors": [
        "Bo Peng",
        "Sirui Chen",
        "Jiaguo Tian",
        "Yu Qiao",
        "Chaochao Lu"
      ],
      "abstract": "Causal discovery is essential for advancing data-driven fields such as scientific AI and data analysis, yet existing approaches face significant time- and space-efficiency bottlenecks when scaling to large graphs. To address this challenge, we present CauScale, a neural architecture designed for efficient causal discovery that scales inference to graphs with up to 1000 nodes. CauScale improves time efficiency via a reduction unit that compresses data embeddings and improves space efficiency by adopting tied attention weights to avoid maintaining axis-specific attention maps. To keep high causal discovery accuracy, CauScale adopts a two-stream design: a data stream extracts relational evidence from high-dimensional observations, while a graph stream integrates statistical graph priors and preserves key structural signals. CauScale successfully scales to 500-node graphs during training, where prior work fails due to space limitations. Across testing data with varying graph scales and causal mechanisms, CauScale achieves 99.6% mAP on in-distribution data and 84.4% on out-of-distribution data, while delivering 4-13,000 times inference speedups over prior methods. Our project page is at https://github.com/OpenCausaLab/CauScale.",
      "pdf_url": "https://arxiv.org/pdf/2602.08629v1",
      "arxiv_url": "http://arxiv.org/abs/2602.08629v1",
      "published": "2026-02-09",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ]
    },
    {
      "title": "Towards Reliable Social A/B Testing: Spillover-Contained Clustering with Robust Post-Experiment Analysis",
      "authors": [
        "Xu Min",
        "Zhaoxu Yang",
        "Kaixuan Tan",
        "Juan Yan",
        "Xunbin Xiong",
        "Zihao Zhu",
        "Kaiyu Zhu",
        "Fenglin Cui",
        "Yang Yang",
        "Sihua Yang",
        "Jianhui Bu"
      ],
      "abstract": "A/B testing is the foundation of decision-making in online platforms, yet social products often suffer from network interference: user interactions cause treatment effects to spill over into the control group. Such spillovers bias causal estimates and undermine experimental conclusions. Existing approaches face key limitations: user-level randomization ignores network structure, while cluster-based methods often rely on general-purpose clustering that is not tailored for spillover containment and has difficulty balancing unbiasedness and statistical power at scale. We propose a spillover-contained experimentation framework with two stages. In the pre-experiment stage, we build social interaction graphs and introduce a Balanced Louvain algorithm that produces stable, size-balanced clusters while minimizing cross-cluster edges, enabling reliable cluster-based randomization. In the post-experiment stage, we develop a tailored CUPAC estimator that leverages pre-experiment behavioral covariates to reduce the variance induced by cluster-level assignment, thereby improving statistical power. Together, these components provide both structural spillover containment and robust statistical inference. We validate our approach through large-scale social sharing experiments on Kuaishou, a platform serving hundreds of millions of users. Results show that our method substantially reduces spillover and yields more accurate assessments of social strategies than traditional user-level designs, establishing a reliable and scalable framework for networked A/B testing.",
      "pdf_url": "https://arxiv.org/pdf/2602.08569v1",
      "arxiv_url": "http://arxiv.org/abs/2602.08569v1",
      "published": "2026-02-09",
      "categories": [
        "cs.SI",
        "cs.IR"
      ]
    },
    {
      "title": "Causal Schrödinger Bridges: Constrained Optimal Transport on Structural Manifolds",
      "authors": [
        "Rui Wu",
        "Li YongJun"
      ],
      "abstract": "Generative modeling typically seeks the path of least action via deterministic flows (ODE). While effective for in-distribution tasks, we argue that these deterministic paths become brittle under causal interventions, which often require transporting probability mass across low-density regions (\"off-manifold\") where the vector field is ill-defined. This leads to numerical instability and spurious correlations. In this work, we introduce the Causal Schrödinger Bridge (CSB), a framework that reformulates counterfactual inference as Entropic Optimal Transport. Unlike deterministic approaches that require strict invertibility, CSB leverages diffusion processes (SDEs) to robustly \"tunnel\" through support mismatches while strictly enforcing structural admissibility constraints. We prove the Structural Decomposition Theorem, showing that the global high-dimensional bridge factorizes into local, robust transitions. Empirical validation on high-dimensional interventions (Morpho-MNIST) demonstrates that CSB significantly outperforms deterministic baselines in structural consistency, particularly in regimes of strong, out-of-distribution treatments.",
      "pdf_url": "https://arxiv.org/pdf/2602.08535v1",
      "arxiv_url": "http://arxiv.org/abs/2602.08535v1",
      "published": "2026-02-09",
      "categories": [
        "cs.LG"
      ]
    },
    {
      "title": "Estimating Aleatoric Uncertainty in the Causal Treatment Effect",
      "authors": [
        "Liyuan Xu",
        "Bijan Mazaheri"
      ],
      "abstract": "Previous work on causal inference has primarily focused on averages and conditional averages of treatment effects, with significantly less attention on variability and uncertainty in individual treatment responses. In this paper, we introduce the variance of the treatment effect (VTE) and conditional variance of treatment effect (CVTE) as the natural measure of aleatoric uncertainty inherent in treatment responses, and we demonstrate that these quantities are identifiable from observed data under mild assumptions, even in the presence of unobserved confounders. We further propose nonparametric kernel-based estimators for VTE and CVTE, and our theoretical analysis establishes their convergence. We also test the performance of our method through extensive empirical experiments on both synthetic and semi-simulated datasets, where it demonstrates superior or comparable performance to naive baselines.",
      "pdf_url": "https://arxiv.org/pdf/2602.08461v1",
      "arxiv_url": "http://arxiv.org/abs/2602.08461v1",
      "published": "2026-02-09",
      "categories": [
        "cs.LG"
      ]
    },
    {
      "title": "Estimation Strategies for Causal Decomposition Analysis with Allowability Specifications",
      "authors": [
        "John W. Jackson",
        "Ting-Hsuan Chang",
        "Aster Meche",
        "Trang Q. Nguyen"
      ],
      "abstract": "Causal decomposition analysis (CDA) is an approach for modeling the impact of hypothetical interventions to reduce disparities. It is useful for identifying foci that future interventions, including multilevel and multimodal interventions, could focus on to reduce disparities. Based within the potential outcomes framework, CDA has a causal interpretation when the identifying assumptions are met. CDA also allows an analyst to consider which covariates are allowable (i.e., fair) for defining the disparity in the outcome and in the point of intervention, so that its interpretation is also meaningful. While the incorporation of causal inference and allowability promotes robustness, transparency, and dialogue in disparities research, it can lead to challenges in estimation such as the need to correctly model densities. Also, how CDA differs from commonly used estimators may not be clear, which may limit its uptake. To address these challenges, we provide a tour of estimation strategies for CDA, reviewing existing proposals and introducing novel estimators that overcome key estimation challenges. Among them we introduce what we call \"bridging\" estimators that avoid directly modeling any density, and weighted sequential regression estimators that are multiply robust. Additionally, we provide diagnostics to assess the quality of the nuisance density models and weighting functions they rely on. We formally establish the estimators' robustness to model mis-specification, demonstrate their performance through a simulation study based on real data, and apply them to study disparities in hypertension control using electronic health records in a large healthcare system.",
      "pdf_url": "https://arxiv.org/pdf/2602.07825v1",
      "arxiv_url": "http://arxiv.org/abs/2602.07825v1",
      "published": "2026-02-08",
      "categories": [
        "stat.ME"
      ]
    },
    {
      "title": "Emergent Structured Representations Support Flexible In-Context Inference in Large Language Models",
      "authors": [
        "Ningyu Xu",
        "Qi Zhang",
        "Xipeng Qiu",
        "Xuanjing Huang"
      ],
      "abstract": "Large language models (LLMs) exhibit emergent behaviors suggestive of human-like reasoning. While recent work has identified structured, human-like conceptual representations within these models, it remains unclear whether they functionally rely on such representations for reasoning. Here we investigate the internal processing of LLMs during in-context concept inference. Our results reveal a conceptual subspace emerging in middle to late layers, whose representational structure persists across contexts. Using causal mediation analyses, we demonstrate that this subspace is not merely an epiphenomenon but is functionally central to model predictions, establishing its causal role in inference. We further identify a layer-wise progression where attention heads in early-to-middle layers integrate contextual cues to construct and refine the subspace, which is subsequently leveraged by later layers to generate predictions. Together, these findings provide evidence that LLMs dynamically construct and use structured, latent representations in context for inference, offering insights into the computational processes underlying flexible adaptation.",
      "pdf_url": "https://arxiv.org/pdf/2602.07794v2",
      "arxiv_url": "http://arxiv.org/abs/2602.07794v2",
      "published": "2026-02-08",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    }
  ]
}