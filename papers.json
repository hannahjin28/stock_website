{
  "last_updated": "2025-06-05T00:54:21.433076",
  "papers": [
    {
      "title": "TalkingMachines: Real-Time Audio-Driven FaceTime-Style Video via Autoregressive Diffusion Models",
      "authors": [
        "Chetwin Low",
        "Weimin Wang"
      ],
      "abstract": "In this paper, we present TalkingMachines -- an efficient framework that\ntransforms pretrained video generation models into real-time, audio-driven\ncharacter animators. TalkingMachines enables natural conversational experiences\nby integrating an audio large language model (LLM) with our video generation\nfoundation model. Our primary contributions include: (1) We adapt a pretrained\nSOTA image-to-video DiT into an audio-driven avatar generation model of 18\nbillion parameters; (2) We enable infinite video streaming without error\naccumulation through asymmetric knowledge distillation from a bidirectional\nteacher model into a sparse causal, autoregressive student model; (3) We design\na high-throughput, low-latency inference pipeline incorporating several key\nengineering optimizations such as: (a) disaggregation of the DiT and VAE\ndecoder across separate devices, (b) efficient overlap of inter-device\ncommunication and computation using CUDA streams, (c) elimination of redundant\nrecomputations to maximize frame-generation throughput. Please see demo videos\nhere - https://aaxwaz.github.io/TalkingMachines/",
      "pdf_url": "http://arxiv.org/pdf/2506.03099v1",
      "arxiv_url": "http://arxiv.org/abs/2506.03099v1",
      "published": "2025-06-03",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.GR"
      ]
    },
    {
      "title": "IP-Dialog: Evaluating Implicit Personalization in Dialogue Systems with Synthetic Data",
      "authors": [
        "Bo Peng",
        "Zhiheng Wang",
        "Heyang Gong",
        "Chaochao Lu"
      ],
      "abstract": "In modern dialogue systems, the ability to implicitly infer user backgrounds\nfrom conversations and leverage this information for personalized assistance is\ncrucial. However, the scarcity of high-quality data remains a fundamental\nchallenge to evaluating and improving this capability. Traditional dataset\nconstruction methods are labor-intensive, resource-demanding, and raise privacy\nconcerns. To address these issues, we propose a novel approach for automatic\nsynthetic data generation and introduce the Implicit Personalized Dialogue\n(IP-Dialog) benchmark along with a training dataset, covering 10 tasks and 12\nuser attribute types. Additionally, we develop a systematic evaluation\nframework with four metrics to assess both attribute awareness and reasoning\ncapabilities. We further propose five causal graphs to elucidate models'\nreasoning pathways during implicit personalization. Extensive experiments yield\ninsightful observations and prove the reliability of our dataset.",
      "pdf_url": "http://arxiv.org/pdf/2506.02449v1",
      "arxiv_url": "http://arxiv.org/abs/2506.02449v1",
      "published": "2025-06-03",
      "categories": [
        "cs.CL",
        "cs.HC"
      ]
    },
    {
      "title": "Spillovers and Effect Attenuation in Firearm Policy Research in the United States",
      "authors": [
        "Lee Kennedy-Shaffer",
        "Alan Hamilton Kennedy"
      ],
      "abstract": "In the United States, firearm-related deaths and injuries are a major public\nhealth issue. Because of limited federal action, state policies are\nparticularly important, and their evaluation informs the actions of other\npolicymakers. The movement of firearms across state and local borders, however,\ncan undermine the effectiveness of these policies and have statistical\nconsequences for their empirical evaluation. This movement causes spillover and\nbypass effects of policies, wherein interventions affect nearby control states\nand the lack of intervention in nearby states reduces the effectiveness in the\nintervention states. While some causal inference methods exist to account for\nspillover effects and reduce bias, these do not necessarily align well with the\ndata available for firearm research or with the most policy-relevant estimands.\nIntegrated data infrastructure and new methods are necessary for a better\nunderstanding of the effects these policies would have if widely adopted. In\nthe meantime, appropriately understanding and interpreting effect estimates\nfrom quasi-experimental analyses is crucial for ensuring that effective\npolicies are not dismissed due to these statistical challenges.",
      "pdf_url": "http://arxiv.org/pdf/2506.01695v1",
      "arxiv_url": "http://arxiv.org/abs/2506.01695v1",
      "published": "2025-06-02",
      "categories": [
        "stat.AP",
        "econ.EM",
        "62P25"
      ]
    },
    {
      "title": "A Diffusion-Based Method for Learning the Multi-Outcome Distribution of Medical Treatments",
      "authors": [
        "Yuchen Ma",
        "Jonas Schweisthal",
        "Hengrui Zhang",
        "Stefan Feuerriegel"
      ],
      "abstract": "In medicine, treatments often influence multiple, interdependent outcomes,\nsuch as primary endpoints, complications, adverse events, or other secondary\nendpoints. Hence, to make optimal treatment decisions, clinicians are\ninterested in learning the distribution of multi-dimensional treatment\noutcomes. However, the vast majority of machine learning methods for predicting\ntreatment effects focus on single-outcome settings, despite the fact that\nmedical data often include multiple, interdependent outcomes. To address this\nlimitation, we propose a novel diffusion-based method called DIME to learn the\njoint distribution of multiple outcomes of medical treatments. We addresses\nthree challenges relevant in medical practice: (i)it is tailored to learn the\njoint interventional distribution of multiple medical outcomes, which enables\nreliable decision-making with uncertainty quantification rather than relying\nsolely on point estimates; (ii)it explicitly captures the dependence structure\nbetween outcomes; (iii)it can handle outcomes of mixed type, including binary,\ncategorical, and continuous variables. In DIME, we take into account the\nfundamental problem of causal inference through causal masking. For training,\nour method decomposes the joint distribution into a series of conditional\ndistributions with a customized conditional masking to account for the\ndependence structure across outcomes. For inference, our method\nauto-regressively generates predictions. This allows our method to move beyond\npoint estimates of causal quantities and thus learn the joint interventional\ndistribution. To the best of our knowledge, DIME is the first neural method\ntailored to learn the joint, multi-outcome distribution of medical treatments.\nAcross various experiments, we demonstrate that our method effectively learns\nthe joint distribution and captures shared information among multiple outcomes.",
      "pdf_url": "http://arxiv.org/pdf/2506.01533v1",
      "arxiv_url": "http://arxiv.org/abs/2506.01533v1",
      "published": "2025-06-02",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Playing with Transformer at 30+ FPS via Next-Frame Diffusion",
      "authors": [
        "Xinle Cheng",
        "Tianyu He",
        "Jiayi Xu",
        "Junliang Guo",
        "Di He",
        "Jiang Bian"
      ],
      "abstract": "Autoregressive video models offer distinct advantages over bidirectional\ndiffusion models in creating interactive video content and supporting streaming\napplications with arbitrary duration. In this work, we present Next-Frame\nDiffusion (NFD), an autoregressive diffusion transformer that incorporates\nblock-wise causal attention, enabling iterative sampling and efficient\ninference via parallel token generation within each frame. Nonetheless,\nachieving real-time video generation remains a significant challenge for such\nmodels, primarily due to the high computational cost associated with diffusion\nsampling and the hardware inefficiencies inherent to autoregressive generation.\nTo address this, we introduce two innovations: (1) We extend consistency\ndistillation to the video domain and adapt it specifically for video models,\nenabling efficient inference with few sampling steps; (2) To fully leverage\nparallel computation, motivated by the observation that adjacent frames often\nshare the identical action input, we propose speculative sampling. In this\napproach, the model generates next few frames using current action input, and\ndiscard speculatively generated frames if the input action differs. Experiments\non a large-scale action-conditioned video generation benchmark demonstrate that\nNFD beats autoregressive baselines in terms of both visual quality and sampling\nefficiency. We, for the first time, achieves autoregressive video generation at\nover 30 Frames Per Second (FPS) on an A100 GPU using a 310M model.",
      "pdf_url": "http://arxiv.org/pdf/2506.01380v1",
      "arxiv_url": "http://arxiv.org/abs/2506.01380v1",
      "published": "2025-06-02",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Uncovering Bias Mechanisms in Observational Studies",
      "authors": [
        "Ilker Demirel",
        "Zeshan Hussain",
        "Piersilvio De Bartolomeis",
        "David Sontag"
      ],
      "abstract": "Observational studies are a key resource for causal inference but are often\naffected by systematic biases. Prior work has focused mainly on detecting these\nbiases, via sensitivity analyses and comparisons with randomized controlled\ntrials, or mitigating them through debiasing techniques. However, there remains\na lack of methodology for uncovering the underlying mechanisms driving these\nbiases, e.g., whether due to hidden confounding or selection of participants.\nIn this work, we show that the relationship between bias magnitude and the\npredictive performance of nuisance function estimators (in the observational\nstudy) can help distinguish among common sources of causal bias. We validate\nour methodology through extensive synthetic experiments and a real-world case\nstudy, demonstrating its effectiveness in revealing the mechanisms behind\nobserved biases. Our framework offers a new lens for understanding and\ncharacterizing bias in observational studies, with practical implications for\nimproving causal inference.",
      "pdf_url": "http://arxiv.org/pdf/2506.01191v1",
      "arxiv_url": "http://arxiv.org/abs/2506.01191v1",
      "published": "2025-06-01",
      "categories": [
        "stat.ME",
        "stat.ML"
      ]
    },
    {
      "title": "zip2zip: Inference-Time Adaptive Vocabularies for Language Models via Token Compression",
      "authors": [
        "Saibo Geng",
        "Nathan Ranchin",
        "Yunzhen yao",
        "Maxime Peyrard",
        "Chris Wendler",
        "Michael Gastpar",
        "Robert West"
      ],
      "abstract": "Tokenization efficiency plays a critical role in the performance and cost of\nlarge language models (LLMs), yet most models rely on static tokenizers\noptimized for general-purpose corpora. These tokenizers' fixed vocabularies\noften fail to adapt to domain- or language-specific inputs, leading to longer\ntoken sequences and higher computational costs. We introduce zip2zip, a\nframework that enables LLMs to dynamically adjust token vocabulary at inference\ntime, allowing for fewer generated tokens and thus faster inference. zip2zip\nconsists of three key components: (1) a tokenizer based on Lempel-Ziv-Welch\n(LZW) compression that incrementally compresses tokens into reusable\n\"hypertokens\" on the fly; (2) an embedding layer that computes embeddings for\nnewly formed hypertokens at runtime; and (3) a causal language modeling variant\nthat trains the model to operate on hypertokenized, compressed sequences. We\nshow that an existing LLM can be zip2zip-fied in 10 GPU-hours via\nparameter-efficient finetuning. The resulting zip2zip LLMs effectively learn to\nuse hypertokens at inference time, reducing input and output sequence length by\n20-60\\%, with significant improvements in inference latency.",
      "pdf_url": "http://arxiv.org/pdf/2506.01084v1",
      "arxiv_url": "http://arxiv.org/abs/2506.01084v1",
      "published": "2025-06-01",
      "categories": [
        "cs.CL",
        "cs.LG"
      ]
    },
    {
      "title": "Projection Pursuit Density Ratio Estimation",
      "authors": [
        "Meilin Wang",
        "Wei Huang",
        "Mingming Gong",
        "Zheng Zhang"
      ],
      "abstract": "Density ratio estimation (DRE) is a paramount task in machine learning, for\nits broad applications across multiple domains, such as covariate shift\nadaptation, causal inference, independence tests and beyond. Parametric methods\nfor estimating the density ratio possibly lead to biased results if models are\nmisspecified, while conventional non-parametric methods suffer from the curse\nof dimensionality when the dimension of data is large. To address these\nchallenges, in this paper, we propose a novel approach for DRE based on the\nprojection pursuit (PP) approximation. The proposed method leverages PP to\nmitigate the impact of high dimensionality while retaining the model\nflexibility needed for the accuracy of DRE. We establish the consistency and\nthe convergence rate for the proposed estimator. Experimental results\ndemonstrate that our proposed method outperforms existing alternatives in\nvarious applications.",
      "pdf_url": "http://arxiv.org/pdf/2506.00866v1",
      "arxiv_url": "http://arxiv.org/abs/2506.00866v1",
      "published": "2025-06-01",
      "categories": [
        "stat.ML",
        "cs.LG",
        "stat.ME"
      ]
    },
    {
      "title": "Recover Experimental Data with Selection Bias using Counterfactual Logic",
      "authors": [
        "Jingyang He",
        "Shuai Wang",
        "Ang Li"
      ],
      "abstract": "Selection bias, arising from the systematic inclusion or exclusion of certain\nsamples, poses a significant challenge to the validity of causal inference.\nWhile Bareinboim et al. introduced methods for recovering unbiased\nobservational and interventional distributions from biased data using partial\nexternal information, the complexity of the backdoor adjustment and the\nmethod's strong reliance on observational data limit its applicability in many\npractical settings. In this paper, we formally discover the recoverability of\n$P(Y^*_{x^*})$ under selection bias with experimental data. By explicitly\nconstructing counterfactual worlds via Structural Causal Models (SCMs), we\nanalyze how selection mechanisms in the observational world propagate to the\ncounterfactual domain. We derive a complete set of graphical and theoretical\ncriteria to determine that the experimental distribution remain unaffected by\nselection bias. Furthermore, we propose principled methods for leveraging\npartially unbiased observational data to recover $P(Y^*_{x^*})$ from biased\nexperimental datasets. Simulation studies replicating realistic research\nscenarios demonstrate the practical utility of our approach, offering concrete\nguidance for mitigating selection bias in applied causal inference.",
      "pdf_url": "http://arxiv.org/pdf/2506.00335v1",
      "arxiv_url": "http://arxiv.org/abs/2506.00335v1",
      "published": "2025-05-31",
      "categories": [
        "stat.ME",
        "cs.AI"
      ]
    },
    {
      "title": "Estimation of Optimal Causal Bounds via Covariate-Assisted Optimal Transport",
      "authors": [
        "Sirui Lin",
        "Zijun Gao",
        "Jose Blanchet",
        "Peter Glynn"
      ],
      "abstract": "We study the estimation of causal estimand involving the joint distribution\nof treatment and control outcomes for a single unit. In typical causal\ninference settings, it is impossible to observe both outcomes simultaneously,\nwhich places our estimation within the domain of partial identification (PI).\nPre-treatment covariates can substantially reduce estimation uncertainty by\nshrinking the partially identified set. Recently, it was shown that\ncovariate-assisted PI sets can be characterized through conditional optimal\ntransport (COT) problems. However, finite-sample estimation of COT poses\nsignificant challenges, primarily because, as we explain, the COT functional is\ndiscontinuous under the weak topology, rendering the direct plug-in estimator\ninconsistent. To address this issue, existing literature relies on relaxations\nor indirect methods involving the estimation of non-parametric nuisance\nstatistics. In this work, we demonstrate the continuity of the COT functional\nunder a stronger topology induced by the adapted Wasserstein distance.\nLeveraging this result, we propose a direct, consistent, non-parametric\nestimator for COT value that avoids nuisance parameter estimation. We derive\nthe convergence rate for our estimator and validate its effectiveness through\ncomprehensive simulations, demonstrating its improved performance compared to\nexisting approaches.",
      "pdf_url": "http://arxiv.org/pdf/2506.00257v1",
      "arxiv_url": "http://arxiv.org/abs/2506.00257v1",
      "published": "2025-05-30",
      "categories": [
        "stat.ME"
      ]
    }
  ]
}