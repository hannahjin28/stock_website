{
  "last_updated": "2026-02-19T01:13:58.186328",
  "papers": [
    {
      "title": "This human study did not involve human subjects: Validating LLM simulations as behavioral evidence",
      "authors": [
        "Jessica Hullman",
        "David Broska",
        "Huaman Sun",
        "Aaron Shaw"
      ],
      "abstract": "A growing literature uses large language models (LLMs) as synthetic participants to generate cost-effective and nearly instantaneous responses in social science experiments. However, there is limited guidance on when such simulations support valid inference about human behavior. We contrast two strategies for obtaining valid estimates of causal effects and clarify the assumptions under which each is suitable for exploratory versus confirmatory research. Heuristic approaches seek to establish that simulated and observed human behavior are interchangeable through prompt engineering, model fine-tuning, and other repair strategies designed to reduce LLM-induced inaccuracies. While useful for many exploratory tasks, heuristic approaches lack the formal statistical guarantees typically required for confirmatory research. In contrast, statistical calibration combines auxiliary human data with statistical adjustments to account for discrepancies between observed and simulated responses. Under explicit assumptions, statistical calibration preserves validity and provides more precise estimates of causal effects at lower cost than experiments that rely solely on human participants. Yet the potential of both approaches depends on how well LLMs approximate the relevant populations. We consider what opportunities are overlooked when researchers focus myopically on substituting LLMs for human participants in a study.",
      "pdf_url": "https://arxiv.org/pdf/2602.15785v1",
      "arxiv_url": "http://arxiv.org/abs/2602.15785v1",
      "published": "2026-02-17",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "The geometry of online conversations and the causal antecedents of conflictual discourse",
      "authors": [
        "Carlo Santagiustina",
        "Caterina Cruciani"
      ],
      "abstract": "This article investigates the causal antecedents of conflictual language and the geometry of interaction in online threaded conversations related to climate change. We employ three annotation dimensions, inferred through LLM prompting and averaging, to capture complementary aspects of discursive conflict (such as stance: agreement vs disagreement; tone: attacking vs respectful; and emotional versus factual framing) and use data from a threaded online forum to examine how these dimensions respond to temporal, conversational, and arborescent structural features of discussions. We show that, as suggested by the literature, longer delays between successive posts in a thread are associated with replies that are, on average, more respectful, whereas longer delays relative to the parent post are associated with slightly less disagreement but more emotional (less factual) language. Second, we characterize alignment with the local conversational environment and find strong convergence both toward the average stance, tone and emotional framing of older sibling posts replying to the same parent and toward those of the parent post itself, with parent post effects generally stronger than sibling effects. We further show that early branch-level responses condition these alignment dynamics, such that parent-child stance alignment is amplified or attenuated depending on whether a branch is initiated in agreement or disagreement with the discussion's root message. These influences are largely additive for civility-related dimensions (attacking vs respectful, disagree vs agree), whereas for emotional versus factual framing there is a significant interaction: alignment with the parent's emotionality is amplified when older siblings are similarly aligned.",
      "pdf_url": "https://arxiv.org/pdf/2602.15600v1",
      "arxiv_url": "http://arxiv.org/abs/2602.15600v1",
      "published": "2026-02-17",
      "categories": [
        "cs.SI",
        "cs.AI",
        "econ.EM",
        "stat.AP"
      ]
    },
    {
      "title": "CEPAE: Conditional Entropy-Penalized Autoencoders for Time Series Counterfactuals",
      "authors": [
        "Tomàs Garriga",
        "Gerard Sanz",
        "Eduard Serrahima de Cambra",
        "Axel Brando"
      ],
      "abstract": "The ability to accurately perform counterfactual inference on time series is crucial for decision-making in fields like finance, healthcare, and marketing, as it allows us to understand the impact of events or treatments on outcomes over time. In this paper, we introduce a new counterfactual inference approach tailored to time series data impacted by market events, which is motivated by an industrial application. Utilizing the abduction-action-prediction procedure and the Structural Causal Model framework, we first adapt methods based on variational autoencoders and adversarial autoencoders, both previously used in counterfactual literature although not in time series settings. Then, we present the Conditional Entropy-Penalized Autoencoder (CEPAE), a novel autoencoder-based approach for counterfactual inference, which employs an entropy penalization loss over the latent space to encourage disentangled data representations. We validate our approach both theoretically and experimentally on synthetic, semi-synthetic, and real-world datasets, showing that CEPAE generally outperforms the other approaches in the evaluated metrics.",
      "pdf_url": "https://arxiv.org/pdf/2602.15546v1",
      "arxiv_url": "http://arxiv.org/abs/2602.15546v1",
      "published": "2026-02-17",
      "categories": [
        "cs.LG"
      ]
    },
    {
      "title": "Joint analysis for multivariate longitudinal and event time data with a change point anchored at interval-censored event time",
      "authors": [
        "Yue Zhan",
        "Cheng Zheng",
        "Ying Zhang"
      ],
      "abstract": "Huntington's disease (HD) is an autosomal dominant neurodegenerative disorder characterized by motor dysfunction, psychiatric disturbances, and cognitive decline. The onset of HD is marked by severe motor impairment, which may be predicted by prior cognitive decline and, in turn, exacerbate cognitive deficits. Clinical data, however, are often collected at discrete time points, so the timing of disease onset is subject to interval censoring. To address the challenges posed by such data, we develop a joint model for multivariate longitudinal biomarkers with a change point anchored at an interval-censored event time. The model simultaneously assesses the effects of longitudinal biomarkers on the event time and the changes in biomarker trajectories following the event. We conduct a comprehensive simulation study to demonstrate the finite-sample performance of the proposed method for causal inference. Finally, we apply the method to PREDICT-HD, a multisite observational cohort study of prodromal HD individuals, to ascertain how cognitive impairment and motor dysfunction interact during disease progression.",
      "pdf_url": "https://arxiv.org/pdf/2602.14991v1",
      "arxiv_url": "http://arxiv.org/abs/2602.14991v1",
      "published": "2026-02-16",
      "categories": [
        "stat.ME"
      ]
    },
    {
      "title": "Use What You Know: Causal Foundation Models with Partial Graphs",
      "authors": [
        "Arik Reuter",
        "Anish Dhir",
        "Cristiana Diaconu",
        "Jake Robertson",
        "Ole Ossen",
        "Frank Hutter",
        "Adrian Weller",
        "Mark van der Wilk",
        "Bernhard Schölkopf"
      ],
      "abstract": "Estimating causal quantities traditionally relies on bespoke estimators tailored to specific assumptions. Recently proposed Causal Foundation Models (CFMs) promise a more unified approach by amortising causal discovery and inference in a single step. However, in their current state, they do not allow for the incorporation of any domain knowledge, which can lead to suboptimal predictions. We bridge this gap by introducing methods to condition CFMs on causal information, such as the causal graph or more readily available ancestral information. When access to complete causal graph information is too strict a requirement, our approach also effectively leverages partial causal information. We systematically evaluate conditioning strategies and find that injecting learnable biases into the attention mechanism is the most effective method to utilise full and partial causal information. Our experiments show that this conditioning allows a general-purpose CFM to match the performance of specialised models trained on specific causal structures. Overall, our approach addresses a central hurdle on the path towards all-in-one causal foundation models: the capability to answer causal queries in a data-driven manner while effectively leveraging any amount of domain expertise.",
      "pdf_url": "https://arxiv.org/pdf/2602.14972v1",
      "arxiv_url": "http://arxiv.org/abs/2602.14972v1",
      "published": "2026-02-16",
      "categories": [
        "cs.LG"
      ]
    },
    {
      "title": "DriveFine: Refining-Augmented Masked Diffusion VLA for Precise and Robust Driving",
      "authors": [
        "Chenxu Dang",
        "Sining Ang",
        "Yongkang Li",
        "Haochen Tian",
        "Jie Wang",
        "Guang Li",
        "Hangjun Ye",
        "Jie Ma",
        "Long Chen",
        "Yan Wang"
      ],
      "abstract": "Vision-Language-Action (VLA) models for autonomous driving increasingly adopt generative planners trained with imitation learning followed by reinforcement learning. Diffusion-based planners suffer from modality alignment difficulties, low training efficiency, and limited generalization. Token-based planners are plagued by cumulative causal errors and irreversible decoding. In summary, the two dominant paradigms exhibit complementary strengths and weaknesses. In this paper, we propose DriveFine, a masked diffusion VLA model that combines flexible decoding with self-correction capabilities. In particular, we design a novel plug-and-play block-MoE, which seamlessly injects a refinement expert on top of the generation expert. By enabling explicit expert selection during inference and gradient blocking during training, the two experts are fully decoupled, preserving the foundational capabilities and generic patterns of the pretrained weights, which highlights the flexibility and extensibility of the block-MoE design. Furthermore, we design a hybrid reinforcement learning strategy that encourages effective exploration of refinement expert while maintaining training stability. Extensive experiments on NAVSIM v1, v2, and Navhard benchmarks demonstrate that DriveFine exhibits strong efficacy and robustness. The code will be released at https://github.com/MSunDYY/DriveFine.",
      "pdf_url": "https://arxiv.org/pdf/2602.14577v1",
      "arxiv_url": "http://arxiv.org/abs/2602.14577v1",
      "published": "2026-02-16",
      "categories": [
        "cs.CV"
      ]
    },
    {
      "title": "Traceable Latent Variable Discovery Based on Multi-Agent Collaboration",
      "authors": [
        "Huaming Du",
        "Tao Hu",
        "Yijie Huang",
        "Yu Zhao",
        "Guisong Liu",
        "Tao Gu",
        "Gang Kou",
        "Carl Yang"
      ],
      "abstract": "Revealing the underlying causal mechanisms in the real world is crucial for scientific and technological progress. Despite notable advances in recent decades, the lack of high-quality data and the reliance of traditional causal discovery algorithms (TCDA) on the assumption of no latent confounders, as well as their tendency to overlook the precise semantics of latent variables, have long been major obstacles to the broader application of causal discovery. To address this issue, we propose a novel causal modeling framework, TLVD, which integrates the metadata-based reasoning capabilities of large language models (LLMs) with the data-driven modeling capabilities of TCDA for inferring latent variables and their semantics. Specifically, we first employ a data-driven approach to construct a causal graph that incorporates latent variables. Then, we employ multi-LLM collaboration for latent variable inference, modeling this process as a game with incomplete information and seeking its Bayesian Nash Equilibrium (BNE) to infer the possible specific latent variables. Finally, to validate the inferred latent variables across multiple real-world web-based data sources, we leverage LLMs for evidence exploration to ensure traceability. We comprehensively evaluate TLVD on three de-identified real patient datasets provided by a hospital and two benchmark datasets. Extensive experimental results confirm the effectiveness and reliability of TLVD, with average improvements of 32.67% in Acc, 62.21% in CAcc, and 26.72% in ECit across the five datasets.",
      "pdf_url": "https://arxiv.org/pdf/2602.14456v1",
      "arxiv_url": "http://arxiv.org/abs/2602.14456v1",
      "published": "2026-02-16",
      "categories": [
        "cs.LG"
      ]
    },
    {
      "title": "Integrating Unstructured Text into Causal Inference: Empirical Evidence from Real Data",
      "authors": [
        "Boning Zhou",
        "Ziyu Wang",
        "Han Hong",
        "Haoqi Hu"
      ],
      "abstract": "Causal inference, a critical tool for informing business decisions, traditionally relies heavily on structured data. However, in many real-world scenarios, such data can be incomplete or unavailable. This paper presents a framework that leverages transformer-based language models to perform causal inference using unstructured text. We demonstrate the effectiveness of our framework by comparing causal estimates derived from unstructured text against those obtained from structured data across population, group, and individual levels. Our findings show consistent results between the two approaches, validating the potential of unstructured text in causal inference tasks. Our approach extends the applicability of causal inference methods to scenarios where only textual data is available, enabling data-driven business decision-making when structured tabular data is scarce.",
      "pdf_url": "https://arxiv.org/pdf/2602.14274v1",
      "arxiv_url": "http://arxiv.org/abs/2602.14274v1",
      "published": "2026-02-15",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "EgoSound: Benchmarking Sound Understanding in Egocentric Videos",
      "authors": [
        "Bingwen Zhu",
        "Yuqian Fu",
        "Qiaole Dong",
        "Guolei Sun",
        "Tianwen Qian",
        "Yuzheng Wu",
        "Danda Pani Paudel",
        "Xiangyang Xue",
        "Yanwei Fu"
      ],
      "abstract": "Multimodal Large Language Models (MLLMs) have recently achieved remarkable progress in vision-language understanding. Yet, human perception is inherently multisensory, integrating sight, sound, and motion to reason about the world. Among these modalities, sound provides indispensable cues about spatial layout, off-screen events, and causal interactions, particularly in egocentric settings where auditory and visual signals are tightly coupled. To this end, we introduce EgoSound, the first benchmark designed to systematically evaluate egocentric sound understanding in MLLMs. EgoSound unifies data from Ego4D and EgoBlind, encompassing both sighted and sound-dependent experiences. It defines a seven-task taxonomy spanning intrinsic sound perception, spatial localization, causal inference, and cross-modal reasoning. Constructed through a multi-stage auto-generative pipeline, EgoSound contains 7315 validated QA pairs across 900 videos. Comprehensive experiments on nine state-of-the-art MLLMs reveal that current models exhibit emerging auditory reasoning abilities but remain limited in fine-grained spatial and causal understanding. EgoSound establishes a challenging foundation for advancing multisensory egocentric intelligence, bridging the gap between seeing and truly hearing the world.",
      "pdf_url": "https://arxiv.org/pdf/2602.14122v1",
      "arxiv_url": "http://arxiv.org/abs/2602.14122v1",
      "published": "2026-02-15",
      "categories": [
        "cs.CV"
      ]
    },
    {
      "title": "BRAIN: Bayesian Reasoning via Active Inference for Agentic and Embodied Intelligence in Mobile Networks",
      "authors": [
        "Osman Tugay Basaran",
        "Martin Maier",
        "Falko Dressler"
      ],
      "abstract": "Future sixth-generation (6G) mobile networks will demand artificial intelligence (AI) agents that are not only autonomous and efficient, but also capable of real-time adaptation in dynamic environments and transparent in their decisionmaking. However, prevailing agentic AI approaches in networking, exhibit significant shortcomings in this regard. Conventional deep reinforcement learning (DRL)-based agents lack explainability and often suffer from brittle adaptation, including catastrophic forgetting of past knowledge under non-stationary conditions. In this paper, we propose an alternative solution for these challenges: Bayesian reasoning via Active Inference (BRAIN) agent. BRAIN harnesses a deep generative model of the network environment and minimizes variational free energy to unify perception and action in a single closed-loop paradigm. We implement BRAIN as O-RAN eXtended application (xApp) on GPU-accelerated testbed and demonstrate its advantages over standard DRL baselines. In our experiments, BRAIN exhibits (i) robust causal reasoning for dynamic radio resource allocation, maintaining slice-specific quality of service (QoS) targets (throughput, latency, reliability) under varying traffic loads, (ii) superior adaptability with up to 28.3% higher robustness to sudden traffic shifts versus benchmarks (achieved without any retraining), and (iii) real-time interpretability of its decisions through human-interpretable belief state diagnostics.",
      "pdf_url": "https://arxiv.org/pdf/2602.14033v1",
      "arxiv_url": "http://arxiv.org/abs/2602.14033v1",
      "published": "2026-02-15",
      "categories": [
        "cs.IT"
      ]
    }
  ]
}