{
  "last_updated": "2025-05-07T00:53:03.631849",
  "papers": [
    {
      "title": "Multi-View Learning with Context-Guided Receptance for Image Denoising",
      "authors": [
        "Binghong Chen",
        "Tingting Chai",
        "Wei Jiang",
        "Yuanrong Xu",
        "Guanglu Zhou",
        "Xiangqian Wu"
      ],
      "abstract": "Image denoising is essential in low-level vision applications such as\nphotography and automated driving. Existing methods struggle with\ndistinguishing complex noise patterns in real-world scenes and consume\nsignificant computational resources due to reliance on Transformer-based\nmodels. In this work, the Context-guided Receptance Weighted Key-Value (\\M)\nmodel is proposed, combining enhanced multi-view feature integration with\nefficient sequence modeling. Our approach introduces the Context-guided Token\nShift (CTS) paradigm, which effectively captures local spatial dependencies and\nenhance the model's ability to model real-world noise distributions.\nAdditionally, the Frequency Mix (FMix) module extracting frequency-domain\nfeatures is designed to isolate noise in high-frequency spectra, and is\nintegrated with spatial representations through a multi-view learning process.\nTo improve computational efficiency, the Bidirectional WKV (BiWKV) mechanism is\nadopted, enabling full pixel-sequence interaction with linear complexity while\novercoming the causal selection constraints. The model is validated on multiple\nreal-world image denoising datasets, outperforming the existing\nstate-of-the-art methods quantitatively and reducing inference time up to 40\\%.\nQualitative results further demonstrate the ability of our model to restore\nfine details in various scenes.",
      "pdf_url": "http://arxiv.org/pdf/2505.02705v1",
      "arxiv_url": "http://arxiv.org/abs/2505.02705v1",
      "published": "2025-05-05",
      "categories": [
        "eess.IV",
        "cs.CV"
      ]
    },
    {
      "title": "Structure Causal Models and LLMs Integration in Medical Visual Question Answering",
      "authors": [
        "Zibo Xu",
        "Qiang Li",
        "Weizhi Nie",
        "Weijie Wang",
        "Anan Liu"
      ],
      "abstract": "Medical Visual Question Answering (MedVQA) aims to answer medical questions\naccording to medical images. However, the complexity of medical data leads to\nconfounders that are difficult to observe, so bias between images and questions\nis inevitable. Such cross-modal bias makes it challenging to infer medically\nmeaningful answers. In this work, we propose a causal inference framework for\nthe MedVQA task, which effectively eliminates the relative confounding effect\nbetween the image and the question to ensure the precision of the\nquestion-answering (QA) session. We are the first to introduce a novel causal\ngraph structure that represents the interaction between visual and textual\nelements, explicitly capturing how different questions influence visual\nfeatures. During optimization, we apply the mutual information to discover\nspurious correlations and propose a multi-variable resampling front-door\nadjustment method to eliminate the relative confounding effect, which aims to\nalign features based on their true causal relevance to the question-answering\ntask. In addition, we also introduce a prompt strategy that combines multiple\nprompt forms to improve the model's ability to understand complex medical data\nand answer accurately. Extensive experiments on three MedVQA datasets\ndemonstrate that 1) our method significantly improves the accuracy of MedVQA,\nand 2) our method achieves true causal correlations in the face of complex\nmedical data.",
      "pdf_url": "http://arxiv.org/pdf/2505.02703v1",
      "arxiv_url": "http://arxiv.org/abs/2505.02703v1",
      "published": "2025-05-05",
      "categories": [
        "cs.CV"
      ]
    },
    {
      "title": "Bayesian inference for cluster-randomized trials with multivariate outcomes subject to both truncation by death and missingness",
      "authors": [
        "Guangyu Tong",
        "Chenxi Li",
        "Eric Velazquez",
        "Michael O. Harhay",
        "Fan Li"
      ],
      "abstract": "Cluster-randomized trials (CRTs) on fragile populations frequently encounter\ncomplex attrition problems where the reasons for missing outcomes can be\nheterogeneous, with participants who are known alive, known to have died, or\nwith unknown survival status, and with complex and distinct missing data\nmechanisms for each group. Although existing methods have been developed to\naddress death truncation in CRTs, no existing methods can jointly accommodate\nparticipants who drop out for reasons unrelated to mortality or serious\nillnesses, or those with an unknown survival status. This paper proposes a\nBayesian framework for estimating survivor average causal effects in CRTs while\naccounting for different types of missingness. Our approach uses a multivariate\noutcome that jointly estimates the causal effects, and in the posterior\nestimates, we distinguish the individual-level and the cluster-level survivor\naverage causal effect. We perform simulation studies to evaluate the\nperformance of our model and found low bias and high coverage on key parameters\nacross several different scenarios. We use data from a geriatric CRT to\nillustrate the use of our model. Although our illustration focuses on the case\nof a bivariate continuous outcome, our model is straightforwardly extended to\naccommodate more than two endpoints as well as other types of endpoints (e.g.,\nbinary). Thus, this work provides a general modeling framework for handling\ncomplex missingness in CRTs and can be applied to a wide range of settings with\naging and palliative care populations.",
      "pdf_url": "http://arxiv.org/pdf/2505.02310v1",
      "arxiv_url": "http://arxiv.org/abs/2505.02310v1",
      "published": "2025-05-05",
      "categories": [
        "stat.ME"
      ]
    },
    {
      "title": "Federated Causal Inference in Healthcare: Methods, Challenges, and Applications",
      "authors": [
        "Haoyang Li",
        "Jie Xu",
        "Kyra Gan",
        "Fei Wang",
        "Chengxi Zang"
      ],
      "abstract": "Federated causal inference enables multi-site treatment effect estimation\nwithout sharing individual-level data, offering a privacy-preserving solution\nfor real-world evidence generation. However, data heterogeneity across sites,\nmanifested in differences in covariate, treatment, and outcome, poses\nsignificant challenges for unbiased and efficient estimation. In this paper, we\npresent a comprehensive review and theoretical analysis of federated causal\neffect estimation across both binary/continuous and time-to-event outcomes. We\nclassify existing methods into weight-based strategies and optimization-based\nframeworks and further discuss extensions including personalized models,\npeer-to-peer communication, and model decomposition. For time-to-event\noutcomes, we examine federated Cox and Aalen-Johansen models, deriving\nasymptotic bias and variance under heterogeneity. Our analysis reveals that\nFedProx-style regularization achieves near-optimal bias-variance trade-offs\ncompared to naive averaging and meta-analysis. We review related software tools\nand conclude by outlining opportunities, challenges, and future directions for\nscalable, fair, and trustworthy federated causal inference in distributed\nhealthcare systems.",
      "pdf_url": "http://arxiv.org/pdf/2505.02238v1",
      "arxiv_url": "http://arxiv.org/abs/2505.02238v1",
      "published": "2025-05-04",
      "categories": [
        "cs.LG"
      ]
    },
    {
      "title": "TV-SurvCaus: Dynamic Representation Balancing for Causal Survival Analysis",
      "authors": [
        "Ayoub Abraich"
      ],
      "abstract": "Estimating the causal effect of time-varying treatments on survival outcomes\nis a challenging task in many domains, particularly in medicine where treatment\nprotocols adapt over time. While recent advances in representation learning\nhave improved causal inference for static treatments, extending these methods\nto dynamic treatment regimes with survival outcomes remains under-explored. In\nthis paper, we introduce TV-SurvCaus, a novel framework that extends\nrepresentation balancing techniques to the time-varying treatment setting for\nsurvival analysis. We provide theoretical guarantees through (1) a generalized\nbound for time-varying precision in estimation of heterogeneous effects, (2)\nvariance control via sequential balancing weights, (3) consistency results for\ndynamic treatment regimes, (4) convergence rates for representation learning\nwith temporal dependencies, and (5) a formal bound on the bias due to\ntreatment-confounder feedback. Our neural architecture incorporates sequence\nmodeling to handle temporal dependencies while balancing time-dependent\nrepresentations. Through extensive experiments on both synthetic and real-world\ndatasets, we demonstrate that TV-SurvCaus outperforms existing methods in\nestimating individualized treatment effects with time-varying covariates and\ntreatments. Our framework advances the field of causal inference by enabling\nmore accurate estimation of treatment effects in dynamic, longitudinal settings\nwith survival outcomes.",
      "pdf_url": "http://arxiv.org/pdf/2505.01785v1",
      "arxiv_url": "http://arxiv.org/abs/2505.01785v1",
      "published": "2025-05-03",
      "categories": [
        "stat.ML",
        "cs.LG"
      ]
    },
    {
      "title": "Causally Fair Node Classification on Non-IID Graph Data",
      "authors": [
        "Yucong Dai",
        "Lu Zhang",
        "Yaowei Hu",
        "Susan Gauch",
        "Yongkai Wu"
      ],
      "abstract": "Fair machine learning seeks to identify and mitigate biases in predictions\nagainst unfavorable populations characterized by demographic attributes, such\nas race and gender. Recently, a few works have extended fairness to graph data,\nsuch as social networks, but most of them neglect the causal relationships\namong data instances. This paper addresses the prevalent challenge in\nfairness-aware ML algorithms, which typically assume Independent and\nIdentically Distributed (IID) data. We tackle the overlooked domain of non-IID,\ngraph-based settings where data instances are interconnected, influencing the\noutcomes of fairness interventions. We base our research on the Network\nStructural Causal Model (NSCM) framework and posit two main assumptions:\nDecomposability and Graph Independence, which enable the computation of\ninterventional distributions in non-IID settings using the $do$-calculus. Based\non that, we develop the Message Passing Variational Autoencoder for Causal\nInference (MPVA) to compute interventional distributions and facilitate\ncausally fair node classification through estimated interventional\ndistributions. Empirical evaluations on semi-synthetic and real-world datasets\ndemonstrate that MPVA outperforms conventional methods by effectively\napproximating interventional distributions and mitigating bias. The\nimplications of our findings underscore the potential of causality-based\nfairness in complex ML applications, setting the stage for further research\ninto relaxing the initial assumptions to enhance model fairness.",
      "pdf_url": "http://arxiv.org/pdf/2505.01652v1",
      "arxiv_url": "http://arxiv.org/abs/2505.01652v1",
      "published": "2025-05-03",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Design-Based Inference under Random Potential Outcomes via Riesz Representation",
      "authors": [
        "Yukai Yang"
      ],
      "abstract": "We introduce a general framework for design-based causal inference that\naccommodates stochastic potential outcomes, thereby extending the classical\nNeyman-Rubin setup in which outcomes are treated as fixed. In our formulation,\neach unit's potential outcome is modelled as a function $\\tilde{y}_i(z,\n\\omega)$, where $\\omega$ denotes latent randomness external to the treatment\nassignment. Building on recent work that connects design-based estimation with\nthe Riesz representation theorem, we construct causal estimators by embedding\npotential outcomes in a Hilbert space and defining treatment effects as linear\nfunctionals. This allows us to derive unbiased and consistent estimators, even\nwhen potential outcomes exhibit random variation. The framework retains the key\nadvantage of design-based analysis, namely, the use of a known randomisation\nscheme for identification, while enabling inference in settings with inherent\nstochasticity. We establish large-sample properties under local dependence,\nprovide a variance estimator compatible with sparse dependency structures, and\nillustrate the method through a simulation. Our results unify design-based\nreasoning with random-outcome modelling, broadening the applicability of causal\ninference in complex experimental environments.",
      "pdf_url": "http://arxiv.org/pdf/2505.01324v2",
      "arxiv_url": "http://arxiv.org/abs/2505.01324v2",
      "published": "2025-05-02",
      "categories": [
        "stat.ME",
        "econ.EM",
        "math.ST",
        "stat.TH",
        "62G20, 62K99, 62D05"
      ]
    },
    {
      "title": "Robust Root Cause Diagnosis using In-Distribution Interventions",
      "authors": [
        "Lokesh Nagalapatti",
        "Ashutosh Srivastava",
        "Sunita Sarawagi",
        "Amit Sharma"
      ],
      "abstract": "Diagnosing the root cause of an anomaly in a complex interconnected system is\na pressing problem in today's cloud services and industrial operations. We\npropose In-Distribution Interventions (IDI), a novel algorithm that predicts\nroot cause as nodes that meet two criteria: 1) **Anomaly:** root cause nodes\nshould take on anomalous values; 2) **Fix:** had the root cause nodes assumed\nusual values, the target node would not have been anomalous. Prior methods of\nassessing the fix condition rely on counterfactuals inferred from a Structural\nCausal Model (SCM) trained on historical data. But since anomalies are rare and\nfall outside the training distribution, the fitted SCMs yield unreliable\ncounterfactual estimates. IDI overcomes this by relying on interventional\nestimates obtained by solely probing the fitted SCM at in-distribution inputs.\nWe present a theoretical analysis comparing and bounding the errors in\nassessing the fix condition using interventional and counterfactual estimates.\nWe then conduct experiments by systematically varying the SCM's complexity to\ndemonstrate the cases where IDI's interventional approach outperforms the\ncounterfactual approach and vice versa. Experiments on both synthetic and\nPetShop RCD benchmark datasets demonstrate that \\our\\ consistently identifies\ntrue root causes more accurately and robustly than nine existing\nstate-of-the-art RCD baselines. Code is released at\nhttps://github.com/nlokeshiisc/IDI_release.",
      "pdf_url": "http://arxiv.org/pdf/2505.00930v1",
      "arxiv_url": "http://arxiv.org/abs/2505.00930v1",
      "published": "2025-05-02",
      "categories": [
        "cs.LG"
      ]
    },
    {
      "title": "Q-Learning with Clustered-SMART (cSMART) Data: Examining Moderators in the Construction of Clustered Adaptive Interventions",
      "authors": [
        "Yao Song",
        "Kelly Speth",
        "Amy Kilbourne",
        "Andrew Quanbeck",
        "Daniel Almirall",
        "Lu Wang"
      ],
      "abstract": "A clustered adaptive intervention (cAI) is a pre-specified sequence of\ndecision rules that guides practitioners on how best - and based on which\nmeasures - to tailor cluster-level intervention to improve outcomes at the\nlevel of individuals within the clusters. A clustered sequential multiple\nassignment randomized trial (cSMART) is a type of trial that is used to inform\nthe empirical development of a cAI. The most common type of secondary aim in a\ncSMART focuses on assessing causal effect moderation by candidate tailoring\nvariables. We introduce a clustered Q-learning framework with the M-out-of-N\nCluster Bootstrap using data from a cSMART to evaluate whether a set of\ncandidate tailoring variables may be useful in defining an optimal cAI. This\napproach could construct confidence intervals (CI) with near-nominal coverage\nto assess parameters indexing the causal effect moderation function.\nSpecifically, it allows reliable inferences concerning the utility of candidate\ntailoring variables in constructing a cAI that maximizes a mean end-of-study\noutcome even when \"non-regularity\", a well-known challenge exists. Simulations\ndemonstrate the numerical performance of the proposed method across varying\nnon-regularity conditions and investigate the impact of varying number of\nclusters and intra-cluster correlation coefficient on CI coverage. Methods are\napplied on ADEPT dataset to inform the construction of a clinic-level cAI for\nimproving evidence-based practice in treating mood disorders.",
      "pdf_url": "http://arxiv.org/pdf/2505.00822v1",
      "arxiv_url": "http://arxiv.org/abs/2505.00822v1",
      "published": "2025-05-01",
      "categories": [
        "stat.ME",
        "cs.LG",
        "stat.ML"
      ]
    },
    {
      "title": "Dual Filter: A Mathematical Framework for Inference using Transformer-like Architectures",
      "authors": [
        "Heng-Sheng Chang",
        "Prashant G. Mehta"
      ],
      "abstract": "This paper presents a mathematical framework for causal nonlinear prediction\nin settings where observations are generated from an underlying hidden Markov\nmodel (HMM). Both the problem formulation and the proposed solution are\nmotivated by the decoder-only transformer architecture, in which a finite\nsequence of observations (tokens) is mapped to the conditional probability of\nthe next token. Our objective is not to construct a mathematical model of a\ntransformer. Rather, our interest lies in deriving, from first principles,\ntransformer-like architectures that solve the prediction problem for which the\ntransformer is designed. The proposed framework is based on an original optimal\ncontrol approach, where the prediction objective (MMSE) is reformulated as an\noptimal control problem. An analysis of the optimal control problem is\npresented leading to a fixed-point equation on the space of probability\nmeasures. To solve the fixed-point equation, we introduce the dual filter, an\niterative algorithm that closely parallels the architecture of decoder-only\ntransformers. These parallels are discussed in detail along with the\nrelationship to prior work on mathematical modeling of transformers as\ntransport on the space of probability measures. Numerical experiments are\nprovided to illustrate the performance of the algorithm using parameter values\nused in researchscale transformer models.",
      "pdf_url": "http://arxiv.org/pdf/2505.00818v1",
      "arxiv_url": "http://arxiv.org/abs/2505.00818v1",
      "published": "2025-05-01",
      "categories": [
        "cs.LG",
        "cs.SY",
        "eess.SY",
        "math.PR"
      ]
    }
  ]
}