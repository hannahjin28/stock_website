{
  "last_updated": "2025-11-18T00:53:50.763011",
  "papers": [
    {
      "title": "Estimating the Effects of Heatwaves on Health: A Causal Inference Framework",
      "authors": [
        "Giulio Grossi",
        "Leo Vanciu",
        "Veronica Ballerini",
        "Danielle Braun",
        "Falco J. Bargagli Stoffi"
      ],
      "abstract": "The harmful relationship between heatwaves and health has been extensively documented in medical and epidemiological literature. However, most evidence is associational and cannot be interpreted causally unless strong assumptions are made. In this paper, we first make explicit the assumptions underlying the statistical methods frequently used in the heatwave literature and demonstrate when these assumptions might break down in heatwave contexts. To address these shortcomings, we propose a causal inference framework that transparently elicits causal identification assumptions. Within this new framework, we first introduce synthetic controls (SC) for estimating heatwave effects, then propose a spatially augmented Bayesian synthetic control (SA-SC) method that accounts for spatial dependence and spillovers. Empirical Monte Carlo simulations show both methods perform well, with SA-SC reducing root mean squared error and improving posterior interval coverage under spillovers and spatial dependence. Finally, we apply the proposed methods to estimate the causal effects of heatwaves on Medicare heat-related hospitalizations among 13,753,273 beneficiaries residing in Northeastern U.S. from 2000 to 2019. This causal inference framework provides spatially coherent counterfactual outcomes and robust, interpretable, and transparent causal estimates while explicitly addressing the unexamined assumptions in existing methods that pervade the heatwave effect literature.",
      "pdf_url": "https://arxiv.org/pdf/2511.11433v1",
      "arxiv_url": "http://arxiv.org/abs/2511.11433v1",
      "published": "2025-11-14",
      "categories": [
        "stat.ME",
        "stat.AP"
      ]
    },
    {
      "title": "Interpolated stochastic interventions based on propensity scores, target policies and treatment-specific costs",
      "authors": [
        "Johan de Aguas"
      ],
      "abstract": "We introduce families of stochastic interventions for discrete treatments that connect causal modeling to cost-sensitive decision making. The interventions arise from a cost-penalized information projection of the independent product of the organic propensity and a user-specified target, yielding closed-form Boltzmann-Gibbs couplings. The induced marginals define modified stochastic policies that interpolate smoothly, via a single tilt parameter, from the organic law or from the target distribution toward a product-of-experts limit when all destination costs are strictly positive. One of these families recovers and extends incremental propensity score interventions, retaining identification without global positivity. For inference, we derive efficient influence functions under a nonparametric model for the expected outcomes after these policies and construct one-step estimators with uniform confidence bands. In simulations, the proposed estimators improve stability and robustness to nuisance misspecification relative to plug-in baselines. The framework can operationalize graded scientific hypotheses under realistic constraints: because inputs are modular, analysts can sweep feasible policy spaces, prototype candidates, and align interventions with budgets and logistics before committing experimental resources. This could help close the loop between observational evidence and resource-aware experimental design.",
      "pdf_url": "https://arxiv.org/pdf/2511.11353v1",
      "arxiv_url": "http://arxiv.org/abs/2511.11353v1",
      "published": "2025-11-14",
      "categories": [
        "stat.ME"
      ]
    },
    {
      "title": "MeCaMIL: Causality-Aware Multiple Instance Learning for Fair and Interpretable Whole Slide Image Diagnosis",
      "authors": [
        "Yiran Song",
        "Yikai Zhang",
        "Shuang Zhou",
        "Guojun Xiong",
        "Xiaofeng Yang",
        "Nian Wang",
        "Fenglong Ma",
        "Rui Zhang",
        "Mingquan Lin"
      ],
      "abstract": "Multiple instance learning (MIL) has emerged as the dominant paradigm for whole slide image (WSI) analysis in computational pathology, achieving strong diagnostic performance through patch-level feature aggregation. However, existing MIL methods face critical limitations: (1) they rely on attention mechanisms that lack causal interpretability, and (2) they fail to integrate patient demographics (age, gender, race), leading to fairness concerns across diverse populations. These shortcomings hinder clinical translation, where algorithmic bias can exacerbate health disparities. We introduce \\textbf{MeCaMIL}, a causality-aware MIL framework that explicitly models demographic confounders through structured causal graphs. Unlike prior approaches treating demographics as auxiliary features, MeCaMIL employs principled causal inference -- leveraging do-calculus and collider structures -- to disentangle disease-relevant signals from spurious demographic correlations. Extensive evaluation on three benchmarks demonstrates state-of-the-art performance across CAMELYON16 (ACC/AUC/F1: 0.939/0.983/0.946), TCGA-Lung (0.935/0.979/0.931), and TCGA-Multi (0.977/0.993/0.970, five cancer types). Critically, MeCaMIL achieves superior fairness -- demographic disparity variance drops by over 65% relative reduction on average across attributes, with notable improvements for underserved populations. The framework generalizes to survival prediction (mean C-index: 0.653, +0.017 over best baseline across five cancer types). Ablation studies confirm causal graph structure is essential -- alternative designs yield 0.048 lower accuracy and 4.2x times worse fairness. These results establish MeCaMIL as a principled framework for fair, interpretable, and clinically actionable AI in digital pathology. Code will be released upon acceptance.",
      "pdf_url": "https://arxiv.org/pdf/2511.11004v1",
      "arxiv_url": "http://arxiv.org/abs/2511.11004v1",
      "published": "2025-11-14",
      "categories": [
        "cs.CV"
      ]
    },
    {
      "title": "GraphFaaS: Serverless GNN Inference for Burst-Resilient, Real-Time Intrusion Detection",
      "authors": [
        "Lingzhi Wang",
        "Vinod Yegneswaran",
        "Xinyi Shi",
        "Ziyu Li",
        "Ashish Gehani",
        "Yan Chen"
      ],
      "abstract": "Provenance-based intrusion detection is an increasingly popular application of graphical machine learning in cybersecurity, where system activities are modeled as provenance graphs to capture causality and correlations among potentially malicious actions. Graph Neural Networks (GNNs) have demonstrated strong performance in this setting. However, traditional statically-provisioned GNN inference architectures fall short in meeting two crucial demands of intrusion detection: (1) maintaining consistently low detection latency, and (2) handling highly irregular and bursty workloads. To holistically address these challenges, we present GraphFaaS, a serverless architecture tailored for GNN-based intrusion detection. GraphFaaS leverages the elasticity and agility of serverless computing to dynamically scale the GNN inference pipeline. We parallelize and adapt GNN workflows to a serverless environment, ensuring that the system can respond in real time to fluctuating workloads. By decoupling compute resources from static provisioning, GraphFaaS delivers stable inference latency, which is critical for dependable intrusion detection and timely incident response in cybersecurity operations. Preliminary evaluation shows GraphFaaS reduces average detection latency by 85% and coefficient of variation (CV) by 64% compared to the baseline.",
      "pdf_url": "https://arxiv.org/pdf/2511.10554v1",
      "arxiv_url": "http://arxiv.org/abs/2511.10554v1",
      "published": "2025-11-13",
      "categories": [
        "cs.CR"
      ]
    },
    {
      "title": "Causal Model-Based Reinforcement Learning for Sample-Efficient IoT Channel Access",
      "authors": [
        "Aswin Arun",
        "Christo Kurisummoottil Thomas",
        "Rimalpudi Sarvendranath",
        "Walid Saad"
      ],
      "abstract": "Despite the advantages of multi-agent reinforcement learning (MARL) for wireless use case such as medium access control (MAC), their real-world deployment in Internet of Things (IoT) is hindered by their sample inefficiency. To alleviate this challenge, one can leverage model-based reinforcement learning (MBRL) solutions, however, conventional MBRL approaches rely on black-box models that are not interpretable and cannot reason. In contrast, in this paper, a novel causal model-based MARL framework is developed by leveraging tools from causal learn- ing. In particular, the proposed model can explicitly represent causal dependencies between network variables using structural causal models (SCMs) and attention-based inference networks. Interpretable causal models are then developed to capture how MAC control messages influence observations, how transmission actions determine outcomes, and how channel observations affect rewards. Data augmentation techniques are then used to generate synthetic rollouts using the learned causal model for policy optimization via proximal policy optimization (PPO). Analytical results demonstrate exponential sample complexity gains of causal MBRL over black-box approaches. Extensive simulations demonstrate that, on average, the proposed approach can reduce environment interactions by 58%, and yield faster convergence compared to model-free baselines. The proposed approach inherently is also shown to provide interpretable scheduling decisions via attention-based causal attribution, revealing which network conditions drive the policy. The resulting combination of sample efficiency and interpretability establishes causal MBRL as a practical approach for resource-constrained wireless systems.",
      "pdf_url": "https://arxiv.org/pdf/2511.10291v1",
      "arxiv_url": "http://arxiv.org/abs/2511.10291v1",
      "published": "2025-11-13",
      "categories": [
        "cs.IT",
        "cs.LG",
        "cs.NI"
      ]
    },
    {
      "title": "Temporal Latent Variable Structural Causal Model for Causal Discovery under External Interferences",
      "authors": [
        "Ruichu Cai",
        "Xiaokai Huang",
        "Wei Chen",
        "Zijian Li",
        "Zhifeng Hao"
      ],
      "abstract": "Inferring causal relationships from observed data is an important task, yet it becomes challenging when the data is subject to various external interferences. Most of these interferences are the additional effects of external factors on observed variables. Since these external factors are often unknown, we introduce latent variables to represent these unobserved factors that affect the observed data. Specifically, to capture the causal strength and adjacency information, we propose a new temporal latent variable structural causal model, incorporating causal strength and adjacency coefficients that represent the causal relationships between variables. Considering that expert knowledge can provide information about unknown interferences in certain scenarios, we develop a method that facilitates the incorporation of prior knowledge into parameter learning based on Variational Inference, to guide the model estimation. Experimental results demonstrate the stability and accuracy of our proposed method.",
      "pdf_url": "https://arxiv.org/pdf/2511.10031v1",
      "arxiv_url": "http://arxiv.org/abs/2511.10031v1",
      "published": "2025-11-13",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "AI-Integrated Decision Support System for Real-Time Market Growth Forecasting and Multi-Source Content Diffusion Analytics",
      "authors": [
        "Ziqing Yin",
        "Xuanjing Chen",
        "Xi Zhang"
      ],
      "abstract": "The rapid proliferation of AI-generated content (AIGC) has reshaped the dynamics of digital marketing and online consumer behavior. However, predicting the diffusion trajectory and market impact of such content remains challenging due to data heterogeneity, non linear propagation mechanisms, and evolving consumer interactions. This study proposes an AI driven Decision Support System (DSS) that integrates multi source data including social media streams, marketing expenditure records, consumer engagement logs, and sentiment dynamics using a hybrid Graph Neural Network (GNN) and Temporal Transformer framework. The model jointly learns the content diffusion structure and temporal influence evolution through a dual channel architecture, while causal inference modules disentangle the effects of marketing stimuli on return on investment (ROI) and market visibility. Experiments on large scale real-world datasets collected from multiple online platforms such as Twitter, TikTok, and YouTube advertising show that our system outperforms existing baselines in all six metrics. The proposed DSS enhances marketing decisions by providing interpretable real-time insights into AIGC driven content dissemination and market growth patterns.",
      "pdf_url": "https://arxiv.org/pdf/2511.09962v1",
      "arxiv_url": "http://arxiv.org/abs/2511.09962v1",
      "published": "2025-11-13",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.MM"
      ]
    },
    {
      "title": "NeuroLingua: A Language-Inspired Hierarchical Framework for Multimodal Sleep Stage Classification Using EEG and EOG",
      "authors": [
        "Mahdi Samaee",
        "Mehran Yazdi",
        "Daniel Massicotte"
      ],
      "abstract": "Automated sleep stage classification from polysomnography remains limited by the lack of expressive temporal hierarchies, challenges in multimodal EEG and EOG fusion, and the limited interpretability of deep learning models. We propose NeuroLingua, a language-inspired framework that conceptualizes sleep as a structured physiological language. Each 30-second epoch is decomposed into overlapping 3-second subwindows (\"tokens\") using a CNN-based tokenizer, enabling hierarchical temporal modeling through dual-level Transformers: intra-segment encoding of local dependencies and inter-segment integration across seven consecutive epochs (3.5 minutes) for extended context. Modality-specific embeddings from EEG and EOG channels are fused via a Graph Convolutional Network, facilitating robust multimodal integration. NeuroLingua is evaluated on the Sleep-EDF Expanded and ISRUC-Sleep datasets, achieving state-of-the-art results on Sleep-EDF (85.3% accuracy, 0.800 macro F1, and 0.796 Cohen's kappa) and competitive performance on ISRUC (81.9% accuracy, 0.802 macro F1, and 0.755 kappa), matching or exceeding published baselines in overall and per-class metrics. The architecture's attention mechanisms enhance the detection of clinically relevant sleep microevents, providing a principled foundation for future interpretability, explainability, and causal inference in sleep research. By framing sleep as a compositional language, NeuroLingua unifies hierarchical sequence modeling and multimodal fusion, advancing automated sleep staging toward more transparent and clinically meaningful applications.",
      "pdf_url": "https://arxiv.org/pdf/2511.09773v1",
      "arxiv_url": "http://arxiv.org/abs/2511.09773v1",
      "published": "2025-11-12",
      "categories": [
        "cs.LG",
        "eess.SP"
      ]
    },
    {
      "title": "Distributional Treatment Effect Estimation across Heterogeneous Sites via Optimal Transport",
      "authors": [
        "Borna Bateni",
        "Yubai Yuan",
        "Qi Xu",
        "Annie Qu"
      ],
      "abstract": "We propose a novel framework for synthesizing counterfactual treatment group data in a target site by integrating full treatment and control group data from a source site with control group data from the target. Departing from conventional average treatment effect estimation, our approach adopts a distributional causal inference perspective by modeling treatment and control as distinct probability measures on the source and target sites. We formalize the cross-site heterogeneity (effect modification) as a push-forward transformation that maps the joint feature-outcome distribution from the source to the target site. This transformation is learned by aligning the control group distributions between sites using an Optimal Transport-based procedure, and subsequently applied to the source treatment group to generate the synthetic target treatment distribution. Under general regularity conditions, we establish theoretical guarantees for the consistency and asymptotic convergence of the synthetic treatment group data to the true target distribution. Simulation studies across multiple data-generating scenarios and a real-world application to patient-derived xenograft data demonstrate that our framework robustly recovers the full distributional properties of treatment effects.",
      "pdf_url": "https://arxiv.org/pdf/2511.09759v1",
      "arxiv_url": "http://arxiv.org/abs/2511.09759v1",
      "published": "2025-11-12",
      "categories": [
        "stat.ME",
        "math.ST",
        "stat.ML"
      ]
    },
    {
      "title": "History Rhymes: Macro-Contextual Retrieval for Robust Financial Forecasting",
      "authors": [
        "Sarthak Khanna",
        "Armin Berger",
        "Muskaan Chopra",
        "Rafet Sifa"
      ],
      "abstract": "Financial markets are inherently non-stationary: structural breaks and macroeconomic regime shifts often cause forecasting models to fail when deployed out of distribution (OOD). Conventional multimodal approaches that simply fuse numerical indicators and textual sentiment rarely adapt to such shifts. We introduce macro-contextual retrieval, a retrieval-augmented forecasting framework that grounds each prediction in historically analogous macroeconomic regimes. The method jointly embeds macro indicators (e.g., CPI, unemployment, yield spread, GDP growth) and financial news sentiment in a shared similarity space, enabling causal retrieval of precedent periods during inference without retraining.\n  Trained on seventeen years of S&P 500 data (2007-2023) and evaluated OOD on AAPL (2024) and XOM (2024), the framework consistently narrows the CV to OOD performance gap. Macro-conditioned retrieval achieves the only positive out-of-sample trading outcomes (AAPL: PF=1.18, Sharpe=0.95; XOM: PF=1.16, Sharpe=0.61), while static numeric, text-only, and naive multimodal baselines collapse under regime shifts. Beyond metric gains, retrieved neighbors form interpretable evidence chains that correspond to recognizable macro contexts, such as inflationary or yield-curve inversion phases, supporting causal interpretability and transparency. By operationalizing the principle that \"financial history may not repeat, but it often rhymes,\" this work demonstrates that macro-aware retrieval yields robust, explainable forecasts under distributional change.\n  All datasets, models, and source code are publicly available.",
      "pdf_url": "https://arxiv.org/pdf/2511.09754v1",
      "arxiv_url": "http://arxiv.org/abs/2511.09754v1",
      "published": "2025-11-12",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    }
  ]
}