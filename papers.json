{
  "last_updated": "2025-09-18T00:48:00.861153",
  "papers": [
    {
      "title": "Semiparametric Causal Inference for Right-Censored Outcomes with Many Weak Invalid Instruments",
      "authors": [
        "Qiushi Bu",
        "Wen Su",
        "Xingqiu Zhao",
        "Zhonghua Liu"
      ],
      "abstract": "We propose a semiparametric framework for causal inference with\nright-censored survival outcomes and many weak invalid instruments, motivated\nby Mendelian randomization in biobank studies where classical methods may fail.\nWe adopt an accelerated failure time model and construct a moment condition\nbased on augmented inverse probability of censoring weighting, incorporating\nboth uncensored and censored observations. Under a heteroscedasticity-based\ncondition on the treatment model, we establish point identification of the\ncausal effect despite censoring and invalid instruments. We propose GEL-NOW\n(Generalized Empirical Likelihood with Non-Orthogonal and Weak moments) for\nvalid inference under these conditions. A divergent number of Neyman orthogonal\nnuisance functions is estimated using deep neural networks. A key challenge is\nthat the conditional censoring distribution is a non-Neyman orthogonal\nnuisance, contributing to the first-order asymptotics of the estimator for the\ntarget causal effect parameter. We derive the asymptotic distribution and\nexplicitly incorporate this additional uncertainty into the asymptotic variance\nformula. We also introduce a censoring-adjusted over-identification test that\naccounts for this variance component. Simulation studies and UK Biobank\napplications demonstrate the method's robustness and practical utility.",
      "pdf_url": "http://arxiv.org/pdf/2509.13176v1",
      "arxiv_url": "http://arxiv.org/abs/2509.13176v1",
      "published": "2025-09-16",
      "categories": [
        "stat.ME",
        "math.ST",
        "stat.TH"
      ]
    },
    {
      "title": "Robust Sensitivity Analysis via Augmented Percentile Bootstrap under Simultaneous Violations of Unconfoundedness and Overlap",
      "authors": [
        "Han Cui",
        "Xinran Li"
      ],
      "abstract": "The identification of causal effects in observational studies typically\nrelies on two standard assumptions: unconfoundedness and overlap. However, both\nassumptions are often questionable in practice: unconfoundedness is inherently\nuntestable, and overlap may fail in the presence of extreme unmeasured\nconfounding. While various approaches have been developed to address unmeasured\nconfounding and extreme propensity scores separately, few methods accommodate\nsimultaneous violations of both assumptions. In this paper, we propose a\nsensitivity analysis framework that relaxes both unconfoundedness and overlap,\nbuilding upon the marginal sensitivity model. Specifically, we allow the bound\non unmeasured confounding to hold for only a subset of the population, thereby\naccommodating heterogeneity in confounding and allowing treatment probabilities\nto be zero or one. Moreover, unlike prior work, our approach does not require\nbounded outcomes and focuses on overlap-weighted average treatment effects,\nwhich are both practically meaningful and robust to non-overlap. We develop\ncomputationally efficient methods to obtain worst-case bounds via linear\nprogramming, and introduce a novel augmented percentile bootstrap procedure for\nstatistical inference. This bootstrap method handles parameters defined through\nover-identified estimating equations involving unobserved variables and may be\nof independent interest. Our work provides a unified and flexible framework for\nsensitivity analysis under violations of both unconfoundedness and overlap.",
      "pdf_url": "http://arxiv.org/pdf/2509.13169v1",
      "arxiv_url": "http://arxiv.org/abs/2509.13169v1",
      "published": "2025-09-16",
      "categories": [
        "stat.ME"
      ]
    },
    {
      "title": "Inverse regression for causal inference with multiple outcomes",
      "authors": [
        "Wei Zhang",
        "Qizhai Li",
        "Peng Ding"
      ],
      "abstract": "With multiple outcomes in empirical research, a common strategy is to define\na composite outcome as a weighted average of the original outcomes. However,\nthe choices of weights are often subjective and can be controversial. We\npropose an inverse regression strategy for causal inference with multiple\noutcomes. The key idea is to regress the treatment on the outcomes, which is\nthe inverse of the standard regression of the outcomes on the treatment.\nAlthough this strategy is simple and even counterintuitive, it has several\nadvantages. First, testing for zero coefficients of the outcomes is equivalent\nto testing for the null hypothesis of zero effects, even though the inverse\nregression is deemed misspecified. Second, the coefficients of the outcomes\nprovide a data-driven choice of the weights for defining a composite outcome.\nWe also discuss the associated inference issues. Third, this strategy is\napplicable to general study designs. We illustrate the theory in both\nrandomized experiments and observational studies.",
      "pdf_url": "http://arxiv.org/pdf/2509.12587v1",
      "arxiv_url": "http://arxiv.org/abs/2509.12587v1",
      "published": "2025-09-16",
      "categories": [
        "stat.ME"
      ]
    },
    {
      "title": "Prediction and Causality of functional MRI and synthetic signal using a Zero-Shot Time-Series Foundation Model",
      "authors": [
        "Alessandro Crimi",
        "Andrea Brovelli"
      ],
      "abstract": "Time-series forecasting and causal discovery are central in neuroscience, as\npredicting brain activity and identifying causal relationships between neural\npopulations and circuits can shed light on the mechanisms underlying cognition\nand disease. With the rise of foundation models, an open question is how they\ncompare to traditional methods for brain signal forecasting and causality\nanalysis, and whether they can be applied in a zero-shot setting. In this work,\nwe evaluate a foundation model against classical methods for inferring\ndirectional interactions from spontaneous brain activity measured with\nfunctional magnetic resonance imaging (fMRI) in humans. Traditional approaches\noften rely on Wiener-Granger causality. We tested the forecasting ability of\nthe foundation model in both zero-shot and fine-tuned settings, and assessed\ncausality by comparing Granger-like estimates from the model with standard\nGranger causality. We validated the approach using synthetic time series\ngenerated from ground-truth causal models, including logistic map coupling and\nOrnstein-Uhlenbeck processes. The foundation model achieved competitive\nzero-shot forecasting fMRI time series (mean absolute percentage error of 0.55\nin controls and 0.27 in patients). Although standard Granger causality did not\nshow clear quantitative differences between models, the foundation model\nprovided a more precise detection of causal interactions.\n  Overall, these findings suggest that foundation models offer versatility,\nstrong zero-shot performance, and potential utility for forecasting and causal\ndiscovery in time-series data.",
      "pdf_url": "http://arxiv.org/pdf/2509.12497v2",
      "arxiv_url": "http://arxiv.org/abs/2509.12497v2",
      "published": "2025-09-15",
      "categories": [
        "cs.LG"
      ]
    },
    {
      "title": "Causal-Symbolic Meta-Learning (CSML): Inducing Causal World Models for Few-Shot Generalization",
      "authors": [
        "Mohamed Zayaan S"
      ],
      "abstract": "Modern deep learning models excel at pattern recognition but remain\nfundamentally limited by their reliance on spurious correlations, leading to\npoor generalization and a demand for massive datasets. We argue that a key\ningredient for human-like intelligence-robust, sample-efficient learning-stems\nfrom an understanding of causal mechanisms. In this work, we introduce\nCausal-Symbolic Meta-Learning (CSML), a novel framework that learns to infer\nthe latent causal structure of a task distribution. CSML comprises three key\nmodules: a perception module that maps raw inputs to disentangled symbolic\nrepresentations; a differentiable causal induction module that discovers the\nunderlying causal graph governing these symbols and a graph-based reasoning\nmodule that leverages this graph to make predictions. By meta-learning a shared\ncausal world model across a distribution of tasks, CSML can rapidly adapt to\nnovel tasks, including those requiring reasoning about interventions and\ncounterfactuals, from only a handful of examples. We introduce CausalWorld, a\nnew physics-based benchmark designed to test these capabilities. Our\nexperiments show that CSML dramatically outperforms state-of-the-art\nmeta-learning and neuro-symbolic baselines, particularly on tasks demanding\ntrue causal inference.",
      "pdf_url": "http://arxiv.org/pdf/2509.12387v1",
      "arxiv_url": "http://arxiv.org/abs/2509.12387v1",
      "published": "2025-09-15",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ]
    },
    {
      "title": "AvatarSync: Rethinking Talking-Head Animation through Autoregressive Perspective",
      "authors": [
        "Yuchen Deng",
        "Xiuyang Wu",
        "Hai-Tao Zheng",
        "Suiyang Zhang",
        "Yi He",
        "Yuxing Han"
      ],
      "abstract": "Existing talking-head animation approaches based on Generative Adversarial\nNetworks (GANs) or diffusion models often suffer from inter-frame flicker,\nidentity drift, and slow inference. These limitations inherent to their video\ngeneration pipelines restrict their suitability for applications. To address\nthis, we introduce AvatarSync, an autoregressive framework on phoneme\nrepresentations that generates realistic and controllable talking-head\nanimations from a single reference image, driven directly text or audio input.\nIn addition, AvatarSync adopts a two-stage generation strategy, decoupling\nsemantic modeling from visual dynamics, which is a deliberate \"Divide and\nConquer\" design. The first stage, Facial Keyframe Generation (FKG), focuses on\nphoneme-level semantic representation by leveraging the many-to-one mapping\nfrom text or audio to phonemes. A Phoneme-to-Visual Mapping is constructed to\nanchor abstract phonemes to character-level units. Combined with a customized\nText-Frame Causal Attention Mask, the keyframes are generated. The second\nstage, inter-frame interpolation, emphasizes temporal coherence and visual\nsmoothness. We introduce a timestamp-aware adaptive strategy based on a\nselective state space model, enabling efficient bidirectional context\nreasoning. To support deployment, we optimize the inference pipeline to reduce\nlatency without compromising visual fidelity. Extensive experiments show that\nAvatarSync outperforms existing talking-head animation methods in visual\nfidelity, temporal consistency, and computational efficiency, providing a\nscalable and controllable solution.",
      "pdf_url": "http://arxiv.org/pdf/2509.12052v1",
      "arxiv_url": "http://arxiv.org/abs/2509.12052v1",
      "published": "2025-09-15",
      "categories": [
        "cs.CV"
      ]
    },
    {
      "title": "A Survey of Reasoning and Agentic Systems in Time Series with Large Language Models",
      "authors": [
        "Ching Chang",
        "Yidan Shi",
        "Defu Cao",
        "Wei Yang",
        "Jeehyun Hwang",
        "Haixin Wang",
        "Jiacheng Pang",
        "Wei Wang",
        "Yan Liu",
        "Wen-Chih Peng",
        "Tien-Fu Chen"
      ],
      "abstract": "Time series reasoning treats time as a first-class axis and incorporates\nintermediate evidence directly into the answer. This survey defines the problem\nand organizes the literature by reasoning topology with three families: direct\nreasoning in one step, linear chain reasoning with explicit intermediates, and\nbranch-structured reasoning that explores, revises, and aggregates. The\ntopology is crossed with the main objectives of the field, including\ntraditional time series analysis, explanation and understanding, causal\ninference and decision making, and time series generation, while a compact tag\nset spans these axes and captures decomposition and verification, ensembling,\ntool use, knowledge access, multimodality, agent loops, and LLM alignment\nregimes. Methods and systems are reviewed across domains, showing what each\ntopology enables and where it breaks down in faithfulness or robustness, along\nwith curated datasets, benchmarks, and resources that support study and\ndeployment (https://github.com/blacksnail789521/Time-Series-Reasoning-Survey).\nEvaluation practices that keep evidence visible and temporally aligned are\nhighlighted, and guidance is distilled on matching topology to uncertainty,\ngrounding with observable artifacts, planning for shift and streaming, and\ntreating cost and latency as design budgets. We emphasize that reasoning\nstructures must balance capacity for grounding and self-correction against\ncomputational cost and reproducibility, while future progress will likely\ndepend on benchmarks that tie reasoning quality to utility and on closed-loop\ntestbeds that trade off cost and risk under shift-aware, streaming, and\nlong-horizon settings. Taken together, these directions mark a shift from\nnarrow accuracy toward reliability at scale, enabling systems that not only\nanalyze but also understand, explain, and act on dynamic worlds with traceable\nevidence and credible outcomes.",
      "pdf_url": "http://arxiv.org/pdf/2509.11575v1",
      "arxiv_url": "http://arxiv.org/abs/2509.11575v1",
      "published": "2025-09-15",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Mendelian Randomization Methods for Causal Inference: Estimands, Identification and Inference",
      "authors": [
        "Minhao Yao",
        "Anqi Wang",
        "Xihao Li",
        "Zhonghua Liu"
      ],
      "abstract": "Mendelian randomization (MR) has become an essential tool for causal\ninference in biomedical and public health research. By using genetic variants\nas instrumental variables, MR helps address unmeasured confounding and reverse\ncausation, offering a quasi-experimental framework to evaluate causal effects\nof modifiable exposures on health outcomes. Despite its promise, MR faces\nsubstantial methodological challenges, including invalid instruments, weak\ninstrument bias, and design complexities across different data structures. In\nthis tutorial review, we provide a comprehensive overview of MR methods for\ncausal inference, emphasizing clarity of causal interpretation, study design\ncomparisons, availability of software tools, and practical guidance for applied\nscientists. We organize the review around causal estimands, ensuring that\nanalyses are anchored to well-defined causal questions. We discuss the problems\nof invalid and weak instruments, comparing available strategies for their\ndetection and correction. We integrate discussions of population-based versus\nfamily-based MR designs, analyses based on individual-level versus\nsummary-level data, and one-sample versus two-sample MR designs, highlighting\ntheir relative advantages and limitations. We also summarize recent\nmethodological advances and software developments that extend MR to settings\nwith many weak or invalid instruments and to modern high-dimensional omics\ndata. Real-data applications, including UK Biobank and Alzheimer's disease\nproteomics studies, illustrate the use of these methods in practice. This\nreview aims to serve as a tutorial-style reference for both methodologists and\napplied scientists.",
      "pdf_url": "http://arxiv.org/pdf/2509.11519v1",
      "arxiv_url": "http://arxiv.org/abs/2509.11519v1",
      "published": "2025-09-15",
      "categories": [
        "stat.ME",
        "stat.AP"
      ]
    },
    {
      "title": "The Honest Truth About Causal Trees: Accuracy Limits for Heterogeneous Treatment Effect Estimation",
      "authors": [
        "Matias D. Cattaneo",
        "Jason M. Klusowski",
        "Ruiqi Rae Yu"
      ],
      "abstract": "Recursive decision trees have emerged as a leading methodology for\nheterogeneous causal treatment effect estimation and inference in experimental\nand observational settings. These procedures are fitted using the celebrated\nCART (Classification And Regression Tree) algorithm [Breiman et al., 1984], or\ncustom variants thereof, and hence are believed to be \"adaptive\" to\nhigh-dimensional data, sparsity, or other specific features of the underlying\ndata generating process. Athey and Imbens [2016] proposed several \"honest\"\ncausal decision tree estimators, which have become the standard in both\nacademia and industry. We study their estimators, and variants thereof, and\nestablish lower bounds on their estimation error. We demonstrate that these\npopular heterogeneous treatment effect estimators cannot achieve a\npolynomial-in-$n$ convergence rate under basic conditions, where $n$ denotes\nthe sample size. Contrary to common belief, honesty does not resolve these\nlimitations and at best delivers negligible logarithmic improvements in sample\nsize or dimension. As a result, these commonly used estimators can exhibit poor\nperformance in practice, and even be inconsistent in some settings. Our\ntheoretical insights are empirically validated through simulations.",
      "pdf_url": "http://arxiv.org/pdf/2509.11381v1",
      "arxiv_url": "http://arxiv.org/abs/2509.11381v1",
      "published": "2025-09-14",
      "categories": [
        "math.ST",
        "econ.EM",
        "stat.ME",
        "stat.ML",
        "stat.TH"
      ]
    },
    {
      "title": "PersonaX: Multimodal Datasets with LLM-Inferred Behavior Traits",
      "authors": [
        "Loka Li",
        "Wong Yu Kang",
        "Minghao Fu",
        "Guangyi Chen",
        "Zhenhao Chen",
        "Gongxu Luo",
        "Yuewen Sun",
        "Salman Khan",
        "Peter Spirtes",
        "Kun Zhang"
      ],
      "abstract": "Understanding human behavior traits is central to applications in\nhuman-computer interaction, computational social science, and personalized AI\nsystems. Such understanding often requires integrating multiple modalities to\ncapture nuanced patterns and relationships. However, existing resources rarely\nprovide datasets that combine behavioral descriptors with complementary\nmodalities such as facial attributes and biographical information. To address\nthis gap, we present PersonaX, a curated collection of multimodal datasets\ndesigned to enable comprehensive analysis of public traits across modalities.\nPersonaX consists of (1) CelebPersona, featuring 9444 public figures from\ndiverse occupations, and (2) AthlePersona, covering 4181 professional athletes\nacross 7 major sports leagues. Each dataset includes behavioral trait\nassessments inferred by three high-performing large language models, alongside\nfacial imagery and structured biographical features. We analyze PersonaX at two\ncomplementary levels. First, we abstract high-level trait scores from text\ndescriptions and apply five statistical independence tests to examine their\nrelationships with other modalities. Second, we introduce a novel causal\nrepresentation learning (CRL) framework tailored to multimodal and\nmulti-measurement data, providing theoretical identifiability guarantees.\nExperiments on both synthetic and real-world data demonstrate the effectiveness\nof our approach. By unifying structured and unstructured analysis, PersonaX\nestablishes a foundation for studying LLM-inferred behavioral traits in\nconjunction with visual and biographical attributes, advancing multimodal trait\nanalysis and causal reasoning.",
      "pdf_url": "http://arxiv.org/pdf/2509.11362v1",
      "arxiv_url": "http://arxiv.org/abs/2509.11362v1",
      "published": "2025-09-14",
      "categories": [
        "cs.LG",
        "cs.CV"
      ]
    }
  ]
}