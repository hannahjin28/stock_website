{
  "last_updated": "2025-04-17T00:51:07.069804",
  "papers": [
    {
      "title": "Looking beyond the next token",
      "authors": [
        "Abitha Thankaraj",
        "Yiding Jiang",
        "J. Zico Kolter",
        "Yonatan Bisk"
      ],
      "abstract": "The structure of causal language model training assumes that each token can\nbe accurately predicted from the previous context. This contrasts with humans'\nnatural writing and reasoning process, where goals are typically known before\nthe exact argument or phrasings. While this mismatch has been well studied in\nthe literature, the working assumption has been that architectural changes are\nneeded to address this mismatch. We argue that rearranging and processing the\ntraining data sequences can allow models to more accurately imitate the true\ndata-generating process, and does not require any other changes to the\narchitecture or training infrastructure. We demonstrate that this technique,\nTrelawney, and the inference algorithms derived from it allow us to improve\nperformance on several key benchmarks that span planning, algorithmic\nreasoning, and story generation tasks. Finally, our method naturally enables\nthe generation of long-term goals at no additional cost. We investigate how\nusing the model's goal-generation capability can further improve planning and\nreasoning. Additionally, we believe Trelawney could potentially open doors to\nnew capabilities beyond the current language modeling paradigm.",
      "pdf_url": "http://arxiv.org/pdf/2504.11336v1",
      "arxiv_url": "http://arxiv.org/abs/2504.11336v1",
      "published": "2025-04-15",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Clinically Interpretable Survival Risk Stratification in Head and Neck Cancer Using Bayesian Networks and Markov Blankets",
      "authors": [
        "Keyur D. Shah",
        "Ibrahim Chamseddine",
        "Xiaohan Yuan",
        "Sibo Tian",
        "Richard Qiu",
        "Jun Zhou",
        "Anees Dhabaan",
        "Hania Al-Hallaq",
        "David S. Yu",
        "Harald Paganetti",
        "Xiaofeng Yang"
      ],
      "abstract": "Purpose: To identify a clinically interpretable subset of survival-relevant\nfeatures in HN cancer using Bayesian Network (BN) and evaluate its prognostic\nand causal utility. Methods and Materials: We used the RADCURE dataset,\nconsisting of 3,346 patients with H&N cancer treated with definitive\n(chemo)radiotherapy. A probabilistic BN was constructed to model dependencies\namong clinical, anatomical, and treatment variables. The Markov Blanket (MB) of\ntwo-year survival (SVy2) was extracted and used to train a logistic regression\nmodel. After excluding incomplete cases, a temporal split yielded a train/test\n(2,174/820) dataset using 2007 as the cutoff year. Model performance was\nassessed using area under the ROC curve (AUC), C-index, and Kaplan-Meier (KM)\nsurvival stratification. Model fit was further evaluated using a log-likelihood\nratio (LLR) test. Causal inference was performed using do-calculus\ninterventions on MB variables. Results: The MB of SVy2 included 6 clinically\nrelevant features: ECOG performance status, T-stage, HPV status, disease site,\nthe primary gross tumor volume (GTVp), and treatment modality. The model\nachieved an AUC of 0.65 and C-index of 0.78 on the test dataset, significantly\nstratifying patients into high- and low-risk groups (log-rank p < 0.01). Model\nfit was further supported by a log-likelihood ratio of 70.32 (p < 0.01).\nSubgroup analyses revealed strong performance in HPV-negative (AUC = 0.69,\nC-index = 0.76), T4 (AUC = 0.69, C-index = 0.80), and large-GTV (AUC = 0.67,\nC-index = 0.75) cohorts, each showing significant KM separation. Causal\nanalysis further supported the positive survival impact of ECOG 0, HPV-positive\nstatus, and chemoradiation. Conclusions: A compact, MB-derived BN model can\nrobustly stratify survival risk in HN cancer. The model enables explainable\nprognostication and supports individualized decision-making across key clinical\nsubgroups.",
      "pdf_url": "http://arxiv.org/pdf/2504.11188v1",
      "arxiv_url": "http://arxiv.org/abs/2504.11188v1",
      "published": "2025-04-15",
      "categories": [
        "physics.med-ph"
      ]
    },
    {
      "title": "On relative universality, regression operator, and conditional independence",
      "authors": [
        "Bing Li",
        "Ben Jones",
        "Andreas Artemiou"
      ],
      "abstract": "The notion of relative universality with respect to a {\\sigma}-field was\nintroduced to establish the unbiasedness and Fisher consistency of an estimator\nin nonlinear sufficient dimension reduction. However, there is a gap in the\nproof of this result in the existing literature. The existing definition of\nrelative universality seems to be too strong for the proof to be valid. In this\nnote we modify the definition of relative universality using the concept of\n\\k{o}-measurability, and rigorously establish the mentioned unbiasedness and\nFisher consistency. The significance of this result is beyond its original\ncontext of sufficient dimension reduction, because relative universality allows\nus to use the regression operator to fully characterize conditional\nindependence, a crucially important statistical relation that sits at the core\nof many areas and methodologies in statistics and machine learning, such as\ndimension reduction, graphical models, probability embedding, causal inference,\nand Bayesian estimation.",
      "pdf_url": "http://arxiv.org/pdf/2504.11044v1",
      "arxiv_url": "http://arxiv.org/abs/2504.11044v1",
      "published": "2025-04-15",
      "categories": [
        "math.ST",
        "stat.ME",
        "stat.ML",
        "stat.TH",
        "62",
        "G.3"
      ]
    },
    {
      "title": "A conceptual synthesis of causal assumptions for causal discovery and inference",
      "authors": [
        "Hannah E. Correia"
      ],
      "abstract": "This work presents a conceptual synthesis of causal discovery and inference\nframeworks, with a focus on how foundational assumptions -- causal sufficiency,\ncausal faithfulness, and the causal Markov condition -- are formalized and\noperationalized across methodological traditions. Through structured tables and\ncomparative summaries, I map core assumptions, tasks, and analytical choices\nfrom multiple causal frameworks, highlighting their connections and\ndifferences. The synthesis provides practical guidance for researchers\ndesigning causal studies, especially in settings where observational or\nexperimental constraints challenge standard approaches. This guide spans all\nphases of causal analysis, including question formulation, formalization of\nbackground knowledge, selection of appropriate frameworks, choice of study\ndesign or algorithm, and interpretation. It is intended as a tool to support\nrigorous causal reasoning across diverse empirical domains.",
      "pdf_url": "http://arxiv.org/pdf/2504.11035v1",
      "arxiv_url": "http://arxiv.org/abs/2504.11035v1",
      "published": "2025-04-15",
      "categories": [
        "stat.ME",
        "q-bio.QM",
        "stat.AP",
        "stat.OT",
        "62A01"
      ]
    },
    {
      "title": "Can LLMs Leverage Observational Data? Towards Data-Driven Causal Discovery with LLMs",
      "authors": [
        "Yuni Susanti",
        "Michael FÃ¤rber"
      ],
      "abstract": "Causal discovery traditionally relies on statistical methods applied to\nobservational data, often requiring large datasets and assumptions about\nunderlying causal structures. Recent advancements in Large Language Models\n(LLMs) have introduced new possibilities for causal discovery by providing\ndomain expert knowledge. However, it remains unclear whether LLMs can\neffectively process observational data for causal discovery. In this work, we\nexplore the potential of LLMs for data-driven causal discovery by integrating\nobservational data for LLM-based reasoning. Specifically, we examine whether\nLLMs can effectively utilize observational data through two prompting\nstrategies: pairwise prompting and breadth first search (BFS)-based prompting.\nIn both approaches, we incorporate the observational data directly into the\nprompt to assess LLMs' ability to infer causal relationships from such data.\nExperiments on benchmark datasets show that incorporating observational data\nenhances causal discovery, boosting F1 scores by up to 0.11 point using both\npairwise and BFS LLM-based prompting, while outperforming traditional\nstatistical causal discovery baseline by up to 0.52 points. Our findings\nhighlight the potential and limitations of LLMs for data-driven causal\ndiscovery, demonstrating their ability to move beyond textual metadata and\neffectively interpret and utilize observational data for more informed causal\nreasoning. Our studies lays the groundwork for future advancements toward fully\nLLM-driven causal discovery.",
      "pdf_url": "http://arxiv.org/pdf/2504.10936v1",
      "arxiv_url": "http://arxiv.org/abs/2504.10936v1",
      "published": "2025-04-15",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Bayesian analysis of regression discontinuity designs with heterogeneous treatment effects",
      "authors": [
        "Kevin Tao",
        "Y. Samuel Wang",
        "David Ruppert"
      ],
      "abstract": "Regression Discontinuity Design (RDD) is a popular framework for estimating a\ncausal effect in settings where treatment is assigned if an observed covariate\nexceeds a fixed threshold. We consider estimation and inference in the common\nsetting where the sample consists of multiple known sub-populations with\npotentially heterogeneous treatment effects. In the applied literature, it is\ncommon to account for heterogeneity by either fitting a parametric model or\nconsidering each sub-population separately. In contrast, we develop a Bayesian\nhierarchical model using Gaussian process regression which allows for\nnon-parametric regression while borrowing information across sub-populations.\nWe derive the posterior distribution, prove posterior consistency, and develop\na Metropolis-Hastings within Gibbs sampling algorithm. In extensive\nsimulations, we show that the proposed procedure outperforms existing methods\nin both estimation and inferential tasks. Finally, we apply our procedure to\nU.S. Senate election data and discover an incumbent party advantage which is\nheterogeneous over different time periods.",
      "pdf_url": "http://arxiv.org/pdf/2504.10652v1",
      "arxiv_url": "http://arxiv.org/abs/2504.10652v1",
      "published": "2025-04-14",
      "categories": [
        "math.ST",
        "stat.ME",
        "stat.TH",
        "62C10"
      ]
    },
    {
      "title": "A Two-Stage Interpretable Matching Framework for Causal Inference",
      "authors": [
        "Sahil Shikalgar",
        "Md. Noor-E-Alam"
      ],
      "abstract": "Matching in causal inference from observational data aims to construct\ntreatment and control groups with similar distributions of covariates, thereby\nreducing confounding and ensuring an unbiased estimation of treatment effects.\nThis matched sample closely mimics a randomized controlled trial (RCT), thus\nimproving the quality of causal estimates. We introduce a novel Two-stage\nInterpretable Matching (TIM) framework for transparent and interpretable\ncovariate matching. In the first stage, we perform exact matching across all\navailable covariates. For treatment and control units without an exact match in\nthe first stage, we proceed to the second stage. Here, we iteratively refine\nthe matching process by removing the least significant confounder in each\niteration and attempting exact matching on the remaining covariates. We learn a\ndistance metric for the dropped covariates to quantify closeness to the\ntreatment unit(s) within the corresponding strata. We used these high- quality\nmatches to estimate the conditional average treatment effects (CATEs). To\nvalidate TIM, we conducted experiments on synthetic datasets with varying\nassociation structures and correlations. We assessed its performance by\nmeasuring bias in CATE estimation and evaluating multivariate overlap between\ntreatment and control groups before and after matching. Additionally, we apply\nTIM to a real-world healthcare dataset from the Centers for Disease Control and\nPrevention (CDC) to estimate the causal effect of high cholesterol on diabetes.\nOur results demonstrate that TIM improves CATE estimates, increases\nmultivariate overlap, and scales effectively to high-dimensional data, making\nit a robust tool for causal inference in observational data.",
      "pdf_url": "http://arxiv.org/pdf/2504.09635v1",
      "arxiv_url": "http://arxiv.org/abs/2504.09635v1",
      "published": "2025-04-13",
      "categories": [
        "cs.AI",
        "stat.ME"
      ]
    },
    {
      "title": "PlugSelect: Pruning Channels with Plug-and-Play Flexibility for Electroencephalography-based Brain Computer Interface",
      "authors": [
        "Xue Yuan",
        "Keren Shi",
        "Ning Jiang",
        "Jiayuan He"
      ],
      "abstract": "Automatic minimization and optimization of the number of the electrodes is\nessential for the practical application of electroencephalography (EEG)-based\nbrain computer interface (BCI). Previous methods typically require additional\ntraining costs or rely on prior knowledge assumptions. This study proposed a\nnovel channel pruning model, plug-and-select (PlugSelect), applicable across a\nbroad range of BCI paradigms with no additional training cost and plug-and-play\nfunctionality. It integrates gradients along the input path to globally infer\nthe causal relationships between input channels and outputs, and ranks the\ncontribution sequences to identify the most highly attributed channels. The\nresults showed that for three BCI paradigms, i.e., auditory attention decoding\n(AAD), motor imagery (MI), affective computation (AC), PlugSelect could reduce\nthe number of channels by at least half while effectively maintaining decoding\nperformance and improving efficiency. The outcome benefits the design of\nwearable EEG-based devices, facilitating the practical application of BCI\ntechnology.",
      "pdf_url": "http://arxiv.org/pdf/2504.08486v1",
      "arxiv_url": "http://arxiv.org/abs/2504.08486v1",
      "published": "2025-04-11",
      "categories": [
        "cs.HC"
      ]
    },
    {
      "title": "Enhanced Marginal Sensitivity Model and Bounds",
      "authors": [
        "Yi Zhang",
        "Wenfu Xu",
        "Zhiqiang Tan"
      ],
      "abstract": "Sensitivity analysis is important to assess the impact of unmeasured\nconfounding in causal inference from observational studies. The marginal\nsensitivity model (MSM) provides a useful approach in quantifying the influence\nof unmeasured confounders on treatment assignment and leading to tractable\nsharp bounds of common causal parameters. In this paper, to tighten MSM sharp\nbounds, we propose the enhanced MSM (eMSM) by incorporating another sensitivity\nconstraint that quantifies the influence of unmeasured confounders on outcomes.\nWe derive sharp population bounds of expected potential outcomes under eMSM,\nwhich are always narrower than the MSM sharp bounds in a simple and\ninterpretable way. We further discuss desirable specifications of sensitivity\nparameters related to the outcome sensitivity constraint, and obtain both\ndoubly robust point estimation and confidence intervals for the eMSM population\nbounds. The effectiveness of eMSM is also demonstrated numerically through two\nreal-data applications. Our development represents, for the first time, a\nsatisfactory extension of MSM to exploit both treatment and outcome sensitivity\nconstraints on unmeasured confounding.",
      "pdf_url": "http://arxiv.org/pdf/2504.08301v1",
      "arxiv_url": "http://arxiv.org/abs/2504.08301v1",
      "published": "2025-04-11",
      "categories": [
        "stat.ME",
        "math.ST",
        "stat.TH"
      ]
    },
    {
      "title": "Causal attribution with confidence",
      "authors": [
        "Ping Zhang",
        "Ruoyu Wang",
        "Wang Miao"
      ],
      "abstract": "To answer questions of \"causes of effects\", the probability of necessity is\nintroduced for assessing whether or not an observed outcome was caused by an\nearlier treatment. However, the statistical inference for probability of\nnecessity is understudied due to several difficulties, which hinders its\napplication in practice. The evaluation of the probability of necessity\ninvolves the joint distribution of potential outcomes, and thus it is in\ngeneral not point identified and one can at best obtain lower and upper bounds\neven in randomized experiments, unless certain monotonicity assumptions on\npotential outcomes are made. Moreover, these bounds are non-smooth functionals\nof the observed data distribution and standard estimation and inference methods\ncannot be directly applied. In this paper, we investigate the statistical\ninference for the probability of necessity in general situations where it may\nnot be point identified. We introduce a mild margin condition to tackle the\nnon-smoothness, under which the bounds become pathwise differentiable. We\nestablish the semiparametric efficiency theory and propose novel asymptotically\nefficient estimators of the bounds, and further construct confidence intervals\nfor the probability of necessity based on the proposed bounds estimators. The\nresultant confidence intervals are less conservative than existing methods and\ncan effectively make use of the observed covariates.",
      "pdf_url": "http://arxiv.org/pdf/2504.08294v1",
      "arxiv_url": "http://arxiv.org/abs/2504.08294v1",
      "published": "2025-04-11",
      "categories": [
        "stat.ME"
      ]
    }
  ]
}