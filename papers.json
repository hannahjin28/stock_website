{
  "last_updated": "2025-08-16T00:53:32.495355",
  "papers": [
    {
      "title": "The Knowledge-Reasoning Dissociation: Fundamental Limitations of LLMs in Clinical Natural Language Inference",
      "authors": [
        "Maël Jullien",
        "Marco Valentino",
        "André Freitas"
      ],
      "abstract": "Large language models are often assumed to acquire increasingly structured,\ngeneralizable internal representations simply by scaling data and parameters.\nWe interrogate this assumption by introducing a Clinical Trial Natural Language\nInference benchmark comprising four reasoning families, Causal Attribution,\nCompositional Grounding, Epistemic Verification, and Risk State Abstraction.\nEach item is paired with a targeted Ground Knowledge and Meta-Level Reasoning\nVerification (GKMRV) probe, allowing us to dissociate failures of factual\naccess from failures of inference. We evaluate six contemporary LLMs under both\ndirect and chain of thought prompting.\n  Models achieve near-ceiling GKMRV accuracy (mean accuracy 0.918) yet perform\npoorly on the main reasoning tasks (mean accuracy 0.25). Despite low accuracy,\noutput inferences are highly consistent across samples (mean 0.87), indicating\na systematic application of underlying heuristics and shortcuts.\n  These results reveal fundamental structural and representational limitations:\ncurrent LLMs often possess the relevant clinical knowledge but lack the\nstructured, composable internal representations needed to deploy it reliably\n(e.g., integrating constraints, weighing evidence, or simulating\ncounterfactuals). Decoupling knowledge from reasoning with GKMRV makes this\ndissociation explicit and measurable, providing an effective framework for\nprobing the reliability of LLMs in high-stakes domains.",
      "pdf_url": "http://arxiv.org/pdf/2508.10777v1",
      "arxiv_url": "http://arxiv.org/abs/2508.10777v1",
      "published": "2025-08-14",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Technical Report: Facilitating the Adoption of Causal Inference Methods Through LLM-Empowered Co-Pilot",
      "authors": [
        "Jeroen Berrevoets",
        "Julianna Piskorz",
        "Robert Davis",
        "Harry Amad",
        "Jim Weatherall",
        "Mihaela van der Schaar"
      ],
      "abstract": "Estimating treatment effects (TE) from observational data is a critical yet\ncomplex task in many fields, from healthcare and economics to public policy.\nWhile recent advances in machine learning and causal inference have produced\npowerful estimation techniques, their adoption remains limited due to the need\nfor deep expertise in causal assumptions, adjustment strategies, and model\nselection. In this paper, we introduce CATE-B, an open-source co-pilot system\nthat uses large language models (LLMs) within an agentic framework to guide\nusers through the end-to-end process of treatment effect estimation. CATE-B\nassists in (i) constructing a structural causal model via causal discovery and\nLLM-based edge orientation, (ii) identifying robust adjustment sets through a\nnovel Minimal Uncertainty Adjustment Set criterion, and (iii) selecting\nappropriate regression methods tailored to the causal structure and dataset\ncharacteristics. To encourage reproducibility and evaluation, we release a\nsuite of benchmark tasks spanning diverse domains and causal complexities. By\ncombining causal inference with intelligent, interactive assistance, CATE-B\nlowers the barrier to rigorous causal analysis and lays the foundation for a\nnew class of benchmarks in automated treatment effect estimation.",
      "pdf_url": "http://arxiv.org/pdf/2508.10581v1",
      "arxiv_url": "http://arxiv.org/abs/2508.10581v1",
      "published": "2025-08-14",
      "categories": [
        "cs.LG"
      ]
    },
    {
      "title": "eDIF: A European Deep Inference Fabric for Remote Interpretability of LLM",
      "authors": [
        "Irma Heithoff. Marc Guggenberger",
        "Sandra Kalogiannis",
        "Susanne Mayer",
        "Fabian Maag",
        "Sigurd Schacht",
        "Carsten Lanquillon"
      ],
      "abstract": "This paper presents a feasibility study on the deployment of a European Deep\nInference Fabric (eDIF), an NDIF-compatible infrastructure designed to support\nmechanistic interpretability research on large language models. The need for\nwidespread accessibility of LLM interpretability infrastructure in Europe\ndrives this initiative to democratize advanced model analysis capabilities for\nthe research community. The project introduces a GPU-based cluster hosted at\nAnsbach University of Applied Sciences and interconnected with partner\ninstitutions, enabling remote model inspection via the NNsight API. A\nstructured pilot study involving 16 researchers from across Europe evaluated\nthe platform's technical performance, usability, and scientific utility. Users\nconducted interventions such as activation patching, causal tracing, and\nrepresentation analysis on models including GPT-2 and DeepSeek-R1-70B. The\nstudy revealed a gradual increase in user engagement, stable platform\nperformance throughout, and a positive reception of the remote experimentation\ncapabilities. It also marked the starting point for building a user community\naround the platform. Identified limitations such as prolonged download\ndurations for activation data as well as intermittent execution interruptions\nare addressed in the roadmap for future development. This initiative marks a\nsignificant step towards widespread accessibility of LLM interpretability\ninfrastructure in Europe and lays the groundwork for broader deployment,\nexpanded tooling, and sustained community collaboration in mechanistic\ninterpretability research.",
      "pdf_url": "http://arxiv.org/pdf/2508.10553v1",
      "arxiv_url": "http://arxiv.org/abs/2508.10553v1",
      "published": "2025-08-14",
      "categories": [
        "cs.CL"
      ]
    },
    {
      "title": "Identifying Unmeasured Confounders in Panel Causal Models: A Two-Stage LM-Wald Approach",
      "authors": [
        "Bang Quan Zheng"
      ],
      "abstract": "Panel data are widely used in political science to draw causal inferences.\nHowever, these models often rely on the strong and untested assumption of\nsequential ignorability--that no unmeasured variables influence both the\nindependent and outcome variables across time. Grounded in psychometric\nliterature on latent variable modeling, this paper introduces the Two-Stage\nLM-Wald (2SLW) approach, a diagnostic tool that extends the Lagrange Multiplier\n(LM) and Wald tests to detect violations of this assumption in panel causal\nmodels. Using Monte Carlo simulations within the Random Intercept Cross-Lagged\nPanel Model (RI-CLPM), which separates within and between person effects, I\ndemonstrate the 2SLW's ability to detect unmeasured confounding across three\nkey scenarios: biased corrections, distorted direct effects, and altered\nmediation pathways. I also illustrate the approach with an empirical\napplication to real-world panel data. By providing a practical and\ntheoretically grounded diagnostic, the 2SLW approach enhances the robustness of\ncausal inferences in the presence of potential time-varying confounders.\nMoreover, it can be readily implemented using the R package lavaan.",
      "pdf_url": "http://arxiv.org/pdf/2508.10342v1",
      "arxiv_url": "http://arxiv.org/abs/2508.10342v1",
      "published": "2025-08-14",
      "categories": [
        "stat.ME"
      ]
    },
    {
      "title": "Synthesizing Evidence: Data-Pooling as a Tool for Treatment Selection in Online Experiments",
      "authors": [
        "Zhenkang Peng",
        "Chengzhang Li",
        "Ying Rong",
        "Renyu",
        "Zhang"
      ],
      "abstract": "Randomized experiments are the gold standard for causal inference but face\nsignificant challenges in business applications, including limited traffic\nallocation, the need for heterogeneous treatment effect estimation, and the\ncomplexity of managing overlapping experiments. These factors lead to high\nvariability in treatment effect estimates, making data-driven policy roll out\ndifficult. To address these issues, we introduce the data pooling treatment\nroll-out (DTR) framework, which enhances policy roll-out by pooling data across\nexperiments rather than focusing narrowly on individual ones. DTR can\neffectively accommodate both overlapping and non-overlapping traffic scenarios,\nregardless of linear or nonlinear model specifications. We demonstrate the\nframework's robustness through a three-pronged validation: (a) theoretical\nanalysis shows that DTR surpasses the traditional difference-in-mean and\nordinary least squares methods under non-overlapping experiments, particularly\nwhen the number of experiments is large; (b) synthetic simulations confirm its\nadaptability in complex scenarios with overlapping traffic, rich covariates and\nnonlinear specifications; and (c) empirical applications to two experimental\ndatasets from real world platforms, demonstrating its effectiveness in guiding\ncustomized policy roll-outs for subgroups within a single experiment, as well\nas in coordinating policy deployments across multiple experiments with\noverlapping scenarios. By reducing estimation variability to improve\ndecision-making effectiveness, DTR provides a scalable, practical solution for\nonline platforms to better leverage their experimental data in today's\nincreasingly complex business environments.",
      "pdf_url": "http://arxiv.org/pdf/2508.10331v1",
      "arxiv_url": "http://arxiv.org/abs/2508.10331v1",
      "published": "2025-08-14",
      "categories": [
        "stat.ME"
      ]
    },
    {
      "title": "Statistical methods: Basic concepts, interpretations, and cautions",
      "authors": [
        "Sander Greenland"
      ],
      "abstract": "The study of associations and their causal explanations is a central research\nactivity whose methodology varies tremendously across fields. Even within\nspecialized subfields, comparisons across textbooks and journals reveals that\nthe basics are subject to considerable variation and controversy. This\nvariation is often obscured by the singular viewpoints presented within\ntextbooks and journal guidelines, which may be deceptively written as if the\nnorms they adopt are unchallenged. Furthermore, human limitations and the\nvastness within fields imply that no one can have expertise across all\nsubfields and that interpretations will be severely constrained by the\nlimitations of studies of human populations.\n  The present chapter outlines an approach to statistical methods that attempts\nto recognize these problems from the start, rather than assume they are absent\nas in the claims of 'statistical significance' and 'confidence' ordinarily\nattached to statistical tests and interval estimates. It does so by grounding\nmodels and statistics in data description, and treating inferences from them as\nspeculations based on assumptions that cannot be fully validated or checked\nusing the analysis data.",
      "pdf_url": "http://arxiv.org/pdf/2508.10168v1",
      "arxiv_url": "http://arxiv.org/abs/2508.10168v1",
      "published": "2025-08-13",
      "categories": [
        "stat.ME",
        "math.ST",
        "stat.TH"
      ]
    },
    {
      "title": "Linking GFAP Levels to Speech Anomalies in Acute Brain Injury: A Simulation Based Study",
      "authors": [
        "Shamaley Aravinthan",
        "Bin Hu"
      ],
      "abstract": "Background: Glial fibrillary acidic protein (GFAP) is a biomarker for\nintracerebral hemorrhage and traumatic brain injury, but its link to acute\nspeech disruption is untested. Speech anomalies often emerge early after\ninjury, enabling rapid triage.\n  Methods: We simulated a cohort of 200 virtual patients stratified by lesion\nlocation, onset time, and severity. GFAP kinetics followed published\ntrajectories; speech anomalies were generated from lesion-specific\nneurophysiological mappings. Ensemble machine-learning models used GFAP,\nspeech, and lesion features; robustness was tested under noise, delays, and\nlabel dropout. Causal inference (inverse probability of treatment weighting and\ntargeted maximum likelihood estimation) estimated directional associations\nbetween GFAP elevation and speech severity.\n  Findings: GFAP correlated with simulated speech anomaly severity (Spearman\nrho = 0.48), strongest for cortical lesions (rho = 0.55). Voice anomalies\npreceded detectable GFAP rise by a median of 42 minutes in cortical injury.\nClassifier area under the curve values were 0.74 (GFAP only), 0.78 (voice\nonly), and 0.86 for the fused multimodal model, which showed higher sensitivity\nin mild or ambiguous cases. Causal estimates indicated higher GFAP increased\nthe modeled probability of moderate-to-severe speech anomalies by 32 to 35\npercent, independent of lesion site and onset time.\n  Conclusion: These results support a link between GFAP elevation and speech\nanomalies in acute brain injury and suggest integrated biochemical-voice\ndiagnostics could improve early triage, especially for cortical injury.\nFindings are simulation-based and require validation in prospective clinical\nstudies with synchronized GFAP assays and speech recordings.",
      "pdf_url": "http://arxiv.org/pdf/2508.10130v1",
      "arxiv_url": "http://arxiv.org/abs/2508.10130v1",
      "published": "2025-08-13",
      "categories": [
        "q-bio.NC"
      ]
    },
    {
      "title": "Developing an Inhaled NEU1 Inhibitor for Cystic Fibrosis via Pharmacokinetic and Biophysical Modeling",
      "authors": [
        "Yousra Hassan Alsaad Almeshale",
        "Abdulelah Hassan Almeshali",
        "Omar Alsaddique",
        "Noura Jandali",
        "Nadeen Garaween",
        "Bin Hu"
      ],
      "abstract": "Background: Cystic fibrosis (CF) airway mucus exhibits reduced mucin\nsialylation, increasing viscosity and impairing mucociliary clearance (MCC).\nNEU1 inhibition has been proposed to restore MCC, but its quantitative\npharmacokinetic and rheological effects, particularly with inhaled delivery,\nremain uncharacterized.\n  Objective: To develop an integrated pharmacokinetic/pharmacodynamic (PK/PD)\nand biophysical model to assess the efficacy of an inhaled NEU1 inhibitor.\n  Methods: Empirical and preclinical NEU1 inhibition data were combined with\ninhalation PK/PD modeling and a biophysical viscosity framework linking mucin\nsialylation and extracellular DNA. Synthetic cohort simulations (N = 200) were\nreconciled with empirical PK benchmarks using Latin hypercube parameter\nsampling. Cross-validation, hold-out testing, and causal inference methods\n(inverse probability of treatment weighting and targeted maximum likelihood\nestimation) quantified predicted effects on lung function (delta FEV1).\n  Results: With reconciled parameters (F_dep = 0.12; k_abs = 0.21 per hour;\nk_muc = 0.24 per hour), epithelial lining fluid drug levels reached a peak\nconcentration of 7.5 micromolar (95 percent CI: 6 to 10 micromolar), achieving\nIC50 coverage for approximately 10 hours per day and greater than 80 percent\nmodeled NEU1 inhibition. Predicted mucus viscosity reduction averaged 25 to 28\npercent. Causal inference estimated delta FEV1 improvement of +0.13 liters (95\npercent CI: 0.10 to 0.15 liters), with about 70 percent mediated via MCC.\n  Conclusions: Empirically anchored PK/PD and biophysical modeling support the\nfeasibility of inhaled NEU1 inhibition as a rheology-targeting strategy in CF,\nprojecting clinically realistic efficacy while maintaining pharmacological\nviability. This calibrated proof of concept warrants in vivo validation in CF\nmodels.",
      "pdf_url": "http://arxiv.org/pdf/2508.10082v1",
      "arxiv_url": "http://arxiv.org/abs/2508.10082v1",
      "published": "2025-08-13",
      "categories": [
        "q-bio.QM"
      ]
    },
    {
      "title": "Embodied Tactile Perception of Soft Objects Properties",
      "authors": [
        "Anirvan Dutta",
        "Alexis WM Devillard",
        "Zhihuan Zhang",
        "Xiaoxiao Cheng",
        "Etienne Burdet"
      ],
      "abstract": "To enable robots to develop human-like fine manipulation, it is essential to\nunderstand how mechanical compliance, multi-modal sensing, and purposeful\ninteraction jointly shape tactile perception. In this study, we use a dedicated\nmodular e-Skin with tunable mechanical compliance and multi-modal sensing\n(normal, shear forces and vibrations) to systematically investigate how sensing\nembodiment and interaction strategies influence robotic perception of objects.\nLeveraging a curated set of soft wave objects with controlled viscoelastic and\nsurface properties, we explore a rich set of palpation primitives-pressing,\nprecession, sliding that vary indentation depth, frequency, and directionality.\nIn addition, we propose the latent filter, an unsupervised, action-conditioned\ndeep state-space model of the sophisticated interaction dynamics and infer\ncausal mechanical properties into a structured latent space. This provides\ngeneralizable and in-depth interpretable representation of how embodiment and\ninteraction determine and influence perception. Our investigation demonstrates\nthat multi-modal sensing outperforms uni-modal sensing. It highlights a nuanced\ninteraction between the environment and mechanical properties of e-Skin, which\nshould be examined alongside the interaction by incorporating temporal\ndynamics.",
      "pdf_url": "http://arxiv.org/pdf/2508.09836v1",
      "arxiv_url": "http://arxiv.org/abs/2508.09836v1",
      "published": "2025-08-13",
      "categories": [
        "cs.RO"
      ]
    },
    {
      "title": "Structured Kernel Regression VAE: A Computationally Efficient Surrogate for GP-VAEs in ICA",
      "authors": [
        "Yuan-Hao Wei",
        "Fu-Hao Deng",
        "Lin-Yong Cui",
        "Yan-Jie Sun"
      ],
      "abstract": "The interpretability of generative models is considered a key factor in\ndemonstrating their effectiveness and controllability. The generated data are\nbelieved to be determined by latent variables that are not directly observable.\nTherefore, disentangling, decoupling, decomposing, causal inference, or\nperforming Independent Component Analysis (ICA) in the latent variable space\nhelps uncover the independent factors that influence the attributes or features\naffecting the generated outputs, thereby enhancing the interpretability of\ngenerative models. As a generative model, Variational Autoencoders (VAEs)\ncombine with variational Bayesian inference algorithms. Using VAEs, the inverse\nprocess of ICA can be equivalently framed as a variational inference process.\nIn some studies, Gaussian processes (GPs) have been introduced as priors for\neach dimension of latent variables in VAEs, structuring and separating each\ndimension from temporal or spatial perspectives, and encouraging different\ndimensions to control various attributes of the generated data. However, GPs\nimpose a significant computational burden, resulting in substantial resource\nconsumption when handling large datasets. Essentially, GPs model different\ntemporal or spatial structures through various kernel functions. Structuring\nthe priors of latent variables via kernel functions-so that different kernel\nfunctions model the correlations among sequence points within different latent\ndimensions-is at the core of achieving disentanglement in VAEs. The proposed\nStructured Kernel Regression VAE (SKR-VAE) leverages this core idea in a more\nefficient way, avoiding the costly kernel matrix inversion required in GPs.\nThis research demonstrates that, while maintaining ICA performance, SKR-VAE\nachieves greater computational efficiency and significantly reduced\ncomputational burden compared to GP-VAE.",
      "pdf_url": "http://arxiv.org/pdf/2508.09721v1",
      "arxiv_url": "http://arxiv.org/abs/2508.09721v1",
      "published": "2025-08-13",
      "categories": [
        "stat.ML",
        "cs.LG"
      ]
    }
  ]
}