{
  "last_updated": "2025-10-30T00:54:31.906322",
  "papers": [
    {
      "title": "Machine-Learning-Assisted Comparison of Regression Functions",
      "authors": [
        "Jian Yan",
        "Zhuoxi Li",
        "Yang Ning",
        "Yong Chen"
      ],
      "abstract": "We revisit the classical problem of comparing regression functions, a\nfundamental question in statistical inference with broad relevance to modern\napplications such as data integration, transfer learning, and causal inference.\nExisting approaches typically rely on smoothing techniques and are thus\nhindered by the curse of dimensionality. We propose a generalized notion of\nkernel-based conditional mean dependence that provides a new characterization\nof the null hypothesis of equal regression functions. Building on this\nreformulation, we develop two novel tests that leverage modern machine learning\nmethods for flexible estimation. We establish the asymptotic properties of the\ntest statistics, which hold under both fixed- and high-dimensional regimes.\nUnlike existing methods that often require restrictive distributional\nassumptions, our framework only imposes mild moment conditions. The efficacy of\nthe proposed tests is demonstrated through extensive numerical studies.",
      "pdf_url": "http://arxiv.org/pdf/2510.24714v1",
      "arxiv_url": "http://arxiv.org/abs/2510.24714v1",
      "published": "2025-10-28",
      "categories": [
        "stat.ME",
        "econ.EM",
        "stat.ML"
      ]
    },
    {
      "title": "Decentralized Causal Discovery using Judo Calculus",
      "authors": [
        "Sridhar Mahadevan"
      ],
      "abstract": "We describe a theory and implementation of an intuitionistic decentralized\nframework for causal discovery using judo calculus, which is formally defined\nas j-stable causal inference using j-do-calculus in a topos of sheaves. In\nreal-world applications -- from biology to medicine and social science --\ncausal effects depend on regime (age, country, dose, genotype, or lab\nprotocol). Our proposed judo calculus formalizes this context dependence\nformally as local truth: a causal claim is proven true on a cover of regimes,\nnot everywhere at once. The Lawvere-Tierney modal operator j chooses which\nregimes are relevant; j-stability means the claim holds constructively and\nconsistently across that family. We describe an algorithmic and implementation\nframework for judo calculus, combining it with standard score-based,\nconstraint-based, and gradient-based causal discovery methods. We describe\nexperimental results on a range of domains, from synthetic to real-world\ndatasets from biology and economics. Our experimental results show the\ncomputational efficiency gained by the decentralized nature of sheaf-theoretic\ncausal discovery, as well as improved performance over classical causal\ndiscovery methods.",
      "pdf_url": "http://arxiv.org/pdf/2510.23942v1",
      "arxiv_url": "http://arxiv.org/abs/2510.23942v1",
      "published": "2025-10-27",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Group Interventions on Deep Networks for Causal Discovery in Subsystems",
      "authors": [
        "Wasim Ahmad",
        "Joachim Denzler",
        "Maha Shadaydeh"
      ],
      "abstract": "Causal discovery uncovers complex relationships between variables, enhancing\npredictions, decision-making, and insights into real-world systems, especially\nin nonlinear multivariate time series. However, most existing methods primarily\nfocus on pairwise cause-effect relationships, overlooking interactions among\ngroups of variables, i.e., subsystems and their collective causal influence. In\nthis study, we introduce gCDMI, a novel multi-group causal discovery method\nthat leverages group-level interventions on trained deep neural networks and\nemploys model invariance testing to infer causal relationships. Our approach\ninvolves three key steps. First, we use deep learning to jointly model the\nstructural relationships among groups of all time series. Second, we apply\ngroup-wise interventions to the trained model. Finally, we conduct model\ninvariance testing to determine the presence of causal links among variable\ngroups. We evaluate our method on simulated datasets, demonstrating its\nsuperior performance in identifying group-level causal relationships compared\nto existing methods. Additionally, we validate our approach on real-world\ndatasets, including brain networks and climate ecosystems. Our results\nhighlight that applying group-level interventions to deep learning models,\ncombined with invariance testing, can effectively reveal complex causal\nstructures, offering valuable insights for domains such as neuroscience and\nclimate science.",
      "pdf_url": "http://arxiv.org/pdf/2510.23906v2",
      "arxiv_url": "http://arxiv.org/abs/2510.23906v2",
      "published": "2025-10-27",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Control VAR: a counterfactual based approach to inference in macroeconomics",
      "authors": [
        "Raimondo Pala"
      ],
      "abstract": "This paper addresses the challenges of giving a causal interpretation to\nvector autoregressions (VARs). I show that under independence assumptions VARs\ncan identify average treatment effects, average causal responses, or a mix of\nthe two, depending on the distribution of the policy. But what about situations\nin which the economist cannot rely on independence assumptions? I propose an\nalternative method, defined as control-VAR, which uses control variables to\nestimate causal effects. Control-VAR can estimate average treatment effects on\nthe treated for dummy policies or average causal responses over time for\ncontinuous policies.\n  The advantages of control-based approaches are demonstrated by examining the\nimpact of natural disasters on the US economy, using Germany as a control.\nContrary to previous literature, the results indicate that natural disasters\nhave a negative economic impact without any cyclical positive effect. These\nfindings suggest that control-VARs provide a viable alternative to strict\nindependence assumptions, offering more credible causal estimates and\nsignificant implications for policy design in response to natural disasters.",
      "pdf_url": "http://arxiv.org/pdf/2510.23762v1",
      "arxiv_url": "http://arxiv.org/abs/2510.23762v1",
      "published": "2025-10-27",
      "categories": [
        "econ.EM"
      ]
    },
    {
      "title": "Game-TARS: Pretrained Foundation Models for Scalable Generalist Multimodal Game Agents",
      "authors": [
        "Zihao Wang",
        "Xujing Li",
        "Yining Ye",
        "Junjie Fang",
        "Haoming Wang",
        "Longxiang Liu",
        "Shihao Liang",
        "Junting Lu",
        "Zhiyong Wu",
        "Jiazhan Feng",
        "Wanjun Zhong",
        "Zili Li",
        "Yu Wang",
        "Yu Miao",
        "Bo Zhou",
        "Yuanfan Li",
        "Hao Wang",
        "Zhongkai Zhao",
        "Faming Wu",
        "Zhengxuan Jiang",
        "Weihao Tan",
        "Heyuan Yao",
        "Shi Yan",
        "Xiangyang Li",
        "Yitao Liang",
        "Yujia Qin",
        "Guang Shi"
      ],
      "abstract": "We present Game-TARS, a generalist game agent trained with a unified,\nscalable action space anchored to human-aligned native keyboard-mouse inputs.\nUnlike API- or GUI-based approaches, this paradigm enables large-scale\ncontinual pre-training across heterogeneous domains, including OS, web, and\nsimulation games. Game-TARS is pre-trained on over 500B tokens with diverse\ntrajectories and multimodal data. Key techniques include a decaying continual\nloss to reduce causal confusion and an efficient Sparse-Thinking strategy that\nbalances reasoning depth and inference cost. Experiments show that Game-TARS\nachieves about 2 times the success rate over the previous sota model on\nopen-world Minecraft tasks, is close to the generality of fresh humans in\nunseen web 3d games, and outperforms GPT-5, Gemini-2.5-Pro, and Claude-4-Sonnet\nin FPS benchmarks. Scaling results on training-time and test-time confirm that\nthe unified action space sustains improvements when scaled to cross-game and\nmultimodal data. Our results demonstrate that simple, scalable action\nrepresentations combined with large-scale pre-training provide a promising path\ntoward generalist agents with broad computer-use abilities.",
      "pdf_url": "http://arxiv.org/pdf/2510.23691v1",
      "arxiv_url": "http://arxiv.org/abs/2510.23691v1",
      "published": "2025-10-27",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Causal Deep Q Network",
      "authors": [
        "Elouanes Khelifi",
        "Amir Saki",
        "Usef Faghihi"
      ],
      "abstract": "Deep Q Networks (DQN) have shown remarkable success in various reinforcement\nlearning tasks. However, their reliance on associative learning often leads to\nthe acquisition of spurious correlations, hindering their problem-solving\ncapabilities. In this paper, we introduce a novel approach to integrate causal\nprinciples into DQNs, leveraging the PEACE (Probabilistic Easy vAriational\nCausal Effect) formula for estimating causal effects. By incorporating causal\nreasoning during training, our proposed framework enhances the DQN's\nunderstanding of the underlying causal structure of the environment, thereby\nmitigating the influence of confounding factors and spurious correlations. We\ndemonstrate that integrating DQNs with causal capabilities significantly\nenhances their problem-solving capabilities without compromising performance.\nExperimental results on standard benchmark environments showcase that our\napproach outperforms conventional DQNs, highlighting the effectiveness of\ncausal reasoning in reinforcement learning. Overall, our work presents a\npromising avenue for advancing the capabilities of deep reinforcement learning\nagents through principled causal inference.",
      "pdf_url": "http://arxiv.org/pdf/2510.23424v1",
      "arxiv_url": "http://arxiv.org/abs/2510.23424v1",
      "published": "2025-10-27",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Beyond Prompt Engineering: Neuro-Symbolic-Causal Architecture for Robust Multi-Objective AI Agents",
      "authors": [
        "Gokturk Aytug Akarlar"
      ],
      "abstract": "Large language models show promise as autonomous decision-making agents, yet\ntheir deployment in high-stakes domains remains fraught with risk. Without\narchitectural safeguards, LLM agents exhibit catastrophic brittleness:\nidentical capabilities produce wildly different outcomes depending solely on\nprompt framing. We present Chimera, a neuro-symbolic-causal architecture that\nintegrates three complementary components - an LLM strategist, a formally\nverified symbolic constraint engine, and a causal inference module for\ncounterfactual reasoning. We benchmark Chimera against baseline architectures\n(LLM-only, LLM with symbolic constraints) across 52-week simulations in a\nrealistic e-commerce environment featuring price elasticity, trust dynamics,\nand seasonal demand. Under organizational biases toward either volume or margin\noptimization, LLM-only agents fail catastrophically (total loss of \\$99K in\nvolume scenarios) or destroy brand trust (-48.6% in margin scenarios). Adding\nsymbolic constraints prevents disasters but achieves only 43-87% of Chimera's\nprofit. Chimera consistently delivers the highest returns (\\$1.52M and \\$1.96M\nrespectively, some cases +\\$2.2M) while improving brand trust (+1.8% and\n+10.8%, some cases +20.86%), demonstrating prompt-agnostic robustness. Our TLA+\nformal verification proves zero constraint violations across all scenarios.\nThese results establish that architectural design not prompt engineering\ndetermines the reliability of autonomous agents in production environments. We\nprovide open-source implementations and interactive demonstrations for\nreproducibility.",
      "pdf_url": "http://arxiv.org/pdf/2510.23682v1",
      "arxiv_url": "http://arxiv.org/abs/2510.23682v1",
      "published": "2025-10-27",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.LO",
        "cs.SE",
        "I.2.11; I.2.6; I.2.8"
      ]
    },
    {
      "title": "The Reasoning Trap: How Enhancing LLM Reasoning Amplifies Tool Hallucination",
      "authors": [
        "Chenlong Yin",
        "Zeyang Sha",
        "Shiwen Cui",
        "Changhua Meng"
      ],
      "abstract": "Enhancing the reasoning capabilities of Large Language Models (LLMs) is a key\nstrategy for building Agents that \"think then act.\" However, recent\nobservations, like OpenAI's o3, suggest a paradox: stronger reasoning often\ncoincides with increased hallucination, yet no prior work has systematically\nexamined whether reasoning enhancement itself causes tool hallucination. To\naddress this gap, we pose the central question: Does strengthening reasoning\nincrease tool hallucination? To answer this, we introduce SimpleToolHalluBench,\na diagnostic benchmark measuring tool hallucination in two failure modes: (i)\nno tool available, and (ii) only distractor tools available. Through controlled\nexperiments, we establish three key findings. First, we demonstrate a causal\nrelationship: progressively enhancing reasoning through RL increases tool\nhallucination proportionally with task performance gains. Second, this effect\ntranscends overfitting - training on non-tool tasks (e.g., mathematics) still\namplifies subsequent tool hallucination. Third, the effect is method-agnostic,\nappearing when reasoning is instilled via supervised fine-tuning and when it is\nmerely elicited at inference by switching from direct answers to step-by-step\nthinking. We also evaluate mitigation strategies including Prompt Engineering\nand Direct Preference Optimization (DPO), revealing a fundamental\nreliability-capability trade-off: reducing hallucination consistently degrades\nutility. Mechanistically, Reasoning RL disproportionately collapses\ntool-reliability-related representations, and hallucinations surface as\namplified divergences concentrated in late-layer residual streams. These\nfindings reveal that current reasoning enhancement methods inherently amplify\ntool hallucination, highlighting the need for new training objectives that\njointly optimize for capability and reliability.",
      "pdf_url": "http://arxiv.org/pdf/2510.22977v1",
      "arxiv_url": "http://arxiv.org/abs/2510.22977v1",
      "published": "2025-10-27",
      "categories": [
        "cs.LG",
        "cs.AI",
        "I.2"
      ]
    },
    {
      "title": "Exploring Structures of Inferential Mechanisms through Simplistic Digital Circuits",
      "authors": [
        "Giovanni Sileno",
        "Jean-Louis Dessalles"
      ],
      "abstract": "Cognitive studies and artificial intelligence have developed distinct models\nfor various inferential mechanisms (categorization, induction, abduction,\ncausal inference, contrast, merge, ...). Yet, both natural and artificial views\non cognition lack apparently a unifying framework. This paper formulates a\nspeculative answer attempting to respond to this gap. To postulate on\nhigher-level activation processes from a material perspective, we consider\ninferential mechanisms informed by symbolic AI modelling techniques, through\nthe simplistic lenses of electronic circuits based on logic gates. We observe\nthat a logic gate view entails a different treatment of implication and\nnegation compared to standard logic and logic programming. Then, by\ncombinatorial exploration, we identify four main forms of dependencies that can\nbe realized by these inferential circuits. Looking at how these forms are\ngenerally used in the context of logic programs, we identify eight common\ninferential patterns, exposing traditionally distinct inferential mechanisms in\nan unifying framework. Finally, following a probabilistic interpretation of\nlogic programs, we unveil inner functional dependencies. The paper concludes\nelaborating in what sense, even if our arguments are mostly informed by\nsymbolic means and digital systems infrastructures, our observations may\npinpoint to more generally applicable structures.",
      "pdf_url": "http://arxiv.org/pdf/2510.22883v1",
      "arxiv_url": "http://arxiv.org/abs/2510.22883v1",
      "published": "2025-10-27",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Unifying regression-based and design-based causal inference in time-series experiments",
      "authors": [
        "Zhexiao Lin",
        "Peng Ding"
      ],
      "abstract": "Time-series experiments, also called switchback experiments or N-of-1 trials,\nplay increasingly important roles in modern applications in medical and\nindustrial areas. Under the potential outcomes framework, recent research has\nstudied time-series experiments from the design-based perspective, relying\nsolely on the randomness in the design to drive the statistical inference.\nFocusing on simpler statistical methods, we examine the design-based properties\nof regression-based methods for estimating treatment effects in time-series\nexperiments. We demonstrate that the treatment effects of interest can be\nconsistently estimated using ordinary least squares with an appropriately\nspecified working model and transformed regressors. Our analysis allows for\nestimating a diverging number of treatment effects simultaneously, and\nestablishes the consistency and asymptotic normality of the regression-based\nestimators. Additionally, we show that asymptotically, the heteroskedasticity\nand autocorrelation consistent variance estimators provide conservative\nestimates of the true, design-based variances. Importantly, although our\napproach relies on regression, our design-based framework allows for\nmisspecification of the regression model.",
      "pdf_url": "http://arxiv.org/pdf/2510.22864v1",
      "arxiv_url": "http://arxiv.org/abs/2510.22864v1",
      "published": "2025-10-26",
      "categories": [
        "stat.ME",
        "econ.EM",
        "math.ST",
        "stat.TH"
      ]
    }
  ]
}