{
  "last_updated": "2025-07-23T00:59:00.226072",
  "papers": [
    {
      "title": "CoLD: Counterfactually-Guided Length Debiasing for Process Reward Models",
      "authors": [
        "Congmin Zheng",
        "Jiachen Zhu",
        "Jianghao Lin",
        "Xinyi Dai",
        "Yong Yu",
        "Weinan Zhang",
        "Mengyue Yang"
      ],
      "abstract": "Process Reward Models (PRMs) play a central role in evaluating and guiding\nmulti-step reasoning in large language models (LLMs), especially for\nmathematical problem solving. However, we identify a pervasive length bias in\nexisting PRMs: they tend to assign higher scores to longer reasoning steps,\neven when the semantic content and logical validity are unchanged. This bias\nundermines the reliability of reward predictions and leads to overly verbose\noutputs during inference. To address this issue, we propose\nCoLD(Counterfactually-Guided Length Debiasing), a unified framework that\nmitigates length bias through three components: an explicit length-penalty\nadjustment, a learned bias estimator trained to capture spurious length-related\nsignals, and a joint training strategy that enforces length-invariance in\nreward predictions. Our approach is grounded in counterfactual reasoning and\ninformed by causal graph analysis. Extensive experiments on MATH500 and\nGSM-Plus show that CoLD consistently reduces reward-length correlation,\nimproves accuracy in step selection, and encourages more concise, logically\nvalid reasoning. These results demonstrate the effectiveness and practicality\nof CoLD in improving the fidelity and robustness of PRMs.",
      "pdf_url": "http://arxiv.org/pdf/2507.15698v1",
      "arxiv_url": "http://arxiv.org/abs/2507.15698v1",
      "published": "2025-07-21",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Inference on Nonlinear Counterfactual Functionals under a Multiplicative IV Model",
      "authors": [
        "Yonghoon Lee",
        "Mengxin Yu",
        "Jiewen Liu",
        "Chan Park",
        "Yunshu Zhang",
        "James M. Robins",
        "Eric J. Tchetgen Tchetgen"
      ],
      "abstract": "Instrumental variable (IV) methods play a central role in causal inference,\nparticularly in settings where treatment assignment is confounded by unobserved\nvariables. IV methods have been extensively developed in recent years and\napplied across diverse domains, from economics to epidemiology. In this work,\nwe study the recently introduced multiplicative IV (MIV) model and demonstrate\nits utility for causal inference beyond the average treatment effect. In\nparticular, we show that it enables identification and inference for a broad\nclass of counterfactual functionals characterized by moment equations. This\nincludes, for example, inference on quantile treatment effects. We develop\nmethods for efficient and multiply robust estimation of such functionals, and\nprovide inference procedures with asymptotic validity. Experimental results\ndemonstrate that the proposed procedure performs well even with moderate sample\nsizes.",
      "pdf_url": "http://arxiv.org/pdf/2507.15612v1",
      "arxiv_url": "http://arxiv.org/abs/2507.15612v1",
      "published": "2025-07-21",
      "categories": [
        "stat.ME"
      ]
    },
    {
      "title": "From Kicking to Causality: Simulating Infant Agency Detection with a Robust Intrinsic Reward",
      "authors": [
        "Xia Xu",
        "Jochen Triesch"
      ],
      "abstract": "While human infants robustly discover their own causal efficacy, standard\nreinforcement learning agents remain brittle, as their reliance on\ncorrelation-based rewards fails in noisy, ecologically valid scenarios. To\naddress this, we introduce the Causal Action Influence Score (CAIS), a novel\nintrinsic reward rooted in causal inference. CAIS quantifies an action's\ninfluence by measuring the 1-Wasserstein distance between the learned\ndistribution of sensory outcomes conditional on that action, $p(h|a)$, and the\nbaseline outcome distribution, $p(h)$. This divergence provides a robust reward\nthat isolates the agent's causal impact from confounding environmental noise.\nWe test our approach in a simulated infant-mobile environment where\ncorrelation-based perceptual rewards fail completely when the mobile is\nsubjected to external forces. In stark contrast, CAIS enables the agent to\nfilter this noise, identify its influence, and learn the correct policy.\nFurthermore, the high-quality predictive model learned for CAIS allows our\nagent, when augmented with a surprise signal, to successfully reproduce the\n\"extinction burst\" phenomenon. We conclude that explicitly inferring causality\nis a crucial mechanism for developing a robust sense of agency, offering a\npsychologically plausible framework for more adaptive autonomous systems.",
      "pdf_url": "http://arxiv.org/pdf/2507.15106v1",
      "arxiv_url": "http://arxiv.org/abs/2507.15106v1",
      "published": "2025-07-20",
      "categories": [
        "cs.AI",
        "cs.RO",
        "F.2.2"
      ]
    },
    {
      "title": "Kernel Based Maximum Entropy Inverse Reinforcement Learning for Mean-Field Games",
      "authors": [
        "Berkay Anahtarci",
        "Can Deha Kariksiz",
        "Naci Saldi"
      ],
      "abstract": "We consider the maximum causal entropy inverse reinforcement learning problem\nfor infinite-horizon stationary mean-field games, in which we model the unknown\nreward function within a reproducing kernel Hilbert space. This allows the\ninference of rich and potentially nonlinear reward structures directly from\nexpert demonstrations, in contrast to most existing inverse reinforcement\nlearning approaches for mean-field games that typically restrict the reward\nfunction to a linear combination of a fixed finite set of basis functions. We\nalso focus on the infinite-horizon cost structure, whereas prior studies\nprimarily rely on finite-horizon formulations. We introduce a Lagrangian\nrelaxation to this maximum causal entropy inverse reinforcement learning\nproblem that enables us to reformulate it as an unconstrained log-likelihood\nmaximization problem, and obtain a solution \\lk{via} a gradient ascent\nalgorithm. To illustrate the theoretical consistency of the algorithm, we\nestablish the smoothness of the log-likelihood objective by proving the\nFr\\'echet differentiability of the related soft Bellman operators with respect\nto the parameters in the reproducing kernel Hilbert space. We demonstrate the\neffectiveness of our method on a mean-field traffic routing game, where it\naccurately recovers expert behavior.",
      "pdf_url": "http://arxiv.org/pdf/2507.14529v1",
      "arxiv_url": "http://arxiv.org/abs/2507.14529v1",
      "published": "2025-07-19",
      "categories": [
        "cs.LG",
        "math.OC",
        "91A16, 68T05, 49N45, 93E20, 46E22"
      ]
    },
    {
      "title": "Positive-Unlabeled Learning for Control Group Construction in Observational Causal Inference",
      "authors": [
        "Ilias Tsoumas",
        "Dimitrios Bormpoudakis",
        "Vasileios Sitokonstantinou",
        "Athanasios Askitopoulos",
        "Andreas Kalogeras",
        "Charalampos Kontoes",
        "Ioannis Athanasiadis"
      ],
      "abstract": "In causal inference, whether through randomized controlled trials or\nobservational studies, access to both treated and control units is essential\nfor estimating the effect of a treatment on an outcome of interest. When\ntreatment assignment is random, the average treatment effect (ATE) can be\nestimated directly by comparing outcomes between groups. In non-randomized\nsettings, various techniques are employed to adjust for confounding and\napproximate the counterfactual scenario to recover an unbiased ATE. A common\nchallenge, especially in observational studies, is the absence of units clearly\nlabeled as controls-that is, units known not to have received the treatment. To\naddress this, we propose positive-unlabeled (PU) learning as a framework for\nidentifying, with high confidence, control units from a pool of unlabeled ones,\nusing only the available treated (positive) units. We evaluate this approach\nusing both simulated and real-world data. We construct a causal graph with\ndiverse relationships and use it to generate synthetic data under various\nscenarios, assessing how reliably the method recovers control groups that allow\nestimates of true ATE. We also apply our approach to real-world data on optimal\nsowing and fertilizer treatments in sustainable agriculture. Our findings show\nthat PU learning can successfully identify control (negative) units from\nunlabeled data based only on treated units and, through the resulting control\ngroup, estimate an ATE that closely approximates the true value. This work has\nimportant implications for observational causal inference, especially in fields\nwhere randomized experiments are difficult or costly. In domains such as earth,\nenvironmental, and agricultural sciences, it enables a plethora of\nquasi-experiments by leveraging available earth observation and climate data,\nparticularly when treated units are available but control units are lacking.",
      "pdf_url": "http://arxiv.org/pdf/2507.14528v1",
      "arxiv_url": "http://arxiv.org/abs/2507.14528v1",
      "published": "2025-07-19",
      "categories": [
        "cs.LG"
      ]
    },
    {
      "title": "How LLMs Comprehend Temporal Meaning in Narratives: A Case Study in Cognitive Evaluation of LLMs",
      "authors": [
        "Karin de Langis",
        "Jong Inn Park",
        "Andreas Schramm",
        "Bin Hu",
        "Khanh Chi Le",
        "Michael Mensink",
        "Ahn Thu Tong",
        "Dongyeop Kang"
      ],
      "abstract": "Large language models (LLMs) exhibit increasingly sophisticated linguistic\ncapabilities, yet the extent to which these behaviors reflect human-like\ncognition versus advanced pattern recognition remains an open question. In this\nstudy, we investigate how LLMs process the temporal meaning of linguistic\naspect in narratives that were previously used in human studies. Using an\nExpert-in-the-Loop probing pipeline, we conduct a series of targeted\nexperiments to assess whether LLMs construct semantic representations and\npragmatic inferences in a human-like manner. Our findings show that LLMs\nover-rely on prototypicality, produce inconsistent aspectual judgments, and\nstruggle with causal reasoning derived from aspect, raising concerns about\ntheir ability to fully comprehend narratives. These results suggest that LLMs\nprocess aspect fundamentally differently from humans and lack robust narrative\nunderstanding. Beyond these empirical findings, we develop a standardized\nexperimental framework for the reliable assessment of LLMs' cognitive and\nlinguistic capabilities.",
      "pdf_url": "http://arxiv.org/pdf/2507.14307v1",
      "arxiv_url": "http://arxiv.org/abs/2507.14307v1",
      "published": "2025-07-18",
      "categories": [
        "cs.CL"
      ]
    },
    {
      "title": "On the importance of tail assumptions in climate extreme event attribution",
      "authors": [
        "Mengran Li",
        "Daniela Castro-Camilo"
      ],
      "abstract": "Extreme weather events are becoming more frequent and intense, posing serious\nthreats to human life, biodiversity, and ecosystems. A key objective of extreme\nevent attribution (EEA) is to assess whether and to what extent anthropogenic\nclimate change influences such events. Central to EEA is the accurate\nstatistical characterization of atmospheric extremes, which are inherently\nmultivariate or spatial due to their measurement over high-dimensional grids.\nWithin the counterfactual causal inference framework of Pearl, we evaluate how\ntail assumptions affect attribution conclusions by comparing three multivariate\nmodeling approaches for estimating causation metrics. These include: (i) the\nmultivariate generalized Pareto distribution, which imposes an invariant tail\ndependence structure; (ii) the factor copula model of Castro-Camilo and Huser\n(2020), which offers flexible subasymptotic behavior; and (iii) the model of\nHuser and Wadsworth (2019), which smoothly transitions between different forms\nof extremal dependence. We assess the implications of these modeling choices in\nboth simulated scenarios (under varying forms of model misspecification) and\nreal data applications, using weekly winter maxima over Europe from the\nM\\'et\\'eo-France CNRM model and daily precipitation from the ACCESS-CM2 model\nover the U.S. Our findings highlight that tail assumptions critically shape\ncausality metrics in EEA. Misspecification of the extremal dependence structure\ncan lead to substantially different and potentially misleading attribution\nconclusions, underscoring the need for careful model selection and evaluation\nwhen quantifying the influence of climate change on extreme events.",
      "pdf_url": "http://arxiv.org/pdf/2507.14019v1",
      "arxiv_url": "http://arxiv.org/abs/2507.14019v1",
      "published": "2025-07-18",
      "categories": [
        "stat.AP",
        "62G32, 62H10, 62P12, 62H20",
        "G.3"
      ]
    },
    {
      "title": "A regression-based approach for bidirectional proximal causal inference in the presence of unmeasured confounding",
      "authors": [
        "Jiaqi Min",
        "Xueyue Zhang",
        "Shanshan Luo"
      ],
      "abstract": "Proxy variables are commonly used in causal inference when unmeasured\nconfounding exists. While most existing proximal methods assume a\nunidirectional causal relationship between two primary variables, many social\nand biological systems exhibit complex feedback mechanisms that imply\nbidirectional causality. In this paper, using regression-based models, we\nextend the proximal framework to identify bidirectional causal effects in the\npresence of unmeasured confounding. We establish the identification of\nbidirectional causal effects and develop a sensitivity analysis method for\nviolations of the proxy structural conditions. Building on this identification\nresult, we derive bidirectional two-stage least squares estimators that are\nconsistent and asymptotically normal under standard regularity conditions.\nSimulation studies demonstrate that our approach delivers unbiased causal\neffect estimates and outperforms some standard methods. The simulation results\nalso confirm the reliability of the sensitivity analysis procedure. Applying\nour methodology to a state-level panel dataset from 1985 to 2014 in the United\nStates, we examine the bidirectional causal effects between abortion rates and\nmurder rates. The analysis reveals a consistent negative effect of abortion\nrates on murder rates, while also detecting a potential reciprocal effect from\nmurder rates to abortion rates that conventional unidirectional analyses have\nnot considered.",
      "pdf_url": "http://arxiv.org/pdf/2507.13965v1",
      "arxiv_url": "http://arxiv.org/abs/2507.13965v1",
      "published": "2025-07-18",
      "categories": [
        "stat.ME"
      ]
    },
    {
      "title": "Reframing attention as a reinforcement learning problem for causal discovery",
      "authors": [
        "Turan Orujlu",
        "Christian Gumbsch",
        "Martin V. Butz",
        "Charley M Wu"
      ],
      "abstract": "Formal frameworks of causality have operated largely parallel to modern\ntrends in deep reinforcement learning (RL). However, there has been a revival\nof interest in formally grounding the representations learned by neural\nnetworks in causal concepts. Yet, most attempts at neural models of causality\nassume static causal graphs and ignore the dynamic nature of causal\ninteractions. In this work, we introduce Causal Process framework as a novel\ntheory for representing dynamic hypotheses about causal structure. Furthermore,\nwe present Causal Process Model as an implementation of this framework. This\nallows us to reformulate the attention mechanism popularized by Transformer\nnetworks within an RL setting with the goal to infer interpretable causal\nprocesses from visual observations. Here, causal inference corresponds to\nconstructing a causal graph hypothesis which itself becomes an RL task nested\nwithin the original RL problem. To create an instance of such hypothesis, we\nemploy RL agents. These agents establish links between units similar to the\noriginal Transformer attention mechanism. We demonstrate the effectiveness of\nour approach in an RL environment where we outperform current alternatives in\ncausal representation learning and agent performance, and uniquely recover\ngraphs of dynamic causal processes.",
      "pdf_url": "http://arxiv.org/pdf/2507.13920v1",
      "arxiv_url": "http://arxiv.org/abs/2507.13920v1",
      "published": "2025-07-18",
      "categories": [
        "cs.LG"
      ]
    },
    {
      "title": "CoTasks: Chain-of-Thought based Video Instruction Tuning Tasks",
      "authors": [
        "Yanan Wang",
        "Julio Vizcarra",
        "Zhi Li",
        "Hao Niu",
        "Mori Kurokawa"
      ],
      "abstract": "Despite recent progress in video large language models (VideoLLMs), a key\nopen challenge remains: how to equip models with chain-of-thought (CoT)\nreasoning abilities grounded in fine-grained object-level video understanding.\nExisting instruction-tuned models, such as the Qwen and LLaVA series, are\ntrained on high-level video-text pairs, often lacking structured annotations\nnecessary for compositional, step-by-step reasoning. We propose CoTasks:\nChain-of-Thought based Video Instruction Tuning Tasks, a new framework that\ndecomposes complex video questions of existing datasets (e.g., NeXT-QA, STAR)\ninto four entity-level foundational tasks: frame localization, entity tracking,\nspatial and temporal relation extraction. By embedding these intermediate\nCoT-style reasoning steps into the input, CoTasks enables models to explicitly\nperform object-centric spatiotemporal reasoning. Experiments on the NeXT-QA\nbenchmark show that CoTasks significantly enhance inference performance:\nLLaVA-video-7B improves by +3.3 points in average GPT-4 evaluation score, and\nQwen2.5-VL-3B gains +17.4, with large boosts in causal (+14.6), temporal\n(+10.9), and descriptive (+48.1) subcategories. These results demonstrate the\neffectiveness of CoTasks as a structured CoT-style supervision framework for\nimproving compositional video reasoning.",
      "pdf_url": "http://arxiv.org/pdf/2507.13609v1",
      "arxiv_url": "http://arxiv.org/abs/2507.13609v1",
      "published": "2025-07-18",
      "categories": [
        "cs.CV",
        "cs.CL"
      ]
    }
  ]
}