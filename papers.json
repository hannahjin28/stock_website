{
  "last_updated": "2025-05-30T00:53:58.571522",
  "papers": [
    {
      "title": "A Synthetic Business Cycle Approach to Counterfactual Analysis with Nonstationary Macroeconomic Data",
      "authors": [
        "Zhentao Shi",
        "Jin Xi",
        "Haitian Xie"
      ],
      "abstract": "This paper investigates the use of synthetic control methods for causal\ninference in macroeconomic settings when dealing with possibly nonstationary\ndata. While the synthetic control approach has gained popularity for estimating\ncounterfactual outcomes, we caution researchers against assuming a common\nnonstationary trend factor across units for macroeconomic outcomes, as doing so\nmay result in misleading causal estimation-a pitfall we refer to as the\nspurious synthetic control problem. To address this issue, we propose a\nsynthetic business cycle framework that explicitly separates trend and cyclical\ncomponents. By leveraging the treated unit's historical data to forecast its\ntrend and using control units only for cyclical fluctuations, our\ndivide-and-conquer strategy eliminates spurious correlations and improves the\nrobustness of counterfactual prediction in macroeconomic applications. As\nempirical illustrations, we examine the cases of German reunification and the\nhandover of Hong Kong, demonstrating the advantages of the proposed approach.",
      "pdf_url": "http://arxiv.org/pdf/2505.22388v1",
      "arxiv_url": "http://arxiv.org/abs/2505.22388v1",
      "published": "2025-05-28",
      "categories": [
        "econ.EM"
      ]
    },
    {
      "title": "Causal Inference for Experiments with Latent Outcomes: Key Results and Their Implications for Design and Analysis",
      "authors": [
        "Jiawei Fu",
        "Donald P. Green"
      ],
      "abstract": "How should we analyze randomized experiments in which the main outcome is\nmeasured in multiple ways and each measure contains some degree of error? Since\nCostner (1971) and Bagozzi (1977), methodological discussions of experiments\nwith latent outcomes have reviewed the modeling assumptions that are invoked\nwhen the quantity of interest is the average treatment effect (ATE) of a\nrandomized intervention on a latent outcome that is measured with error. Many\nauthors have proposed methods to estimate this ATE when multiple measures of an\noutcome are available. Despite this extensive literature, social scientists\nrarely use these modeling approaches when analyzing experimental data, perhaps\nbecause the surge of interest in experiments coincides with increased\nskepticism about the modeling assumptions that these methods invoke. The\npresent paper takes a fresh look at the use of latent variable models to\nanalyze experiments. Like the skeptics, we seek to minimize reliance on ad hoc\nassumptions that are not rooted in the experimental design and measurement\nstrategy. At the same time, we think that some of the misgivings that are\nfrequently expressed about latent variable models can be addressed by modifying\nthe research design in ways that make the underlying assumptions defensible or\ntestable. We describe modeling approaches that enable researchers to identify\nand estimate key parameters of interest, suggest ways that experimental designs\ncan be augmented so as to make the modeling requirements more credible, and\ndiscuss empirical tests of key modeling assumptions. Simulations and an\nempirical application illustrate the gains in terms of precision and\nrobustness.",
      "pdf_url": "http://arxiv.org/pdf/2505.21909v1",
      "arxiv_url": "http://arxiv.org/abs/2505.21909v1",
      "published": "2025-05-28",
      "categories": [
        "econ.EM",
        "stat.AP",
        "stat.ME"
      ]
    },
    {
      "title": "MAMBO-NET: Multi-Causal Aware Modeling Backdoor-Intervention Optimization for Medical Image Segmentation Network",
      "authors": [
        "Ruiguo Yu",
        "Yiyang Zhang",
        "Yuan Tian",
        "Yujie Diao",
        "Di Jin",
        "Witold Pedrycz"
      ],
      "abstract": "Medical image segmentation methods generally assume that the process from\nmedical image to segmentation is unbiased, and use neural networks to establish\nconditional probability models to complete the segmentation task. This\nassumption does not consider confusion factors, which can affect medical\nimages, such as complex anatomical variations and imaging modality limitations.\nConfusion factors obfuscate the relevance and causality of medical image\nsegmentation, leading to unsatisfactory segmentation results. To address this\nissue, we propose a multi-causal aware modeling backdoor-intervention\noptimization (MAMBO-NET) network for medical image segmentation. Drawing\ninsights from causal inference, MAMBO-NET utilizes self-modeling with\nmulti-Gaussian distributions to fit the confusion factors and introduce causal\nintervention into the segmentation process. Moreover, we design appropriate\nposterior probability constraints to effectively train the distributions of\nconfusion factors. For the distributions to effectively guide the segmentation\nand mitigate and eliminate the Impact of confusion factors on the segmentation,\nwe introduce classical backdoor intervention techniques and analyze their\nfeasibility in the segmentation task. To evaluate the effectiveness of our\napproach, we conducted extensive experiments on five medical image datasets.\nThe results demonstrate that our method significantly reduces the influence of\nconfusion factors, leading to enhanced segmentation accuracy.",
      "pdf_url": "http://arxiv.org/pdf/2505.21874v1",
      "arxiv_url": "http://arxiv.org/abs/2505.21874v1",
      "published": "2025-05-28",
      "categories": [
        "eess.IV",
        "cs.CV"
      ]
    },
    {
      "title": "Causal Posterior Estimation",
      "authors": [
        "Simon Dirmeier",
        "Antonietta Mira"
      ],
      "abstract": "We present Causal Posterior Estimation (CPE), a novel method for Bayesian\ninference in simulator models, i.e., models where the evaluation of the\nlikelihood function is intractable or too computationally expensive, but where\none can simulate model outputs given parameter values. CPE utilizes a\nnormalizing flow-based (NF) approximation to the posterior distribution which\ncarefully incorporates the conditional dependence structure induced by the\ngraphical representation of the model into the neural network. Thereby it is\npossible to improve the accuracy of the approximation. We introduce both\ndiscrete and continuous NF architectures for CPE and propose a constant-time\nsampling procedure for the continuous case which reduces the computational\ncomplexity of drawing samples to O(1) as for discrete NFs. We show, through an\nextensive experimental evaluation, that by incorporating the conditional\ndependencies induced by the graphical model directly into the neural network,\nrather than learning them from data, CPE is able to conduct highly accurate\nposterior inference either outperforming or matching the state of the art in\nthe field.",
      "pdf_url": "http://arxiv.org/pdf/2505.21468v1",
      "arxiv_url": "http://arxiv.org/abs/2505.21468v1",
      "published": "2025-05-27",
      "categories": [
        "cs.LG",
        "stat.ML"
      ]
    },
    {
      "title": "A Bayesian approach to the survivor average causal effect in cluster-randomized crossover trials",
      "authors": [
        "Dane Isenberg",
        "Michael O. Harhay",
        "Andrew B. Forbes",
        "Paul J. Young",
        "Fan Li",
        "Nandita Mitra"
      ],
      "abstract": "In cluster-randomized crossover (CRXO) trials, groups of individuals are\nrandomly assigned to two or more sequences of alternating treatments. Since\nclusters act as their own control, the CRXO design is typically more\nstatistically efficient than the usual parallel-arm trial. CRXO trials are\nincreasingly popular in many areas of health research where the number of\navailable clusters is limited. Further, in trials among severely ill patients,\nresearchers often want to assess the effect of treatments on secondary\nnon-terminal outcomes, but frequently in these studies, there are patients who\ndo not survive to have these measurements fully recorded. In this paper, we\nprovide a causal inference framework and treatment effect estimation methods\nfor addressing truncation by death in the setting of CRXO trials. We target the\nsurvivor average causal effect (SACE) estimand, a well-defined subgroup\ntreatment effect obtained via principal stratification. We propose novel\nstructural and standard modeling assumptions to enable SACE identification\nfollowed by estimation within a Bayesian paradigm. We evaluate the small-sample\nperformance of our proposed Bayesian approach for the estimation of the SACE in\nCRXO trial settings via simulation studies. We apply our methods to a\npreviously conducted two-period cross-sectional CRXO study examining the impact\nof proton pump inhibitors compared to histamine-2 receptor blockers on length\nof hospitalization among adults requiring invasive mechanical ventilation.",
      "pdf_url": "http://arxiv.org/pdf/2505.21447v1",
      "arxiv_url": "http://arxiv.org/abs/2505.21447v1",
      "published": "2025-05-27",
      "categories": [
        "stat.ME"
      ]
    },
    {
      "title": "The Effect of the Gotthard Base Tunnel on Road Traffic: A Synthetic Control Approach",
      "authors": [
        "Hannes Wallimann",
        "Widar von Arx",
        "Ann Hesse"
      ],
      "abstract": "The opening of the Gotthard Base Tunnel in 2017, the longest railway tunnel\nin the world, marked a milestone in Swiss transport policy. The tunnel, a part\nof the New Rail Link through the Alps, serves as a key instrument of the\nso-called \"modal shift policy,\" which aims to transfer transalpine freight\ntraffic from road to rail. The reduction in travel time by train between\nnorthern and southern Switzerland raised expectations that a substantial share\nof tourist-oriented passenger traffic would also shift from car to rail. In\nthis paper, we conduct a causal analysis of the impact of the Gotthard Base\nTunnel's opening at the end of 2016 on the number of cars using the parallel\nGotthard motorway section in the subsequent years. To this end, we apply the\nsynthetic control and the synthetic difference-in-differences methods to\nconstruct a synthetic Gotthard motorway section based on a weighted combination\nof other alpine road crossings (a so-called donor pool) that did not experience\nthe construction of a competing rail infrastructure. Our results reveal only a\nmodest but statistically significant decline in the number of cars between the\nactual and the synthetic Gotthard motorway in the short run. Given the\nconsistently strong and increasing demand for the new rail connection through\nthe Gotthard Base Tunnel, we infer a substantial induced short-run demand\neffect resulting from the rail travel time savings.",
      "pdf_url": "http://arxiv.org/pdf/2505.21129v1",
      "arxiv_url": "http://arxiv.org/abs/2505.21129v1",
      "published": "2025-05-27",
      "categories": [
        "econ.GN",
        "q-fin.EC"
      ]
    },
    {
      "title": "Study of Lightweight Transformer Architectures for Single-Channel Speech Enhancement",
      "authors": [
        "Haixin Zhao",
        "Nilesh Madhu"
      ],
      "abstract": "In speech enhancement, achieving state-of-the-art (SotA) performance while\nadhering to the computational constraints on edge devices remains a formidable\nchallenge. Networks integrating stacked temporal and spectral modelling\neffectively leverage improved architectures such as transformers; however, they\ninevitably incur substantial computational complexity and model expansion.\nThrough systematic ablation analysis on transformer-based temporal and spectral\nmodelling, we demonstrate that the architecture employing streamlined\nFrequency-Time-Frequency (FTF) stacked transformers efficiently learns global\ndependencies within causal context, while avoiding considerable computational\ndemands. Utilising discriminators in training further improves learning\nefficacy and enhancement without introducing additional complexity during\ninference. The proposed lightweight, causal, transformer-based architecture\nwith adversarial training (LCT-GAN) yields SoTA performance on instrumental\nmetrics among contemporary lightweight models, but with far less overhead.\nCompared to DeepFilterNet2, the LCT-GAN only requires 6% of the parameters, at\nsimilar complexity and performance. Against CCFNet+(Lite), LCT-GAN saves 9% in\nparameters and 10% in multiply-accumulate operations yet yielding improved\nperformance. Further, the LCT-GAN even outperforms more complex, common\nbaseline models on widely used test datasets.",
      "pdf_url": "http://arxiv.org/pdf/2505.21057v1",
      "arxiv_url": "http://arxiv.org/abs/2505.21057v1",
      "published": "2025-05-27",
      "categories": [
        "eess.AS",
        "cs.SD"
      ]
    },
    {
      "title": "Debiased Ill-Posed Regression",
      "authors": [
        "AmirEmad Ghassami",
        "James M. Robins",
        "Andrea Rotnitzky"
      ],
      "abstract": "In various statistical settings, the goal is to estimate a function which is\nrestricted by the statistical model only through a conditional moment\nrestriction. Prominent examples include the nonparametric instrumental variable\nframework for estimating the structural function of the outcome variable, and\nthe proximal causal inference framework for estimating the bridge functions. A\ncommon strategy in the literature is to find the minimizer of the projected\nmean squared error. However, this approach can be sensitive to misspecification\nor slow convergence rate of the estimators of the involved nuisance components.\nIn this work, we propose a debiased estimation strategy based on the influence\nfunction of a modification of the projected error and demonstrate its\nfinite-sample convergence rate. Our proposed estimator possesses a second-order\nbias with respect to the involved nuisance functions and a desirable robustness\nproperty with respect to the misspecification of one of the nuisance functions.\nThe proposed estimator involves a hyper-parameter, for which the optimal value\ndepends on potentially unknown features of the underlying data-generating\nprocess. Hence, we further propose a hyper-parameter selection approach based\non cross-validation and derive an error bound for the resulting estimator. This\nanalysis highlights the potential rate loss due to hyper-parameter selection\nand underscore the importance and advantages of incorporating debiasing in this\nsetting. We also study the application of our approach to the estimation of\nregular parameters in a specific parameter class, which are linear functionals\nof the solutions to the conditional moment restrictions and provide sufficient\nconditions for achieving root-n consistency using our debiased estimator.",
      "pdf_url": "http://arxiv.org/pdf/2505.20787v1",
      "arxiv_url": "http://arxiv.org/abs/2505.20787v1",
      "published": "2025-05-27",
      "categories": [
        "stat.ME",
        "econ.EM",
        "stat.ML"
      ]
    },
    {
      "title": "Causal inference with dyadic data in randomized experiments",
      "authors": [
        "Yilin Li",
        "Lu Deng",
        "Yong Wang",
        "Wang Miao"
      ],
      "abstract": "Estimating the treatment effect within network structures is a key focus in\nonline controlled experiments, particularly for social media platforms. We\ninvestigate a scenario where the unit-level outcome of interest comprises a\nseries of dyadic outcomes, which is pervasive in many social network sources,\nspanning from microscale point-to-point messaging to macroscale international\ntrades. Dyadic outcomes are of particular interest in online controlled\nexperiments, capturing pairwise interactions as basic units for analysis. The\ndyadic nature of the data induces interference, as treatment assigned to one\nunit may affect outcomes involving connected pairs. We propose a novel\ndesign-based causal inference framework for dyadic outcomes in randomized\nexperiments, develop estimators of the global average causal effect, and\nestablish their asymptotic properties under different randomization designs. We\nprove the central limit theorem for the estimators and propose variance\nestimators to quantify the estimation uncertainty. The advantages of\nintegrating dyadic data in randomized experiments are manifested in a variety\nof numerical experiments, especially in correcting interference bias. We\nimplement our proposed method in a large-scale experiment on WeChat Channels,\nassessing the impact of a recommendation algorithm on users' interaction\nmetrics.",
      "pdf_url": "http://arxiv.org/pdf/2505.20780v1",
      "arxiv_url": "http://arxiv.org/abs/2505.20780v1",
      "published": "2025-05-27",
      "categories": [
        "stat.ME",
        "stat.AP"
      ]
    },
    {
      "title": "Mamba-Driven Topology Fusion for Monocular 3-D Human Pose Estimation",
      "authors": [
        "Zenghao Zheng",
        "Lianping Yang",
        "Jinshan Pan",
        "Hegui Zhu"
      ],
      "abstract": "Transformer-based methods for 3-D human pose estimation face significant\ncomputational challenges due to the quadratic growth of self-attention\nmechanism complexity with sequence length. Recently, the Mamba model has\nsubstantially reduced computational overhead and demonstrated outstanding\nperformance in modeling long sequences by leveraging state space model (SSM).\nHowever, the ability of SSM to process sequential data is not suitable for 3-D\njoint sequences with topological structures, and the causal convolution\nstructure in Mamba also lacks insight into local joint relationships. To\naddress these issues, we propose the Mamba-Driven Topology Fusion framework in\nthis paper. Specifically, the proposed Bone Aware Module infers the direction\nand length of bone vectors in the spherical coordinate system, providing\neffective topological guidance for the Mamba model in processing joint\nsequences. Furthermore, we enhance the convolutional structure within the Mamba\nmodel by integrating forward and backward graph convolutional network, enabling\nit to better capture local joint dependencies. Finally, we design a\nSpatiotemporal Refinement Module to model both temporal and spatial\nrelationships within the sequence. Through the incorporation of skeletal\ntopology, our approach effectively alleviates Mamba's limitations in capturing\nhuman structural relationships. We conduct extensive experiments on the\nHuman3.6M and MPI-INF-3DHP datasets for testing and comparison, and the results\nshow that the proposed method greatly reduces computational cost while\nachieving higher accuracy. Ablation studies further demonstrate the\neffectiveness of each proposed module. The code and models will be released.",
      "pdf_url": "http://arxiv.org/pdf/2505.20611v1",
      "arxiv_url": "http://arxiv.org/abs/2505.20611v1",
      "published": "2025-05-27",
      "categories": [
        "cs.CV"
      ]
    }
  ]
}