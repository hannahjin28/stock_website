{
  "last_updated": "2026-01-23T01:00:09.637940",
  "papers": [
    {
      "title": "Many Experiments, Few Repetitions, Unpaired Data, and Sparse Effects: Is Causal Inference Possible?",
      "authors": [
        "Felix Schur",
        "Niklas Pfister",
        "Peng Ding",
        "Sach Mukherjee",
        "Jonas Peters"
      ],
      "abstract": "We study the problem of estimating causal effects under hidden confounding in the following unpaired data setting: we observe some covariates $X$ and an outcome $Y$ under different experimental conditions (environments) but do not observe them jointly; we either observe $X$ or $Y$. Under appropriate regularity conditions, the problem can be cast as an instrumental variable (IV) regression with the environment acting as a (possibly high-dimensional) instrument. When there are many environments but only a few observations per environment, standard two-sample IV estimators fail to be consistent. We propose a GMM-type estimator based on cross-fold sample splitting of the instrument-covariate sample and prove that it is consistent as the number of environments grows but the sample size per environment remains constant. We further extend the method to sparse causal effects via $\\ell_1$-regularized estimation and post-selection refitting.",
      "pdf_url": "https://arxiv.org/pdf/2601.15254v1",
      "arxiv_url": "http://arxiv.org/abs/2601.15254v1",
      "published": "2026-01-21",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "RadixMLP -- Intra-batch Deduplication for Causal Transformers",
      "authors": [
        "Michael Feil",
        "Julius Lipp"
      ],
      "abstract": "Batch inference workloads for causal transformer models frequently process sequences that share common prefixes, such as system prompts, few-shot examples, or shared queries. Standard inference engines treat each sequence independently, redundantly recomputing identical MLP activations for every copy of the shared prefix. We introduce RadixMLP, a technique that exploits the position-wise nature of MLPs, LayerNorms, linear projections, and embeddings to eliminate this redundancy. RadixMLP dynamically maps batches to a prefix trie, gathering shared segments into a compressed representation for position-wise computation and scattering results back only at attention boundaries. RadixMLP is stateless and operates within a single forward pass. In end-to-end serving benchmarks on MS~MARCO v1.1 with Qwen3 models (0.6B to 8B parameters), RadixMLP achieves 1.44-1.59$\\times$ speedups in realistic reranking workloads, with up to $5\\times$ speedups on synthetic benchmarks with longer shared prefixes. Our code is available at https://github.com/michaelfeil/radix-mlp.",
      "pdf_url": "https://arxiv.org/pdf/2601.15013v1",
      "arxiv_url": "http://arxiv.org/abs/2601.15013v1",
      "published": "2026-01-21",
      "categories": [
        "cs.LG",
        "cs.DC"
      ]
    },
    {
      "title": "Mirai: Autoregressive Visual Generation Needs Foresight",
      "authors": [
        "Yonghao Yu",
        "Lang Huang",
        "Zerun Wang",
        "Runyi Li",
        "Toshihiko Yamasaki"
      ],
      "abstract": "Autoregressive (AR) visual generators model images as sequences of discrete tokens and are trained with next token likelihood. This strict causality supervision optimizes each step only by its immediate next token, which diminishes global coherence and slows convergence. We ask whether foresight, training signals that originate from later tokens, can help AR visual generation. We conduct a series of controlled diagnostics along the injection level, foresight layout, and foresight source axes, unveiling a key insight: aligning foresight to AR models' internal representation on the 2D image grids improves causality modeling. We formulate this insight with Mirai (meaning \"future\" in Japanese), a general framework that injects future information into AR training with no architecture change and no extra inference overhead: Mirai-E uses explicit foresight from multiple future positions of unidirectional representations, whereas Mirai-I leverages implicit foresight from matched bidirectional representations. Extensive experiments show that Mirai significantly accelerates convergence and improves generation quality. For instance, Mirai can speed up LlamaGen-B's convergence by up to 10$\\times$ and reduce the generation FID from 5.34 to 4.34 on the ImageNet class-condition image generation benchmark. Our study highlights that visual autoregressive models need foresight.",
      "pdf_url": "https://arxiv.org/pdf/2601.14671v1",
      "arxiv_url": "http://arxiv.org/abs/2601.14671v1",
      "published": "2026-01-21",
      "categories": [
        "cs.CV"
      ]
    },
    {
      "title": "Causal feature selection framework for stable soft sensor modeling based on time-delayed cross mapping",
      "authors": [
        "Shi-Shun Chen",
        "Xiao-Yang Li",
        "Enrico Zio"
      ],
      "abstract": "Soft sensor modeling plays a crucial role in process monitoring. Causal feature selection can enhance the performance of soft sensor models in industrial applications. However, existing methods ignore two critical characteristics of industrial processes. Firstly, causal relationships between variables always involve time delays, whereas most causal feature selection methods investigate causal relationships in the same time dimension. Secondly, variables in industrial processes are often interdependent, which contradicts the decorrelation assumption of traditional causal inference methods. Consequently, soft sensor models based on existing causal feature selection approaches often lack sufficient accuracy and stability. To overcome these challenges, this paper proposes a causal feature selection framework based on time-delayed cross mapping. Time-delayed cross mapping employs state space reconstruction to effectively handle interdependent variables in causality analysis, and considers varying causal strength across time delay. Time-delayed convergent cross mapping (TDCCM) is introduced for total causal inference, and time-delayed partial cross mapping (TDPCM) is developed for direct causal inference. Then, in order to achieve automatic feature selection, an objective feature selection strategy is presented. The causal threshold is automatically determined based on the model performance on the validation set, and the causal features are then selected. Two real-world case studies show that TDCCM achieves the highest average performance, while TDPCM improves soft sensor stability and performance in the worst scenario. The code is publicly available at https://github.com/dirge1/TDPCM.",
      "pdf_url": "https://arxiv.org/pdf/2601.14099v1",
      "arxiv_url": "http://arxiv.org/abs/2601.14099v1",
      "published": "2026-01-20",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Component systems: do null models explain everything?",
      "authors": [
        "Andrea Mazzolini",
        "Mattia Corigliano",
        "Rossana Droghetti",
        "Matteo Osella",
        "Marco Cosentino-Lagomarsino"
      ],
      "abstract": "Component systems - ensembles of realizations built from a shared repertoire of modular parts - are ubiquitous in biological, ecological, technological, and socio-cultural domains. From genomes to texts, cities, and software, these systems exhibit statistical regularities that often meet the \"bona fide\" requirements of laws in the physical sciences. Here, we argue that the generality and simplicity of those laws are often due to basic combinatorial or sampling constraints, raising the question of whether such patterns are actually revealing system-specific mechanisms and how we might move beyond them. To this end, we first present a unifying mathematical framework, which allows us to compare modular systems in different fields and highlights the common \"null\" trends as well as the system-specific uniqueness, which, arguably, are signatures of the underlying generative dynamics. Next, we can exploit the framework with statistical mechanics and modern machine-learning tools for a twofold objective. (i) Explaining why the general regularities emerge, highlighting the constraints between them and the general principles at their origins, and (ii) \"subtracting\" them from data, which will isolate the informative features for inferring hidden system-specific generative processes, mechanistic and causal aspects.",
      "pdf_url": "https://arxiv.org/pdf/2601.13985v1",
      "arxiv_url": "http://arxiv.org/abs/2601.13985v1",
      "published": "2026-01-20",
      "categories": [
        "cond-mat.stat-mech",
        "q-bio.OT"
      ]
    },
    {
      "title": "Are Large Language Models able to Predict Highly Cited Papers? Evidence from Statistical Publications",
      "authors": [
        "Zhanshuo Ye",
        "Yiming Hou",
        "Rui Pan",
        "Tianchen Gao",
        "Hansheng Wang"
      ],
      "abstract": "Predicting highly-cited papers is a long-standing challenge due to the complex interactions of research content, scholarly communities, and temporal dynamics. Recent advances in large language models (LLMs) raise the question of whether early-stage textual information can provide useful signals of long-term scientific impact. Focusing on statistical publications, we propose a flexible, text-centered framework that leverages LLMs and structured prompt design to predict highly cited papers. Specifically, we utilize information available at the time of publication, including titles, abstracts, keywords, and limited bibliographic metadata. Using a large corpus of statistical papers, we evaluate predictive performance across multiple publication periods and alternative definitions of highly cited papers. The proposed approach achieves stable and competitive performance relative to existing methods and demonstrates strong generalization over time. Textual analysis further reveals that papers predicted as highly cited concentrate on recurring topics such as causal inference and deep learning. To facilitate practical use of the proposed approach, we further develop a WeChat mini program, \\textit{Stat Highly Cited Papers}, which provides an accessible interface for early-stage citation impact assessment. Overall, our results provide empirical evidence that LLMs can capture meaningful early signals of long-term citation impact, while also highlighting their limitations as tools for research impact assessment.",
      "pdf_url": "https://arxiv.org/pdf/2601.13627v1",
      "arxiv_url": "http://arxiv.org/abs/2601.13627v1",
      "published": "2026-01-20",
      "categories": [
        "stat.AP"
      ]
    },
    {
      "title": "What is Overlap Weighting, How Has it Evolved, and When to Use It for Causal Inference?",
      "authors": [
        "Haidong Lu",
        "Fan Li",
        "Laine E. Thomas",
        "Fan Li"
      ],
      "abstract": "The growing availability of large health databases has expanded the use of observational studies for comparative effectiveness research. Unlike randomized trials, observational studies must adjust for systematic differences in patient characteristics between treatment groups. Propensity score methods, including matching, weighting, stratification, and regression adjustment, address this issue by creating groups that are comparable with respect to measured covariates. Among these approaches, overlap weighting (OW) has emerged as a principled and efficient method that emphasizes individuals at empirical equipoise, those who could plausibly receive either treatment. By assigning weights proportional to the probability of receiving the opposite treatment, OW targets the Average Treatment Effect in the Overlap population (ATO), achieves exact mean covariate balance under logistic propensity score models, and minimizes asymptotic variance. Over the last decade, the OW method has been recognized as a valuable confounding adjustment tool across the statistical, epidemiologic, and clinical research communities, and is increasingly applied in clinical and health studies. Given the growing interest in using observational data to emulate randomized trials and the capacity of OW to prioritize populations at clinical equipoise while achieving covariate balance (fundamental attributes of randomized studies), this article provides a concise overview of recent methodological developments in OW and practical guidance on when it represents a suitable choice for causal inference.",
      "pdf_url": "https://arxiv.org/pdf/2601.13535v1",
      "arxiv_url": "http://arxiv.org/abs/2601.13535v1",
      "published": "2026-01-20",
      "categories": [
        "stat.ME",
        "stat.AP"
      ]
    },
    {
      "title": "Two-stage least squares with clustered data",
      "authors": [
        "Anqi Zhao",
        "Peng Ding",
        "Fan Li"
      ],
      "abstract": "Clustered data -- where units of observation are nested within higher-level groups, such as repeated measurements on users, or panel data of firms, industries, or geographic regions -- are ubiquitous in business research. When the objective is to estimate the causal effect of a potentially endogenous treatment, a common approach -- which we call the canonical two-stage least squares (2sls) -- is to fit a 2sls regression of the outcome on treatment status with instrumental variables (IVs) for point estimation, and apply cluster-robust standard errors to account for clustering in inference. When both the treatment and IVs vary within clusters, a natural alternative -- which we call the two-stage least squares with fixed effects (2sfe) -- is to include cluster indicators in the 2sls specification, thereby incorporating cluster information in point estimation as well. This paper clarifies the trade-off between these two approaches within the local average treatment effect (LATE) framework, and makes three contributions. First, we establish the validity of both approaches for Wald-type inference of the LATE when clusters are homogeneous, and characterize their relative efficiency. We show that, when the true outcome model includes cluster-specific effects, 2sfe is more efficient than the canonical 2sls only when the variation in cluster-specific effects dominates that in unit-level errors. Second, we show that with heterogeneous clusters, 2sfe recovers a weighted average of cluster-specific LATEs, whereas the canonical 2sls generally does not. Third, to guide empirical choice between the two procedures, we develop a joint asymptotic theory for the two estimators under homogeneous clusters, and propose a Wald-type test for detecting cluster heterogeneity.",
      "pdf_url": "https://arxiv.org/pdf/2601.13507v1",
      "arxiv_url": "http://arxiv.org/abs/2601.13507v1",
      "published": "2026-01-20",
      "categories": [
        "stat.ME"
      ]
    },
    {
      "title": "Propensity Score Propagation: A General Framework for Design-Based Inference with Unknown Propensity Scores",
      "authors": [
        "Siyu Heng",
        "Yanxin Shen",
        "Zijian Guo"
      ],
      "abstract": "Design-based inference, also known as randomization-based or finite-population inference, provides a principled framework for causal and descriptive analyses that attribute randomness solely to the design mechanism (e.g., treatment assignment, sampling, or missingness) without imposing distributional or modeling assumptions on the outcome data of study units. Despite its conceptual appeal and long history, this framework becomes challenging to apply when the underlying design probabilities (i.e., propensity scores) are unknown, as is common in observational studies, real-world surveys, and missing-data settings. Existing plug-in or matching-based approaches either ignore the uncertainty stemming from estimated propensity scores or rely on the post-matching uniform-propensity condition (an assumption typically violated when there are multiple or continuous covariates), leading to systematic under-coverage. Finite-population M-estimation partially mitigates these issues but remains limited to parametric propensity score models. In this work, we introduce propensity score propagation, a general framework for valid design-based inference with unknown propensity scores. The framework introduces a regeneration-and-union procedure that automatically propagates uncertainty in propensity score estimation into downstream design-based inference. It accommodates both parametric and nonparametric propensity score models, integrates seamlessly with standard tools in design-based inference with known propensity scores, and is universally applicable to various important design-based inference problems, such as observational studies, real-world surveys, and missing-data analyses, among many others. Simulation studies demonstrate that the proposed framework restores nominal coverage levels in settings where conventional methods suffer from severe under-coverage.",
      "pdf_url": "https://arxiv.org/pdf/2601.13150v1",
      "arxiv_url": "http://arxiv.org/abs/2601.13150v1",
      "published": "2026-01-19",
      "categories": [
        "stat.ME"
      ]
    },
    {
      "title": "Rerandomization for quantile treatment effects",
      "authors": [
        "Tingxuan Han",
        "Yuhao Wang"
      ],
      "abstract": "Although complete randomization is widely regarded as the gold standard for causal inference, covariate imbalance can still arise by chance in finite samples. Rerandomization has emerged as an effective tool to improve covariate balance across treatment groups and enhance the precision of causal effect estimation. While existing work focuses on average treatment effects, quantile treatment effects (QTEs) provide a richer characterization of treatment heterogeneity by capturing distributional shifts in outcomes, which is crucial for policy evaluation and equity-oriented research. In this article, we establish the asymptotic properties of the QTE estimator under rerandomization within a finite-population framework, without imposing any distributional or modeling assumptions on the covariates or outcomes.The estimator exhibits a non-Gaussian asymptotic distribution, represented as a linear combination of Gaussian and truncated Gaussian random variables. To facilitate inference, we propose a conservative variance estimator and construct corresponding confidence interval. Our theoretical analysis demonstrates that rerandomization improves efficiency over complete randomization under mild regularity conditions. Simulation studies further support the theoretical findings and illustrate the practical advantages of rerandomization for QTE estimation.",
      "pdf_url": "https://arxiv.org/pdf/2601.12540v1",
      "arxiv_url": "http://arxiv.org/abs/2601.12540v1",
      "published": "2026-01-18",
      "categories": [
        "stat.ME",
        "math.ST"
      ]
    }
  ]
}