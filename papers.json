{
  "last_updated": "2025-06-06T00:53:56.687836",
  "papers": [
    {
      "title": "What Makes Treatment Effects Identifiable? Characterizations and Estimators Beyond Unconfoundedness",
      "authors": [
        "Yang Cai",
        "Alkis Kalavasis",
        "Katerina Mamali",
        "Anay Mehrotra",
        "Manolis Zampetakis"
      ],
      "abstract": "Most of the widely used estimators of the average treatment effect (ATE) in\ncausal inference rely on the assumptions of unconfoundedness and overlap.\nUnconfoundedness requires that the observed covariates account for all\ncorrelations between the outcome and treatment. Overlap requires the existence\nof randomness in treatment decisions for all individuals. Nevertheless, many\ntypes of studies frequently violate unconfoundedness or overlap, for instance,\nobservational studies with deterministic treatment decisions -- popularly known\nas Regression Discontinuity designs -- violate overlap.\n  In this paper, we initiate the study of general conditions that enable the\nidentification of the average treatment effect, extending beyond\nunconfoundedness and overlap. In particular, following the paradigm of\nstatistical learning theory, we provide an interpretable condition that is\nsufficient and nearly necessary for the identification of ATE. Moreover, this\ncondition characterizes the identification of the average treatment effect on\nthe treated (ATT) and can be used to characterize other treatment effects as\nwell. To illustrate the utility of our condition, we present several\nwell-studied scenarios where our condition is satisfied and, hence, we prove\nthat ATE can be identified in regimes that prior works could not capture. For\nexample, under mild assumptions on the data distributions, this holds for the\nmodels proposed by Tan (2006) and Rosenbaum (2002), and the Regression\nDiscontinuity design model introduced by Thistlethwaite and Campbell (1960).\nFor each of these scenarios, we also show that, under natural additional\nassumptions, ATE can be estimated from finite samples.\n  We believe these findings open new avenues for bridging learning-theoretic\ninsights and causal inference methodologies, particularly in observational\nstudies with complex treatment mechanisms.",
      "pdf_url": "http://arxiv.org/pdf/2506.04194v1",
      "arxiv_url": "http://arxiv.org/abs/2506.04194v1",
      "published": "2025-06-04",
      "categories": [
        "math.ST",
        "cs.LG",
        "econ.EM",
        "stat.ME",
        "stat.ML",
        "stat.TH"
      ]
    },
    {
      "title": "N$^2$: A Unified Python Package and Test Bench for Nearest Neighbor-Based Matrix Completion",
      "authors": [
        "Caleb Chin",
        "Aashish Khubchandani",
        "Harshvardhan Maskara",
        "Kyuseong Choi",
        "Jacob Feitelberg",
        "Albert Gong",
        "Manit Paul",
        "Tathagata Sadhukhan",
        "Anish Agarwal",
        "Raaz Dwivedi"
      ],
      "abstract": "Nearest neighbor (NN) methods have re-emerged as competitive tools for matrix\ncompletion, offering strong empirical performance and recent theoretical\nguarantees, including entry-wise error bounds, confidence intervals, and\nminimax optimality. Despite their simplicity, recent work has shown that NN\napproaches are robust to a range of missingness patterns and effective across\ndiverse applications. This paper introduces N$^2$, a unified Python package and\ntestbed that consolidates a broad class of NN-based methods through a modular,\nextensible interface. Built for both researchers and practitioners, N$^2$\nsupports rapid experimentation and benchmarking. Using this framework, we\nintroduce a new NN variant that achieves state-of-the-art results in several\nsettings. We also release a benchmark suite of real-world datasets, from\nhealthcare and recommender systems to causal inference and LLM evaluation,\ndesigned to stress-test matrix completion methods beyond synthetic scenarios.\nOur experiments demonstrate that while classical methods excel on idealized\ndata, NN-based techniques consistently outperform them in real-world settings.",
      "pdf_url": "http://arxiv.org/pdf/2506.04166v1",
      "arxiv_url": "http://arxiv.org/abs/2506.04166v1",
      "published": "2025-06-04",
      "categories": [
        "cs.LG",
        "stat.CO",
        "stat.ML"
      ]
    },
    {
      "title": "Causal Inference with Missing Exposures, Missing Outcomes, and Dependence",
      "authors": [
        "Kirsten E. Landsiedel",
        "Rachel Abbott",
        "Atukunda Mucunguzi",
        "Florence Mwangwa",
        "Elijah Kakande",
        "Edwin D. Charlebois",
        "Carina Marquez",
        "Moses R. Kamya",
        "Laura B. Balzer"
      ],
      "abstract": "Missing data are ubiquitous in public health research. The\nmissing-completely-at-random (MCAR) assumption is often unrealistic and can\nlead to meaningful bias when violated. The missing-at-random (MAR) assumption\ntends to be more reasonable, but guidance on conducting causal analyses under\nMAR is limited when there is missingness on multiple variables. We present a\nseries of causal graphs and identification results to demonstrate the handling\nof missing exposures and outcomes in observational studies. For estimation and\ninference, we highlight the use of targeted minimum loss-based estimation\n(TMLE) with Super Learner to flexibly and robustly address confounding, missing\ndata, and dependence. Our work is motivated by SEARCH-TB's investigation of the\neffect of alcohol consumption on the risk of incident tuberculosis (TB)\ninfection in rural Uganda. This study posed notable challenges due to\nconfounding, missingness on the exposure (alcohol use), missingness on the\nbaseline outcome (defining who was at risk of TB), missingness on the outcome\nat follow-up (capturing who acquired TB), and clustering within households.\nApplication to real data from SEARCH-TB highlighted the real-world consequences\nof the discussed methods. Estimates from TMLE suggested that alcohol use was\nassociated with a 49% increase in the relative risk (RR) of incident TB\ninfection (RR=1.49, 95%CI: 1.39-1.59). These estimates were notably larger and\nmore precise than estimates from inverse probability weighting (RR=1.13, 95%CI:\n1.00-1.27) and unadjusted, complete case analyses (RR=1.18, 95%CI: 0.89-1.57).\nOur work demonstrates the utility of causal models for describing the missing\ndata mechanism and TMLE for flexible inference.",
      "pdf_url": "http://arxiv.org/pdf/2506.03336v1",
      "arxiv_url": "http://arxiv.org/abs/2506.03336v1",
      "published": "2025-06-03",
      "categories": [
        "stat.ME"
      ]
    },
    {
      "title": "TalkingMachines: Real-Time Audio-Driven FaceTime-Style Video via Autoregressive Diffusion Models",
      "authors": [
        "Chetwin Low",
        "Weimin Wang"
      ],
      "abstract": "In this paper, we present TalkingMachines -- an efficient framework that\ntransforms pretrained video generation models into real-time, audio-driven\ncharacter animators. TalkingMachines enables natural conversational experiences\nby integrating an audio large language model (LLM) with our video generation\nfoundation model. Our primary contributions include: (1) We adapt a pretrained\nSOTA image-to-video DiT into an audio-driven avatar generation model of 18\nbillion parameters; (2) We enable infinite video streaming without error\naccumulation through asymmetric knowledge distillation from a bidirectional\nteacher model into a sparse causal, autoregressive student model; (3) We design\na high-throughput, low-latency inference pipeline incorporating several key\nengineering optimizations such as: (a) disaggregation of the DiT and VAE\ndecoder across separate devices, (b) efficient overlap of inter-device\ncommunication and computation using CUDA streams, (c) elimination of redundant\nrecomputations to maximize frame-generation throughput. Please see demo videos\nhere - https://aaxwaz.github.io/TalkingMachines/",
      "pdf_url": "http://arxiv.org/pdf/2506.03099v1",
      "arxiv_url": "http://arxiv.org/abs/2506.03099v1",
      "published": "2025-06-03",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.GR"
      ]
    },
    {
      "title": "IP-Dialog: Evaluating Implicit Personalization in Dialogue Systems with Synthetic Data",
      "authors": [
        "Bo Peng",
        "Zhiheng Wang",
        "Heyang Gong",
        "Chaochao Lu"
      ],
      "abstract": "In modern dialogue systems, the ability to implicitly infer user backgrounds\nfrom conversations and leverage this information for personalized assistance is\ncrucial. However, the scarcity of high-quality data remains a fundamental\nchallenge to evaluating and improving this capability. Traditional dataset\nconstruction methods are labor-intensive, resource-demanding, and raise privacy\nconcerns. To address these issues, we propose a novel approach for automatic\nsynthetic data generation and introduce the Implicit Personalized Dialogue\n(IP-Dialog) benchmark along with a training dataset, covering 10 tasks and 12\nuser attribute types. Additionally, we develop a systematic evaluation\nframework with four metrics to assess both attribute awareness and reasoning\ncapabilities. We further propose five causal graphs to elucidate models'\nreasoning pathways during implicit personalization. Extensive experiments yield\ninsightful observations and prove the reliability of our dataset.",
      "pdf_url": "http://arxiv.org/pdf/2506.02449v1",
      "arxiv_url": "http://arxiv.org/abs/2506.02449v1",
      "published": "2025-06-03",
      "categories": [
        "cs.CL",
        "cs.HC"
      ]
    },
    {
      "title": "Spillovers and Effect Attenuation in Firearm Policy Research in the United States",
      "authors": [
        "Lee Kennedy-Shaffer",
        "Alan Hamilton Kennedy"
      ],
      "abstract": "In the United States, firearm-related deaths and injuries are a major public\nhealth issue. Because of limited federal action, state policies are\nparticularly important, and their evaluation informs the actions of other\npolicymakers. The movement of firearms across state and local borders, however,\ncan undermine the effectiveness of these policies and have statistical\nconsequences for their empirical evaluation. This movement causes spillover and\nbypass effects of policies, wherein interventions affect nearby control states\nand the lack of intervention in nearby states reduces the effectiveness in the\nintervention states. While some causal inference methods exist to account for\nspillover effects and reduce bias, these do not necessarily align well with the\ndata available for firearm research or with the most policy-relevant estimands.\nIntegrated data infrastructure and new methods are necessary for a better\nunderstanding of the effects these policies would have if widely adopted. In\nthe meantime, appropriately understanding and interpreting effect estimates\nfrom quasi-experimental analyses is crucial for ensuring that effective\npolicies are not dismissed due to these statistical challenges.",
      "pdf_url": "http://arxiv.org/pdf/2506.01695v1",
      "arxiv_url": "http://arxiv.org/abs/2506.01695v1",
      "published": "2025-06-02",
      "categories": [
        "stat.AP",
        "econ.EM",
        "62P25"
      ]
    },
    {
      "title": "A Diffusion-Based Method for Learning the Multi-Outcome Distribution of Medical Treatments",
      "authors": [
        "Yuchen Ma",
        "Jonas Schweisthal",
        "Hengrui Zhang",
        "Stefan Feuerriegel"
      ],
      "abstract": "In medicine, treatments often influence multiple, interdependent outcomes,\nsuch as primary endpoints, complications, adverse events, or other secondary\nendpoints. Hence, to make optimal treatment decisions, clinicians are\ninterested in learning the distribution of multi-dimensional treatment\noutcomes. However, the vast majority of machine learning methods for predicting\ntreatment effects focus on single-outcome settings, despite the fact that\nmedical data often include multiple, interdependent outcomes. To address this\nlimitation, we propose a novel diffusion-based method called DIME to learn the\njoint distribution of multiple outcomes of medical treatments. We addresses\nthree challenges relevant in medical practice: (i)it is tailored to learn the\njoint interventional distribution of multiple medical outcomes, which enables\nreliable decision-making with uncertainty quantification rather than relying\nsolely on point estimates; (ii)it explicitly captures the dependence structure\nbetween outcomes; (iii)it can handle outcomes of mixed type, including binary,\ncategorical, and continuous variables. In DIME, we take into account the\nfundamental problem of causal inference through causal masking. For training,\nour method decomposes the joint distribution into a series of conditional\ndistributions with a customized conditional masking to account for the\ndependence structure across outcomes. For inference, our method\nauto-regressively generates predictions. This allows our method to move beyond\npoint estimates of causal quantities and thus learn the joint interventional\ndistribution. To the best of our knowledge, DIME is the first neural method\ntailored to learn the joint, multi-outcome distribution of medical treatments.\nAcross various experiments, we demonstrate that our method effectively learns\nthe joint distribution and captures shared information among multiple outcomes.",
      "pdf_url": "http://arxiv.org/pdf/2506.01533v1",
      "arxiv_url": "http://arxiv.org/abs/2506.01533v1",
      "published": "2025-06-02",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Playing with Transformer at 30+ FPS via Next-Frame Diffusion",
      "authors": [
        "Xinle Cheng",
        "Tianyu He",
        "Jiayi Xu",
        "Junliang Guo",
        "Di He",
        "Jiang Bian"
      ],
      "abstract": "Autoregressive video models offer distinct advantages over bidirectional\ndiffusion models in creating interactive video content and supporting streaming\napplications with arbitrary duration. In this work, we present Next-Frame\nDiffusion (NFD), an autoregressive diffusion transformer that incorporates\nblock-wise causal attention, enabling iterative sampling and efficient\ninference via parallel token generation within each frame. Nonetheless,\nachieving real-time video generation remains a significant challenge for such\nmodels, primarily due to the high computational cost associated with diffusion\nsampling and the hardware inefficiencies inherent to autoregressive generation.\nTo address this, we introduce two innovations: (1) We extend consistency\ndistillation to the video domain and adapt it specifically for video models,\nenabling efficient inference with few sampling steps; (2) To fully leverage\nparallel computation, motivated by the observation that adjacent frames often\nshare the identical action input, we propose speculative sampling. In this\napproach, the model generates next few frames using current action input, and\ndiscard speculatively generated frames if the input action differs. Experiments\non a large-scale action-conditioned video generation benchmark demonstrate that\nNFD beats autoregressive baselines in terms of both visual quality and sampling\nefficiency. We, for the first time, achieves autoregressive video generation at\nover 30 Frames Per Second (FPS) on an A100 GPU using a 310M model.",
      "pdf_url": "http://arxiv.org/pdf/2506.01380v1",
      "arxiv_url": "http://arxiv.org/abs/2506.01380v1",
      "published": "2025-06-02",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Uncovering Bias Mechanisms in Observational Studies",
      "authors": [
        "Ilker Demirel",
        "Zeshan Hussain",
        "Piersilvio De Bartolomeis",
        "David Sontag"
      ],
      "abstract": "Observational studies are a key resource for causal inference but are often\naffected by systematic biases. Prior work has focused mainly on detecting these\nbiases, via sensitivity analyses and comparisons with randomized controlled\ntrials, or mitigating them through debiasing techniques. However, there remains\na lack of methodology for uncovering the underlying mechanisms driving these\nbiases, e.g., whether due to hidden confounding or selection of participants.\nIn this work, we show that the relationship between bias magnitude and the\npredictive performance of nuisance function estimators (in the observational\nstudy) can help distinguish among common sources of causal bias. We validate\nour methodology through extensive synthetic experiments and a real-world case\nstudy, demonstrating its effectiveness in revealing the mechanisms behind\nobserved biases. Our framework offers a new lens for understanding and\ncharacterizing bias in observational studies, with practical implications for\nimproving causal inference.",
      "pdf_url": "http://arxiv.org/pdf/2506.01191v1",
      "arxiv_url": "http://arxiv.org/abs/2506.01191v1",
      "published": "2025-06-01",
      "categories": [
        "stat.ME",
        "stat.ML"
      ]
    },
    {
      "title": "zip2zip: Inference-Time Adaptive Vocabularies for Language Models via Token Compression",
      "authors": [
        "Saibo Geng",
        "Nathan Ranchin",
        "Yunzhen yao",
        "Maxime Peyrard",
        "Chris Wendler",
        "Michael Gastpar",
        "Robert West"
      ],
      "abstract": "Tokenization efficiency plays a critical role in the performance and cost of\nlarge language models (LLMs), yet most models rely on static tokenizers\noptimized for general-purpose corpora. These tokenizers' fixed vocabularies\noften fail to adapt to domain- or language-specific inputs, leading to longer\ntoken sequences and higher computational costs. We introduce zip2zip, a\nframework that enables LLMs to dynamically adjust token vocabulary at inference\ntime, allowing for fewer generated tokens and thus faster inference. zip2zip\nconsists of three key components: (1) a tokenizer based on Lempel-Ziv-Welch\n(LZW) compression that incrementally compresses tokens into reusable\n\"hypertokens\" on the fly; (2) an embedding layer that computes embeddings for\nnewly formed hypertokens at runtime; and (3) a causal language modeling variant\nthat trains the model to operate on hypertokenized, compressed sequences. We\nshow that an existing LLM can be zip2zip-fied in 10 GPU-hours via\nparameter-efficient finetuning. The resulting zip2zip LLMs effectively learn to\nuse hypertokens at inference time, reducing input and output sequence length by\n20-60\\%, with significant improvements in inference latency.",
      "pdf_url": "http://arxiv.org/pdf/2506.01084v1",
      "arxiv_url": "http://arxiv.org/abs/2506.01084v1",
      "published": "2025-06-01",
      "categories": [
        "cs.CL",
        "cs.LG"
      ]
    }
  ]
}