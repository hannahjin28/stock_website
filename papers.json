{
  "last_updated": "2025-06-04T00:55:09.907194",
  "papers": [
    {
      "title": "Density Ratio Permutation Tests with connections to distributional shifts and conditional two-sample testing",
      "authors": [
        "Alberto Bordino",
        "Thomas B. Berrett"
      ],
      "abstract": "We introduce novel hypothesis tests to allow for statistical inference for\ndensity ratios. More precisely, we introduce the Density Ratio Permutation Test\n(DRPT) for testing $H_0: g \\propto r f$ based on independent data drawn from\ndistributions with densities $f$ and $g$, where the hypothesised density ratio\n$r$ is a fixed function. The proposed test employs an efficient Markov Chain\nMonte Carlo algorithm to draw permutations of the combined dataset according to\na distribution determined by $r$, producing exchangeable versions of the whole\nsample and thereby establishing finite-sample validity. Regarding the test's\nbehaviour under the alternative hypothesis, we begin by demonstrating that if\nthe test statistic is chosen as an Integral Probability Metric (IPM), the DRPT\nis consistent under mild assumptions on the function class that defines the\nIPM. We then narrow our focus to the setting where the function class is a\nReproducing Kernel Hilbert Space, and introduce a generalisation of the\nclassical Maximum Mean Discrepancy (MMD), which we term Shifted-MMD. For\ncontinuous data, assuming that a normalised version of $g - rf$ lies in a\nSobolev ball, we establish the minimax optimality of the DRPT based on the\nShifted-MMD. We further extend our approach to scenarios with an unknown shift\nfactor $r$, estimating it from part of the data using Density Ratio Estimation\ntechniques, and derive Type-I error bounds based on estimation error.\nAdditionally, we demonstrate how the DRPT can be adapted for conditional\ntwo-sample testing, establishing it as a versatile tool for assessing modelling\nassumptions on importance weights, covariate shifts and related scenarios,\nwhich frequently arise in contexts such as transfer learning and causal\ninference. Finally, we validate our theoretical findings through experiments on\nboth simulated and real-world datasets.",
      "pdf_url": "http://arxiv.org/pdf/2505.24529v1",
      "arxiv_url": "http://arxiv.org/abs/2505.24529v1",
      "published": "2025-05-30",
      "categories": [
        "stat.ME",
        "math.ST",
        "stat.TH",
        "62G09 62G10"
      ]
    },
    {
      "title": "Data Fusion for Partial Identification of Causal Effects",
      "authors": [
        "Quinn Lanners",
        "Cynthia Rudin",
        "Alexander Volfovsky",
        "Harsh Parikh"
      ],
      "abstract": "Data fusion techniques integrate information from heterogeneous data sources\nto improve learning, generalization, and decision making across data sciences.\nIn causal inference, these methods leverage rich observational data to improve\ncausal effect estimation, while maintaining the trustworthiness of randomized\ncontrolled trials. Existing approaches often relax the strong no unobserved\nconfounding assumption by instead assuming exchangeability of counterfactual\noutcomes across data sources. However, when both assumptions simultaneously\nfail - a common scenario in practice - current methods cannot identify or\nestimate causal effects. We address this limitation by proposing a novel\npartial identification framework that enables researchers to answer key\nquestions such as: Is the causal effect positive or negative? and How severe\nmust assumption violations be to overturn this conclusion? Our approach\nintroduces interpretable sensitivity parameters that quantify assumption\nviolations and derives corresponding causal effect bounds. We develop doubly\nrobust estimators for these bounds and operationalize breakdown frontier\nanalysis to understand how causal conclusions change as assumption violations\nincrease. We apply our framework to the Project STAR study, which investigates\nthe effect of classroom size on students' third-grade standardized test\nperformance. Our analysis reveals that the Project STAR results are robust to\nsimultaneous violations of key assumptions, both on average and across various\nsubgroups of interest. This strengthens confidence in the study's conclusions\ndespite potential unmeasured biases in the data.",
      "pdf_url": "http://arxiv.org/pdf/2505.24296v1",
      "arxiv_url": "http://arxiv.org/abs/2505.24296v1",
      "published": "2025-05-30",
      "categories": [
        "stat.ME",
        "cs.LG",
        "econ.EM"
      ]
    },
    {
      "title": "Estimation of Gender Wage Gap in the University of North Carolina System",
      "authors": [
        "Zihan Zhang",
        "Jan Hannig"
      ],
      "abstract": "Gender pay equity remains an open challenge in academia despite decades of\nmovements. Prior studies, however, have relied largely on descriptive\nregressions, leaving causal analysis underexplored. This study examines\ngender-based wage disparities among tenure-track faculty in the University of\nNorth Carolina system using both parametric and non-parametric causal inference\nmethods. In particular, we employed propensity score matching and causal\nforests to estimate the causal effect of gender on academic salary while\ncontrolling for university type, discipline, titles, working years, and\nscholarly productivity metrics. The results indicate that on average female\nprofessors earn approximately 6% less than their male colleagues with similar\nqualifications and positions.",
      "pdf_url": "http://arxiv.org/pdf/2505.24078v1",
      "arxiv_url": "http://arxiv.org/abs/2505.24078v1",
      "published": "2025-05-29",
      "categories": [
        "stat.AP",
        "econ.GN",
        "q-fin.EC"
      ]
    },
    {
      "title": "Test-Time Training Done Right",
      "authors": [
        "Tianyuan Zhang",
        "Sai Bi",
        "Yicong Hong",
        "Kai Zhang",
        "Fujun Luan",
        "Songlin Yang",
        "Kalyan Sunkavalli",
        "William T. Freeman",
        "Hao Tan"
      ],
      "abstract": "Test-Time Training (TTT) models context dependencies by adapting part of the\nmodel's weights (referred to as fast weights) during inference. This fast\nweight, akin to recurrent states in RNNs, stores temporary memories of past\ntokens in the current sequence. Existing TTT methods struggled to show\neffectiveness in handling long-context data, due to their inefficiency on\nmodern GPUs. The TTT layers in many of these approaches operate with extremely\nlow FLOPs utilization (often <5%) because they deliberately apply small online\nminibatch sizes (e.g., updating fast weights every 16 or 64 tokens). Moreover,\na small minibatch implies fine-grained block-wise causal dependencies in the\ndata, unsuitable for data beyond 1D ordered sequences, like sets or\nN-dimensional grids such as images or videos. In contrast, we pursue the\nopposite direction by using an extremely large chunk update, ranging from 2K to\n1M tokens across tasks of varying modalities, which we refer to as Large Chunk\nTest-Time Training (LaCT). It improves hardware utilization by orders of\nmagnitude, and more importantly, facilitates scaling of nonlinear state size\n(up to 40% of model parameters), hence substantially improving state capacity,\nall without requiring cumbersome and error-prone kernel implementations. It\nalso allows easy integration of sophisticated optimizers, e.g. Muon for online\nupdates. We validate our approach across diverse modalities and tasks,\nincluding novel view synthesis with image set, language models, and\nauto-regressive video diffusion. Our approach can scale up to 14B-parameter AR\nvideo diffusion model on sequences up to 56K tokens. In our longest sequence\nexperiment, we perform novel view synthesis with 1 million context length. We\nhope this work will inspire and accelerate new research in the field of\nlong-context modeling and test-time training. Website:\nhttps://tianyuanzhang.com/projects/ttt-done-right",
      "pdf_url": "http://arxiv.org/pdf/2505.23884v1",
      "arxiv_url": "http://arxiv.org/abs/2505.23884v1",
      "published": "2025-05-29",
      "categories": [
        "cs.LG",
        "cs.CL",
        "cs.CV"
      ]
    },
    {
      "title": "D-AR: Diffusion via Autoregressive Models",
      "authors": [
        "Ziteng Gao",
        "Mike Zheng Shou"
      ],
      "abstract": "This paper presents Diffusion via Autoregressive models (D-AR), a new\nparadigm recasting the image diffusion process as a vanilla autoregressive\nprocedure in the standard next-token-prediction fashion. We start by designing\nthe tokenizer that converts images into sequences of discrete tokens, where\ntokens in different positions can be decoded into different diffusion denoising\nsteps in the pixel space. Thanks to the diffusion properties, these tokens\nnaturally follow a coarse-to-fine order, which directly lends itself to\nautoregressive modeling. Therefore, we apply standard next-token prediction on\nthese tokens, without modifying any underlying designs (either causal masks or\ntraining/inference strategies), and such sequential autoregressive token\ngeneration directly mirrors the diffusion procedure in image space. That is,\nonce the autoregressive model generates an increment of tokens, we can directly\ndecode these tokens into the corresponding diffusion denoising step in the\nstreaming manner. Our pipeline naturally reveals several intriguing properties,\nfor example, it supports consistent previews when generating only a subset of\ntokens and enables zero-shot layout-controlled synthesis. On the standard\nImageNet benchmark, our method achieves 2.09 FID using a 775M Llama backbone\nwith 256 discrete tokens. We hope our work can inspire future research on\nunified autoregressive architectures of visual synthesis, especially with large\nlanguage models. Code and models will be available at\nhttps://github.com/showlab/D-AR",
      "pdf_url": "http://arxiv.org/pdf/2505.23660v1",
      "arxiv_url": "http://arxiv.org/abs/2505.23660v1",
      "published": "2025-05-29",
      "categories": [
        "cs.CV"
      ]
    },
    {
      "title": "BinauralFlow: A Causal and Streamable Approach for High-Quality Binaural Speech Synthesis with Flow Matching Models",
      "authors": [
        "Susan Liang",
        "Dejan Markovic",
        "Israel D. Gebru",
        "Steven Krenn",
        "Todd Keebler",
        "Jacob Sandakly",
        "Frank Yu",
        "Samuel Hassel",
        "Chenliang Xu",
        "Alexander Richard"
      ],
      "abstract": "Binaural rendering aims to synthesize binaural audio that mimics natural\nhearing based on a mono audio and the locations of the speaker and listener.\nAlthough many methods have been proposed to solve this problem, they struggle\nwith rendering quality and streamable inference. Synthesizing high-quality\nbinaural audio that is indistinguishable from real-world recordings requires\nprecise modeling of binaural cues, room reverb, and ambient sounds.\nAdditionally, real-world applications demand streaming inference. To address\nthese challenges, we propose a flow matching based streaming binaural speech\nsynthesis framework called BinauralFlow. We consider binaural rendering to be a\ngeneration problem rather than a regression problem and design a conditional\nflow matching model to render high-quality audio. Moreover, we design a causal\nU-Net architecture that estimates the current audio frame solely based on past\ninformation to tailor generative models for streaming inference. Finally, we\nintroduce a continuous inference pipeline incorporating streaming STFT/ISTFT\noperations, a buffer bank, a midpoint solver, and an early skip schedule to\nimprove rendering continuity and speed. Quantitative and qualitative\nevaluations demonstrate the superiority of our method over SOTA approaches. A\nperceptual study further reveals that our model is nearly indistinguishable\nfrom real-world recordings, with a $42\\%$ confusion rate.",
      "pdf_url": "http://arxiv.org/pdf/2505.22865v1",
      "arxiv_url": "http://arxiv.org/abs/2505.22865v1",
      "published": "2025-05-28",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ]
    },
    {
      "title": "A Synthetic Business Cycle Approach to Counterfactual Analysis with Nonstationary Macroeconomic Data",
      "authors": [
        "Zhentao Shi",
        "Jin Xi",
        "Haitian Xie"
      ],
      "abstract": "This paper investigates the use of synthetic control methods for causal\ninference in macroeconomic settings when dealing with possibly nonstationary\ndata. While the synthetic control approach has gained popularity for estimating\ncounterfactual outcomes, we caution researchers against assuming a common\nnonstationary trend factor across units for macroeconomic outcomes, as doing so\nmay result in misleading causal estimation-a pitfall we refer to as the\nspurious synthetic control problem. To address this issue, we propose a\nsynthetic business cycle framework that explicitly separates trend and cyclical\ncomponents. By leveraging the treated unit's historical data to forecast its\ntrend and using control units only for cyclical fluctuations, our\ndivide-and-conquer strategy eliminates spurious correlations and improves the\nrobustness of counterfactual prediction in macroeconomic applications. As\nempirical illustrations, we examine the cases of German reunification and the\nhandover of Hong Kong, demonstrating the advantages of the proposed approach.",
      "pdf_url": "http://arxiv.org/pdf/2505.22388v1",
      "arxiv_url": "http://arxiv.org/abs/2505.22388v1",
      "published": "2025-05-28",
      "categories": [
        "econ.EM"
      ]
    },
    {
      "title": "Causal Inference for Experiments with Latent Outcomes: Key Results and Their Implications for Design and Analysis",
      "authors": [
        "Jiawei Fu",
        "Donald P. Green"
      ],
      "abstract": "How should researchers analyze randomized experiments in which the main\noutcome is measured in multiple ways but each measure contains some degree of\nerror? We describe modeling approaches that enable researchers to identify\ncausal parameters of interest, suggest ways that experimental designs can be\naugmented so as to make linear latent variable models more credible, and\ndiscuss empirical tests of key modeling assumptions. We show that when\nexperimental researchers invest appropriately in multiple outcome measures, an\noptimally weighted index of the outcome measures enables researchers to obtain\nefficient and interpretable estimates of causal parameters by applying standard\nregression methods, and that weights may be obtained using instrumental\nvariables regression. Maximum likelihood and generalized method of moments\nestimators can be used to obtain estimates and standard errors in a single\nstep. An empirical application illustrates the gains in precision and\nrobustness that multiple outcome measures can provide.",
      "pdf_url": "http://arxiv.org/pdf/2505.21909v2",
      "arxiv_url": "http://arxiv.org/abs/2505.21909v2",
      "published": "2025-05-28",
      "categories": [
        "econ.EM",
        "stat.AP",
        "stat.ME"
      ]
    },
    {
      "title": "MAMBO-NET: Multi-Causal Aware Modeling Backdoor-Intervention Optimization for Medical Image Segmentation Network",
      "authors": [
        "Ruiguo Yu",
        "Yiyang Zhang",
        "Yuan Tian",
        "Yujie Diao",
        "Di Jin",
        "Witold Pedrycz"
      ],
      "abstract": "Medical image segmentation methods generally assume that the process from\nmedical image to segmentation is unbiased, and use neural networks to establish\nconditional probability models to complete the segmentation task. This\nassumption does not consider confusion factors, which can affect medical\nimages, such as complex anatomical variations and imaging modality limitations.\nConfusion factors obfuscate the relevance and causality of medical image\nsegmentation, leading to unsatisfactory segmentation results. To address this\nissue, we propose a multi-causal aware modeling backdoor-intervention\noptimization (MAMBO-NET) network for medical image segmentation. Drawing\ninsights from causal inference, MAMBO-NET utilizes self-modeling with\nmulti-Gaussian distributions to fit the confusion factors and introduce causal\nintervention into the segmentation process. Moreover, we design appropriate\nposterior probability constraints to effectively train the distributions of\nconfusion factors. For the distributions to effectively guide the segmentation\nand mitigate and eliminate the Impact of confusion factors on the segmentation,\nwe introduce classical backdoor intervention techniques and analyze their\nfeasibility in the segmentation task. To evaluate the effectiveness of our\napproach, we conducted extensive experiments on five medical image datasets.\nThe results demonstrate that our method significantly reduces the influence of\nconfusion factors, leading to enhanced segmentation accuracy.",
      "pdf_url": "http://arxiv.org/pdf/2505.21874v1",
      "arxiv_url": "http://arxiv.org/abs/2505.21874v1",
      "published": "2025-05-28",
      "categories": [
        "eess.IV",
        "cs.CV"
      ]
    },
    {
      "title": "Causal Posterior Estimation",
      "authors": [
        "Simon Dirmeier",
        "Antonietta Mira"
      ],
      "abstract": "We present Causal Posterior Estimation (CPE), a novel method for Bayesian\ninference in simulator models, i.e., models where the evaluation of the\nlikelihood function is intractable or too computationally expensive, but where\none can simulate model outputs given parameter values. CPE utilizes a\nnormalizing flow-based (NF) approximation to the posterior distribution which\ncarefully incorporates the conditional dependence structure induced by the\ngraphical representation of the model into the neural network. Thereby it is\npossible to improve the accuracy of the approximation. We introduce both\ndiscrete and continuous NF architectures for CPE and propose a constant-time\nsampling procedure for the continuous case which reduces the computational\ncomplexity of drawing samples to O(1) as for discrete NFs. We show, through an\nextensive experimental evaluation, that by incorporating the conditional\ndependencies induced by the graphical model directly into the neural network,\nrather than learning them from data, CPE is able to conduct highly accurate\nposterior inference either outperforming or matching the state of the art in\nthe field.",
      "pdf_url": "http://arxiv.org/pdf/2505.21468v1",
      "arxiv_url": "http://arxiv.org/abs/2505.21468v1",
      "published": "2025-05-27",
      "categories": [
        "cs.LG",
        "stat.ML"
      ]
    }
  ]
}