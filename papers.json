{
  "last_updated": "2025-08-07T01:00:28.565721",
  "papers": [
    {
      "title": "EmbedGrad: Gradient-Based Prompt Optimization in Embedding Space for Large Language Models",
      "authors": [
        "Xiaoming Hou",
        "Jiquan Zhang",
        "Zibin Lin",
        "DaCheng Tao",
        "Shengli Zhang"
      ],
      "abstract": "Effectively adapting powerful pretrained foundation models to diverse tasks\nremains a key challenge in AI deployment. Current approaches primarily follow\ntwo paradigms:discrete optimization of text prompts through prompt engineering,\nor continuous adaptation via additional trainable parameters. Both exhibit\nlimitations-discrete methods lack refinement precision while parameter-based\ntechniques increase complexity and reduce interpretability. To address these\nconstraints, we propose EmbedGrad, a novel framework that optimizes text prompt\nembeddings through gradient-based refinement. Our approach uniquely decouples\ntraining from deployment:during optimization,labeled examples guide precise\nembedding adjustments while preserving semantic meaning; during inference, only\noptimized embeddings integrate with user queries. This enables fine-grained\ncalibration impossible in text space, such as enhancing the reasoning\ncapability of prompts like please reason step by step. Comprehensive\nevaluations across mathematical reasoning, sentiment analysis, and causal\njudgment tasks demonstrate EmbedGrad's effectiveness:optimizing this reasoning\nprompt for Qwen2.5-Math-1.5B increased accuracy from 14.74\\% to 58.96\\% on\nmathematical problems. Consistent improvements were observed across model\nscales (0.5B-14B) and all tasks, with particularly significant gains for\nsmaller models on complex problems like causal judgment. By bridging prompt\nengineering and parameter efficiency without architectural changes, our work\nestablishes embedding refinement as a powerful new paradigm for task\nadaptation.",
      "pdf_url": "http://arxiv.org/pdf/2508.03533v1",
      "arxiv_url": "http://arxiv.org/abs/2508.03533v1",
      "published": "2025-08-05",
      "categories": [
        "cs.CL"
      ]
    },
    {
      "title": "Causal identification with $Y_0$",
      "authors": [
        "Charles Tapley Hoyt",
        "Craig Bakker",
        "Richard J. Callahan",
        "Joseph Cottam",
        "August George",
        "Benjamin M. Gyori",
        "Haley M. Hummel",
        "Nathaniel Merrill",
        "Sara Mohammad Taheri",
        "Pruthvi Prakash Navada",
        "Marc-Antoine Parent",
        "Adam Rupe",
        "Olga Vitek",
        "Jeremy Zucker"
      ],
      "abstract": "We present the $Y_0$ Python package, which implements causal identification\nalgorithms that apply interventional, counterfactual, and transportability\nqueries to data from (randomized) controlled trials, observational studies, or\nmixtures thereof. $Y_0$ focuses on the qualitative investigation of causation,\nhelping researchers determine whether a causal relationship can be estimated\nfrom available data before attempting to estimate how strong that relationship\nis. Furthermore, $Y_0$ provides guidance on how to transform the causal query\ninto a symbolic estimand that can be non-parametrically estimated from the\navailable data. $Y_0$ provides a domain-specific language for representing\ncausal queries and estimands as symbolic probabilistic expressions, tools for\nrepresenting causal graphical models with unobserved confounders, such as\nacyclic directed mixed graphs (ADMGs), and implementations of numerous\nidentification algorithms from the recent causal inference literature. The\n$Y_0$ source code can be found under the MIT License at\nhttps://github.com/y0-causal-inference/y0 and it can be installed with pip\ninstall y0.",
      "pdf_url": "http://arxiv.org/pdf/2508.03167v1",
      "arxiv_url": "http://arxiv.org/abs/2508.03167v1",
      "published": "2025-08-05",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Bayesian Sensitivity Analyses for Policy Evaluation with Difference-in-Differences under Violations of Parallel Trends",
      "authors": [
        "Seong Woo Han",
        "Nandita Mitra",
        "Gary Hettinger",
        "Arman Oganisian"
      ],
      "abstract": "Violations of the parallel trends assumption pose significant challenges for\ncausal inference in difference-in-differences (DiD) studies, especially in\npolicy evaluations where pre-treatment dynamics and external shocks may bias\nestimates. In this work, we propose a Bayesian DiD framework to allow us to\nestimate the effect of policies when parallel trends is violated. To address\npotential deviations from the parallel trends assumption, we introduce a formal\nsensitivity parameter representing the extent of the violation, specify an\nautoregressive AR(1) prior on this term to robustly model temporal correlation,\nand explore a range of prior specifications - including fixed, fully Bayesian,\nand empirical Bayes (EB) approaches calibrated from pre-treatment data. By\nsystematically comparing posterior treatment effect estimates across prior\nconfigurations when evaluating Philadelphia's sweetened beverage tax using\nBaltimore as a control, we show how Bayesian sensitivity analyses support\nrobust and interpretable policy conclusions under violations of parallel\ntrends.",
      "pdf_url": "http://arxiv.org/pdf/2508.02970v1",
      "arxiv_url": "http://arxiv.org/abs/2508.02970v1",
      "published": "2025-08-05",
      "categories": [
        "stat.ME"
      ]
    },
    {
      "title": "Sensitivity of weighted least squares estimators to omitted variables",
      "authors": [
        "Leonard Wainstein",
        "Chad Hazlett"
      ],
      "abstract": "This paper introduces tools for assessing the sensitivity, to unobserved\nconfounding, of a common estimator of the causal effect of a treatment on an\noutcome that employs weights: the weighted linear regression of the outcome on\nthe treatment and observed covariates. We demonstrate through the omitted\nvariable bias framework that the bias of this estimator is a function of two\nintuitive sensitivity parameters: (i) the proportion of weighted variance in\nthe treatment that unobserved confounding explains given the covariates and\n(ii) the proportion of weighted variance in the outcome that unobserved\nconfounding explains given the covariates and the treatment, i.e., two weighted\npartial $R^2$ values. Following previous work, we define sensitivity statistics\nthat lend themselves well to routine reporting, and derive formal bounds on the\nstrength of the unobserved confounding with (a multiple of) the strength of\nselect dimensions of the covariates, which help the user determine if\nunobserved confounding that would alter one's conclusions is plausible. We also\npropose tools for adjusted inference. A key choice we make is to examine only\nhow the (weighted) outcome model is influenced by unobserved confounding,\nrather than examining how the weights have been biased by omitted confounding.\nOne benefit of this choice is that the resulting tool applies with any weights\n(e.g., inverse-propensity score, matching, or covariate balancing weights).\nAnother benefit is that we can rely on simple omitted variable bias approaches\nthat, for example, impose no distributional assumptions on the data or\nunobserved confounding, and can address bias from misspecification in the\nobserved data. We make these tools available in the weightsense package for the\nR computing language.",
      "pdf_url": "http://arxiv.org/pdf/2508.02954v1",
      "arxiv_url": "http://arxiv.org/abs/2508.02954v1",
      "published": "2025-08-04",
      "categories": [
        "stat.ME"
      ]
    },
    {
      "title": "Causality and Interpretability for Electrical Distribution System faults",
      "authors": [
        "Karthik Peddi",
        "Sai Ram Aditya Parisineni",
        "Hemanth Macharla",
        "Mayukha Pal"
      ],
      "abstract": "Causal analysis helps us understand variables that are responsible for system\nfailures. This improves fault detection and makes system more reliable. In this\nwork, we present a new method that combines causal inference with machine\nlearning to classify faults in electrical distribution systems (EDS) using\ngraph-based models. We first build causal graphs using transfer entropy (TE).\nEach fault case is represented as a graph, where the nodes are features such as\nvoltage and current, and the edges demonstrate how these features influence\neach other. Then, the graphs are classified using machine learning and\nGraphSAGE where the model learns from both the node values and the structure of\nthe graph to predict the type of fault. To make the predictions understandable,\nwe further developed an integrated approach using GNNExplainer and Captums\nIntegrated Gradients to highlight the nodes (features) that influences the most\non the final prediction. This gives us clear insights into the possible causes\nof the fault. Our experiments show high accuracy: 99.44% on the EDS fault\ndataset, which is better than state of art models. By combining causal graphs\nwith machine learning, our method not only predicts faults accurately but also\nhelps understand their root causes. This makes it a strong and practical tool\nfor improving system reliability.",
      "pdf_url": "http://arxiv.org/pdf/2508.02524v1",
      "arxiv_url": "http://arxiv.org/abs/2508.02524v1",
      "published": "2025-08-04",
      "categories": [
        "eess.SY",
        "cs.LG",
        "cs.SY"
      ]
    },
    {
      "title": "CITS: Nonparametric Statistical Causal Modeling for High-Resolution Neural Time Series",
      "authors": [
        "Rahul Biswas",
        "SuryaNarayana Sripada",
        "Somabha Mukherjee",
        "Reza Abbasi-Asl"
      ],
      "abstract": "Understanding how signals propagate through neural circuits is central to\ndeciphering brain computation. While functional connectivity captures\nstatistical associations, it does not reveal directionality or causal\nmechanisms. We introduce CITS (Causal Inference in Time Series), a\nnon-parametric method for inferring statistically causal neural circuitry from\nhigh-resolution time series data. CITS models neural dynamics using a\nstructural causal model with arbitrary Markov order and tests for time-lagged\nconditional independence using either Gaussian or distribution-free statistics.\nUnlike classical Granger Causality, which assumes linear autoregressive models\nand Gaussian noise, or the Peter-Clark algorithm, which assumes i.i.d. data and\nno temporal structure, CITS handles temporally dependent, potentially\nnon-Gaussian data with flexible testing procedures. We prove consistency under\nmild mixing assumptions and validate CITS on simulated linear, nonlinear, and\ncontinuous-time recurrent neural network data, where it outperforms\nstate-of-the-art methods. We then apply CITS to Neuropixels recordings from\nmouse brain during visual tasks. CITS uncovers interpretable, stimulus-specific\ncausal circuits linking cortical, thalamic, and hippocampal regions, consistent\nwith experimental literature. It also reveals that neurons with similar\norientation selectivity indices are more likely to be causally connected. Our\nresults demonstrate the utility of CITS in uncovering biologically meaningful\npathways and generating hypotheses for future experimental studies.",
      "pdf_url": "http://arxiv.org/pdf/2508.01920v1",
      "arxiv_url": "http://arxiv.org/abs/2508.01920v1",
      "published": "2025-08-03",
      "categories": [
        "q-bio.NC",
        "q-bio.QM",
        "stat.AP"
      ]
    },
    {
      "title": "Structure Maintained Representation Learning Neural Network for Causal Inference",
      "authors": [
        "Yang Sun",
        "Wenbin Lu",
        "Yi-Hui Zhou"
      ],
      "abstract": "Recent developments in causal inference have greatly shifted the interest\nfrom estimating the average treatment effect to the individual treatment\neffect. In this article, we improve the predictive accuracy of representation\nlearning and adversarial networks in estimating individual treatment effects by\nintroducing a structure keeper which maintains the correlation between the\nbaseline covariates and their corresponding representations in the high\ndimensional space. We train a discriminator at the end of representation layers\nto trade off representation balance and information loss. We show that the\nproposed discriminator minimizes an upper bound of the treatment estimation\nerror. We can address the tradeoff between distribution balance and information\nloss by considering the correlations between the learned representation space\nand the original covariate feature space. We conduct extensive experiments with\nsimulated and real-world observational data to show that our proposed Structure\nMaintained Representation Learning (SMRL) algorithm outperforms\nstate-of-the-art methods. We also demonstrate the algorithms on real electronic\nhealth record data from the MIMIC-III database.",
      "pdf_url": "http://arxiv.org/pdf/2508.01865v1",
      "arxiv_url": "http://arxiv.org/abs/2508.01865v1",
      "published": "2025-08-03",
      "categories": [
        "stat.ML",
        "cs.LG",
        "stat.ME"
      ]
    },
    {
      "title": "Anchoring-Based Causal Design (ABCD): Estimating the Effects of Beliefs",
      "authors": [
        "Raanan Sulitzeanu-Kenan",
        "Micha Mandel",
        "Yosef Rinott"
      ],
      "abstract": "A central challenge in any study of the effects of beliefs on outcomes, such\nas decisions and behavior, is the risk of omitted variables bias. Omitted\nvariables, frequently unmeasured or even unknown, can induce correlations\nbetween beliefs and decisions that are not genuinely causal, in which case the\nomitted variables are referred to as confounders. To address the challenge of\ncausal inference, researchers frequently rely on information provision\nexperiments to randomly manipulate beliefs. The information supplied in these\nexperiments can serve as an instrumental variable (IV), enabling causal\ninference, so long as it influences decisions exclusively through its impact on\nbeliefs. However, providing varying information to participants to shape their\nbeliefs can raise both methodological and ethical concerns. Methodological\nconcerns arise from potential violations of the exclusion restriction\nassumption. Such violations may stem from information source effects, when\nattitudes toward the source affect the outcome decision directly, thereby\nintroducing a confounder. An ethical concern arises from manipulating the\nprovided information, as it may involve deceiving participants. This paper\nproposes and empirically demonstrates a new method for treating beliefs and\nestimating their effects, the Anchoring-Based Causal Design (ABCD), which\navoids deception and source influences. ABCD combines the cognitive mechanism\nknown as anchoring with instrumental variable (IV) estimation. Instead of\nproviding substantive information, the method employs a deliberately\nnon-informative procedure in which participants compare their self-assessment\nof a concept to a randomly assigned anchor value. We present the method and the\nresults of eight experiments demonstrating its application, strengths, and\nlimitations. We conclude by discussing the potential of this design for\nadvancing experimental social science.",
      "pdf_url": "http://arxiv.org/pdf/2508.01677v1",
      "arxiv_url": "http://arxiv.org/abs/2508.01677v1",
      "published": "2025-08-03",
      "categories": [
        "econ.GN",
        "q-fin.EC",
        "stat.ME"
      ]
    },
    {
      "title": "Debiasing Machine Learning Predictions for Causal Inference Without Additional Ground Truth Data: \"One Map, Many Trials\" in Satellite-Driven Poverty Analysis",
      "authors": [
        "Markus Pettersson",
        "Connor T. Jerzak",
        "Adel Daoud"
      ],
      "abstract": "Machine learning models trained on Earth observation data, such as satellite\nimagery, have demonstrated significant promise in predicting household-level\nwealth indices, enabling the creation of high-resolution wealth maps that can\nbe leveraged across multiple causal trials. However, because standard training\nobjectives prioritize overall predictive accuracy, these predictions inherently\nsuffer from shrinkage toward the mean, leading to attenuated estimates of\ncausal treatment effects and limiting their utility in policy. Existing\ndebiasing methods, such as Prediction-Powered Inference, can handle this\nattenuation bias but require additional fresh ground-truth data at the\ndownstream stage of causal inference, which restricts their applicability in\ndata-scarce environments. Here, we introduce and evaluate two correction\nmethods -- linear calibration correction and Tweedie's correction -- that\nsubstantially reduce prediction bias without relying on newly collected labeled\ndata. Linear calibration corrects bias through a straightforward linear\ntransformation derived from held-out calibration data, whereas Tweedie's\ncorrection leverages empirical Bayes principles to directly address\nshrinkage-induced biases by exploiting score functions derived from the model's\nlearning patterns. Through analytical exercises and experiments using\nDemographic and Health Survey data, we demonstrate that the proposed methods\nmeet or outperform existing approaches that either require (a) adjustments to\ntraining pipelines or (b) additional labeled data. These approaches may\nrepresent a promising avenue for improving the reliability of causal inference\nwhen direct outcome measures are limited or unavailable, enabling a \"one map,\nmany trials\" paradigm where a single upstream data creation team produces\npredictions usable by many downstream teams across diverse ML pipelines.",
      "pdf_url": "http://arxiv.org/pdf/2508.01341v1",
      "arxiv_url": "http://arxiv.org/abs/2508.01341v1",
      "published": "2025-08-02",
      "categories": [
        "stat.ML",
        "cs.LG",
        "62C12",
        "H.3"
      ]
    },
    {
      "title": "Flow IV: Counterfactual Inference In Nonseparable Outcome Models Using Instrumental Variables",
      "authors": [
        "Marc Braun",
        "Jose M. Pe√±a",
        "Adel Daoud"
      ],
      "abstract": "To reach human level intelligence, learning algorithms need to incorporate\ncausal reasoning. But identifying causality, and particularly counterfactual\nreasoning, remains an elusive task. In this paper, we make progress on this\ntask by utilizing instrumental variables (IVs). IVs are a classic tool for\nmitigating bias from unobserved confounders when estimating causal effects.\nWhile IV methods have been extended to non-separable structural models at the\npopulation level, existing approaches to counterfactual prediction typically\nassume additive noise in the outcome. In this paper, we show that under\nstandard IV assumptions, along with the assumptions that latent noises in\ntreatment and outcome are strictly monotonic and jointly Gaussian, the\ntreatment-outcome relationship becomes uniquely identifiable from observed\ndata. This enables counterfactual inference even in nonseparable models. We\nimplement our approach by training a normalizing flow to maximize the\nlikelihood of the observed data, demonstrating accurate recovery of the\nunderlying outcome function. We call our method Flow IV.",
      "pdf_url": "http://arxiv.org/pdf/2508.01321v1",
      "arxiv_url": "http://arxiv.org/abs/2508.01321v1",
      "published": "2025-08-02",
      "categories": [
        "stat.ML",
        "cs.LG"
      ]
    }
  ]
}