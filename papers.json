{
  "last_updated": "2025-07-24T00:58:44.112621",
  "papers": [
    {
      "title": "Ballot Design and Electoral Outcomes: The Role of Candidate Order and Party Affiliation",
      "authors": [
        "Alessandro Arlotto",
        "Alexandre Belloni",
        "Fei Fang",
        "Saša Pekeč"
      ],
      "abstract": "We use causal inference to study how designing ballots with and without party\ndesignations impacts electoral outcomes when partisan voters rely on\nparty-order cues to infer candidate affiliation in races without designations.\nIf the party orders of candidates in races with and without party designations\ndiffer, these voters might cast their votes incorrectly. We identify a\nquasi-randomized natural experiment with contest-level treatment assignment\npertaining to North Carolina judicial elections and use double machine learning\nto accurately capture the magnitude of such incorrectly cast votes. Using\nprecinct-level election and demographic data, we estimate that 11.8% (95%\nconfidence interval: [4.0%, 19.6%]) of democratic partisan voters and 15.4%\n(95% confidence interval: [7.8%, 23.1%]) of republican partisan voters cast\ntheir votes incorrectly due to the difference in party orders. Our results\nindicate that ballots mixing contests with and without party designations\nmislead many voters, leading to outcomes that do not reflect true voter\npreferences. To accurately capture voter intent, such ballot designs should be\navoided.",
      "pdf_url": "http://arxiv.org/pdf/2507.16722v1",
      "arxiv_url": "http://arxiv.org/abs/2507.16722v1",
      "published": "2025-07-22",
      "categories": [
        "stat.AP"
      ]
    },
    {
      "title": "On Causal Inference for the Survivor Function",
      "authors": [
        "Benjamin R. Baer",
        "Ashkan Ertefaie",
        "Robert L. Strawderman"
      ],
      "abstract": "In this expository paper, we consider the problem of causal inference and\nefficient estimation for the counterfactual survivor function. This problem has\npreviously been considered in the literature in several papers, each relying on\nthe imposition of conditions meant to identify the desired estimand from the\nobserved data. These conditions, generally referred to as either implying or\nsatisfying coarsening at random, are inconsistently imposed across this\nliterature and, in all cases, fail to imply coarsening at random. We establish\nthe first general characterization of coarsening at random, and also sequential\ncoarsening at random, for this estimation problem. Other contributions include\nthe first general characterization of the set of all influence functions for\nthe counterfactual survival probability under sequential coarsening at random,\nand the corresponding nonparametric efficient influence function. These\ncharacterizations are general in that neither impose continuity assumptions on\neither the underlying failure or censoring time distributions. We further show\nhow the latter compares to alternative forms recently derived in the\nliterature, including establishing the pointwise equivalence of the influence\nfunctions for our nonparametric efficient estimator and that recently given in\nWestling et al (2024, Journal of the American Statistical Association).",
      "pdf_url": "http://arxiv.org/pdf/2507.16691v1",
      "arxiv_url": "http://arxiv.org/abs/2507.16691v1",
      "published": "2025-07-22",
      "categories": [
        "stat.ME"
      ]
    },
    {
      "title": "Estimating Treatment Effects with Independent Component Analysis",
      "authors": [
        "Patrik Reizinger",
        "Lester Mackey",
        "Wieland Brendel",
        "Rahul Krishnan"
      ],
      "abstract": "The field of causal inference has developed a variety of methods to\naccurately estimate treatment effects in the presence of nuisance. Meanwhile,\nthe field of identifiability theory has developed methods like Independent\nComponent Analysis (ICA) to identify latent sources and mixing weights from\ndata. While these two research communities have developed largely\nindependently, they aim to achieve similar goals: the accurate and\nsample-efficient estimation of model parameters. In the partially linear\nregression (PLR) setting, Mackey et al. (2018) recently found that estimation\nconsistency can be improved with non-Gaussian treatment noise. Non-Gaussianity\nis also a crucial assumption for identifying latent factors in ICA. We provide\nthe first theoretical and empirical insights into this connection, showing that\nICA can be used for causal effect estimation in the PLR model. Surprisingly, we\nfind that linear ICA can accurately estimate multiple treatment effects even in\nthe presence of Gaussian confounders or nonlinear nuisance.",
      "pdf_url": "http://arxiv.org/pdf/2507.16467v1",
      "arxiv_url": "http://arxiv.org/abs/2507.16467v1",
      "published": "2025-07-22",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Do Large Language Models Have a Planning Theory of Mind? Evidence from MindGames: a Multi-Step Persuasion Task",
      "authors": [
        "Jared Moore",
        "Ned Cooper",
        "Rasmus Overmark",
        "Beba Cibralic",
        "Nick Haber",
        "Cameron R. Jones"
      ],
      "abstract": "Recent evidence suggests Large Language Models (LLMs) display Theory of Mind\n(ToM) abilities. Most ToM experiments place participants in a spectatorial\nrole, wherein they predict and interpret other agents' behavior. However, human\nToM also contributes to dynamically planning action and strategically\nintervening on others' mental states. We present MindGames: a novel `planning\ntheory of mind' (PToM) task which requires agents to infer an interlocutor's\nbeliefs and desires to persuade them to alter their behavior. Unlike previous\nevaluations, we explicitly evaluate use cases of ToM. We find that humans\nsignificantly outperform o1-preview (an LLM) at our PToM task (11% higher;\n$p=0.006$). We hypothesize this is because humans have an implicit causal model\nof other agents (e.g., they know, as our task requires, to ask about people's\npreferences). In contrast, o1-preview outperforms humans in a baseline\ncondition which requires a similar amount of planning but minimal mental state\ninferences (e.g., o1-preview is better than humans at planning when already\ngiven someone's preferences). These results suggest a significant gap between\nhuman-like social reasoning and LLM abilities.",
      "pdf_url": "http://arxiv.org/pdf/2507.16196v1",
      "arxiv_url": "http://arxiv.org/abs/2507.16196v1",
      "published": "2025-07-22",
      "categories": [
        "cs.CL"
      ]
    },
    {
      "title": "CoLD: Counterfactually-Guided Length Debiasing for Process Reward Models",
      "authors": [
        "Congmin Zheng",
        "Jiachen Zhu",
        "Jianghao Lin",
        "Xinyi Dai",
        "Yong Yu",
        "Weinan Zhang",
        "Mengyue Yang"
      ],
      "abstract": "Process Reward Models (PRMs) play a central role in evaluating and guiding\nmulti-step reasoning in large language models (LLMs), especially for\nmathematical problem solving. However, we identify a pervasive length bias in\nexisting PRMs: they tend to assign higher scores to longer reasoning steps,\neven when the semantic content and logical validity are unchanged. This bias\nundermines the reliability of reward predictions and leads to overly verbose\noutputs during inference. To address this issue, we propose\nCoLD(Counterfactually-Guided Length Debiasing), a unified framework that\nmitigates length bias through three components: an explicit length-penalty\nadjustment, a learned bias estimator trained to capture spurious length-related\nsignals, and a joint training strategy that enforces length-invariance in\nreward predictions. Our approach is grounded in counterfactual reasoning and\ninformed by causal graph analysis. Extensive experiments on MATH500 and\nGSM-Plus show that CoLD consistently reduces reward-length correlation,\nimproves accuracy in step selection, and encourages more concise, logically\nvalid reasoning. These results demonstrate the effectiveness and practicality\nof CoLD in improving the fidelity and robustness of PRMs.",
      "pdf_url": "http://arxiv.org/pdf/2507.15698v1",
      "arxiv_url": "http://arxiv.org/abs/2507.15698v1",
      "published": "2025-07-21",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Inference on Nonlinear Counterfactual Functionals under a Multiplicative IV Model",
      "authors": [
        "Yonghoon Lee",
        "Mengxin Yu",
        "Jiewen Liu",
        "Chan Park",
        "Yunshu Zhang",
        "James M. Robins",
        "Eric J. Tchetgen Tchetgen"
      ],
      "abstract": "Instrumental variable (IV) methods play a central role in causal inference,\nparticularly in settings where treatment assignment is confounded by unobserved\nvariables. IV methods have been extensively developed in recent years and\napplied across diverse domains, from economics to epidemiology. In this work,\nwe study the recently introduced multiplicative IV (MIV) model and demonstrate\nits utility for causal inference beyond the average treatment effect. In\nparticular, we show that it enables identification and inference for a broad\nclass of counterfactual functionals characterized by moment equations. This\nincludes, for example, inference on quantile treatment effects. We develop\nmethods for efficient and multiply robust estimation of such functionals, and\nprovide inference procedures with asymptotic validity. Experimental results\ndemonstrate that the proposed procedure performs well even with moderate sample\nsizes.",
      "pdf_url": "http://arxiv.org/pdf/2507.15612v1",
      "arxiv_url": "http://arxiv.org/abs/2507.15612v1",
      "published": "2025-07-21",
      "categories": [
        "stat.ME"
      ]
    },
    {
      "title": "Structural DID with ML: Theory, Simulation, and a Roadmap for Applied Research",
      "authors": [
        "Yile Yu",
        "Anzhi Xu",
        "Yi Wang"
      ],
      "abstract": "Causal inference in observational panel data has become a central concern in\neconomics,policy analysis,and the broader social sciences.To address the core\ncontradiction where traditional difference-in-differences (DID) struggles with\nhigh-dimensional confounding variables in observational panel data,while\nmachine learning (ML) lacks causal structure interpretability,this paper\nproposes an innovative framework called S-DIDML that integrates structural\nidentification with high-dimensional estimation.Building upon the structure of\ntraditional DID methods,S-DIDML employs structured residual orthogonalization\ntechniques (Neyman orthogonality+cross-fitting) to retain the group-time\ntreatment effect (ATT) identification structure while resolving\nhigh-dimensional covariate interference issues.It designs a dynamic\nheterogeneity estimation module combining causal forests and semi-parametric\nmodels to capture spatiotemporal heterogeneity effects.The framework\nestablishes a complete modular application process with standardized Stata\nimplementation paths.The introduction of S-DIDML enriches methodological\nresearch on DID and DDML innovations, shifting causal inference from method\nstacking to architecture integration.This advancement enables social sciences\nto precisely identify policy-sensitive groups and optimize resource\nallocation.The framework provides replicable evaluation tools, decision\noptimization references,and methodological paradigms for complex intervention\nscenarios such as digital transformation policies and environmental\nregulations.",
      "pdf_url": "http://arxiv.org/pdf/2507.15899v1",
      "arxiv_url": "http://arxiv.org/abs/2507.15899v1",
      "published": "2025-07-21",
      "categories": [
        "stat.ML",
        "cs.LG",
        "91-01"
      ]
    },
    {
      "title": "From Kicking to Causality: Simulating Infant Agency Detection with a Robust Intrinsic Reward",
      "authors": [
        "Xia Xu",
        "Jochen Triesch"
      ],
      "abstract": "While human infants robustly discover their own causal efficacy, standard\nreinforcement learning agents remain brittle, as their reliance on\ncorrelation-based rewards fails in noisy, ecologically valid scenarios. To\naddress this, we introduce the Causal Action Influence Score (CAIS), a novel\nintrinsic reward rooted in causal inference. CAIS quantifies an action's\ninfluence by measuring the 1-Wasserstein distance between the learned\ndistribution of sensory outcomes conditional on that action, $p(h|a)$, and the\nbaseline outcome distribution, $p(h)$. This divergence provides a robust reward\nthat isolates the agent's causal impact from confounding environmental noise.\nWe test our approach in a simulated infant-mobile environment where\ncorrelation-based perceptual rewards fail completely when the mobile is\nsubjected to external forces. In stark contrast, CAIS enables the agent to\nfilter this noise, identify its influence, and learn the correct policy.\nFurthermore, the high-quality predictive model learned for CAIS allows our\nagent, when augmented with a surprise signal, to successfully reproduce the\n\"extinction burst\" phenomenon. We conclude that explicitly inferring causality\nis a crucial mechanism for developing a robust sense of agency, offering a\npsychologically plausible framework for more adaptive autonomous systems.",
      "pdf_url": "http://arxiv.org/pdf/2507.15106v1",
      "arxiv_url": "http://arxiv.org/abs/2507.15106v1",
      "published": "2025-07-20",
      "categories": [
        "cs.AI",
        "cs.RO",
        "F.2.2"
      ]
    },
    {
      "title": "Kernel Based Maximum Entropy Inverse Reinforcement Learning for Mean-Field Games",
      "authors": [
        "Berkay Anahtarci",
        "Can Deha Kariksiz",
        "Naci Saldi"
      ],
      "abstract": "We consider the maximum causal entropy inverse reinforcement learning problem\nfor infinite-horizon stationary mean-field games, in which we model the unknown\nreward function within a reproducing kernel Hilbert space. This allows the\ninference of rich and potentially nonlinear reward structures directly from\nexpert demonstrations, in contrast to most existing inverse reinforcement\nlearning approaches for mean-field games that typically restrict the reward\nfunction to a linear combination of a fixed finite set of basis functions. We\nalso focus on the infinite-horizon cost structure, whereas prior studies\nprimarily rely on finite-horizon formulations. We introduce a Lagrangian\nrelaxation to this maximum causal entropy inverse reinforcement learning\nproblem that enables us to reformulate it as an unconstrained log-likelihood\nmaximization problem, and obtain a solution \\lk{via} a gradient ascent\nalgorithm. To illustrate the theoretical consistency of the algorithm, we\nestablish the smoothness of the log-likelihood objective by proving the\nFr\\'echet differentiability of the related soft Bellman operators with respect\nto the parameters in the reproducing kernel Hilbert space. We demonstrate the\neffectiveness of our method on a mean-field traffic routing game, where it\naccurately recovers expert behavior.",
      "pdf_url": "http://arxiv.org/pdf/2507.14529v1",
      "arxiv_url": "http://arxiv.org/abs/2507.14529v1",
      "published": "2025-07-19",
      "categories": [
        "cs.LG",
        "math.OC",
        "91A16, 68T05, 49N45, 93E20, 46E22"
      ]
    },
    {
      "title": "Positive-Unlabeled Learning for Control Group Construction in Observational Causal Inference",
      "authors": [
        "Ilias Tsoumas",
        "Dimitrios Bormpoudakis",
        "Vasileios Sitokonstantinou",
        "Athanasios Askitopoulos",
        "Andreas Kalogeras",
        "Charalampos Kontoes",
        "Ioannis Athanasiadis"
      ],
      "abstract": "In causal inference, whether through randomized controlled trials or\nobservational studies, access to both treated and control units is essential\nfor estimating the effect of a treatment on an outcome of interest. When\ntreatment assignment is random, the average treatment effect (ATE) can be\nestimated directly by comparing outcomes between groups. In non-randomized\nsettings, various techniques are employed to adjust for confounding and\napproximate the counterfactual scenario to recover an unbiased ATE. A common\nchallenge, especially in observational studies, is the absence of units clearly\nlabeled as controls-that is, units known not to have received the treatment. To\naddress this, we propose positive-unlabeled (PU) learning as a framework for\nidentifying, with high confidence, control units from a pool of unlabeled ones,\nusing only the available treated (positive) units. We evaluate this approach\nusing both simulated and real-world data. We construct a causal graph with\ndiverse relationships and use it to generate synthetic data under various\nscenarios, assessing how reliably the method recovers control groups that allow\nestimates of true ATE. We also apply our approach to real-world data on optimal\nsowing and fertilizer treatments in sustainable agriculture. Our findings show\nthat PU learning can successfully identify control (negative) units from\nunlabeled data based only on treated units and, through the resulting control\ngroup, estimate an ATE that closely approximates the true value. This work has\nimportant implications for observational causal inference, especially in fields\nwhere randomized experiments are difficult or costly. In domains such as earth,\nenvironmental, and agricultural sciences, it enables a plethora of\nquasi-experiments by leveraging available earth observation and climate data,\nparticularly when treated units are available but control units are lacking.",
      "pdf_url": "http://arxiv.org/pdf/2507.14528v1",
      "arxiv_url": "http://arxiv.org/abs/2507.14528v1",
      "published": "2025-07-19",
      "categories": [
        "cs.LG"
      ]
    }
  ]
}