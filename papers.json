{
  "last_updated": "2025-08-20T00:52:00.929258",
  "papers": [
    {
      "title": "Reinforced Context Order Recovery for Adaptive Reasoning and Planning",
      "authors": [
        "Long Ma",
        "Fangwei Zhong",
        "Yizhou Wang"
      ],
      "abstract": "Modern causal language models, followed by rapid developments in discrete\ndiffusion models, can now produce a wide variety of interesting and useful\ncontent. However, these families of models are predominantly trained to output\ntokens with a fixed (left-to-right) or random order, which may deviate from the\nlogical order in which tokens are generated originally. In this paper, we\nobserve that current causal and diffusion models encounter difficulties in\nproblems that require adaptive token generation orders to solve tractably,\nwhich we characterize with the $\\mathcal{V}$-information framework. Motivated\nby this, we propose Reinforced Context Order Recovery (ReCOR), a\nreinforcement-learning-based framework to extract adaptive, data-dependent\ntoken generation orders from text data without annotations. Self-supervised by\ntoken prediction statistics, ReCOR estimates the hardness of predicting every\nunfilled token and adaptively selects the next token during both training and\ninference. Experiments on challenging reasoning and planning datasets\ndemonstrate the superior performance of ReCOR compared with baselines,\nsometimes outperforming oracle models supervised with the ground-truth order.",
      "pdf_url": "http://arxiv.org/pdf/2508.13070v1",
      "arxiv_url": "http://arxiv.org/abs/2508.13070v1",
      "published": "2025-08-18",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "Bayesian Double Machine Learning for Causal Inference",
      "authors": [
        "Francis J. DiTraglia",
        "Laura Liu"
      ],
      "abstract": "This paper proposes a simple, novel, and fully-Bayesian approach for causal\ninference in partially linear models with high-dimensional control variables.\nOff-the-shelf machine learning methods can introduce biases in the causal\nparameter known as regularization-induced confounding. To address this, we\npropose a Bayesian Double Machine Learning (BDML) method, which modifies a\nstandard Bayesian multivariate regression model and recovers the causal effect\nof interest from the reduced-form covariance matrix. Our BDML is related to the\nburgeoning frequentist literature on DML while addressing its limitations in\nfinite-sample inference. Moreover, the BDML is based on a fully generative\nprobability model in the DML context, adhering to the likelihood principle. We\nshow that in high dimensional setups the naive estimator implicitly assumes no\nselection on observables--unlike our BDML. The BDML exhibits lower asymptotic\nbias and achieves asymptotic normality and semiparametric efficiency as\nestablished by a Bernstein-von Mises theorem, thereby ensuring robustness to\nmisspecification. In simulations, our BDML achieves lower RMSE, better\nfrequentist coverage, and shorter confidence interval width than alternatives\nfrom the literature, both Bayesian and frequentist.",
      "pdf_url": "http://arxiv.org/pdf/2508.12688v1",
      "arxiv_url": "http://arxiv.org/abs/2508.12688v1",
      "published": "2025-08-18",
      "categories": [
        "econ.EM"
      ]
    },
    {
      "title": "Root Cause Analysis of Hydrogen Bond Separation in Spatio-Temporal Molecular Dynamics using Causal Models",
      "authors": [
        "Rahmat K. Adesunkanmi",
        "Ashfaq Khokhar",
        "Goce Trajcevski",
        "Sohail Murad"
      ],
      "abstract": "Molecular dynamics simulations (MDS) face challenges, including\nresource-heavy computations and the need to manually scan outputs to detect\n\"interesting events,\" such as the formation and persistence of hydrogen bonds\nbetween atoms of different molecules. A critical research gap lies in\nidentifying the underlying causes of hydrogen bond formation and separation\n-understanding which interactions or prior events contribute to their emergence\nover time. With this challenge in mind, we propose leveraging spatio-temporal\ndata analytics and machine learning models to enhance the detection of these\nphenomena. In this paper, our approach is inspired by causal modeling and aims\nto identify the root cause variables of hydrogen bond formation and separation\nevents. Specifically, we treat the separation of hydrogen bonds as an\n\"intervention\" occurring and represent the causal structure of the bonding and\nseparation events in the MDS as graphical causal models. These causal models\nare built using a variational autoencoder-inspired architecture that enables us\nto infer causal relationships across samples with diverse underlying causal\ngraphs while leveraging shared dynamic information. We further include a step\nto infer the root causes of changes in the joint distribution of the causal\nmodels. By constructing causal models that capture shifts in the conditional\ndistributions of molecular interactions during bond formation or separation,\nthis framework provides a novel perspective on root cause analysis in molecular\ndynamic systems. We validate the efficacy of our model empirically on the\natomic trajectories that used MDS for chiral separation, demonstrating that we\ncan predict many steps in the future and also find the variables driving the\nobserved changes in the system.",
      "pdf_url": "http://arxiv.org/pdf/2508.12500v1",
      "arxiv_url": "http://arxiv.org/abs/2508.12500v1",
      "published": "2025-08-17",
      "categories": [
        "cs.AI",
        "cs.LG",
        "q-bio.QM"
      ]
    },
    {
      "title": "GALA: Can Graph-Augmented Large Language Model Agentic Workflows Elevate Root Cause Analysis?",
      "authors": [
        "Yifang Tian",
        "Yaming Liu",
        "Zichun Chong",
        "Zihang Huang",
        "Hans-Arno Jacobsen"
      ],
      "abstract": "Root cause analysis (RCA) in microservice systems is challenging, requiring\non-call engineers to rapidly diagnose failures across heterogeneous telemetry\nsuch as metrics, logs, and traces. Traditional RCA methods often focus on\nsingle modalities or merely rank suspect services, falling short of providing\nactionable diagnostic insights with remediation guidance. This paper introduces\nGALA, a novel multi-modal framework that combines statistical causal inference\nwith LLM-driven iterative reasoning for enhanced RCA. Evaluated on an\nopen-source benchmark, GALA achieves substantial improvements over\nstate-of-the-art methods of up to 42.22% accuracy. Our novel human-guided LLM\nevaluation score shows GALA generates significantly more causally sound and\nactionable diagnostic outputs than existing methods. Through comprehensive\nexperiments and a case study, we show that GALA bridges the gap between\nautomated failure diagnosis and practical incident resolution by providing both\naccurate root cause identification and human-interpretable remediation\nguidance.",
      "pdf_url": "http://arxiv.org/pdf/2508.12472v1",
      "arxiv_url": "http://arxiv.org/abs/2508.12472v1",
      "published": "2025-08-17",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "LoraxBench: A Multitask, Multilingual Benchmark Suite for 20 Indonesian Languages",
      "authors": [
        "Alham Fikri Aji",
        "Trevor Cohn"
      ],
      "abstract": "As one of the world's most populous countries, with 700 languages spoken,\nIndonesia is behind in terms of NLP progress. We introduce LoraxBench, a\nbenchmark that focuses on low-resource languages of Indonesia and covers 6\ndiverse tasks: reading comprehension, open-domain QA, language inference,\ncausal reasoning, translation, and cultural QA. Our dataset covers 20\nlanguages, with the addition of two formality registers for three languages. We\nevaluate a diverse set of multilingual and region-focused LLMs and found that\nthis benchmark is challenging. We note a visible discrepancy between\nperformance in Indonesian and other languages, especially the low-resource\nones. There is no clear lead when using a region-specific model as opposed to\nthe general multilingual model. Lastly, we show that a change in register\naffects model performance, especially with registers not commonly found in\nsocial media, such as high-level politeness `Krama' Javanese.",
      "pdf_url": "http://arxiv.org/pdf/2508.12459v1",
      "arxiv_url": "http://arxiv.org/abs/2508.12459v1",
      "published": "2025-08-17",
      "categories": [
        "cs.CL"
      ]
    },
    {
      "title": "MedKGent: A Large Language Model Agent Framework for Constructing Temporally Evolving Medical Knowledge Graph",
      "authors": [
        "Duzhen Zhang",
        "Zixiao Wang",
        "Zhong-Zhi Li",
        "Yahan Yu",
        "Shuncheng Jia",
        "Jiahua Dong",
        "Haotian Xu",
        "Xing Wu",
        "Yingying Zhang",
        "Tielin Zhang",
        "Jie Yang",
        "Xiuying Chen",
        "Le Song"
      ],
      "abstract": "The rapid expansion of medical literature presents growing challenges for\nstructuring and integrating domain knowledge at scale. Knowledge Graphs (KGs)\noffer a promising solution by enabling efficient retrieval, automated\nreasoning, and knowledge discovery. However, current KG construction methods\noften rely on supervised pipelines with limited generalizability or naively\naggregate outputs from Large Language Models (LLMs), treating biomedical\ncorpora as static and ignoring the temporal dynamics and contextual uncertainty\nof evolving knowledge. To address these limitations, we introduce MedKGent, a\nLLM agent framework for constructing temporally evolving medical KGs.\nLeveraging over 10 million PubMed abstracts published between 1975 and 2023, we\nsimulate the emergence of biomedical knowledge via a fine-grained daily time\nseries. MedKGent incrementally builds the KG in a day-by-day manner using two\nspecialized agents powered by the Qwen2.5-32B-Instruct model. The Extractor\nAgent identifies knowledge triples and assigns confidence scores via\nsampling-based estimation, which are used to filter low-confidence extractions\nand inform downstream processing. The Constructor Agent incrementally\nintegrates the retained triples into a temporally evolving graph, guided by\nconfidence scores and timestamps to reinforce recurring knowledge and resolve\nconflicts. The resulting KG contains 156,275 entities and 2,971,384 relational\ntriples. Quality assessments by two SOTA LLMs and three domain experts\ndemonstrate an accuracy approaching 90%, with strong inter-rater agreement. To\nevaluate downstream utility, we conduct RAG across seven medical question\nanswering benchmarks using five leading LLMs, consistently observing\nsignificant improvements over non-augmented baselines. Case studies further\ndemonstrate the KG's value in literature-based drug repurposing via\nconfidence-aware causal inference.",
      "pdf_url": "http://arxiv.org/pdf/2508.12393v2",
      "arxiv_url": "http://arxiv.org/abs/2508.12393v2",
      "published": "2025-08-17",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "CarelessWhisper: Turning Whisper into a Causal Streaming Model",
      "authors": [
        "Tomer Krichli",
        "Bhiksha Raj",
        "Joseph Keshet"
      ],
      "abstract": "Automatic Speech Recognition (ASR) has seen remarkable progress, with models\nlike OpenAI Whisper and NVIDIA Canary achieving state-of-the-art (SOTA)\nperformance in offline transcription. However, these models are not designed\nfor streaming (online or real-time) transcription, due to limitations in their\narchitecture and training methodology. We propose a method to turn the\ntransformer encoder-decoder model into a low-latency streaming model that is\ncareless about future context. We present an analysis explaining why it is not\nstraightforward to convert an encoder-decoder transformer to a low-latency\nstreaming model. Our proposed method modifies the existing (non-causal) encoder\nto a causal encoder by fine-tuning both the encoder and decoder using Low-Rank\nAdaptation (LoRA) and a weakly aligned dataset. We then propose an updated\ninference mechanism that utilizes the fine-tune causal encoder and decoder to\nyield greedy and beam-search decoding, and is shown to be locally optimal.\nExperiments on low-latency chunk sizes (less than 300 msec) show that our\nfine-tuned model outperforms existing non-fine-tuned streaming approaches in\nmost cases, while using a lower complexity. Additionally, we observe that our\ntraining process yields better alignment, enabling a simple method for\nextracting word-level timestamps. We release our training and inference code,\nalong with the fine-tuned models, to support further research and development\nin streaming ASR.",
      "pdf_url": "http://arxiv.org/pdf/2508.12301v1",
      "arxiv_url": "http://arxiv.org/abs/2508.12301v1",
      "published": "2025-08-17",
      "categories": [
        "cs.CL",
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ]
    },
    {
      "title": "Conditional mutual information: A generalization of causal inference in quantum systems",
      "authors": [
        "Anupam Ghosh"
      ],
      "abstract": "The concept of causality is fundamental to numerous scientific explanations;\nnonetheless, its extension to the quantum regime has yet to be explored\nrigorously. This paper introduces the development of a quantum causal index, a\nnovel extension of the classical causal inference framework, tailored to grasp\nthe causal relationships inherent in quantum systems. Our study focuses on the\nasymmetric quantum conditional mutual information (QCMI), incorporating the von\nNeumann entropy, as a directional metric of causal influence in quantum\nmany-body systems. We analyze spin chains using the QCMI, implementing a\nprojective measurement on one site as the intervention and monitoring its\neffect on a distant site conditioned on intermediate spins. Additionally, we\nstudy the effective causal propagation velocity, which is the speed at which\nQCMI becomes significant at distant sites. These findings indicate the presence\nof finite-speed propagation of causal influence, along with the emergence of\ncoherent oscillations.",
      "pdf_url": "http://arxiv.org/pdf/2508.12160v1",
      "arxiv_url": "http://arxiv.org/abs/2508.12160v1",
      "published": "2025-08-16",
      "categories": [
        "quant-ph",
        "nlin.CD",
        "physics.app-ph",
        "physics.comp-ph"
      ]
    },
    {
      "title": "Applied causality to infer protein dynamics and kinetics",
      "authors": [
        "Akashnathan Aranganathan",
        "Eric R. Beyerle"
      ],
      "abstract": "The use of generative machine learning models, trained on the experimentally\nresolved structures deposited in the protein data bank, is an attractive\napproach to sampling conformational ensembles of proteins. Unfortunately, since\nthe machine-learned model utilized to generate these ensembles is not tied to\nan equation of motion, such as a molecular dynamics integrator or other causal\ngenerator of the dynamics, there is no timescale or causal information encoded\nin them. As such, with this work, we use the structural ensembles generated\nfrom AlphaFold2 at a range of reduced MSA depths to parameterize the potential\nof mean force of an overdamped, memory-free, coarse-grained Langevin equation.\nThis approach couples the AlphaFold2 ensembles to a causal model, allowing us\nto estimate the timescales spanned by the AlphaFold2-generated ensembles at\neach MSA depth. Performing this analysis on six variants of HIV-1 protease, we\nconfirm an inverse relationship between MSA depth and the timescale of an\nensemble's conformational fluctuations, since the MSA depth essentially serves\nas a conformational restraint, and AlphaFold2 is generally able to probe\ntimescales at or below those seen in microsecond-long, unbiased molecular\ndynamics simulations. We conclude by generalizing this approach to other\nmachine-learned structure-prediction methods.",
      "pdf_url": "http://arxiv.org/pdf/2508.12060v1",
      "arxiv_url": "http://arxiv.org/abs/2508.12060v1",
      "published": "2025-08-16",
      "categories": [
        "q-bio.BM",
        "physics.bio-ph"
      ]
    },
    {
      "title": "Deconfounding via Profiled Transfer Learning",
      "authors": [
        "Ziyuan Chen",
        "Yifan Jiang",
        "Jingyuan Liu",
        "Fang Yao"
      ],
      "abstract": "Unmeasured confounders are a major source of bias in regression-based effect\nestimation and causal inference. In this paper, we advocate a new profiled\ntransfer learning framework, ProTrans, to address confounding effects in the\ntarget dataset, when additional source datasets that possess similar\nconfounding structures are available. We introduce the concept of profiled\nresiduals to characterize the shared confounding patterns between source and\ntarget datasets. By incorporating these profiled residuals into the target\ndebiasing step, we effectively mitigates the latent confounding effects. We\nalso propose a source selection strategy to enhance robustness of ProTrans\nagainst noninformative sources. As a byproduct, ProTrans can also be utilized\nto estimate treatment effects when potential confounders exist, without the use\nof auxiliary features such as instrumental or proxy variables, which are often\nchallenging to select in practice. Theoretically, we prove that the resulting\nestimated model shift from sources to target is confounding-free without any\nassumptions imposed on the true confounding structure, and that the target\nparameter estimation achieves the minimax optimal rate under mild conditions.\nSimulated and real-world experiments validate the effectiveness of ProTrans and\nsupport the theoretical findings.",
      "pdf_url": "http://arxiv.org/pdf/2508.11622v1",
      "arxiv_url": "http://arxiv.org/abs/2508.11622v1",
      "published": "2025-08-15",
      "categories": [
        "stat.ME"
      ]
    }
  ]
}