{
  "last_updated": "2026-01-06T00:58:34.629504",
  "papers": [
    {
      "title": "Fair Policy Learning under Bipartite Network Interference: Learning Fair and Cost-Effective Environmental Policies",
      "authors": [
        "Raphael C. Kim",
        "Rachel C. Nethery",
        "Kevin L. Chen",
        "Falco J. Bargagli-Stoffi"
      ],
      "abstract": "Numerous studies have shown the harmful effects of airborne pollutants on human health. Vulnerable groups and communities often bear a disproportionately larger health burden due to exposure to airborne pollutants. Thus, there is a need to design policies that effectively reduce the public health burdens while ensuring cost-effective policy interventions. Designing policies that optimally benefit the population while ensuring equity between groups under cost constraints is a challenging statistical and causal inference problem. In the context of environmental policy this is further complicated by the fact that interventions target emission sources but health impacts occur in potentially distant communities due to atmospheric pollutant transport -- a setting known as bipartite network interference (BNI). To address these issues, we propose a fair policy learning approach under BNI. Our approach allows to learn cost-effective policies under fairness constraints even accounting for complex BNI data structures. We derive asymptotic properties and demonstrate finite sample performance via Monte Carlo simulations. Finally, we apply the proposed method to a real-world dataset linking power plant scrubber installations to Medicare health records for more than 2 million individuals in the U.S. Our method determine fair scrubber allocations to reduce mortality under fairness and cost constraints.",
      "pdf_url": "https://arxiv.org/pdf/2601.00531v1",
      "arxiv_url": "http://arxiv.org/abs/2601.00531v1",
      "published": "2026-01-02",
      "categories": [
        "stat.ME"
      ]
    },
    {
      "title": "Bio-inspired Agentic Self-healing Framework for Resilient Distributed Computing Continuum Systems",
      "authors": [
        "Alaa Saleh",
        "Praveen Kumar Donta",
        "Roberto Morabito",
        "Sasu Tarkoma",
        "Anders Lindgren",
        "Qiyang Zhang",
        "Schahram Dustdar",
        "Susanna Pirttikangas",
        "Lauri Lov√©n"
      ],
      "abstract": "Human biological systems sustain life through extraordinary resilience, continually detecting damage, orchestrating targeted responses, and restoring function through self-healing. Inspired by these capabilities, this paper introduces ReCiSt, a bio-inspired agentic self-healing framework designed to achieve resilience in Distributed Computing Continuum Systems (DCCS). Modern DCCS integrate heterogeneous computing resources, ranging from resource-constrained IoT devices to high-performance cloud infrastructures, and their inherent complexity, mobility, and dynamic operating conditions expose them to frequent faults that disrupt service continuity. These challenges underscore the need for scalable, adaptive, and self-regulated resilience strategies. ReCiSt reconstructs the biological phases of Hemostasis, Inflammation, Proliferation, and Remodeling into the computational layers Containment, Diagnosis, Meta-Cognitive, and Knowledge for DCCS. These four layers perform autonomous fault isolation, causal diagnosis, adaptive recovery, and long-term knowledge consolidation through Language Model (LM)-powered agents. These agents interpret heterogeneous logs, infer root causes, refine reasoning pathways, and reconfigure resources with minimal human intervention. The proposed ReCiSt framework is evaluated on public fault datasets using multiple LMs, and no baseline comparison is included due to the scarcity of similar approaches. Nevertheless, our results, evaluated under different LMs, confirm ReCiSt's self-healing capabilities within tens of seconds with minimum of 10% of agent CPU usage. Our results also demonstrated depth of analysis to over come uncertainties and amount of micro-agents invoked to achieve resilience.",
      "pdf_url": "https://arxiv.org/pdf/2601.00339v1",
      "arxiv_url": "http://arxiv.org/abs/2601.00339v1",
      "published": "2026-01-01",
      "categories": [
        "cs.AI",
        "cs.DC",
        "cs.ET",
        "cs.MA",
        "cs.NE"
      ]
    },
    {
      "title": "Identification and Estimation under Multiple Versions of Treatment: Mixture-of-Experts Approach",
      "authors": [
        "Kohei Yoshikawa",
        "Shuichi Kawano"
      ],
      "abstract": "The Stable Unit Treatment Value Assumption (SUTVA) includes the condition that there are no multiple versions of treatment in causal inference. Though we could not control the implementation of treatment in observational studies, multiple versions may exist in the treatment. It has been pointed out that ignoring such multiple versions of treatment can lead to biased estimates of causal effects, but a causal inference framework that explicitly deals with the unbiased identification and estimation of version-specific causal effects has not been fully developed yet. Thus, obtaining a deeper understanding for mechanisms of the complex treatments is difficult. In this paper, we introduce the Mixture-of-Experts framework into causal inference and develop a methodology for estimating the causal effects of latent versions. This approach enables explicit estimation of version-specific causal effects even if the versions are not observed. Numerical experiments demonstrate the effectiveness of the proposed method.",
      "pdf_url": "https://arxiv.org/pdf/2601.00287v1",
      "arxiv_url": "http://arxiv.org/abs/2601.00287v1",
      "published": "2026-01-01",
      "categories": [
        "stat.ME",
        "stat.ML"
      ]
    },
    {
      "title": "Detecting Unobserved Confounders: A Kernelized Regression Approach",
      "authors": [
        "Yikai Chen",
        "Yunxin Mao",
        "Chunyuan Zheng",
        "Hao Zou",
        "Shanzhi Gu",
        "Shixuan Liu",
        "Yang Shi",
        "Wenjing Yang",
        "Kun Kuang",
        "Haotian Wang"
      ],
      "abstract": "Detecting unobserved confounders is crucial for reliable causal inference in observational studies. Existing methods require either linearity assumptions or multiple heterogeneous environments, limiting applicability to nonlinear single-environment settings. To bridge this gap, we propose Kernel Regression Confounder Detection (KRCD), a novel method for detecting unobserved confounding in nonlinear observational data under single-environment conditions. KRCD leverages reproducing kernel Hilbert spaces to model complex dependencies. By comparing standard and higherorder kernel regressions, we derive a test statistic whose significant deviation from zero indicates unobserved confounding. Theoretically, we prove two key results: First, in infinite samples, regression coefficients coincide if and only if no unobserved confounders exist. Second, finite-sample differences converge to zero-mean Gaussian distributions with tractable variance. Extensive experiments on synthetic benchmarks and the Twins dataset demonstrate that KRCD not only outperforms existing baselines but also achieves superior computational efficiency.",
      "pdf_url": "https://arxiv.org/pdf/2601.00200v1",
      "arxiv_url": "http://arxiv.org/abs/2601.00200v1",
      "published": "2026-01-01",
      "categories": [
        "stat.ML",
        "cs.LG"
      ]
    },
    {
      "title": "The Agentic Leash: Extracting Causal Feedback Fuzzy Cognitive Maps with LLMs",
      "authors": [
        "Akash Kumar Panda",
        "Olaoluwa Adigun",
        "Bart Kosko"
      ],
      "abstract": "We design a large-language-model (LLM) agent that extracts causal feedback fuzzy cognitive maps (FCMs) from raw text. The causal learning or extraction process is agentic both because of the LLM's semi-autonomy and because ultimately the FCM dynamical system's equilibria drive the LLM agents to fetch and process causal text. The fetched text can in principle modify the adaptive FCM causal structure and so modify the source of its quasi-autonomy--its equilibrium limit cycles and fixed-point attractors. This bidirectional process endows the evolving FCM dynamical system with a degree of autonomy while still staying on its agentic leash. We show in particular that a sequence of three finely tuned system instructions guide an LLM agent as it systematically extracts key nouns and noun phrases from text, as it extracts FCM concept nodes from among those nouns and noun phrases, and then as it extracts or infers partial or fuzzy causal edges between those FCM nodes. We test this FCM generation on a recent essay about the promise of AI from the late diplomat and political theorist Henry Kissinger and his colleagues. This three-step process produced FCM dynamical systems that converged to the same equilibrium limit cycles as did the human-generated FCMs even though the human-generated FCM differed in the number of nodes and edges. A final FCM mixed generated FCMs from separate Gemini and ChatGPT LLM agents. The mixed FCM absorbed the equilibria of its dominant mixture component but also created new equilibria of its own to better approximate the underlying causal dynamical system.",
      "pdf_url": "https://arxiv.org/pdf/2601.00097v1",
      "arxiv_url": "http://arxiv.org/abs/2601.00097v1",
      "published": "2025-12-31",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.HC",
        "cs.IR"
      ]
    },
    {
      "title": "CPR: Causal Physiological Representation Learning for Robust ECG Analysis under Distribution Shifts",
      "authors": [
        "Shunbo Jia",
        "Caizhi Liao"
      ],
      "abstract": "Deep learning models for Electrocardiogram (ECG) diagnosis have achieved remarkable accuracy but exhibit fragility against adversarial perturbations, particularly Smooth Adversarial Perturbations (SAP) that mimic biological morphology. Existing defenses face a critical dilemma: Adversarial Training (AT) provides robustness but incurs a prohibitive computational burden, while certified methods like Randomized Smoothing (RS) introduce significant inference latency, rendering them impractical for real-time clinical monitoring. We posit that this vulnerability stems from the models' reliance on non-robust spurious correlations rather than invariant pathological features. To address this, we propose Causal Physiological Representation Learning (CPR). Unlike standard denoising approaches that operate without semantic constraints, CPR incorporates a Physiological Structural Prior within a causal disentanglement framework. By modeling ECG generation via a Structural Causal Model (SCM), CPR enforces a structural intervention that strictly separates invariant pathological morphology (P-QRS-T complex) from non-causal artifacts. Empirical results on PTB-XL demonstrate that CPR significantly outperforms standard clinical preprocessing methods. Specifically, under SAP attacks, CPR achieves an F1 score of 0.632, surpassing Median Smoothing (0.541 F1) by 9.1%. Crucially, CPR matches the certified robustness of Randomized Smoothing while maintaining single-pass inference efficiency, offering a superior trade-off between robustness, efficiency, and clinical interpretability.",
      "pdf_url": "https://arxiv.org/pdf/2512.24564v1",
      "arxiv_url": "http://arxiv.org/abs/2512.24564v1",
      "published": "2025-12-31",
      "categories": [
        "cs.LG",
        "eess.SP"
      ]
    },
    {
      "title": "Demystifying Proximal Causal Inference",
      "authors": [
        "Grace V. Ringlein",
        "Trang Quynh Nguyen",
        "Peter P. Zandi",
        "Elizabeth A. Stuart",
        "Harsh Parikh"
      ],
      "abstract": "Proximal causal inference (PCI) has emerged as a promising framework for identifying and estimating causal effects in the presence of unobserved confounders. While many traditional causal inference methods rely on the assumption of no unobserved confounding, this assumption is likely often violated. PCI mitigates this challenge by relying on an alternative set of assumptions regarding the relationships between treatment, outcome, and auxiliary variables that serve as proxies for unmeasured confounders. We review existing identification results, discuss the assumptions necessary for valid causal effect estimation via PCI, and compare different PCI estimation methods. We offer practical guidance on operationalizing PCI, with a focus on selecting and evaluating proxy variables using domain knowledge, measurement error perspectives, and negative control analogies. Through conceptual examples, we demonstrate tensions in proxy selection and discuss the importance of clearly defining the unobserved confounding mechanism. By bridging formal results with applied considerations, this work aims to demystify PCI, encourage thoughtful use in practice, and identify open directions for methodological development and empirical research.",
      "pdf_url": "https://arxiv.org/pdf/2512.24413v1",
      "arxiv_url": "http://arxiv.org/abs/2512.24413v1",
      "published": "2025-12-30",
      "categories": [
        "stat.ME"
      ]
    },
    {
      "title": "Stream-DiffVSR: Low-Latency Streamable Video Super-Resolution via Auto-Regressive Diffusion",
      "authors": [
        "Hau-Shiang Shiu",
        "Chin-Yang Lin",
        "Zhixiang Wang",
        "Chi-Wei Hsiao",
        "Po-Fan Yu",
        "Yu-Chih Chen",
        "Yu-Lun Liu"
      ],
      "abstract": "Diffusion-based video super-resolution (VSR) methods achieve strong perceptual quality but remain impractical for latency-sensitive settings due to reliance on future frames and expensive multi-step denoising. We propose Stream-DiffVSR, a causally conditioned diffusion framework for efficient online VSR. Operating strictly on past frames, it combines a four-step distilled denoiser for fast inference, an Auto-regressive Temporal Guidance (ARTG) module that injects motion-aligned cues during latent denoising, and a lightweight temporal-aware decoder with a Temporal Processor Module (TPM) that enhances detail and temporal coherence. Stream-DiffVSR processes 720p frames in 0.328 seconds on an RTX4090 GPU and significantly outperforms prior diffusion-based methods. Compared with the online SOTA TMP, it boosts perceptual quality (LPIPS +0.095) while reducing latency by over 130x. Stream-DiffVSR achieves the lowest latency reported for diffusion-based VSR, reducing initial delay from over 4600 seconds to 0.328 seconds, thereby making it the first diffusion VSR method suitable for low-latency online deployment. Project page: https://jamichss.github.io/stream-diffvsr-project-page/",
      "pdf_url": "https://arxiv.org/pdf/2512.23709v1",
      "arxiv_url": "http://arxiv.org/abs/2512.23709v1",
      "published": "2025-12-29",
      "categories": [
        "cs.CV"
      ]
    },
    {
      "title": "Propensity Patchwork Kriging for Scalable Inference on Heterogeneous Treatment Effects",
      "authors": [
        "Hajime Ogawa",
        "Shonosuke Sugasawa"
      ],
      "abstract": "Gaussian process-based models are attractive for estimating heterogeneous treatment effects (HTE), but their computational cost limits scalability in causal inference settings. In this work, we address this challenge by extending Patchwork Kriging into the causal inference framework. Our proposed method partitions the data according to the estimated propensity score and applies Patchwork Kriging to enforce continuity of HTE estimates across adjacent regions. By imposing continuity constraints only along the propensity score dimension, rather than the full covariate space, the proposed approach substantially reduces computational cost while avoiding discontinuities inherent in simple local approximations. The resulting method can be interpreted as a smoothing extension of stratification and provides an efficient approach to HTE estimation. The proposed method is demonstrated through simulation studies and a real data application.",
      "pdf_url": "https://arxiv.org/pdf/2512.23467v1",
      "arxiv_url": "http://arxiv.org/abs/2512.23467v1",
      "published": "2025-12-29",
      "categories": [
        "stat.ME"
      ]
    },
    {
      "title": "Probabilistic Modelling is Sufficient for Causal Inference",
      "authors": [
        "Bruno Mlodozeniec",
        "David Krueger",
        "Richard E. Turner"
      ],
      "abstract": "Causal inference is a key research area in machine learning, yet confusion reigns over the tools needed to tackle it. There are prevalent claims in the machine learning literature that you need a bespoke causal framework or notation to answer causal questions. In this paper, we want to make it clear that you \\emph{can} answer any causal inference question within the realm of probabilistic modelling and inference, without causal-specific tools or notation. Through concrete examples, we demonstrate how causal questions can be tackled by writing down the probability of everything. Lastly, we reinterpret causal tools as emerging from standard probabilistic modelling and inference, elucidating their necessity and utility.",
      "pdf_url": "https://arxiv.org/pdf/2512.23408v1",
      "arxiv_url": "http://arxiv.org/abs/2512.23408v1",
      "published": "2025-12-29",
      "categories": [
        "stat.ML",
        "cs.LG",
        "stat.ME"
      ]
    }
  ]
}