{
  "last_updated": "2026-02-10T01:22:53.030696",
  "papers": [
    {
      "title": "Endogenous Resistance to Activation Steering in Language Models",
      "authors": [
        "Alex McKenzie",
        "Keenan Pepper",
        "Stijn Servaes",
        "Martin Leitgab",
        "Murat Cubuktepe",
        "Mike Vaiana",
        "Diogo de Lucena",
        "Judd Rosenblatt",
        "Michael S. A. Graziano"
      ],
      "abstract": "Large language models can resist task-misaligned activation steering during inference, sometimes recovering mid-generation to produce improved responses even when steering remains active. We term this Endogenous Steering Resistance (ESR). Using sparse autoencoder (SAE) latents to steer model activations, we find that Llama-3.3-70B shows substantial ESR, while smaller models from the Llama-3 and Gemma-2 families exhibit the phenomenon less frequently. We identify 26 SAE latents that activate differentially during off-topic content and are causally linked to ESR in Llama-3.3-70B. Zero-ablating these latents reduces the multi-attempt rate by 25%, providing causal evidence for dedicated internal consistency-checking circuits. We demonstrate that ESR can be deliberately enhanced through both prompting and training: meta-prompts instructing the model to self-monitor increase the multi-attempt rate by 4x for Llama-3.3-70B, and fine-tuning on self-correction examples successfully induces ESR-like behavior in smaller models. These findings have dual implications: ESR could protect against adversarial manipulation but might also interfere with beneficial safety interventions that rely on activation steering. Understanding and controlling these resistance mechanisms is important for developing transparent and controllable AI systems. Code is available at github.com/agencyenterprise/endogenous-steering-resistance.",
      "pdf_url": "https://arxiv.org/pdf/2602.06941v1",
      "arxiv_url": "http://arxiv.org/abs/2602.06941v1",
      "published": "2026-02-06",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Can Post-Training Transform LLMs into Causal Reasoners?",
      "authors": [
        "Junqi Chen",
        "Sirui Chen",
        "Chaochao Lu"
      ],
      "abstract": "Causal inference is essential for decision-making but remains challenging for non-experts. While large language models (LLMs) show promise in this domain, their precise causal estimation capabilities are still limited, and the impact of post-training on these abilities is insufficiently explored. This paper examines the extent to which post-training can enhance LLMs' capacity for causal inference. We introduce CauGym, a comprehensive dataset comprising seven core causal tasks for training and five diverse test sets. Using this dataset, we systematically evaluate five post-training approaches: SFT, DPO, KTO, PPO, and GRPO. Across five in-domain and four existing benchmarks, our experiments demonstrate that appropriate post-training enables smaller LLMs to perform causal inference competitively, often surpassing much larger models. Our 14B parameter model achieves 93.5% accuracy on the CaLM benchmark, compared to 55.4% by OpenAI o3. Furthermore, the post-trained LLMs exhibit strong generalization and robustness under real-world conditions such as distribution shifts and noisy data. Collectively, these findings provide the first systematic evidence that targeted post-training can produce reliable and robust LLM-based causal reasoners. Our data and GRPO-model are available at https://github.com/OpenCausaLab/CauGym.",
      "pdf_url": "https://arxiv.org/pdf/2602.06337v1",
      "arxiv_url": "http://arxiv.org/abs/2602.06337v1",
      "published": "2026-02-06",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Latent variation in pathogen strain-specific effects under multiple-versions-of-treatment theory",
      "authors": [
        "Bronner P. Gon√ßalves"
      ],
      "abstract": "Evidence-informed policy on infections requires estimates of their effects on health. However, pathogenic variation, whereby occurrence of adverse outcomes depends on the infecting strain, might complicate the study of many infectious agents. Here, we consider the interpretation of epidemiologic studies on effects of infections on health when there is heterogeneity in strain-specific effects and information on strain composition is unavailable. We use potential outcomes and causal inference theory for analyses in the presence of multiple versions of treatment to argue that oft-reported quantities in these studies have a causal interpretation that depends on population frequencies of infecting strains. Moreover, as in other contexts where the treatment-variation-irrelevance assumption might be violated, transportability requires additional considerations, beyond those needed for non-compound exposures. This discussion, that considers potential heterogeneity in strain-specific effects, will facilitate interpretation of these studies, and for the reasons mentioned above, also highlights the value of pathogen subtype data.",
      "pdf_url": "https://arxiv.org/pdf/2602.06262v1",
      "arxiv_url": "http://arxiv.org/abs/2602.06262v1",
      "published": "2026-02-05",
      "categories": [
        "stat.ME",
        "stat.AP"
      ]
    },
    {
      "title": "Age-Dependent Causal Effects of Mandibular Dose on Osteoradionecrosis Risk After Head and Neck Radiotherapy",
      "authors": [
        "Jingyuan Chen",
        "Yunze Yang",
        "Olivia M. Muller",
        "Lei Zeng",
        "Zhengliang Liu",
        "Tianming Liu",
        "Robert L",
        "Foote",
        "Daniel J",
        "Ma",
        "Samir H",
        "Patel",
        "Zhong Liu",
        "Wei Liu"
      ],
      "abstract": "Distinguishing causal relationships from statistical correlations remains a fundamental challenge in clinical research, limiting the translation of observational findings into interventional treatment guidelines. Here we apply causal machine learning to establish causal effects of radiation dose parameters on mandibular osteoradionecrosis (ORN) in 931 head and neck cancer patients treated with volumetric-modulated arc therapy. Using generalized random forests, we demonstrate that all examined dosimetric factors exhibit significant positive causal effects on ORN development (average treatment effects: 0.092-0.141). Integration with explainable machine learning reveals substantial treatment effect heterogeneity, with patients aged 50-60 years showing the strongest causal dose-response relationships (conditional average treatment effects up to 0.229), while patients over 70 years demonstrate minimal effects. These results suggest that age-stratified treatment optimization and personalized treatment planning for the dosimetric factors could reduce ORN risk. Our findings demonstrate that causal inference methods can transform clinical retrospective radiotherapy data into personalized treatment recommendations, providing a methodological framework applicable to toxicity prediction across oncology and other clinical domains where treatment decisions depend on complex dose-response relationships.",
      "pdf_url": "https://arxiv.org/pdf/2602.06212v1",
      "arxiv_url": "http://arxiv.org/abs/2602.06212v1",
      "published": "2026-02-05",
      "categories": [
        "physics.med-ph"
      ]
    },
    {
      "title": "DeDPO: Debiased Direct Preference Optimization for Diffusion Models",
      "authors": [
        "Khiem Pham",
        "Quang Nguyen",
        "Tung Nguyen",
        "Jingsen Zhu",
        "Michele Santacatterina",
        "Dimitris Metaxas",
        "Ramin Zabih"
      ],
      "abstract": "Direct Preference Optimization (DPO) has emerged as a predominant alignment method for diffusion models, facilitating off-policy training without explicit reward modeling. However, its reliance on large-scale, high-quality human preference labels presents a severe cost and scalability bottleneck. To overcome this, We propose a semi-supervised framework augmenting limited human data with a large corpus of unlabeled pairs annotated via cost-effective synthetic AI feedback. Our paper introduces Debiased DPO (DeDPO), which uniquely integrates a debiased estimation technique from causal inference into the DPO objective. By explicitly identifying and correcting the systematic bias and noise inherent in synthetic annotators, DeDPO ensures robust learning from imperfect feedback sources, including self-training and Vision-Language Models (VLMs). Experiments demonstrate that DeDPO is robust to the variations in synthetic labeling methods, achieving performance that matches and occasionally exceeds the theoretical upper bound of models trained on fully human-labeled data. This establishes DeDPO as a scalable solution for human-AI alignment using inexpensive synthetic supervision.",
      "pdf_url": "https://arxiv.org/pdf/2602.06195v1",
      "arxiv_url": "http://arxiv.org/abs/2602.06195v1",
      "published": "2026-02-05",
      "categories": [
        "cs.CV"
      ]
    },
    {
      "title": "Causal Inference on Stopped Random Walks in Online Advertising",
      "authors": [
        "Jia Yuan Yu"
      ],
      "abstract": "We consider a causal inference problem frequently encountered in online advertising systems, where a publisher (e.g., Instagram, TikTok) interacts repeatedly with human users and advertisers by sporadically displaying to each user an advertisement selected through an auction. Each treatment corresponds to a parameter value of the advertising mechanism (e.g., auction reserve-price), and we want to estimate through experiments the corresponding long-term treatment effect (e.g., annual advertising revenue). In our setting, the treatment affects not only the instantaneous revenue from showing an ad, but also changes each user's interaction-trajectory, and each advertiser's bidding policy -- as the latter is constrained by a finite budget. In particular, each a treatment may even affect the size of the population, since users interact longer with a tolerable advertising mechanism. We drop the classical i.i.d. assumption and model the experiment measurements (e.g., advertising revenue) as a stopped random walk, and use a budget-splitting experimental design, the Anscombe Theorem, a Wald-like equation, and a Central Limit Theorem to construct confidence intervals for the long-term treatment effect.",
      "pdf_url": "https://arxiv.org/pdf/2602.05997v1",
      "arxiv_url": "http://arxiv.org/abs/2602.05997v1",
      "published": "2026-02-05",
      "categories": [
        "stat.ML",
        "cs.LG",
        "stat.ME"
      ]
    },
    {
      "title": "BABE: Biology Arena BEnchmark",
      "authors": [
        "Junting Zhou",
        "Jin Chen",
        "Linfeng Hao",
        "Denghui Cao",
        "Zheyu Wang",
        "Qiguang Chen",
        "Chaoyou Fu",
        "Jiaze Chen",
        "Yuchen Wu",
        "Ge Zhang",
        "Mingxuan Wang",
        "Wenhao Huang",
        "Tong Yang"
      ],
      "abstract": "The rapid evolution of large language models (LLMs) has expanded their capabilities from basic dialogue to advanced scientific reasoning. However, existing benchmarks in biology often fail to assess a critical skill required of researchers: the ability to integrate experimental results with contextual knowledge to derive meaningful conclusions. To address this gap, we introduce BABE(Biology Arena BEnchmark), a comprehensive benchmark designed to evaluate the experimental reasoning capabilities of biological AI systems. BABE is uniquely constructed from peer-reviewed research papers and real-world biological studies, ensuring that tasks reflect the complexity and interdisciplinary nature of actual scientific inquiry. BABE challenges models to perform causal reasoning and cross-scale inference. Our benchmark provides a robust framework for assessing how well AI systems can reason like practicing scientists, offering a more authentic measure of their potential to contribute to biological research.",
      "pdf_url": "https://arxiv.org/pdf/2602.05857v1",
      "arxiv_url": "http://arxiv.org/abs/2602.05857v1",
      "published": "2026-02-05",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "NEX: Neuron Explore-Exploit Scoring for Label-Free Chain-of-Thought Selection and Model Ranking",
      "authors": [
        "Kang Chen",
        "Zhuoka Feng",
        "Sihan Zhao",
        "Kai Xiong",
        "Junjie Nian",
        "Yaoning Wang",
        "Changyi Xiao",
        "Yixin Cao"
      ],
      "abstract": "Large language models increasingly spend inference compute sampling multiple chain-of-thought traces or searching over merged checkpoints. This shifts the bottleneck from generation to selection, often without supervision on the target distribution. We show entropy-based exploration proxies follow an inverted-U with accuracy, suggesting extra exploration can become redundant and induce overthinking. We propose NEX, a white-box label-free unsupervised scoring framework that views reasoning as alternating E-phase (exploration) and X-phase (exploitation). NEX detects E-phase as spikes in newly activated MLP neurons per token from sparse activation caches, then uses a sticky two-state HMM to infer E-X phases and credits E-introduced neurons by whether they are reused in the following X span. These signals yield interpretable neuron weights and a single Good-Mass Fraction score to rank candidate responses and merged variants without task answers. Across reasoning benchmarks and Qwen3 merge families, NEX computed on a small unlabeled activation set predicts downstream accuracy and identifies better variants; we further validate the E-X signal with human annotations and provide causal evidence via \"Effective-vs-Redundant\" neuron transfer.",
      "pdf_url": "https://arxiv.org/pdf/2602.05805v1",
      "arxiv_url": "http://arxiv.org/abs/2602.05805v1",
      "published": "2026-02-05",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Causal Front-Door Adjustment for Robust Jailbreak Attacks on LLMs",
      "authors": [
        "Yao Zhou",
        "Zeen Song",
        "Wenwen Qiang",
        "Fengge Wu",
        "Shuyi Zhou",
        "Changwen Zheng",
        "Hui Xiong"
      ],
      "abstract": "Safety alignment mechanisms in Large Language Models (LLMs) often operate as latent internal states, obscuring the model's inherent capabilities. Building on this observation, we model the safety mechanism as an unobserved confounder from a causal perspective. Then, we propose the Causal Front-Door Adjustment Attack (CFA{$^2$}) to jailbreak LLM, which is a framework that leverages Pearl's Front-Door Criterion to sever the confounding associations for robust jailbreaking. Specifically, we employ Sparse Autoencoders (SAEs) to physically strip defense-related features, isolating the core task intent. We further reduce computationally expensive marginalization to a deterministic intervention with low inference complexity. Experiments demonstrate that CFA{$^2$} achieves state-of-the-art attack success rates while offering a mechanistic interpretation of the jailbreaking process.",
      "pdf_url": "https://arxiv.org/pdf/2602.05444v2",
      "arxiv_url": "http://arxiv.org/abs/2602.05444v2",
      "published": "2026-02-05",
      "categories": [
        "cs.CL"
      ]
    },
    {
      "title": "ProAct: Agentic Lookahead in Interactive Environments",
      "authors": [
        "Yangbin Yu",
        "Mingyu Yang",
        "Junyou Li",
        "Yiming Gao",
        "Feiyu Liu",
        "Yijun Yang",
        "Zichuan Lin",
        "Jiafei Lyu",
        "Yicheng Liu",
        "Zhicong Lu",
        "Deheng Ye",
        "Jie Jiang"
      ],
      "abstract": "Existing Large Language Model (LLM) agents struggle in interactive environments requiring long-horizon planning, primarily due to compounding errors when simulating future states. To address this, we propose ProAct, a framework that enables agents to internalize accurate lookahead reasoning through a two-stage training paradigm. First, we introduce Grounded LookAhead Distillation (GLAD), where the agent undergoes supervised fine-tuning on trajectories derived from environment-based search. By compressing complex search trees into concise, causal reasoning chains, the agent learns the logic of foresight without the computational overhead of inference-time search. Second, to further refine decision accuracy, we propose the Monte-Carlo Critic (MC-Critic), a plug-and-play auxiliary value estimator designed to enhance policy-gradient algorithms like PPO and GRPO. By leveraging lightweight environment rollouts to calibrate value estimates, MC-Critic provides a low-variance signal that facilitates stable policy optimization without relying on expensive model-based value approximation. Experiments on both stochastic (e.g., 2048) and deterministic (e.g., Sokoban) environments demonstrate that ProAct significantly improves planning accuracy. Notably, a 4B parameter model trained with ProAct outperforms all open-source baselines and rivals state-of-the-art closed-source models, while demonstrating robust generalization to unseen environments. The codes and models are available at https://github.com/GreatX3/ProAct",
      "pdf_url": "https://arxiv.org/pdf/2602.05327v1",
      "arxiv_url": "http://arxiv.org/abs/2602.05327v1",
      "published": "2026-02-05",
      "categories": [
        "cs.AI"
      ]
    }
  ]
}