{
  "last_updated": "2026-01-28T00:59:55.541815",
  "papers": [
    {
      "title": "From Fuzzy to Exact: The Halo Architecture for Infinite-Depth Reasoning via Rational Arithmetic",
      "authors": [
        "Hansheng Ren"
      ],
      "abstract": "Current paradigms in Deep Learning prioritize computational throughput over numerical precision, relying on the assumption that intelligence emerges from statistical correlation at scale. In this paper, we challenge this orthodoxy. We propose the Exactness Hypothesis: that General Intelligence (AGI), specifically high-order causal inference, requires a computational substrate capable of Arbitrary Precision Arithmetic. We argue that the \"hallucinations\" and logical incoherence seen in current Large Language Models (LLMs) are artifacts of IEEE 754 floating-point approximation errors accumulating over deep compositional functions. To mitigate this, we introduce the Halo Architecture, a paradigm shift to Rational Arithmetic ($\\mathbb{Q}$) supported by a novel Exact Inference Unit (EIU). Empirical validation on the Huginn-0125 prototype demonstrates that while 600B-parameter scale BF16 baselines collapse in chaotic systems, Halo maintains zero numerical divergence indefinitely. This work establishes exact arithmetic as a prerequisite for reducing logical uncertainty in System 2 AGI.",
      "pdf_url": "https://arxiv.org/pdf/2601.18702v1",
      "arxiv_url": "http://arxiv.org/abs/2601.18702v1",
      "published": "2026-01-26",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.AR"
      ]
    },
    {
      "title": "CASSANDRA: Programmatic and Probabilistic Learning and Inference for Stochastic World Modeling",
      "authors": [
        "Panagiotis Lymperopoulos",
        "Abhiramon Rajasekharan",
        "Ian Berlot-Attwell",
        "St√©phane Aroca-Ouellette",
        "Kaheer Suleman"
      ],
      "abstract": "Building world models is essential for planning in real-world domains such as businesses. Since such domains have rich semantics, we can leverage world knowledge to effectively model complex action effects and causal relationships from limited data. In this work, we propose CASSANDRA, a neurosymbolic world modeling approach that leverages an LLM as a knowledge prior to construct lightweight transition models for planning. CASSANDRA integrates two components: (1) LLM-synthesized code to model deterministic features, and (2) LLM-guided structure learning of a probabilistic graphical model to capture causal relationships among stochastic variables. We evaluate CASSANDRA in (i) a small-scale coffee-shop simulator and (ii) a complex theme park business simulator, where we demonstrate significant improvements in transition prediction and planning over baselines.",
      "pdf_url": "https://arxiv.org/pdf/2601.18620v1",
      "arxiv_url": "http://arxiv.org/abs/2601.18620v1",
      "published": "2026-01-26",
      "categories": [
        "cs.LG"
      ]
    },
    {
      "title": "Nearly Optimal Bayesian Inference for Structural Missingness",
      "authors": [
        "Chen Liang",
        "Donghua Yang",
        "Yutong Wang",
        "Tianle Zhang",
        "Shenghe Zhou",
        "Zhiyu Liang",
        "Hengtong Zhang",
        "Hongzhi Wang",
        "Ziqi Li",
        "Xiyang Zhang",
        "Zheng Liang",
        "Yifei Li"
      ],
      "abstract": "Structural missingness breaks 'just impute and train': values can be undefined by causal or logical constraints, and the mask may depend on observed variables, unobserved variables (MNAR), and other missingness indicators. It simultaneously brings (i) a catch-22 situation with causal loop, prediction needs the missing features, yet inferring them depends on the missingness mechanism, (ii) under MNAR, the unseen are different, the missing part can come from a shifted distribution, and (iii) plug-in imputation, a single fill-in can lock in uncertainty and yield overconfident, biased decisions. In the Bayesian view, prediction via the posterior predictive distribution integrates over the full model posterior uncertainty, rather than relying on a single point estimate. This framework decouples (i) learning an in-model missing-value posterior from (ii) label prediction by optimizing the predictive posterior distribution, enabling posterior integration. This decoupling yields an in-model almost-free-lunch: once the posterior is learned, prediction is plug-and-play while preserving uncertainty propagation. It achieves SOTA on 43 classification and 15 imputation benchmarks, with finite-sample near Bayes-optimality guarantees under our SCM prior.",
      "pdf_url": "https://arxiv.org/pdf/2601.18500v1",
      "arxiv_url": "http://arxiv.org/abs/2601.18500v1",
      "published": "2026-01-26",
      "categories": [
        "cs.LG"
      ]
    },
    {
      "title": "Making medical vision-language models think causally across modalities with retrieval-augmented cross-modal reasoning",
      "authors": [
        "Weiqin Yang",
        "Haowen Xue",
        "Qingyi Peng",
        "Hexuan Hu",
        "Qian Huang",
        "Tingbo Zhang"
      ],
      "abstract": "Medical vision-language models (VLMs) achieve strong performance in diagnostic reporting and image-text alignment, yet their underlying reasoning mechanisms remain fundamentally correlational, exhibiting reliance on superficial statistical associations that fail to capture the causal pathophysiological mechanisms central to clinical decision-making. This limitation makes them fragile, prone to hallucinations, and sensitive to dataset biases. Retrieval-augmented generation (RAG) offers a partial remedy by grounding predictions in external knowledge. However, conventional RAG depends on semantic similarity, introducing new spurious correlations. We propose Multimodal Causal Retrieval-Augmented Generation, a framework that integrates causal inference principles with multimodal retrieval. It retrieves clinically relevant exemplars and causal graphs from external sources, conditioning model reasoning on counterfactual and interventional evidence rather than correlations alone. Applied to radiology report generation, diagnosis prediction, and visual question answering, it improves factual accuracy, robustness to distribution shifts, and interpretability. Our results highlight causal retrieval as a scalable path toward medical VLMs that think beyond pattern matching, enabling trustworthy multimodal reasoning in high-stakes clinical settings.",
      "pdf_url": "https://arxiv.org/pdf/2601.18356v1",
      "arxiv_url": "http://arxiv.org/abs/2601.18356v1",
      "published": "2026-01-26",
      "categories": [
        "cs.LG"
      ]
    },
    {
      "title": "LLM-ForcedAligner: A Non-Autoregressive and Accurate LLM-Based Forced Aligner for Multilingual and Long-Form Speech",
      "authors": [
        "Bingshen Mu",
        "Xian Shi",
        "Xiong Wang",
        "Hexin Liu",
        "Jin Xu",
        "Lei Xie"
      ],
      "abstract": "Forced alignment (FA) predicts start and end timestamps for words or characters in speech, but existing methods are language-specific and prone to cumulative temporal shifts. The multilingual speech understanding and long-sequence processing abilities of speech large language models (SLLMs) make them promising for FA in multilingual, crosslingual, and long-form speech settings. However, directly applying the next-token prediction paradigm of SLLMs to FA results in hallucinations and slow inference. To bridge the gap, we propose LLM-ForcedAligner, reformulating FA as a slot-filling paradigm: timestamps are treated as discrete indices, and special timestamp tokens are inserted as slots into the transcript. Conditioned on the speech embeddings and the transcript with slots, the SLLM directly predicts the time indices at slots. During training, causal attention masking with non-shifted input and label sequences allows each slot to predict its own timestamp index based on itself and preceding context, with loss computed only at slot positions. Dynamic slot insertion enables FA at arbitrary positions. Moreover, non-autoregressive inference is supported, avoiding hallucinations and improving speed. Experiments across multilingual, crosslingual, and long-form speech scenarios show that LLM-ForcedAligner achieves a 69%~78% relative reduction in accumulated averaging shift compared with prior methods. The checkpoint and inference code will be released later.",
      "pdf_url": "https://arxiv.org/pdf/2601.18220v1",
      "arxiv_url": "http://arxiv.org/abs/2601.18220v1",
      "published": "2026-01-26",
      "categories": [
        "cs.SD",
        "eess.AS"
      ]
    },
    {
      "title": "\\textsc{NaVIDA}: Vision-Language Navigation with Inverse Dynamics Augmentation",
      "authors": [
        "Weiye Zhu",
        "Zekai Zhang",
        "Xiangchen Wang",
        "Hewei Pan",
        "Teng Wang",
        "Tiantian Geng",
        "Rongtao Xu",
        "Feng Zheng"
      ],
      "abstract": "Vision-and-Language Navigation (VLN) requires agents to interpret natural language instructions and act coherently in visually rich environments. However, most existing methods rely on reactive state-action mappings without explicitly modeling how actions causally transform subsequent visual observations. Lacking such vision-action causality, agents cannot anticipate the visual changes induced by its own actions, leading to unstable behaviors, weak generalization, and cumulative error along trajectory. To address these issues, we introduce \\textsc{NaVIDA} (\\textbf{Nav}igation with \\textbf{I}nverse \\textbf{D}ynamics \\textbf{A}ugmentation), a unified VLN framework that couples policy learning with action-grounded visual dynamics and adaptive execution. \\textsc{NaVIDA} augments training with chunk-based inverse-dynamics supervision to learn causal relationship between visual changes and corresponding actions. To structure this supervision and extend the effective planning range, \\textsc{NaVIDA} employs hierarchical probabilistic action chunking (HPAC), which organizes trajectories into multi-step chunks and provides discriminative, longer-range visual-change cues. To further curb error accumulation and stabilize behavior at inference, an entropy-guided mechanism adaptively sets the execution horizon of action chunks. Extensive experiments show that \\textsc{NaVIDA} achieves superior navigation performance compared to state-of-the-art methods with fewer parameters (3B vs. 8B). Real-world robot evaluations further validate the practical feasibility and effectiveness of our approach. Code and data will be available upon acceptance.",
      "pdf_url": "https://arxiv.org/pdf/2601.18188v1",
      "arxiv_url": "http://arxiv.org/abs/2601.18188v1",
      "published": "2026-01-26",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "VidLaDA: Bidirectional Diffusion Large Language Models for Efficient Video Understanding",
      "authors": [
        "Zhihao He",
        "Tieyuan Chen",
        "Kangyu Wang",
        "Ziran Qin",
        "Yang Shao",
        "Chaofan Gan",
        "Shijie Li",
        "Zuxuan Wu",
        "Weiyao Lin"
      ],
      "abstract": "Standard Autoregressive Video LLMs inevitably suffer from causal masking biases that hinder global spatiotemporal modeling, leading to suboptimal understanding efficiency. We propose VidLaDA, a Video LLM based on Diffusion Language Model utilizing bidirectional attention to capture bidirectional dependencies. To further tackle the inference bottleneck of diffusion decoding on massive video tokens, we introduce MARS-Cache. This framework accelerates inference by combining asynchronous visual cache refreshing with frame-wise chunk attention, effectively pruning redundancy while preserving global connectivity via anchor tokens. Extensive experiments show VidLaDA outperforms diffusion baselines and rivals state-of-the-art autoregressive models (e.g., Qwen2.5-VL and LLaVA-Video), with MARS-Cache delivering over 12x speedup without compromising reasoning accuracy. Code and checkpoints are open-sourced at https://github.com/ziHoHe/VidLaDA.",
      "pdf_url": "https://arxiv.org/pdf/2601.17868v1",
      "arxiv_url": "http://arxiv.org/abs/2601.17868v1",
      "published": "2026-01-25",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "S$^3$-Attention:Attention-Aligned Endogenous Retrieval for Memory-Bounded Long-Context Inference",
      "authors": [
        "Qingsen Ma",
        "Dianyun Wang",
        "Yaoye Wang",
        "Lechen Ning",
        "Sujie Zhu",
        "Xiaohang Zhang",
        "Jiaming Lyu",
        "Linhao Ren",
        "Zhenbo Xu",
        "Zhaofeng He"
      ],
      "abstract": "Large language models are increasingly applied to multi-document and long-form inputs, yet long-context inference remains memory- and noise-inefficient. Key-value (KV) caching scales linearly with context length, while external retrieval methods often return lexically similar but causally irrelevant passages.\n  We present S3-Attention, a memory-first inference-time framework that treats long-context processing as attention-aligned endogenous retrieval. S3-Attention decodes transient key and query projections into top-k sparse feature identifiers using lightweight sparse autoencoders, and constructs a CPU-based inverted index mapping features to token positions or spans during a single streaming scan. This design allows the KV cache to be discarded entirely and bounds GPU memory usage by the scan chunk size.\n  At generation time, feature co-activation is used to retrieve compact evidence spans, optionally fused with BM25 for exact lexical matching. Under a unified LongBench evaluation protocol with fixed prompting, decoding, and matched token budgets, S3-Hybrid closely matches full-context inference across multiple model families and improves robustness in several information-dense settings. We also report an engineering limitation of the current prototype, which incurs higher wall-clock latency than optimized full-KV baselines, motivating future kernel-level optimization.",
      "pdf_url": "https://arxiv.org/pdf/2601.17702v1",
      "arxiv_url": "http://arxiv.org/abs/2601.17702v1",
      "published": "2026-01-25",
      "categories": [
        "cs.CL",
        "cs.LG"
      ]
    },
    {
      "title": "Bidirectional causal inference for binary outcomes in the presence of unmeasured confounding",
      "authors": [
        "Yafang Deng",
        "Kang Shuai",
        "Shanshan Luo"
      ],
      "abstract": "Bidirectional causal relationships arising from mutual interactions between variables are commonly observed within biomedical, econometrical, and social science contexts. When such relationships are further complicated by unobserved factors, identifying causal effects in both directions becomes especially challenging. For continuous variables, methods that utilize two instrumental variables from both directions have been proposed to explore bidirectional causal effects in linear models. However, the existing techniques are not applicable when the key variables of interest are binary. To address these issues, we propose a structural equation modeling approach that links observed binary variables to continuous latent variables through a constrained mapping. We further establish identification results for bidirectional causal effects using a pair of instrumental variables. Additionally, we develop an estimation method for the corresponding causal parameters. We also conduct sensitivity analysis under scenarios where certain identification conditions are violated. Finally, we apply our approach to investigate the bidirectional causal relationship between heart disease and diabetes, demonstrating its practical utility in biomedical research.",
      "pdf_url": "https://arxiv.org/pdf/2601.17695v1",
      "arxiv_url": "http://arxiv.org/abs/2601.17695v1",
      "published": "2026-01-25",
      "categories": [
        "stat.ME"
      ]
    },
    {
      "title": "Revisiting Modality Invariance in a Multilingual Speech-Text Model via Neuron-Level Analysis",
      "authors": [
        "Toshiki Nakai",
        "Varsha Suresh",
        "Vera Demberg"
      ],
      "abstract": "Multilingual speech-text foundation models aim to process language uniformly across both modality and language, yet it remains unclear whether they internally represent the same language consistently when it is spoken versus written. We investigate this question in SeamlessM4T v2 through three complementary analyses that probe where language and modality information is encoded, how selective neurons causally influence decoding, and how concentrated this influence is across the network. We identify language- and modality-selective neurons using average-precision ranking, investigate their functional role via median-replacement interventions at inference time, and analyze activation-magnitude inequality across languages and modalities. Across experiments, we find evidence of incomplete modality invariance. Although encoder representations become increasingly language-agnostic, this compression makes it more difficult for the shared decoder to recover the language of origin when constructing modality-agnostic representations, particularly when adapting from speech to text. We further observe sharply localized modality-selective structure in cross-attention key and value projections. Finally, speech-conditioned decoding and non-dominant scripts exhibit higher activation concentration, indicating heavier reliance on a small subset of neurons, which may underlie increased brittleness across modalities and languages.",
      "pdf_url": "https://arxiv.org/pdf/2601.17387v1",
      "arxiv_url": "http://arxiv.org/abs/2601.17387v1",
      "published": "2026-01-24",
      "categories": [
        "cs.CL"
      ]
    }
  ]
}