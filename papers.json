{
  "last_updated": "2025-05-29T00:54:16.121974",
  "papers": [
    {
      "title": "Causal Posterior Estimation",
      "authors": [
        "Simon Dirmeier",
        "Antonietta Mira"
      ],
      "abstract": "We present Causal Posterior Estimation (CPE), a novel method for Bayesian\ninference in simulator models, i.e., models where the evaluation of the\nlikelihood function is intractable or too computationally expensive, but where\none can simulate model outputs given parameter values. CPE utilizes a\nnormalizing flow-based (NF) approximation to the posterior distribution which\ncarefully incorporates the conditional dependence structure induced by the\ngraphical representation of the model into the neural network. Thereby it is\npossible to improve the accuracy of the approximation. We introduce both\ndiscrete and continuous NF architectures for CPE and propose a constant-time\nsampling procedure for the continuous case which reduces the computational\ncomplexity of drawing samples to O(1) as for discrete NFs. We show, through an\nextensive experimental evaluation, that by incorporating the conditional\ndependencies induced by the graphical model directly into the neural network,\nrather than learning them from data, CPE is able to conduct highly accurate\nposterior inference either outperforming or matching the state of the art in\nthe field.",
      "pdf_url": "http://arxiv.org/pdf/2505.21468v1",
      "arxiv_url": "http://arxiv.org/abs/2505.21468v1",
      "published": "2025-05-27",
      "categories": [
        "cs.LG",
        "stat.ML"
      ]
    },
    {
      "title": "A Bayesian approach to the survivor average causal effect in cluster-randomized crossover trials",
      "authors": [
        "Dane Isenberg",
        "Michael O. Harhay",
        "Andrew B. Forbes",
        "Paul J. Young",
        "Fan Li",
        "Nandita Mitra"
      ],
      "abstract": "In cluster-randomized crossover (CRXO) trials, groups of individuals are\nrandomly assigned to two or more sequences of alternating treatments. Since\nclusters act as their own control, the CRXO design is typically more\nstatistically efficient than the usual parallel-arm trial. CRXO trials are\nincreasingly popular in many areas of health research where the number of\navailable clusters is limited. Further, in trials among severely ill patients,\nresearchers often want to assess the effect of treatments on secondary\nnon-terminal outcomes, but frequently in these studies, there are patients who\ndo not survive to have these measurements fully recorded. In this paper, we\nprovide a causal inference framework and treatment effect estimation methods\nfor addressing truncation by death in the setting of CRXO trials. We target the\nsurvivor average causal effect (SACE) estimand, a well-defined subgroup\ntreatment effect obtained via principal stratification. We propose novel\nstructural and standard modeling assumptions to enable SACE identification\nfollowed by estimation within a Bayesian paradigm. We evaluate the small-sample\nperformance of our proposed Bayesian approach for the estimation of the SACE in\nCRXO trial settings via simulation studies. We apply our methods to a\npreviously conducted two-period cross-sectional CRXO study examining the impact\nof proton pump inhibitors compared to histamine-2 receptor blockers on length\nof hospitalization among adults requiring invasive mechanical ventilation.",
      "pdf_url": "http://arxiv.org/pdf/2505.21447v1",
      "arxiv_url": "http://arxiv.org/abs/2505.21447v1",
      "published": "2025-05-27",
      "categories": [
        "stat.ME"
      ]
    },
    {
      "title": "The Effect of the Gotthard Base Tunnel on Road Traffic: A Synthetic Control Approach",
      "authors": [
        "Hannes Wallimann",
        "Widar von Arx",
        "Ann Hesse"
      ],
      "abstract": "The opening of the Gotthard Base Tunnel in 2017, the longest railway tunnel\nin the world, marked a milestone in Swiss transport policy. The tunnel, a part\nof the New Rail Link through the Alps, serves as a key instrument of the\nso-called \"modal shift policy,\" which aims to transfer transalpine freight\ntraffic from road to rail. The reduction in travel time by train between\nnorthern and southern Switzerland raised expectations that a substantial share\nof tourist-oriented passenger traffic would also shift from car to rail. In\nthis paper, we conduct a causal analysis of the impact of the Gotthard Base\nTunnel's opening at the end of 2016 on the number of cars using the parallel\nGotthard motorway section in the subsequent years. To this end, we apply the\nsynthetic control and the synthetic difference-in-differences methods to\nconstruct a synthetic Gotthard motorway section based on a weighted combination\nof other alpine road crossings (a so-called donor pool) that did not experience\nthe construction of a competing rail infrastructure. Our results reveal only a\nmodest but statistically significant decline in the number of cars between the\nactual and the synthetic Gotthard motorway in the short run. Given the\nconsistently strong and increasing demand for the new rail connection through\nthe Gotthard Base Tunnel, we infer a substantial induced short-run demand\neffect resulting from the rail travel time savings.",
      "pdf_url": "http://arxiv.org/pdf/2505.21129v1",
      "arxiv_url": "http://arxiv.org/abs/2505.21129v1",
      "published": "2025-05-27",
      "categories": [
        "econ.GN",
        "q-fin.EC"
      ]
    },
    {
      "title": "Study of Lightweight Transformer Architectures for Single-Channel Speech Enhancement",
      "authors": [
        "Haixin Zhao",
        "Nilesh Madhu"
      ],
      "abstract": "In speech enhancement, achieving state-of-the-art (SotA) performance while\nadhering to the computational constraints on edge devices remains a formidable\nchallenge. Networks integrating stacked temporal and spectral modelling\neffectively leverage improved architectures such as transformers; however, they\ninevitably incur substantial computational complexity and model expansion.\nThrough systematic ablation analysis on transformer-based temporal and spectral\nmodelling, we demonstrate that the architecture employing streamlined\nFrequency-Time-Frequency (FTF) stacked transformers efficiently learns global\ndependencies within causal context, while avoiding considerable computational\ndemands. Utilising discriminators in training further improves learning\nefficacy and enhancement without introducing additional complexity during\ninference. The proposed lightweight, causal, transformer-based architecture\nwith adversarial training (LCT-GAN) yields SoTA performance on instrumental\nmetrics among contemporary lightweight models, but with far less overhead.\nCompared to DeepFilterNet2, the LCT-GAN only requires 6% of the parameters, at\nsimilar complexity and performance. Against CCFNet+(Lite), LCT-GAN saves 9% in\nparameters and 10% in multiply-accumulate operations yet yielding improved\nperformance. Further, the LCT-GAN even outperforms more complex, common\nbaseline models on widely used test datasets.",
      "pdf_url": "http://arxiv.org/pdf/2505.21057v1",
      "arxiv_url": "http://arxiv.org/abs/2505.21057v1",
      "published": "2025-05-27",
      "categories": [
        "eess.AS",
        "cs.SD"
      ]
    },
    {
      "title": "Debiased Ill-Posed Regression",
      "authors": [
        "AmirEmad Ghassami",
        "James M. Robins",
        "Andrea Rotnitzky"
      ],
      "abstract": "In various statistical settings, the goal is to estimate a function which is\nrestricted by the statistical model only through a conditional moment\nrestriction. Prominent examples include the nonparametric instrumental variable\nframework for estimating the structural function of the outcome variable, and\nthe proximal causal inference framework for estimating the bridge functions. A\ncommon strategy in the literature is to find the minimizer of the projected\nmean squared error. However, this approach can be sensitive to misspecification\nor slow convergence rate of the estimators of the involved nuisance components.\nIn this work, we propose a debiased estimation strategy based on the influence\nfunction of a modification of the projected error and demonstrate its\nfinite-sample convergence rate. Our proposed estimator possesses a second-order\nbias with respect to the involved nuisance functions and a desirable robustness\nproperty with respect to the misspecification of one of the nuisance functions.\nThe proposed estimator involves a hyper-parameter, for which the optimal value\ndepends on potentially unknown features of the underlying data-generating\nprocess. Hence, we further propose a hyper-parameter selection approach based\non cross-validation and derive an error bound for the resulting estimator. This\nanalysis highlights the potential rate loss due to hyper-parameter selection\nand underscore the importance and advantages of incorporating debiasing in this\nsetting. We also study the application of our approach to the estimation of\nregular parameters in a specific parameter class, which are linear functionals\nof the solutions to the conditional moment restrictions and provide sufficient\nconditions for achieving root-n consistency using our debiased estimator.",
      "pdf_url": "http://arxiv.org/pdf/2505.20787v1",
      "arxiv_url": "http://arxiv.org/abs/2505.20787v1",
      "published": "2025-05-27",
      "categories": [
        "stat.ME",
        "econ.EM",
        "stat.ML"
      ]
    },
    {
      "title": "Causal inference with dyadic data in randomized experiments",
      "authors": [
        "Yilin Li",
        "Lu Deng",
        "Yong Wang",
        "Wang Miao"
      ],
      "abstract": "Estimating the treatment effect within network structures is a key focus in\nonline controlled experiments, particularly for social media platforms. We\ninvestigate a scenario where the unit-level outcome of interest comprises a\nseries of dyadic outcomes, which is pervasive in many social network sources,\nspanning from microscale point-to-point messaging to macroscale international\ntrades. Dyadic outcomes are of particular interest in online controlled\nexperiments, capturing pairwise interactions as basic units for analysis. The\ndyadic nature of the data induces interference, as treatment assigned to one\nunit may affect outcomes involving connected pairs. We propose a novel\ndesign-based causal inference framework for dyadic outcomes in randomized\nexperiments, develop estimators of the global average causal effect, and\nestablish their asymptotic properties under different randomization designs. We\nprove the central limit theorem for the estimators and propose variance\nestimators to quantify the estimation uncertainty. The advantages of\nintegrating dyadic data in randomized experiments are manifested in a variety\nof numerical experiments, especially in correcting interference bias. We\nimplement our proposed method in a large-scale experiment on WeChat Channels,\nassessing the impact of a recommendation algorithm on users' interaction\nmetrics.",
      "pdf_url": "http://arxiv.org/pdf/2505.20780v1",
      "arxiv_url": "http://arxiv.org/abs/2505.20780v1",
      "published": "2025-05-27",
      "categories": [
        "stat.ME",
        "stat.AP"
      ]
    },
    {
      "title": "Mamba-Driven Topology Fusion for Monocular 3-D Human Pose Estimation",
      "authors": [
        "Zenghao Zheng",
        "Lianping Yang",
        "Jinshan Pan",
        "Hegui Zhu"
      ],
      "abstract": "Transformer-based methods for 3-D human pose estimation face significant\ncomputational challenges due to the quadratic growth of self-attention\nmechanism complexity with sequence length. Recently, the Mamba model has\nsubstantially reduced computational overhead and demonstrated outstanding\nperformance in modeling long sequences by leveraging state space model (SSM).\nHowever, the ability of SSM to process sequential data is not suitable for 3-D\njoint sequences with topological structures, and the causal convolution\nstructure in Mamba also lacks insight into local joint relationships. To\naddress these issues, we propose the Mamba-Driven Topology Fusion framework in\nthis paper. Specifically, the proposed Bone Aware Module infers the direction\nand length of bone vectors in the spherical coordinate system, providing\neffective topological guidance for the Mamba model in processing joint\nsequences. Furthermore, we enhance the convolutional structure within the Mamba\nmodel by integrating forward and backward graph convolutional network, enabling\nit to better capture local joint dependencies. Finally, we design a\nSpatiotemporal Refinement Module to model both temporal and spatial\nrelationships within the sequence. Through the incorporation of skeletal\ntopology, our approach effectively alleviates Mamba's limitations in capturing\nhuman structural relationships. We conduct extensive experiments on the\nHuman3.6M and MPI-INF-3DHP datasets for testing and comparison, and the results\nshow that the proposed method greatly reduces computational cost while\nachieving higher accuracy. Ablation studies further demonstrate the\neffectiveness of each proposed module. The code and models will be released.",
      "pdf_url": "http://arxiv.org/pdf/2505.20611v1",
      "arxiv_url": "http://arxiv.org/abs/2505.20611v1",
      "published": "2025-05-27",
      "categories": [
        "cs.CV"
      ]
    },
    {
      "title": "Long-Context State-Space Video World Models",
      "authors": [
        "Ryan Po",
        "Yotam Nitzan",
        "Richard Zhang",
        "Berlin Chen",
        "Tri Dao",
        "Eli Shechtman",
        "Gordon Wetzstein",
        "Xun Huang"
      ],
      "abstract": "Video diffusion models have recently shown promise for world modeling through\nautoregressive frame prediction conditioned on actions. However, they struggle\nto maintain long-term memory due to the high computational cost associated with\nprocessing extended sequences in attention layers. To overcome this limitation,\nwe propose a novel architecture leveraging state-space models (SSMs) to extend\ntemporal memory without compromising computational efficiency. Unlike previous\napproaches that retrofit SSMs for non-causal vision tasks, our method fully\nexploits the inherent advantages of SSMs in causal sequence modeling. Central\nto our design is a block-wise SSM scanning scheme, which strategically trades\noff spatial consistency for extended temporal memory, combined with dense local\nattention to ensure coherence between consecutive frames. We evaluate the\nlong-term memory capabilities of our model through spatial retrieval and\nreasoning tasks over extended horizons. Experiments on Memory Maze and\nMinecraft datasets demonstrate that our approach surpasses baselines in\npreserving long-range memory, while maintaining practical inference speeds\nsuitable for interactive applications.",
      "pdf_url": "http://arxiv.org/pdf/2505.20171v1",
      "arxiv_url": "http://arxiv.org/abs/2505.20171v1",
      "published": "2025-05-26",
      "categories": [
        "cs.CV"
      ]
    },
    {
      "title": "Causal Meta-Analysis: Rethinking the Foundations of Evidence-Based Medicine",
      "authors": [
        "Clément Berenfeld",
        "Ahmed Boughdiri",
        "Bénédicte Colnet",
        "Wouter A. C. van Amsterdam",
        "Aurélien Bellet",
        "Rémi Khellaf",
        "Erwan Scornet",
        "Julie Josse"
      ],
      "abstract": "Meta-analysis, by synthesizing effect estimates from multiple studies\nconducted in diverse settings, stands at the top of the evidence hierarchy in\nclinical research. Yet, conventional approaches based on fixed- or\nrandom-effects models lack a causal framework, which may limit their\ninterpretability and utility for public policy. Incorporating causal inference\nreframes meta-analysis as the estimation of well-defined causal effects on\nclearly specified populations, enabling a principled approach to handling study\nheterogeneity. We show that classical meta-analysis estimators have a clear\ncausal interpretation when effects are measured as risk differences. However,\nthis breaks down for nonlinear measures like the risk ratio and odds ratio. To\naddress this, we introduce novel causal aggregation formulas that remain\ncompatible with standard meta-analysis practices and do not require access to\nindividual-level data. To evaluate real-world impact, we apply both classical\nand causal meta-analysis methods to 500 published meta-analyses. While the\nconclusions often align, notable discrepancies emerge, revealing cases where\nconventional methods may suggest a treatment is beneficial when, under a causal\nlens, it is in fact harmful.",
      "pdf_url": "http://arxiv.org/pdf/2505.20168v1",
      "arxiv_url": "http://arxiv.org/abs/2505.20168v1",
      "published": "2025-05-26",
      "categories": [
        "stat.ME"
      ]
    },
    {
      "title": "An Explainable Diagnostic Framework for Neurodegenerative Dementias via Reinforcement-Optimized LLM Reasoning",
      "authors": [
        "Andrew Zamai",
        "Nathanael Fijalkow",
        "Boris Mansencal",
        "Laurent Simon",
        "Eloi Navet",
        "Pierrick Coupe"
      ],
      "abstract": "The differential diagnosis of neurodegenerative dementias is a challenging\nclinical task, mainly because of the overlap in symptom presentation and the\nsimilarity of patterns observed in structural neuroimaging. To improve\ndiagnostic efficiency and accuracy, deep learning-based methods such as\nConvolutional Neural Networks and Vision Transformers have been proposed for\nthe automatic classification of brain MRIs. However, despite their strong\npredictive performance, these models find limited clinical utility due to their\nopaque decision making. In this work, we propose a framework that integrates\ntwo core components to enhance diagnostic transparency. First, we introduce a\nmodular pipeline for converting 3D T1-weighted brain MRIs into textual\nradiology reports. Second, we explore the potential of modern Large Language\nModels (LLMs) to assist clinicians in the differential diagnosis between\nFrontotemporal dementia subtypes, Alzheimer's disease, and normal aging based\non the generated reports. To bridge the gap between predictive accuracy and\nexplainability, we employ reinforcement learning to incentivize diagnostic\nreasoning in LLMs. Without requiring supervised reasoning traces or\ndistillation from larger models, our approach enables the emergence of\nstructured diagnostic rationales grounded in neuroimaging findings. Unlike\npost-hoc explainability methods that retrospectively justify model decisions,\nour framework generates diagnostic rationales as part of the inference\nprocess-producing causally grounded explanations that inform and guide the\nmodel's decision-making process. In doing so, our framework matches the\ndiagnostic performance of existing deep learning methods while offering\nrationales that support its diagnostic conclusions.",
      "pdf_url": "http://arxiv.org/pdf/2505.19954v1",
      "arxiv_url": "http://arxiv.org/abs/2505.19954v1",
      "published": "2025-05-26",
      "categories": [
        "cs.LG",
        "cs.CL"
      ]
    }
  ]
}