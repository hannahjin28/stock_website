{
  "last_updated": "2025-02-19T00:45:21.558634",
  "papers": [
    {
      "title": "Unifying Explainable Anomaly Detection and Root Cause Analysis in Dynamical Systems",
      "authors": [
        "Yue Sun",
        "Rick S. Blum",
        "Parv Venkitasubramaniam"
      ],
      "abstract": "Dynamical systems, prevalent in various scientific and engineering domains,\nare susceptible to anomalies that can significantly impact their performance\nand reliability. This paper addresses the critical challenges of anomaly\ndetection, root cause localization, and anomaly type classification in\ndynamical systems governed by ordinary differential equations (ODEs). We define\ntwo categories of anomalies: cyber anomalies, which propagate through\ninterconnected variables, and measurement anomalies, which remain localized to\nindividual variables. To address these challenges, we propose the Interpretable\nCausality Ordinary Differential Equation (ICODE) Networks, a model-intrinsic\nexplainable learning framework. ICODE leverages Neural ODEs for anomaly\ndetection while employing causality inference through an explanation channel to\nperform root cause analysis (RCA), elucidating why specific time periods are\nflagged as anomalous. ICODE is designed to simultaneously perform anomaly\ndetection, RCA, and anomaly type classification within a single, interpretable\nframework. Our approach is grounded in the hypothesis that anomalies alter the\nunderlying ODEs of the system, manifesting as changes in causal relationships\nbetween variables. We provide a theoretical analysis of how perturbations in\nlearned model parameters can be utilized to identify anomalies and their root\ncauses in time series data. Comprehensive experimental evaluations demonstrate\nthe efficacy of ICODE across various dynamical systems, showcasing its ability\nto accurately detect anomalies, classify their types, and pinpoint their\norigins.",
      "pdf_url": "http://arxiv.org/pdf/2502.12086v1",
      "arxiv_url": "http://arxiv.org/abs/2502.12086v1",
      "published": "2025-02-17",
      "categories": [
        "cs.LG",
        "stat.ML"
      ]
    },
    {
      "title": "Causal Inference for Qualitative Outcomes",
      "authors": [
        "Riccardo Di Francesco",
        "Giovanni Mellace"
      ],
      "abstract": "Causal inference methods such as instrumental variables, regression\ndiscontinuity, and difference-in-differences are widely used to estimate\ntreatment effects. However, their application to qualitative outcomes poses\nfundamental challenges, as standard causal estimands are ill-defined in this\ncontext. This paper highlights these issues and introduces an alternative\nframework that focuses on well-defined and interpretable estimands that\nquantify how treatment affects the probability distribution over outcome\ncategories. We establish that standard identification assumptions are\nsufficient for identification and propose simple, intuitive estimation\nstrategies that remain fully compatible with conventional econometric methods.\nTo facilitate implementation, we provide an open-source R package,\n$\\texttt{causalQual}$, which is publicly available on GitHub.",
      "pdf_url": "http://arxiv.org/pdf/2502.11691v1",
      "arxiv_url": "http://arxiv.org/abs/2502.11691v1",
      "published": "2025-02-17",
      "categories": [
        "econ.EM"
      ]
    },
    {
      "title": "CounterBench: A Benchmark for Counterfactuals Reasoning in Large Language Models",
      "authors": [
        "Yuefei Chen",
        "Vivek K. Singh",
        "Jing Ma",
        "Ruxiang Tang"
      ],
      "abstract": "Counterfactual reasoning is widely recognized as one of the most challenging\nand intricate aspects of causality in artificial intelligence. In this paper,\nwe evaluate the performance of large language models (LLMs) in counterfactual\nreasoning. In contrast to previous studies that primarily focus on commonsense\ncausal reasoning, where LLMs often rely on prior knowledge for inference, we\nspecifically assess their ability to perform counterfactual inference using a\nset of formal rules. To support this evaluation, we introduce a new benchmark\ndataset, CounterBench, comprising 1K counterfactual reasoning questions. The\ndataset is designed with varying levels of difficulty, diverse causal graph\nstructures, distinct types of counterfactual questions, and multiple\nnonsensical name variants. Our experiments demonstrate that counterfactual\nreasoning poses a significant challenge for LLMs, with most models performing\nat levels comparable to random guessing. To enhance LLM's counterfactual\nreasoning ability, we propose a novel reasoning paradigm, CoIn, which guides\nLLMs through iterative reasoning and backtracking to systematically explore\ncounterfactual solutions. Experimental results show that our method\nsignificantly improves LLM performance on counterfactual reasoning tasks and\nconsistently enhances performance across different LLMs.Our dataset is\navailable at https://huggingface.co/datasets/CounterBench/CounterBench.",
      "pdf_url": "http://arxiv.org/pdf/2502.11008v1",
      "arxiv_url": "http://arxiv.org/abs/2502.11008v1",
      "published": "2025-02-16",
      "categories": [
        "cs.CL"
      ]
    },
    {
      "title": "Model-assisted inference for dynamic causal effects in staggered rollout cluster randomized experiments",
      "authors": [
        "Xinyuan Chen",
        "Fan Li"
      ],
      "abstract": "Staggered rollout cluster randomized experiments (SR-CREs) are increasingly\nused for their practical feasibility and logistical convenience. These designs\ninvolve staggered treatment adoption across clusters, requiring analysis\nmethods that account for an exhaustive class of dynamic causal effects,\nanticipation, and non-ignorable cluster-period sizes. Without imposing outcome\nmodeling assumptions, we study regression estimators using individual data,\ncluster-period averages, and scaled cluster-period totals, with and without\ncovariate adjustment from a design-based perspective, where only the treatment\nadoption time is random. We establish consistency and asymptotic normality of\neach regression estimator under a finite-population framework and formally\nprove that the associated variance estimators are asymptotically conservative\nin the Lowner ordering. Furthermore, we conduct a unified efficiency comparison\nof the estimators and provide practical recommendations. We highlight the\nefficiency advantage of using estimators based on scaled cluster-period totals\nwith covariate adjustment over their counterparts using individual-level data\nand cluster-period averages. Our results rigorously justify linear regression\nestimators as model-assisted methods to address an entire class of dynamic\ncausal effects in SR-CREs and significantly expand those developed for\nparallel-arm CREs by Su and Ding (JRSSB, 2021) to accommodate a wider class of\ncomplex experimental settings with staggered randomization.",
      "pdf_url": "http://arxiv.org/pdf/2502.10939v1",
      "arxiv_url": "http://arxiv.org/abs/2502.10939v1",
      "published": "2025-02-16",
      "categories": [
        "stat.ME"
      ]
    },
    {
      "title": "Batch-Adaptive Annotations for Causal Inference with Complex-Embedded Outcomes",
      "authors": [
        "Ezinne Nwankwo",
        "Lauri Goldkind",
        "Angela Zhou"
      ],
      "abstract": "Estimating the causal effects of an intervention on outcomes is crucial. But\noften in domains such as healthcare and social services, this critical\ninformation about outcomes is documented by unstructured text, e.g. clinical\nnotes in healthcare or case notes in social services. For example, street\noutreach to homeless populations is a common social services intervention, with\nambiguous and hard-to-measure outcomes. Outreach workers compile case note\nrecords which are informative of outcomes. Although experts can succinctly\nextract relevant information from such unstructured case notes, it is costly or\ninfeasible to do so for an entire corpus, which can span millions of notes.\nRecent advances in large language models (LLMs) enable scalable but potentially\ninaccurate annotation of unstructured text data. We leverage the decision of\nwhich datapoints should receive expert annotation vs. noisy imputation under\nbudget constraints in a \"design-based\" estimator combining limited expert and\nplentiful noisy imputation data via \\textit{causal inference with missing\noutcomes}. We develop a two-stage adaptive algorithm that optimizes the expert\nannotation probabilities, estimating the ATE with optimal asymptotic variance.\nWe demonstrate how expert labels and LLM annotations can be combined\nstrategically, efficiently and responsibly in a causal estimator. We run\nexperiments on simulated data and two real-world datasets, including one on\nstreet outreach, to show the versatility of our proposed method.",
      "pdf_url": "http://arxiv.org/pdf/2502.10605v1",
      "arxiv_url": "http://arxiv.org/abs/2502.10605v1",
      "published": "2025-02-14",
      "categories": [
        "stat.ML",
        "cs.LG"
      ]
    },
    {
      "title": "A Mechanistic Framework for Collider Detection in Observational Data",
      "authors": [
        "Soumik Purkayastha",
        "Peter X. -K. Song"
      ],
      "abstract": "Understanding directionality is crucial for identifying causal structures\nfrom observational data. A key challenge lies in detecting collider structures,\nwhere a $V$--structure is formed between a child node $Z$ receiving directed\nedges from parents $X$ and $Y$, denoted by $X \\rightarrow Z \\leftarrow Y$.\nTraditional causal discovery approaches, such as constraint-based and\nscore-based structure learning algorithms, do not provide statistical inference\non estimated pathways and are often sensitive to latent confounding. To\novercome these issues, we introduce methodology to quantify directionality in\ncollider structures using a pair of conditional asymmetry coefficients to\nsimultaneously examine validity of the pathways $Y \\rightarrow Z$ and $X\n\\rightarrow Z$ in the collider structure. These coefficients are based on\nShannon's differential entropy. Leveraging kernel-based conditional density\nestimation and a nonparametric smoothing technique, we utilise our proposed\nmethod to estimate collider structures and provide uncertainty quantification.\n  Simulation studies demonstrate that our method outperforms existing structure\nlearning algorithms in accurately identifying collider structures. We further\napply our approach to investigate the role of blood pressure as a collider in\nepigenetic DNA methylation, uncovering novel insights into the genetic\nregulation of blood pressure. This framework represents a significant\nadvancement in causal structure learning, offering a robust, nonparametric\nmethod for collider detection with practical applications in biostatistics and\nepidemiology.",
      "pdf_url": "http://arxiv.org/pdf/2502.10317v1",
      "arxiv_url": "http://arxiv.org/abs/2502.10317v1",
      "published": "2025-02-14",
      "categories": [
        "stat.ME",
        "math.ST",
        "stat.TH"
      ]
    },
    {
      "title": "A Latent Causal Inference Framework for Ordinal Variables",
      "authors": [
        "Martina Scauda",
        "Jack Kuipers",
        "Giusi Moffa"
      ],
      "abstract": "Ordinal variables, such as on the Likert scale, are common in applied\nresearch. Yet, existing methods for causal inference tend to target nominal or\ncontinuous data. When applied to ordinal data, this fails to account for the\ninherent ordering or imposes well-defined relative magnitudes. Hence, there is\na need for specialised methods to compute interventional effects between\nordinal variables while accounting for their ordinality. One potential\nframework is to presume a latent Gaussian Directed Acyclic Graph (DAG) model:\nthat the ordinal variables originate from marginally discretizing a set of\nGaussian variables whose latent covariance matrix is constrained to satisfy the\nconditional independencies inherent in a DAG. Conditioned on a given latent\ncovariance matrix and discretisation thresholds, we derive a closed-form\nfunction for ordinal causal effects in terms of interventional distributions in\nthe latent space. Our causal estimation combines naturally with algorithms to\nlearn the latent DAG and its parameters, like the Ordinal Structural EM\nalgorithm. Simulations demonstrate the applicability of the proposed approach\nin estimating ordinal causal effects both for known and unknown structures of\nthe latent graph. As an illustration of a real-world use case, the method is\napplied to survey data of 408 patients from a study on the functional\nrelationships between symptoms of obsessive-compulsive disorder and depression.",
      "pdf_url": "http://arxiv.org/pdf/2502.10276v1",
      "arxiv_url": "http://arxiv.org/abs/2502.10276v1",
      "published": "2025-02-14",
      "categories": [
        "stat.ME",
        "stat.ML"
      ]
    },
    {
      "title": "Do Large Language Models Reason Causally Like Us? Even Better?",
      "authors": [
        "Hanna M. Dettki",
        "Brenden M. Lake",
        "Charley M. Wu",
        "Bob Rehder"
      ],
      "abstract": "Causal reasoning is a core component of intelligence. Large language models\n(LLMs) have shown impressive capabilities in generating human-like text,\nraising questions about whether their responses reflect true understanding or\nstatistical patterns. We compared causal reasoning in humans and four LLMs\nusing tasks based on collider graphs, rating the likelihood of a query variable\noccurring given evidence from other variables. We find that LLMs reason\ncausally along a spectrum from human-like to normative inference, with\nalignment shifting based on model, context, and task. Overall, GPT-4o and\nClaude showed the most normative behavior, including \"explaining away\", whereas\nGemini-Pro and GPT-3.5 did not. Although all agents deviated from the expected\nindependence of causes - Claude the least - they exhibited strong associative\nreasoning and predictive inference when assessing the likelihood of the effect\ngiven its causes. These findings underscore the need to assess AI biases as\nthey increasingly assist human decision-making.",
      "pdf_url": "http://arxiv.org/pdf/2502.10215v1",
      "arxiv_url": "http://arxiv.org/abs/2502.10215v1",
      "published": "2025-02-14",
      "categories": [
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Causal Information Prioritization for Efficient Reinforcement Learning",
      "authors": [
        "Hongye Cao",
        "Fan Feng",
        "Tianpei Yang",
        "Jing Huo",
        "Yang Gao"
      ],
      "abstract": "Current Reinforcement Learning (RL) methods often suffer from\nsample-inefficiency, resulting from blind exploration strategies that neglect\ncausal relationships among states, actions, and rewards. Although recent causal\napproaches aim to address this problem, they lack grounded modeling of\nreward-guided causal understanding of states and actions for goal-orientation,\nthus impairing learning efficiency. To tackle this issue, we propose a novel\nmethod named Causal Information Prioritization (CIP) that improves sample\nefficiency by leveraging factored MDPs to infer causal relationships between\ndifferent dimensions of states and actions with respect to rewards, enabling\nthe prioritization of causal information. Specifically, CIP identifies and\nleverages causal relationships between states and rewards to execute\ncounterfactual data augmentation to prioritize high-impact state features under\nthe causal understanding of the environments. Moreover, CIP integrates a\ncausality-aware empowerment learning objective, which significantly enhances\nthe agent's execution of reward-guided actions for more efficient exploration\nin complex environments. To fully assess the effectiveness of CIP, we conduct\nextensive experiments across 39 tasks in 5 diverse continuous control\nenvironments, encompassing both locomotion and manipulation skills learning\nwith pixel-based and sparse reward settings. Experimental results demonstrate\nthat CIP consistently outperforms existing RL methods across a wide range of\nscenarios.",
      "pdf_url": "http://arxiv.org/pdf/2502.10097v1",
      "arxiv_url": "http://arxiv.org/abs/2502.10097v1",
      "published": "2025-02-14",
      "categories": [
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "Object-Centric Latent Action Learning",
      "authors": [
        "Albina Klepach",
        "Alexander Nikulin",
        "Ilya Zisman",
        "Denis Tarasov",
        "Alexander Derevyagin",
        "Andrei Polubarov",
        "Nikita Lyubaykin",
        "Vladislav Kurenkov"
      ],
      "abstract": "Leveraging vast amounts of internet video data for Embodied AI is currently\nbottle-necked by the lack of action annotations and the presence of\naction-correlated distractors. We propose a novel object-centric latent action\nlearning approach, based on VideoSaur and LAPO, that employs self-supervised\ndecomposition of scenes into object representations and annotates video data\nwith proxy-action labels. This method effectively disentangles causal\nagent-object interactions from irrelevant background noise and reduces the\nperformance degradation of latent action learning approaches caused by\ndistractors. Our preliminary experiments with the Distracting Control Suite\nshow that latent action pretraining based on object decompositions improve the\nquality of inferred latent actions by x2.7 and efficiency of downstream\nfine-tuning with a small set of labeled actions, increasing return by x2.6 on\naverage.",
      "pdf_url": "http://arxiv.org/pdf/2502.09680v1",
      "arxiv_url": "http://arxiv.org/abs/2502.09680v1",
      "published": "2025-02-13",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    }
  ]
}