{
  "last_updated": "2025-09-29T00:51:48.210709",
  "papers": [
    {
      "title": "WAVECLIP: Wavelet Tokenization for Adaptive-Resolution CLIP",
      "authors": [
        "Moshe Kimhi",
        "Erez Koifman",
        "Ehud Rivlin",
        "Eli Schwartz",
        "Chaim Baskin"
      ],
      "abstract": "We introduce WAVECLIP, a single unified model for adaptive resolution\ninference in CLIP, enabled by wavelet-based tokenization. WAVECLIP replaces\nstandard patch embeddings with a multi-level wavelet decomposition, enabling\nthe model to process images coarse to fine while naturally supporting multiple\nresolutions within the same model. At inference time, the model begins with low\nresolution tokens and refines only when needed, using key-value caching and\ncausal cross-level attention to reuse computation, effectively introducing to\nthe model only new information when needed. We evaluate WAVECLIP in zero-shot\nclassification, demonstrating that a simple confidence-based gating mechanism\nenables adaptive early exits. This allows users to dynamically choose a\ncompute-accuracy trade-off using a single deployed model. Our approach requires\nonly lightweight distillation from a frozen CLIP teacher and achieves\ncompetitive accuracy with significant computational savings.",
      "pdf_url": "http://arxiv.org/pdf/2509.21153v1",
      "arxiv_url": "http://arxiv.org/abs/2509.21153v1",
      "published": "2025-09-25",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.MM"
      ]
    },
    {
      "title": "Incorporating External Controls for Estimating the Average Treatment Effect on the Treated with High-Dimensional Data: Retaining Double Robustness and Ensuring Double Safety",
      "authors": [
        "Chi-Shian Dai",
        "Chao Ying",
        "Yang Ning",
        "Jiwei Zhao"
      ],
      "abstract": "Randomized controlled trials (RCTs) are widely regarded as the gold standard\nfor causal inference in biomedical research. For instance, when estimating the\naverage treatment effect on the treated (ATT), a doubly robust estimation\nprocedure can be applied, requiring either the propensity score model or the\ncontrol outcome model to be correctly specified. In this paper, we address\nscenarios where external control data, often with a much larger sample size,\nare available. Such data are typically easier to obtain from historical records\nor third-party sources. However, we find that incorporating external controls\ninto the standard doubly robust estimator for ATT may paradoxically result in\nreduced efficiency compared to using the estimator without external controls.\nThis counterintuitive outcome suggests that the naive incorporation of external\ncontrols could be detrimental to estimation efficiency. To resolve this issue,\nwe propose a novel doubly robust estimator that guarantees higher efficiency\nthan the standard approach without external controls, even under model\nmisspecification. When all models are correctly specified, this estimator\naligns with the standard doubly robust estimator that incorporates external\ncontrols and achieves semiparametric efficiency. The asymptotic theory\ndeveloped in this work applies to high-dimensional confounder settings, which\nare increasingly common with the growing prevalence of electronic health record\ndata. We demonstrate the effectiveness of our methodology through extensive\nsimulation studies and a real-world data application.",
      "pdf_url": "http://arxiv.org/pdf/2509.20586v1",
      "arxiv_url": "http://arxiv.org/abs/2509.20586v1",
      "published": "2025-09-24",
      "categories": [
        "stat.ME",
        "stat.ML"
      ]
    },
    {
      "title": "Identification and Estimation of Joint Potential Outcome Distributions from a Single Study",
      "authors": [
        "Zach Shahn",
        "David Madigan"
      ],
      "abstract": "Most causal inference methods focus on estimating marginal average treatment\neffects, but many important causal estimands depend on the joint distribution\nof potential outcomes, including the probability of causation and proportions\nbenefiting from or harmed by treatment. Wu et al (2025) recently established\nnonparametric identification of this joint distribution for categorical\noutcomes under binary treatment by leveraging variation across multiple\nstudies. We demonstrate that their multi-study framework can be implemented\nwithin a single study by using a baseline covariate that is associated with\nuntreated potential outcomes but does not modify treatment effects conditional\non those outcomes. This reframing substantially broadens the practical\napplicability of their results, as it eliminates the need for multiple\nindependent datasets and gives analysts control over covariate selection to\nsatisfy key identifying assumptions. We provide complete identification and\nestimation theory for the single-study setting, including a Neyman-orthogonal\nestimator for cases where the conditional independence assumption only holds\nafter adjusting for covariates. We validate the estimator in a simulation and\napply it to data from a large field experiment assessing the effect of mailings\non voter turnout.",
      "pdf_url": "http://arxiv.org/pdf/2509.20506v1",
      "arxiv_url": "http://arxiv.org/abs/2509.20506v1",
      "published": "2025-09-24",
      "categories": [
        "stat.ME"
      ]
    },
    {
      "title": "Practical do-Shapley Explanations with Estimand-Agnostic Causal Inference",
      "authors": [
        "Álvaro Parafita",
        "Tomas Garriga",
        "Axel Brando",
        "Francisco J. Cazorla"
      ],
      "abstract": "Among explainability techniques, SHAP stands out as one of the most popular,\nbut often overlooks the causal structure of the problem. In response, do-SHAP\nemploys interventional queries, but its reliance on estimands hinders its\npractical application. To address this problem, we propose the use of\nestimand-agnostic approaches, which allow for the estimation of any\nidentifiable query from a single model, making do-SHAP feasible on complex\ngraphs. We also develop a novel algorithm to significantly accelerate its\ncomputation at a negligible cost, as well as a method to explain inaccessible\nData Generating Processes. We demonstrate the estimation and computational\nperformance of our approach, and validate it on two real-world datasets,\nhighlighting its potential in obtaining reliable explanations.",
      "pdf_url": "http://arxiv.org/pdf/2509.20211v1",
      "arxiv_url": "http://arxiv.org/abs/2509.20211v1",
      "published": "2025-09-24",
      "categories": [
        "cs.LG"
      ]
    },
    {
      "title": "From Text to Talk: Audio-Language Model Needs Non-Autoregressive Joint Training",
      "authors": [
        "Tianqiao Liu",
        "Xueyi Li",
        "Hao Wang",
        "Haoxuan Li",
        "Zhichao Chen",
        "Weiqi Luo",
        "Zitao Liu"
      ],
      "abstract": "Recent advances in large language models (LLMs) have attracted significant\ninterest in extending their capabilities to multimodal scenarios, particularly\nfor speech-to-speech conversational systems. However, existing multimodal\nmodels handling interleaved audio and text rely on autoregressive methods,\noverlooking that text depends on target-target relations whereas audio depends\nmainly on source-target relations. In this work, we propose Text-to-Talk (TtT),\na unified audio-text framework that integrates autoregressive (AR) text\ngeneration with non-autoregressive (NAR) audio diffusion in a single\nTransformer. By leveraging the any-order autoregressive property of absorbing\ndiscrete diffusion, our approach provides a unified training objective for text\nand audio. To support this hybrid generation paradigm, we design a\nmodality-aware attention mechanism that enforces causal decoding for text while\nallowing bidirectional modeling within audio spans, and further introduce three\ntraining strategies that reduce train-test discrepancies. During inference, TtT\nemploys block-wise diffusion to synthesize audio in parallel while flexibly\nhandling variable-length outputs. Extensive experiments across Audio-QA and ASR\ntasks demonstrate the effectiveness of our approach, with detailed ablation\nstudies validating each proposed component. We will open-source our models,\ndata and code to facilitate future research in this direction.",
      "pdf_url": "http://arxiv.org/pdf/2509.20072v2",
      "arxiv_url": "http://arxiv.org/abs/2509.20072v2",
      "published": "2025-09-24",
      "categories": [
        "cs.CL"
      ]
    },
    {
      "title": "Structuring Collective Action with LLM-Guided Evolution: From Ill-Structured Problems to Executable Heuristics",
      "authors": [
        "Kevin Bradley Dsouza",
        "Graham Alexander Watt",
        "Yuri Leonenko",
        "Juan Moreno-Cruz"
      ],
      "abstract": "Collective action problems, which require aligning individual incentives with\ncollective goals, are classic examples of Ill-Structured Problems (ISPs). For\nan individual agent, the causal links between local actions and global outcomes\nare unclear, stakeholder objectives often conflict, and no single, clear\nalgorithm can bridge micro-level choices with macro-level welfare. We present\nECHO-MIMIC, a computational framework that converts this global complexity into\na tractable, Well-Structured Problem (WSP) for each agent by discovering\ncompact, executable heuristics and persuasive rationales. The framework\noperates in two stages: ECHO (Evolutionary Crafting of Heuristics from\nOutcomes) evolves snippets of Python code that encode candidate behavioral\npolicies, while MIMIC (Mechanism Inference & Messaging for\nIndividual-to-Collective Alignment) evolves companion natural language messages\nthat motivate agents to adopt those policies. Both phases employ a\nlarge-language-model-driven evolutionary search: the LLM proposes diverse and\ncontext-aware code or text variants, while population-level selection retains\nthose that maximize collective performance in a simulated environment. We\ndemonstrate this framework on a canonical ISP in agricultural landscape\nmanagement, where local farming decisions impact global ecological\nconnectivity. Results show that ECHO-MIMIC discovers high-performing heuristics\ncompared to baselines and crafts tailored messages that successfully align\nsimulated farmer behavior with landscape-level ecological goals. By coupling\nalgorithmic rule discovery with tailored communication, ECHO-MIMIC transforms\nthe cognitive burden of collective action into a simple set of agent-level\ninstructions, making previously ill-structured problems solvable in practice\nand opening a new path toward scalable, adaptive policy design.",
      "pdf_url": "http://arxiv.org/pdf/2509.20412v1",
      "arxiv_url": "http://arxiv.org/abs/2509.20412v1",
      "published": "2025-09-24",
      "categories": [
        "cs.MA",
        "cs.LG"
      ]
    },
    {
      "title": "Causal Inference under Threshold Manipulation: Bayesian Mixture Modeling and Heterogeneous Treatment Effects",
      "authors": [
        "Kohsuke Kubota",
        "Shonosuke Sugasawa"
      ],
      "abstract": "Many marketing applications, including credit card incentive programs, offer\nrewards to customers who exceed specific spending thresholds to encourage\nincreased consumption. Quantifying the causal effect of these thresholds on\ncustomers is crucial for effective marketing strategy design. Although\nregression discontinuity design is a standard method for such causal inference\ntasks, its assumptions can be violated when customers, aware of the thresholds,\nstrategically manipulate their spending to qualify for the rewards. To address\nthis issue, we propose a novel framework for estimating the causal effect under\nthreshold manipulation. The main idea is to model the observed spending\ndistribution as a mixture of two distributions: one representing customers\nstrategically affected by the threshold, and the other representing those\nunaffected. To fit the mixture model, we adopt a two-step Bayesian approach\nconsisting of modeling non-bunching customers and fitting a mixture model to a\nsample around the threshold. We show posterior contraction of the resulting\nposterior distribution of the causal effect under large samples. Furthermore,\nwe extend this framework to a hierarchical Bayesian setting to estimate\nheterogeneous causal effects across customer subgroups, allowing for stable\ninference even with small subgroup sample sizes. We demonstrate the\neffectiveness of our proposed methods through simulation studies and illustrate\ntheir practical implications using a real-world marketing dataset.",
      "pdf_url": "http://arxiv.org/pdf/2509.19814v1",
      "arxiv_url": "http://arxiv.org/abs/2509.19814v1",
      "published": "2025-09-24",
      "categories": [
        "stat.ME",
        "cs.AI"
      ]
    },
    {
      "title": "iFinder: Structured Zero-Shot Vision-Based LLM Grounding for Dash-Cam Video Reasoning",
      "authors": [
        "Manyi Yao",
        "Bingbing Zhuang",
        "Sparsh Garg",
        "Amit Roy-Chowdhury",
        "Christian Shelton",
        "Manmohan Chandraker",
        "Abhishek Aich"
      ],
      "abstract": "Grounding large language models (LLMs) in domain-specific tasks like post-hoc\ndash-cam driving video analysis is challenging due to their general-purpose\ntraining and lack of structured inductive biases. As vision is often the sole\nmodality available for such analysis (i.e., no LiDAR, GPS, etc.), existing\nvideo-based vision-language models (V-VLMs) struggle with spatial reasoning,\ncausal inference, and explainability of events in the input video. To this end,\nwe introduce iFinder, a structured semantic grounding framework that decouples\nperception from reasoning by translating dash-cam videos into a hierarchical,\ninterpretable data structure for LLMs. iFinder operates as a modular,\ntraining-free pipeline that employs pretrained vision models to extract\ncritical cues -- object pose, lane positions, and object trajectories -- which\nare hierarchically organized into frame- and video-level structures. Combined\nwith a three-block prompting strategy, it enables step-wise, grounded reasoning\nfor the LLM to refine a peer V-VLM's outputs and provide accurate reasoning.\nEvaluations on four public dash-cam video benchmarks show that iFinder's\nproposed grounding with domain-specific cues, especially object orientation and\nglobal context, significantly outperforms end-to-end V-VLMs on four zero-shot\ndriving benchmarks, with up to 39% gains in accident reasoning accuracy. By\ngrounding LLMs with driving domain-specific representations, iFinder offers a\nzero-shot, interpretable, and reliable alternative to end-to-end V-VLMs for\npost-hoc driving video understanding.",
      "pdf_url": "http://arxiv.org/pdf/2509.19552v1",
      "arxiv_url": "http://arxiv.org/abs/2509.19552v1",
      "published": "2025-09-23",
      "categories": [
        "cs.CV"
      ]
    },
    {
      "title": "Chiseling: Powerful and Valid Subgroup Selection via Interactive Machine Learning",
      "authors": [
        "Nathan Cheng",
        "Asher Spector",
        "Lucas Janson"
      ],
      "abstract": "In regression and causal inference, controlled subgroup selection aims to\nidentify, with inferential guarantees, a subgroup (defined as a subset of the\ncovariate space) on which the average response or treatment effect is above a\ngiven threshold. E.g., in a clinical trial, it may be of interest to find a\nsubgroup with a positive average treatment effect. However, existing methods\neither lack inferential guarantees, heavily restrict the search for the\nsubgroup, or sacrifice efficiency by naive data splitting. We propose a novel\nframework called chiseling that allows the analyst to interactively refine and\ntest a candidate subgroup by iteratively shrinking it. The sole restriction is\nthat the shrinkage direction only depends on the points outside the current\nsubgroup, but otherwise the analyst may leverage any prior information or\nmachine learning algorithm. Despite this flexibility, chiseling controls the\nprobability that the discovered subgroup is null (e.g., has a non-positive\naverage treatment effect) under minimal assumptions: for example, in randomized\nexperiments, this inferential validity guarantee holds under only bounded\nmoment conditions. When applied to a variety of simulated datasets and a real\nsurvey experiment, chiseling identifies substantially better subgroups than\nexisting methods with inferential guarantees.",
      "pdf_url": "http://arxiv.org/pdf/2509.19490v2",
      "arxiv_url": "http://arxiv.org/abs/2509.19490v2",
      "published": "2025-09-23",
      "categories": [
        "stat.ME",
        "stat.ML"
      ]
    },
    {
      "title": "Track-On2: Enhancing Online Point Tracking with Memory",
      "authors": [
        "Görkay Aydemir",
        "Weidi Xie",
        "Fatma Güney"
      ],
      "abstract": "In this paper, we consider the problem of long-term point tracking, which\nrequires consistent identification of points across video frames under\nsignificant appearance changes, motion, and occlusion. We target the online\nsetting, i.e. tracking points frame-by-frame, making it suitable for real-time\nand streaming applications. We extend our prior model Track-On into Track-On2,\na simple and efficient transformer-based model for online long-term tracking.\nTrack-On2 improves both performance and efficiency through architectural\nrefinements, more effective use of memory, and improved synthetic training\nstrategies. Unlike prior approaches that rely on full-sequence access or\niterative updates, our model processes frames causally and maintains temporal\ncoherence via a memory mechanism, which is key to handling drift and occlusions\nwithout requiring future frames. At inference, we perform coarse patch-level\nclassification followed by refinement. Beyond architecture, we systematically\nstudy synthetic training setups and their impact on memory behavior, showing\nhow they shape temporal robustness over long sequences. Through comprehensive\nexperiments, Track-On2 achieves state-of-the-art results across five synthetic\nand real-world benchmarks, surpassing prior online trackers and even strong\noffline methods that exploit bidirectional context. These results highlight the\neffectiveness of causal, memory-based architectures trained purely on synthetic\ndata as scalable solutions for real-world point tracking. Project page:\nhttps://kuis-ai.github.io/track_on2",
      "pdf_url": "http://arxiv.org/pdf/2509.19115v1",
      "arxiv_url": "http://arxiv.org/abs/2509.19115v1",
      "published": "2025-09-23",
      "categories": [
        "cs.CV"
      ]
    }
  ]
}