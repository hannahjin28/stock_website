{
  "last_updated": "2025-06-18T00:55:10.336528",
  "papers": [
    {
      "title": "Mixture of Weight-shared Heterogeneous Group Attention Experts for Dynamic Token-wise KV Optimization",
      "authors": [
        "Guanghui Song",
        "Dongping Liao",
        "Yiren Zhao",
        "Kejiang Ye",
        "Cheng-zhong Xu",
        "Xitong Gao"
      ],
      "abstract": "Transformer models face scalability challenges in causal language modeling\n(CLM) due to inefficient memory allocation for growing key-value (KV) caches,\nwhich strains compute and storage resources. Existing methods like Grouped\nQuery Attention (GQA) and token-level KV optimization improve efficiency but\nrely on rigid resource allocation, often discarding \"low-priority\" tokens or\nstatically grouping them, failing to address the dynamic spectrum of token\nimportance. We propose mixSGA, a novel mixture-of-expert (MoE) approach that\ndynamically optimizes token-wise computation and memory allocation. Unlike\nprior approaches, mixSGA retains all tokens while adaptively routing them to\nspecialized experts with varying KV group sizes, balancing granularity and\nefficiency. Our key novelties include: (1) a token-wise expert-choice routing\nmechanism guided by learned importance scores, enabling proportional resource\nallocation without token discard; (2) weight-sharing across grouped attention\nprojections to minimize parameter overhead; and (3) an auxiliary loss to ensure\none-hot routing decisions for training-inference consistency in CLMs. Extensive\nevaluations across Llama3, TinyLlama, OPT, and Gemma2 model families show\nmixSGA's superiority over static baselines. On instruction-following and\ncontinued pretraining tasks, mixSGA achieves higher ROUGE-L and lower\nperplexity under the same KV budgets.",
      "pdf_url": "http://arxiv.org/pdf/2506.13541v1",
      "arxiv_url": "http://arxiv.org/abs/2506.13541v1",
      "published": "2025-06-16",
      "categories": [
        "cs.CL",
        "cs.LG"
      ]
    },
    {
      "title": "Chaos, coherence and turbulence",
      "authors": [
        "Javier Jimenez"
      ],
      "abstract": "This paper is a personal overview of the efforts over the last half century\nto understand fluid turbulence in terms of simpler coherent units. The\nconsequences of chaos and the concept of coherence are first reviewed, using\nexamples from free-shear and wall-bounded shear flows, and including how the\nsimplifications due to coherent structures have been useful in the\nconceptualization and control of turbulence. It is remarked that, even if this\napproach has revolutionized our understanding of the flow, most of turbulence\ncannot yet be described by structures. This includes cascades, both direct and\ninverse, and possibly junk turbulence, whose role, if any, is currently\nunknown. This part of the paper is mostly a catalog of questions, some of them\nanswered and others still open. A second part of the paper examines which new\ntechniques can be expected to help in attacking the open questions, and which,\nin the opinion of the author, are the strengths and limitations of current\napproaches, such as data-driven science and causal inference.",
      "pdf_url": "http://arxiv.org/pdf/2506.13417v1",
      "arxiv_url": "http://arxiv.org/abs/2506.13417v1",
      "published": "2025-06-16",
      "categories": [
        "physics.flu-dyn"
      ]
    },
    {
      "title": "Fortified Proximal Causal Inference with Many Invalid Proxies",
      "authors": [
        "Myeonghun Yu",
        "Xu Shi",
        "Eric J. Tchetgen Tchetgen"
      ],
      "abstract": "Causal inference from observational data often relies on the assumption of no\nunmeasured confounding, an assumption frequently violated in practice due to\nunobserved or poorly measured covariates. Proximal causal inference (PCI)\noffers a promising framework for addressing unmeasured confounding using a pair\nof outcome and treatment confounding proxies. However, existing PCI methods\ntypically assume all specified proxies are valid, which may be unrealistic and\nis untestable without extra assumptions. In this paper, we develop a\nsemiparametric approach for a many-proxy PCI setting that accommodates\npotentially invalid treatment confounding proxies. We introduce a new class of\nfortified confounding bridge functions and establish nonparametric\nidentification of the population average treatment effect (ATE) under the\nassumption that at least $\\gamma$ out of $K$ candidate treatment confounding\nproxies are valid, for any $\\gamma \\leq K$ set by the analyst without requiring\nknowledge of which proxies are valid. We establish a local semiparametric\nefficiency bound and develop a class of multiply robust, locally efficient\nestimators for the ATE. These estimators are thus simultaneously robust to\ninvalid treatment confounding proxies and model misspecification of nuisance\nparameters. The proposed methods are evaluated through simulation and applied\nto assess the effect of right heart catheterization in critically ill patients.",
      "pdf_url": "http://arxiv.org/pdf/2506.13152v1",
      "arxiv_url": "http://arxiv.org/abs/2506.13152v1",
      "published": "2025-06-16",
      "categories": [
        "stat.ME"
      ]
    },
    {
      "title": "Honesty in Causal Forests: When It Helps and When It Hurts",
      "authors": [
        "Yanfang Hou",
        "Carlos Fernández-Loría"
      ],
      "abstract": "Causal forests are increasingly used to personalize decisions based on\nestimated treatment effects. A distinctive modeling choice in this method is\nhonest estimation: using separate data for splitting and for estimating effects\nwithin leaves. This practice is the default in most implementations and is\nwidely seen as desirable for causal inference. But we show that honesty can\nhurt the accuracy of individual-level effect estimates. The reason is a classic\nbias-variance trade-off: honesty reduces variance by preventing overfitting,\nbut increases bias by limiting the model's ability to discover and exploit\nmeaningful heterogeneity in treatment effects. This trade-off depends on the\nsignal-to-noise ratio (SNR): honesty helps when effect heterogeneity is hard to\ndetect (low SNR), but hurts when the signal is strong (high SNR). In essence,\nhonesty acts as a form of regularization, and like any regularization choice,\nit should be guided by out-of-sample performance, not adopted by default.",
      "pdf_url": "http://arxiv.org/pdf/2506.13107v1",
      "arxiv_url": "http://arxiv.org/abs/2506.13107v1",
      "published": "2025-06-16",
      "categories": [
        "cs.LG",
        "stat.ML"
      ]
    },
    {
      "title": "Discussion of \"Causal and counterfactual views of missing data models\" by Razieh Nabi, Rohit Bhattacharya, Ilya Shpitser, & James M. Robins",
      "authors": [
        "Alex W. Levis",
        "Edward H. Kennedy"
      ],
      "abstract": "We congratulate Nabi et al. (2022) on their impressive and insightful paper,\nwhich illustrates the benefits of using causal/counterfactual perspectives and\ntools in missing data problems. This paper represents an important approach to\nmissing-not-at-random (MNAR) problems, exploiting nonparametric independence\nrestrictions for identification, as opposed to parametric/semiparametric\nmodels, or resorting to sensitivity analysis. Crucially, the authors represent\nthese restrictions with missing data directed acyclic graphs (m-DAGs), which\ncan be useful to determine identification in complex and interesting MNAR\nmodels. In this discussion we consider: (i) how/whether other tools from causal\ninference could be useful in missing data problems, (ii) problems that combine\nboth missing data and causal inference together, and (iii) some work on\nestimation in one of the authors' example MNAR models.",
      "pdf_url": "http://arxiv.org/pdf/2506.13025v1",
      "arxiv_url": "http://arxiv.org/abs/2506.13025v1",
      "published": "2025-06-16",
      "categories": [
        "stat.ME"
      ]
    },
    {
      "title": "Finite sample-optimal adjustment sets in linear Gaussian causal models",
      "authors": [
        "Nadja Rutsch",
        "Sara Magliacane",
        "Stéphanie van der Pas"
      ],
      "abstract": "Traditional covariate selection methods for causal inference focus on\nachieving unbiasedness and asymptotic efficiency. In many practical scenarios,\nresearchers must estimate causal effects from observational data with limited\nsample sizes or in cases where covariates are difficult or costly to measure.\nTheir needs might be better met by selecting adjustment sets that are finite\nsample-optimal in terms of mean squared error. In this paper, we aim to find\nthe adjustment set that minimizes the mean squared error of the causal effect\nestimator, taking into account the joint distribution of the variables and the\nsample size. We call this finite sample-optimal set the MSE-optimal adjustment\nset and present examples in which the MSE-optimal adjustment set differs from\nthe asymptotically optimal adjustment set. To identify the MSE-optimal\nadjustment set, we then introduce a sample size criterion for comparing\nadjustment sets in linear Gaussian models. We also develop graphical criteria\nto reduce the search space for this adjustment set based on the causal graph.\nIn experiments with simulated data, we show that the MSE-optimal adjustment set\ncan outperform the asymptotically optimal adjustment set in finite sample size\nsettings, making causal inference more practical in such scenarios.",
      "pdf_url": "http://arxiv.org/pdf/2506.12869v1",
      "arxiv_url": "http://arxiv.org/abs/2506.12869v1",
      "published": "2025-06-15",
      "categories": [
        "math.ST",
        "stat.ME",
        "stat.TH"
      ]
    },
    {
      "title": "Leveraging MIMIC Datasets for Better Digital Health: A Review on Open Problems, Progress Highlights, and Future Promises",
      "authors": [
        "Afifa Khaled",
        "Mohammed Sabir",
        "Rizwan Qureshi",
        "Camillo Maria Caruso",
        "Valerio Guarrasi",
        "Suncheng Xiang",
        "S Kevin Zhou"
      ],
      "abstract": "The Medical Information Mart for Intensive Care (MIMIC) datasets have become\nthe Kernel of Digital Health Research by providing freely accessible,\ndeidentified records from tens of thousands of critical care admissions,\nenabling a broad spectrum of applications in clinical decision support, outcome\nprediction, and healthcare analytics. Although numerous studies and surveys\nhave explored the predictive power and clinical utility of MIMIC based models,\ncritical challenges in data integration, representation, and interoperability\nremain underexplored. This paper presents a comprehensive survey that focuses\nuniquely on open problems. We identify persistent issues such as data\ngranularity, cardinality limitations, heterogeneous coding schemes, and ethical\nconstraints that hinder the generalizability and real-time implementation of\nmachine learning models. We highlight key progress in dimensionality reduction,\ntemporal modelling, causal inference, and privacy preserving analytics, while\nalso outlining promising directions including hybrid modelling, federated\nlearning, and standardized preprocessing pipelines. By critically examining\nthese structural limitations and their implications, this survey offers\nactionable insights to guide the next generation of MIMIC powered digital\nhealth innovations.",
      "pdf_url": "http://arxiv.org/pdf/2506.12808v1",
      "arxiv_url": "http://arxiv.org/abs/2506.12808v1",
      "published": "2025-06-15",
      "categories": [
        "cs.CV"
      ]
    },
    {
      "title": "A Residual Prediction Test for the Well-Specification of Linear Instrumental Variable Models",
      "authors": [
        "Cyrill Scheidegger",
        "Malte Londschien",
        "Peter Bühlmann"
      ],
      "abstract": "The linear instrumental variable (IV) model is widely applied in\nobservational studies. The corresponding assumptions are critical for valid\ncausal inference, and hence, it is important to have tools to assess the\nmodel's well-specification. The classical Sargan-Hansen J-test is limited to\nthe overidentified setting, where the number of instruments is larger than the\nnumber of endogenous variables. Here, we propose a novel and simple test for\nthe well-specification of the linear IV model under the assumption that the\nstructural error is mean independent of the instruments. Importantly, assuming\nmean independence allows the construction of such a test even in the\njust-identified setting. We use the idea of residual prediction tests: if the\nresiduals from two-stage least squares can be predicted from the instruments\nbetter than randomly, this signals misspecification. We construct a test\nstatistic based on sample splitting and a user-chosen machine learning method.\nWe show asymptotic type I error control. Furthermore, by relying on machine\nlearning tools, our test has good power for detecting alternatives from a broad\nclass of scenarios. We also address heteroskedasticity- and cluster-robust\ninference. The test is implemented in the R package RPIV and in the ivmodels\nsoftware package for Python.",
      "pdf_url": "http://arxiv.org/pdf/2506.12771v1",
      "arxiv_url": "http://arxiv.org/abs/2506.12771v1",
      "published": "2025-06-15",
      "categories": [
        "stat.ME"
      ]
    },
    {
      "title": "Interpretable Causal Representation Learning for Biological Data in the Pathway Space",
      "authors": [
        "Jesus de la Fuente",
        "Robert Lehmann",
        "Carlos Ruiz-Arenas",
        "Jan Voges",
        "Irene Marin-Goñi",
        "Xabier Martinez-de-Morentin",
        "David Gomez-Cabrero",
        "Idoia Ochoa",
        "Jesper Tegner",
        "Vincenzo Lagani",
        "Mikel Hernaez"
      ],
      "abstract": "Predicting the impact of genomic and drug perturbations in cellular function\nis crucial for understanding gene functions and drug effects, ultimately\nleading to improved therapies. To this end, Causal Representation Learning\n(CRL) constitutes one of the most promising approaches, as it aims to identify\nthe latent factors that causally govern biological systems, thus facilitating\nthe prediction of the effect of unseen perturbations. Yet, current CRL methods\nfail in reconciling their principled latent representations with known\nbiological processes, leading to models that are not interpretable. To address\nthis major issue, we present SENA-discrepancy-VAE, a model based on the\nrecently proposed CRL method discrepancy-VAE, that produces representations\nwhere each latent factor can be interpreted as the (linear) combination of the\nactivity of a (learned) set of biological processes. To this extent, we present\nan encoder, SENA-{\\delta}, that efficiently compute and map biological\nprocesses' activity levels to the latent causal factors. We show that\nSENA-discrepancy-VAE achieves predictive performances on unseen combinations of\ninterventions that are comparable with its original, non-interpretable\ncounterpart, while inferring causal latent factors that are biologically\nmeaningful.",
      "pdf_url": "http://arxiv.org/pdf/2506.12439v1",
      "arxiv_url": "http://arxiv.org/abs/2506.12439v1",
      "published": "2025-06-14",
      "categories": [
        "cs.LG",
        "q-bio.QM",
        "stat.ML"
      ]
    },
    {
      "title": "Path-specific effects for pulse-oximetry guided decisions in critical care",
      "authors": [
        "Kevin Zhang",
        "Yonghan Jung",
        "Divyat Mahajan",
        "Karthikeyan Shanmugam",
        "Shalmali Joshi"
      ],
      "abstract": "Identifying and measuring biases associated with sensitive attributes is a\ncrucial consideration in healthcare to prevent treatment disparities. One\nprominent issue is inaccurate pulse oximeter readings, which tend to\noverestimate oxygen saturation for dark-skinned patients and misrepresent\nsupplemental oxygen needs. Most existing research has revealed statistical\ndisparities linking device errors to patient outcomes in intensive care units\n(ICUs) without causal formalization. In contrast, this study causally\ninvestigates how racial discrepancies in oximetry measurements affect invasive\nventilation in ICU settings. We employ a causal inference-based approach using\npath-specific effects to isolate the impact of bias by race on clinical\ndecision-making. To estimate these effects, we leverage a doubly robust\nestimator, propose its self-normalized variant for improved sample efficiency,\nand provide novel finite-sample guarantees. Our methodology is validated on\nsemi-synthetic data and applied to two large real-world health datasets:\nMIMIC-IV and eICU. Contrary to prior work, our analysis reveals minimal impact\nof racial discrepancies on invasive ventilation rates. However, path-specific\neffects mediated by oxygen saturation disparity are more pronounced on\nventilation duration, and the severity differs by dataset. Our work provides a\nnovel and practical pipeline for investigating potential disparities in the ICU\nand, more crucially, highlights the necessity of causal methods to robustly\nassess fairness in decision-making.",
      "pdf_url": "http://arxiv.org/pdf/2506.12371v1",
      "arxiv_url": "http://arxiv.org/abs/2506.12371v1",
      "published": "2025-06-14",
      "categories": [
        "cs.LG",
        "stat.ML"
      ]
    }
  ]
}