{
  "last_updated": "2025-09-26T00:49:17.441831",
  "papers": [
    {
      "title": "Practical do-Shapley Explanations with Estimand-Agnostic Causal Inference",
      "authors": [
        "Álvaro Parafita",
        "Tomas Garriga",
        "Axel Brando",
        "Francisco J. Cazorla"
      ],
      "abstract": "Among explainability techniques, SHAP stands out as one of the most popular,\nbut often overlooks the causal structure of the problem. In response, do-SHAP\nemploys interventional queries, but its reliance on estimands hinders its\npractical application. To address this problem, we propose the use of\nestimand-agnostic approaches, which allow for the estimation of any\nidentifiable query from a single model, making do-SHAP feasible on complex\ngraphs. We also develop a novel algorithm to significantly accelerate its\ncomputation at a negligible cost, as well as a method to explain inaccessible\nData Generating Processes. We demonstrate the estimation and computational\nperformance of our approach, and validate it on two real-world datasets,\nhighlighting its potential in obtaining reliable explanations.",
      "pdf_url": "http://arxiv.org/pdf/2509.20211v1",
      "arxiv_url": "http://arxiv.org/abs/2509.20211v1",
      "published": "2025-09-24",
      "categories": [
        "cs.LG"
      ]
    },
    {
      "title": "Causal Inference under Threshold Manipulation: Bayesian Mixture Modeling and Heterogeneous Treatment Effects",
      "authors": [
        "Kohsuke Kubota",
        "Shonosuke Sugasawa"
      ],
      "abstract": "Many marketing applications, including credit card incentive programs, offer\nrewards to customers who exceed specific spending thresholds to encourage\nincreased consumption. Quantifying the causal effect of these thresholds on\ncustomers is crucial for effective marketing strategy design. Although\nregression discontinuity design is a standard method for such causal inference\ntasks, its assumptions can be violated when customers, aware of the thresholds,\nstrategically manipulate their spending to qualify for the rewards. To address\nthis issue, we propose a novel framework for estimating the causal effect under\nthreshold manipulation. The main idea is to model the observed spending\ndistribution as a mixture of two distributions: one representing customers\nstrategically affected by the threshold, and the other representing those\nunaffected. To fit the mixture model, we adopt a two-step Bayesian approach\nconsisting of modeling non-bunching customers and fitting a mixture model to a\nsample around the threshold. We show posterior contraction of the resulting\nposterior distribution of the causal effect under large samples. Furthermore,\nwe extend this framework to a hierarchical Bayesian setting to estimate\nheterogeneous causal effects across customer subgroups, allowing for stable\ninference even with small subgroup sample sizes. We demonstrate the\neffectiveness of our proposed methods through simulation studies and illustrate\ntheir practical implications using a real-world marketing dataset.",
      "pdf_url": "http://arxiv.org/pdf/2509.19814v1",
      "arxiv_url": "http://arxiv.org/abs/2509.19814v1",
      "published": "2025-09-24",
      "categories": [
        "stat.ME",
        "cs.AI"
      ]
    },
    {
      "title": "iFinder: Structured Zero-Shot Vision-Based LLM Grounding for Dash-Cam Video Reasoning",
      "authors": [
        "Manyi Yao",
        "Bingbing Zhuang",
        "Sparsh Garg",
        "Amit Roy-Chowdhury",
        "Christian Shelton",
        "Manmohan Chandraker",
        "Abhishek Aich"
      ],
      "abstract": "Grounding large language models (LLMs) in domain-specific tasks like post-hoc\ndash-cam driving video analysis is challenging due to their general-purpose\ntraining and lack of structured inductive biases. As vision is often the sole\nmodality available for such analysis (i.e., no LiDAR, GPS, etc.), existing\nvideo-based vision-language models (V-VLMs) struggle with spatial reasoning,\ncausal inference, and explainability of events in the input video. To this end,\nwe introduce iFinder, a structured semantic grounding framework that decouples\nperception from reasoning by translating dash-cam videos into a hierarchical,\ninterpretable data structure for LLMs. iFinder operates as a modular,\ntraining-free pipeline that employs pretrained vision models to extract\ncritical cues -- object pose, lane positions, and object trajectories -- which\nare hierarchically organized into frame- and video-level structures. Combined\nwith a three-block prompting strategy, it enables step-wise, grounded reasoning\nfor the LLM to refine a peer V-VLM's outputs and provide accurate reasoning.\nEvaluations on four public dash-cam video benchmarks show that iFinder's\nproposed grounding with domain-specific cues, especially object orientation and\nglobal context, significantly outperforms end-to-end V-VLMs on four zero-shot\ndriving benchmarks, with up to 39% gains in accident reasoning accuracy. By\ngrounding LLMs with driving domain-specific representations, iFinder offers a\nzero-shot, interpretable, and reliable alternative to end-to-end V-VLMs for\npost-hoc driving video understanding.",
      "pdf_url": "http://arxiv.org/pdf/2509.19552v1",
      "arxiv_url": "http://arxiv.org/abs/2509.19552v1",
      "published": "2025-09-23",
      "categories": [
        "cs.CV"
      ]
    },
    {
      "title": "Chiseling: Powerful and Valid Subgroup Selection via Interactive Machine Learning",
      "authors": [
        "Nathan Cheng",
        "Asher Spector",
        "Lucas Janson"
      ],
      "abstract": "In regression and causal inference, controlled subgroup selection aims to\nidentify, with inferential guarantees, a subgroup (defined as a subset of the\ncovariate space) on which the average response or treatment effect is above a\ngiven threshold. E.g., in a clinical trial, it may be of interest to find a\nsubgroup with a positive average treatment effect. However, existing methods\neither lack inferential guarantees, heavily restrict the search for the\nsubgroup, or sacrifice efficiency by naive data splitting. We propose a novel\nframework called chiseling that allows the analyst to interactively refine and\ntest a candidate subgroup by iteratively shrinking it. The sole restriction is\nthat the shrinkage direction only depends on the points outside the current\nsubgroup, but otherwise the analyst may leverage any prior information or\nmachine learning algorithm. Despite this flexibility, chiseling controls the\nprobability that the discovered subgroup is null (e.g., has a non-positive\naverage treatment effect) under minimal assumptions: for example, in randomized\nexperiments, this inferential validity guarantee holds under only bounded\nmoment conditions. When applied to a variety of simulated datasets and a real\nsurvey experiment, chiseling identifies substantially better subgroups than\nexisting methods with inferential guarantees.",
      "pdf_url": "http://arxiv.org/pdf/2509.19490v1",
      "arxiv_url": "http://arxiv.org/abs/2509.19490v1",
      "published": "2025-09-23",
      "categories": [
        "stat.ME",
        "stat.ML"
      ]
    },
    {
      "title": "Track-On2: Enhancing Online Point Tracking with Memory",
      "authors": [
        "Görkay Aydemir",
        "Weidi Xie",
        "Fatma Güney"
      ],
      "abstract": "In this paper, we consider the problem of long-term point tracking, which\nrequires consistent identification of points across video frames under\nsignificant appearance changes, motion, and occlusion. We target the online\nsetting, i.e. tracking points frame-by-frame, making it suitable for real-time\nand streaming applications. We extend our prior model Track-On into Track-On2,\na simple and efficient transformer-based model for online long-term tracking.\nTrack-On2 improves both performance and efficiency through architectural\nrefinements, more effective use of memory, and improved synthetic training\nstrategies. Unlike prior approaches that rely on full-sequence access or\niterative updates, our model processes frames causally and maintains temporal\ncoherence via a memory mechanism, which is key to handling drift and occlusions\nwithout requiring future frames. At inference, we perform coarse patch-level\nclassification followed by refinement. Beyond architecture, we systematically\nstudy synthetic training setups and their impact on memory behavior, showing\nhow they shape temporal robustness over long sequences. Through comprehensive\nexperiments, Track-On2 achieves state-of-the-art results across five synthetic\nand real-world benchmarks, surpassing prior online trackers and even strong\noffline methods that exploit bidirectional context. These results highlight the\neffectiveness of causal, memory-based architectures trained purely on synthetic\ndata as scalable solutions for real-world point tracking. Project page:\nhttps://kuis-ai.github.io/track_on2",
      "pdf_url": "http://arxiv.org/pdf/2509.19115v1",
      "arxiv_url": "http://arxiv.org/abs/2509.19115v1",
      "published": "2025-09-23",
      "categories": [
        "cs.CV"
      ]
    },
    {
      "title": "Towards Practical Multi-label Causal Discovery in High-Dimensional Event Sequences via One-Shot Graph Aggregation",
      "authors": [
        "Hugo Math",
        "Rainer Lienhart"
      ],
      "abstract": "Understanding causality in event sequences where outcome labels such as\ndiseases or system failures arise from preceding events like symptoms or error\ncodes is critical. Yet remains an unsolved challenge across domains like\nhealthcare or vehicle diagnostics. We introduce CARGO, a scalable multi-label\ncausal discovery method for sparse, high-dimensional event sequences comprising\nof thousands of unique event types. Using two pretrained causal Transformers as\ndomain-specific foundation models for event sequences. CARGO infers in\nparallel, per sequence one-shot causal graphs and aggregates them using an\nadaptive frequency fusion to reconstruct the global Markov boundaries of\nlabels. This two-stage approach enables efficient probabilistic reasoning at\nscale while bypassing the intractable cost of full-dataset conditional\nindependence testing. Our results on a challenging real-world automotive fault\nprediction dataset with over 29,100 unique event types and 474 imbalanced\nlabels demonstrate CARGO's ability to perform structured reasoning.",
      "pdf_url": "http://arxiv.org/pdf/2509.19112v1",
      "arxiv_url": "http://arxiv.org/abs/2509.19112v1",
      "published": "2025-09-23",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Memory in Large Language Models: Mechanisms, Evaluation and Evolution",
      "authors": [
        "Dianxing Zhang",
        "Wendong Li",
        "Kani Song",
        "Jiaye Lu",
        "Gang Li",
        "Liuchun Yang",
        "Sheng Li"
      ],
      "abstract": "Under a unified operational definition, we define LLM memory as a persistent\nstate written during pretraining, finetuning, or inference that can later be\naddressed and that stably influences outputs. We propose a four-part taxonomy\n(parametric, contextual, external, procedural/episodic) and a memory quadruple\n(location, persistence, write/access path, controllability). We link mechanism,\nevaluation, and governance via the chain write -> read -> inhibit/update. To\navoid distorted comparisons across heterogeneous setups, we adopt a\nthree-setting protocol (parametric only, offline retrieval, online retrieval)\nthat decouples capability from information availability on the same data and\ntimeline. On this basis we build a layered evaluation: parametric (closed-book\nrecall, edit differential, memorization/privacy), contextual (position curves\nand the mid-sequence drop), external (answer correctness vs snippet\nattribution/faithfulness), and procedural/episodic (cross-session consistency\nand timeline replay, E MARS+). The framework integrates temporal governance and\nleakage auditing (freshness hits, outdated answers, refusal slices) and\nuncertainty reporting via inter-rater agreement plus paired tests with\nmultiple-comparison correction. For updating and forgetting, we present DMM\nGov: coordinating DAPT/TAPT, PEFT, model editing (ROME, MEND, MEMIT, SERAC),\nand RAG to form an auditable loop covering admission thresholds, rollout,\nmonitoring, rollback, and change audits, with specs for timeliness, conflict\nhandling, and long-horizon consistency. Finally, we give four testable\npropositions: minimum identifiability; a minimal evaluation card; causally\nconstrained editing with verifiable forgetting; and when retrieval with\nsmall-window replay outperforms ultra-long-context reading. This yields a\nreproducible, comparable, and governable coordinate system for research and\ndeployment.",
      "pdf_url": "http://arxiv.org/pdf/2509.18868v1",
      "arxiv_url": "http://arxiv.org/abs/2509.18868v1",
      "published": "2025-09-23",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Optimization-centric cutting feedback for semiparametric models",
      "authors": [
        "Linda S. L. Tan",
        "David J. Nott",
        "David T. Frazier"
      ],
      "abstract": "Modern statistics deals with complex models from which the joint model used\nfor inference is built by coupling submodels, called modules. We consider\nmodular inference where the modules may depend on parametric and nonparametric\ncomponents. In such cases, a joint Bayesian inference is highly susceptible to\nmisspecification across any module, and inappropriate priors for nonparametric\ncomponents may deliver subpar inferences for parametric components, and vice\nversa. We propose a novel ``optimization-centric'' approach to cutting feedback\nfor semiparametric modular inference, which can address misspecification and\nprior-data conflicts. The proposed generalized cut posteriors are defined\nthrough a variational optimization problem for generalized posteriors where\nregularization is based on R\\'{e}nyi divergence, rather than Kullback-Leibler\ndivergence (KLD), and variational computational methods are developed. We show\nempirically that using R\\'{e}nyi divergence to define the cut posterior\ndelivers more robust inferences than KLD. We derive novel posterior\nconcentration results that accommodate the R\\'{e}nyi divergence and allow for\nsemiparametric components, greatly extending existing results for cut\nposteriors that were derived for parametric models and KLD. We demonstrate\nthese new methods in a benchmark toy example and two real examples: Gaussian\nprocess adjustments for confounding in causal inference and misspecified copula\nmodels with nonparametric marginals.",
      "pdf_url": "http://arxiv.org/pdf/2509.18708v1",
      "arxiv_url": "http://arxiv.org/abs/2509.18708v1",
      "published": "2025-09-23",
      "categories": [
        "stat.ME",
        "math.ST",
        "stat.CO",
        "stat.TH"
      ]
    },
    {
      "title": "Estimating Heterogeneous Causal Effect on Networks via Orthogonal Learning",
      "authors": [
        "Yuanchen Wu",
        "Yubai Yuan"
      ],
      "abstract": "Estimating causal effects on networks is important for both scientific\nresearch and practical applications. Unlike traditional settings that assume\nthe Stable Unit Treatment Value Assumption (SUTVA), interference allows an\nintervention/treatment on one unit to affect the outcomes of others.\nUnderstanding both direct and spillover effects is critical in fields such as\nepidemiology, political science, and economics. Causal inference on networks\nfaces two main challenges. First, causal effects are typically heterogeneous,\nvarying with unit features and local network structure. Second, connected units\noften exhibit dependence due to network homophily, creating confounding between\nstructural correlations and causal effects. In this paper, we propose a\ntwo-stage method to estimate heterogeneous direct and spillover effects on\nnetworks. The first stage uses graph neural networks to estimate nuisance\ncomponents that depend on the complex network topology. In the second stage, we\nadjust for network confounding using these estimates and infer causal effects\nthrough a novel attention-based interference model. Our approach balances\nexpressiveness and interpretability, enabling downstream tasks such as\nidentifying influential neighborhoods and recovering the sign of spillover\neffects. We integrate the two stages using Neyman orthogonalization and\ncross-fitting, which ensures that errors from nuisance estimation contribute\nonly at higher order. As a result, our causal effect estimates are robust to\nbias and misspecification in modeling causal effects under network\ndependencies.",
      "pdf_url": "http://arxiv.org/pdf/2509.18484v1",
      "arxiv_url": "http://arxiv.org/abs/2509.18484v1",
      "published": "2025-09-23",
      "categories": [
        "stat.ML",
        "cs.LG"
      ]
    },
    {
      "title": "The Narcissus Hypothesis: Descending to the Rung of Illusion",
      "authors": [
        "Riccardo Cadei",
        "Christian Internò"
      ],
      "abstract": "Modern foundational models increasingly reflect not just world knowledge, but\npatterns of human preference embedded in their training data. We hypothesize\nthat recursive alignment-via human feedback and model-generated corpora-induces\na social desirability bias, nudging models to favor agreeable or flattering\nresponses over objective reasoning. We refer to it as the Narcissus Hypothesis\nand test it across 31 models using standardized personality assessments and a\nnovel Social Desirability Bias score. Results reveal a significant drift toward\nsocially conforming traits, with profound implications for corpus integrity and\nthe reliability of downstream inferences. We then offer a novel epistemological\ninterpretation, tracing how recursive bias may collapse higher-order reasoning\ndown Pearl's Ladder of Causality, culminating in what we refer to as the Rung\nof Illusion.",
      "pdf_url": "http://arxiv.org/pdf/2509.17999v2",
      "arxiv_url": "http://arxiv.org/abs/2509.17999v2",
      "published": "2025-09-22",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.HC",
        "cs.LG"
      ]
    }
  ]
}