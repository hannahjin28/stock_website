{
  "last_updated": "2026-01-25T01:05:28.576944",
  "papers": [
    {
      "title": "Distributional Balancing for Causal Inference: A Unified Framework via Characteristic Function Distance",
      "authors": [
        "Diptanil Santra",
        "Guanhua Chen",
        "Chan Park"
      ],
      "abstract": "Weighting methods are essential tools for estimating causal effects in observational studies, with the goal of balancing pre-treatment covariates across treatment groups. Traditional approaches pursue this objective indirectly, for example, via inverse propensity score weighting or by matching a finite number of covariate moments, and therefore do not guarantee balance of the full joint covariate distributions. Recently, distributional balancing methods have emerged as robust, nonparametric alternatives that directly target alignment of entire covariate distributions, but they lack a unified framework, formal theoretical guarantees, and valid inferential procedures. We introduce a unified framework for nonparametric distributional balancing based on the characteristic function distance (CFD) and show that widely used discrepancy measures, including the maximum mean discrepancy and energy distance, arise as special cases. Our theoretical analysis establishes conditions under which the resulting CFD-based weighting estimator achieves $\\sqrt{n}$-consistency. Since the standard bootstrap may fail for this estimator, we propose subsampling as a valid alternative for inference. We further extend our approach to an instrumental variable setting to address potential unmeasured confounding. Finally, we evaluate the performance of our method through simulation studies and a real-world application, where the proposed estimator performs well and exhibits results consistent with our theoretical predictions.",
      "pdf_url": "https://arxiv.org/pdf/2601.15449v1",
      "arxiv_url": "http://arxiv.org/abs/2601.15449v1",
      "published": "2026-01-21",
      "categories": [
        "stat.ME"
      ]
    },
    {
      "title": "Improving MoE Compute Efficiency by Composing Weight and Data Sparsity",
      "authors": [
        "Maciej Kilian",
        "Oleg Mkrtchyan",
        "Luke Zettlemoyer",
        "Akshat Shrivastava",
        "Armen Aghajanyan"
      ],
      "abstract": "Mixture-of-Experts layers achieve compute efficiency through weight sparsity: each token activates only a subset of experts. Data sparsity, where each expert processes only a subset of tokens, offers a complementary axis. Expert-choice routing implements data sparsity directly but violates causality in autoregressive models, creating train-inference mismatch. We recover data sparsity within causal token-choice MoE by leveraging zero-compute (null) experts within the routing pool. When a token routes to null experts, those slots consume no compute. The standard load balancing objective trains the model to uniformly use all experts (real and null) therefore creating data sparsity in expectation without the causality violations. We evaluate on vision-language model training, where data heterogeneity is pronounced: vision encoders produce many low-information tokens while text tokens are denser. At matched expected FLOPs, composing weight and data sparsity yields a more compute-efficient frontier than weight sparsity alone, with gains in training loss and downstream performance. The model learns implicit modality-aware allocation, routing vision tokens to null experts more aggressively than text, without explicit modality routing.",
      "pdf_url": "https://arxiv.org/pdf/2601.15370v1",
      "arxiv_url": "http://arxiv.org/abs/2601.15370v1",
      "published": "2026-01-21",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Many Experiments, Few Repetitions, Unpaired Data, and Sparse Effects: Is Causal Inference Possible?",
      "authors": [
        "Felix Schur",
        "Niklas Pfister",
        "Peng Ding",
        "Sach Mukherjee",
        "Jonas Peters"
      ],
      "abstract": "We study the problem of estimating causal effects under hidden confounding in the following unpaired data setting: we observe some covariates $X$ and an outcome $Y$ under different experimental conditions (environments) but do not observe them jointly; we either observe $X$ or $Y$. Under appropriate regularity conditions, the problem can be cast as an instrumental variable (IV) regression with the environment acting as a (possibly high-dimensional) instrument. When there are many environments but only a few observations per environment, standard two-sample IV estimators fail to be consistent. We propose a GMM-type estimator based on cross-fold sample splitting of the instrument-covariate sample and prove that it is consistent as the number of environments grows but the sample size per environment remains constant. We further extend the method to sparse causal effects via $\\ell_1$-regularized estimation and post-selection refitting.",
      "pdf_url": "https://arxiv.org/pdf/2601.15254v1",
      "arxiv_url": "http://arxiv.org/abs/2601.15254v1",
      "published": "2026-01-21",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "RadixMLP -- Intra-batch Deduplication for Causal Transformers",
      "authors": [
        "Michael Feil",
        "Julius Lipp"
      ],
      "abstract": "Batch inference workloads for causal transformer models frequently process sequences that share common prefixes, such as system prompts, few-shot examples, or shared queries. Standard inference engines treat each sequence independently, redundantly recomputing identical MLP activations for every copy of the shared prefix. We introduce RadixMLP, a technique that exploits the position-wise nature of MLPs, LayerNorms, linear projections, and embeddings to eliminate this redundancy. RadixMLP dynamically maps batches to a prefix trie, gathering shared segments into a compressed representation for position-wise computation and scattering results back only at attention boundaries. RadixMLP is stateless and operates within a single forward pass. In end-to-end serving benchmarks on MS~MARCO v1.1 with Qwen3 models (0.6B to 8B parameters), RadixMLP achieves 1.44-1.59$\\times$ speedups in realistic reranking workloads, with up to $5\\times$ speedups on synthetic benchmarks with longer shared prefixes. Our code is available at https://github.com/michaelfeil/radix-mlp.",
      "pdf_url": "https://arxiv.org/pdf/2601.15013v1",
      "arxiv_url": "http://arxiv.org/abs/2601.15013v1",
      "published": "2026-01-21",
      "categories": [
        "cs.LG",
        "cs.DC"
      ]
    },
    {
      "title": "Mirai: Autoregressive Visual Generation Needs Foresight",
      "authors": [
        "Yonghao Yu",
        "Lang Huang",
        "Zerun Wang",
        "Runyi Li",
        "Toshihiko Yamasaki"
      ],
      "abstract": "Autoregressive (AR) visual generators model images as sequences of discrete tokens and are trained with next token likelihood. This strict causality supervision optimizes each step only by its immediate next token, which diminishes global coherence and slows convergence. We ask whether foresight, training signals that originate from later tokens, can help AR visual generation. We conduct a series of controlled diagnostics along the injection level, foresight layout, and foresight source axes, unveiling a key insight: aligning foresight to AR models' internal representation on the 2D image grids improves causality modeling. We formulate this insight with Mirai (meaning \"future\" in Japanese), a general framework that injects future information into AR training with no architecture change and no extra inference overhead: Mirai-E uses explicit foresight from multiple future positions of unidirectional representations, whereas Mirai-I leverages implicit foresight from matched bidirectional representations. Extensive experiments show that Mirai significantly accelerates convergence and improves generation quality. For instance, Mirai can speed up LlamaGen-B's convergence by up to 10$\\times$ and reduce the generation FID from 5.34 to 4.34 on the ImageNet class-condition image generation benchmark. Our study highlights that visual autoregressive models need foresight.",
      "pdf_url": "https://arxiv.org/pdf/2601.14671v1",
      "arxiv_url": "http://arxiv.org/abs/2601.14671v1",
      "published": "2026-01-21",
      "categories": [
        "cs.CV"
      ]
    },
    {
      "title": "Latent Causal Diffusions for Single-Cell Perturbation Modeling",
      "authors": [
        "Lars Lorch",
        "Jiaqi Zhang",
        "Charlotte Bunne",
        "Andreas Krause",
        "Bernhard Sch√∂lkopf",
        "Caroline Uhler"
      ],
      "abstract": "Perturbation screens hold the potential to systematically map regulatory processes at single-cell resolution, yet modeling and predicting transcriptome-wide responses to perturbations remains a major computational challenge. Existing methods often underperform simple baselines, fail to disentangle measurement noise from biological signal, and provide limited insight into the causal structure governing cellular responses. Here, we present the latent causal diffusion (LCD), a generative model that frames single-cell gene expression as a stationary diffusion process observed under measurement noise. LCD outperforms established approaches in predicting the distributional shifts of unseen perturbation combinations in single-cell RNA-sequencing screens while simultaneously learning a mechanistic dynamical system of gene regulation. To interpret these learned dynamics, we develop an approach we call causal linearization via perturbation responses (CLIPR), which yields an approximation of the direct causal effects between all genes modeled by the diffusion. CLIPR provably identifies causal effects under a linear drift assumption and recovers causal structure in both simulated systems and a genome-wide perturbation screen, where it clusters genes into coherent functional modules and resolves causal relationships that standard differential expression analysis cannot. The LCD-CLIPR framework bridges generative modeling with causal inference to predict unseen perturbation effects and map the underlying regulatory mechanisms of the transcriptome.",
      "pdf_url": "https://arxiv.org/pdf/2601.15341v1",
      "arxiv_url": "http://arxiv.org/abs/2601.15341v1",
      "published": "2026-01-20",
      "categories": [
        "q-bio.MN"
      ]
    },
    {
      "title": "Causal feature selection framework for stable soft sensor modeling based on time-delayed cross mapping",
      "authors": [
        "Shi-Shun Chen",
        "Xiao-Yang Li",
        "Enrico Zio"
      ],
      "abstract": "Soft sensor modeling plays a crucial role in process monitoring. Causal feature selection can enhance the performance of soft sensor models in industrial applications. However, existing methods ignore two critical characteristics of industrial processes. Firstly, causal relationships between variables always involve time delays, whereas most causal feature selection methods investigate causal relationships in the same time dimension. Secondly, variables in industrial processes are often interdependent, which contradicts the decorrelation assumption of traditional causal inference methods. Consequently, soft sensor models based on existing causal feature selection approaches often lack sufficient accuracy and stability. To overcome these challenges, this paper proposes a causal feature selection framework based on time-delayed cross mapping. Time-delayed cross mapping employs state space reconstruction to effectively handle interdependent variables in causality analysis, and considers varying causal strength across time delay. Time-delayed convergent cross mapping (TDCCM) is introduced for total causal inference, and time-delayed partial cross mapping (TDPCM) is developed for direct causal inference. Then, in order to achieve automatic feature selection, an objective feature selection strategy is presented. The causal threshold is automatically determined based on the model performance on the validation set, and the causal features are then selected. Two real-world case studies show that TDCCM achieves the highest average performance, while TDPCM improves soft sensor stability and performance in the worst scenario. The code is publicly available at https://github.com/dirge1/TDPCM.",
      "pdf_url": "https://arxiv.org/pdf/2601.14099v1",
      "arxiv_url": "http://arxiv.org/abs/2601.14099v1",
      "published": "2026-01-20",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Component systems: do null models explain everything?",
      "authors": [
        "Andrea Mazzolini",
        "Mattia Corigliano",
        "Rossana Droghetti",
        "Matteo Osella",
        "Marco Cosentino-Lagomarsino"
      ],
      "abstract": "Component systems - ensembles of realizations built from a shared repertoire of modular parts - are ubiquitous in biological, ecological, technological, and socio-cultural domains. From genomes to texts, cities, and software, these systems exhibit statistical regularities that often meet the \"bona fide\" requirements of laws in the physical sciences. Here, we argue that the generality and simplicity of those laws are often due to basic combinatorial or sampling constraints, raising the question of whether such patterns are actually revealing system-specific mechanisms and how we might move beyond them. To this end, we first present a unifying mathematical framework, which allows us to compare modular systems in different fields and highlights the common \"null\" trends as well as the system-specific uniqueness, which, arguably, are signatures of the underlying generative dynamics. Next, we can exploit the framework with statistical mechanics and modern machine-learning tools for a twofold objective. (i) Explaining why the general regularities emerge, highlighting the constraints between them and the general principles at their origins, and (ii) \"subtracting\" them from data, which will isolate the informative features for inferring hidden system-specific generative processes, mechanistic and causal aspects.",
      "pdf_url": "https://arxiv.org/pdf/2601.13985v1",
      "arxiv_url": "http://arxiv.org/abs/2601.13985v1",
      "published": "2026-01-20",
      "categories": [
        "cond-mat.stat-mech",
        "q-bio.OT"
      ]
    },
    {
      "title": "Are Large Language Models able to Predict Highly Cited Papers? Evidence from Statistical Publications",
      "authors": [
        "Zhanshuo Ye",
        "Yiming Hou",
        "Rui Pan",
        "Tianchen Gao",
        "Hansheng Wang"
      ],
      "abstract": "Predicting highly-cited papers is a long-standing challenge due to the complex interactions of research content, scholarly communities, and temporal dynamics. Recent advances in large language models (LLMs) raise the question of whether early-stage textual information can provide useful signals of long-term scientific impact. Focusing on statistical publications, we propose a flexible, text-centered framework that leverages LLMs and structured prompt design to predict highly cited papers. Specifically, we utilize information available at the time of publication, including titles, abstracts, keywords, and limited bibliographic metadata. Using a large corpus of statistical papers, we evaluate predictive performance across multiple publication periods and alternative definitions of highly cited papers. The proposed approach achieves stable and competitive performance relative to existing methods and demonstrates strong generalization over time. Textual analysis further reveals that papers predicted as highly cited concentrate on recurring topics such as causal inference and deep learning. To facilitate practical use of the proposed approach, we further develop a WeChat mini program, \\textit{Stat Highly Cited Papers}, which provides an accessible interface for early-stage citation impact assessment. Overall, our results provide empirical evidence that LLMs can capture meaningful early signals of long-term citation impact, while also highlighting their limitations as tools for research impact assessment.",
      "pdf_url": "https://arxiv.org/pdf/2601.13627v1",
      "arxiv_url": "http://arxiv.org/abs/2601.13627v1",
      "published": "2026-01-20",
      "categories": [
        "stat.AP"
      ]
    },
    {
      "title": "What is Overlap Weighting, How Has it Evolved, and When to Use It for Causal Inference?",
      "authors": [
        "Haidong Lu",
        "Fan Li",
        "Laine E. Thomas",
        "Fan Li"
      ],
      "abstract": "The growing availability of large health databases has expanded the use of observational studies for comparative effectiveness research. Unlike randomized trials, observational studies must adjust for systematic differences in patient characteristics between treatment groups. Propensity score methods, including matching, weighting, stratification, and regression adjustment, address this issue by creating groups that are comparable with respect to measured covariates. Among these approaches, overlap weighting (OW) has emerged as a principled and efficient method that emphasizes individuals at empirical equipoise, those who could plausibly receive either treatment. By assigning weights proportional to the probability of receiving the opposite treatment, OW targets the Average Treatment Effect in the Overlap population (ATO), achieves exact mean covariate balance under logistic propensity score models, and minimizes asymptotic variance. Over the last decade, the OW method has been recognized as a valuable confounding adjustment tool across the statistical, epidemiologic, and clinical research communities, and is increasingly applied in clinical and health studies. Given the growing interest in using observational data to emulate randomized trials and the capacity of OW to prioritize populations at clinical equipoise while achieving covariate balance (fundamental attributes of randomized studies), this article provides a concise overview of recent methodological developments in OW and practical guidance on when it represents a suitable choice for causal inference.",
      "pdf_url": "https://arxiv.org/pdf/2601.13535v1",
      "arxiv_url": "http://arxiv.org/abs/2601.13535v1",
      "published": "2026-01-20",
      "categories": [
        "stat.ME",
        "stat.AP"
      ]
    }
  ]
}