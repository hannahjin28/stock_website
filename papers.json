{
  "last_updated": "2025-09-09T00:50:24.172700",
  "papers": [
    {
      "title": "Semi-supervised inference for treatment heterogeneity",
      "authors": [
        "Yilizhati Anniwaer",
        "Yuqian Zhang"
      ],
      "abstract": "In causal inference, measuring treatment heterogeneity is crucial as it\nprovides scientific insights into how treatments influence outcomes and guides\npersonalized decision-making. In this work, we study semi-supervised settings\nwhere a labeled dataset is accompanied by a large unlabeled dataset, and\ndevelop semi-supervised estimators for two measures of treatment heterogeneity:\nthe total treatment heterogeneity (TTH) and the explained treatment\nheterogeneity (ETH) of a simplified working model. We propose semi-supervised\nestimators for both quantities and demonstrate their improved robustness and\nefficiency compared with supervised methods. For ETH estimation, we show that\ndirect semi-supervised approaches may result in efficiency loss relative to\nsupervised counterparts. To address this, we introduce a re-weighting strategy\nthat assigns data-dependent weights to labeled and unlabeled samples to\noptimize efficiency. The proposed approach guarantees an asymptotic variance no\nlarger than that of the supervised method, ensuring its safe use. We evaluate\nthe performance of the proposed estimators through simulation studies and a\nreal-data application based on an AIDS clinical trial.",
      "pdf_url": "http://arxiv.org/pdf/2509.05048v1",
      "arxiv_url": "http://arxiv.org/abs/2509.05048v1",
      "published": "2025-09-05",
      "categories": [
        "stat.ME"
      ]
    },
    {
      "title": "Detecting extreme event-driven causality",
      "authors": [
        "Siyang Yu",
        "Yu Huang",
        "Zuntao Fu"
      ],
      "abstract": "The occurrence of some extreme events (such as marine heatwaves or\nexceptional circulations) can cause other extreme events (such as heatwave,\ndrought and flood). These concurrent extreme events have a great impact on\nenvironment and human health. However, how to detect and quantify the causes\nand impacts of these extreme events by a data-driven way is still unsolved. In\nthis study, the dynamic system method is extended to develop a method for\ndetecting the causality between extreme events. Taking the coupled\nLorenz-Lorenz systems with extreme event-driven coupling as an example, it is\ndemonstrated that this proposed detecting method is able to capture the extreme\nevent-driven causality, with even better causality detecting performance\nbetween concurrent extreme events. Comparison among three kinds of measured\nseries, full measurements outperform partial ones in event-to-event causality\ndetecting. The successful applicability of our proposed approach in Walker\ncirculation phenomenon indicates that our method contributes a novel way to the\nstudy of causal inference in complex systems. This method offers valuable\ninsights into multi-scale, nonlinear dynamics, particularly in uncovering\nassociations among extreme events.",
      "pdf_url": "http://arxiv.org/pdf/2509.05043v1",
      "arxiv_url": "http://arxiv.org/abs/2509.05043v1",
      "published": "2025-09-05",
      "categories": [
        "physics.ao-ph",
        "math-ph",
        "math.DS",
        "math.MP"
      ]
    },
    {
      "title": "DarkStream: real-time speech anonymization with low latency",
      "authors": [
        "Waris Quamer",
        "Ricardo Gutierrez-Osuna"
      ],
      "abstract": "We propose DarkStream, a streaming speech synthesis model for real-time\nspeaker anonymization. To improve content encoding under strict latency\nconstraints, DarkStream combines a causal waveform encoder, a short lookahead\nbuffer, and transformer-based contextual layers. To further reduce inference\ntime, the model generates waveforms directly via a neural vocoder, thus\nremoving intermediate mel-spectrogram conversions. Finally, DarkStream\nanonymizes speaker identity by injecting a GAN-generated pseudo-speaker\nembedding into linguistic features from the content encoder. Evaluations show\nour model achieves strong anonymization, yielding close to 50% speaker\nverification EER (near-chance performance) on the lazy-informed attack\nscenario, while maintaining acceptable linguistic intelligibility (WER within\n9%). By balancing low-latency, robust privacy, and minimal intelligibility\ndegradation, DarkStream provides a practical solution for privacy-preserving\nreal-time speech communication.",
      "pdf_url": "http://arxiv.org/pdf/2509.04667v1",
      "arxiv_url": "http://arxiv.org/abs/2509.04667v1",
      "published": "2025-09-04",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.LG"
      ]
    },
    {
      "title": "Precision Mental Health: Predicting Heterogeneous Treatment Effects for Depression through Data Integration",
      "authors": [
        "Carly L. Brantner",
        "Trang Quynh Nguyen",
        "Harsh Parikh",
        "Congwen Zhao",
        "Hwanhee Hong",
        "Elizabeth A. Stuart"
      ],
      "abstract": "When treating depression, clinicians are interested in determining the\noptimal treatment for a given patient, which is challenging given the amount of\ntreatments available. To advance individualized treatment allocation,\nintegrating data across multiple randomized controlled trials (RCTs) can\nenhance our understanding of treatment effect heterogeneity by increasing\navailable information. However, extending these inferences to individuals\noutside of the original RCTs remains crucial for clinical decision-making. We\nintroduce a two-stage meta-analytic method that predicts conditional average\ntreatment effects (CATEs) in target patient populations by leveraging the\ndistribution of CATEs across RCTs. Our approach generates 95\\% prediction\nintervals for CATEs in target settings using first-stage models that can\nincorporate parametric regression or non-parametric methods such as causal\nforests or Bayesian additive regression trees (BART). We validate our method\nthrough simulation studies and operationalize it to integrate multiple RCTs\ncomparing depression treatments, duloxetine and vortioxetine, to generate\nprediction intervals for target patient profiles. Our analysis reveals no\nstrong evidence of effect heterogeneity across trials, with the exception of\npotential age-related variability. Importantly, we show that CATE prediction\nintervals capture broader uncertainty than study-specific confidence intervals\nwhen warranted, reflecting both within-study and between-study variability.",
      "pdf_url": "http://arxiv.org/pdf/2509.04604v1",
      "arxiv_url": "http://arxiv.org/abs/2509.04604v1",
      "published": "2025-09-04",
      "categories": [
        "stat.AP",
        "stat.ME"
      ]
    },
    {
      "title": "Interpretable Clustering with Adaptive Heterogeneous Causal Structure Learning in Mixed Observational Data",
      "authors": [
        "Wenrui Li",
        "Qinghao Zhang",
        "Xiaowo Wang"
      ],
      "abstract": "Understanding causal heterogeneity is essential for scientific discovery in\ndomains such as biology and medicine. However, existing methods lack causal\nawareness, with insufficient modeling of heterogeneity, confounding, and\nobservational constraints, leading to poor interpretability and difficulty\ndistinguishing true causal heterogeneity from spurious associations. We propose\nan unsupervised framework, HCL (Interpretable Causal Mechanism-Aware Clustering\nwith Adaptive Heterogeneous Causal Structure Learning), that jointly infers\nlatent clusters and their associated causal structures from mixed-type\nobservational data without requiring temporal ordering, environment labels,\ninterventions or other prior knowledge. HCL relaxes the homogeneity and\nsufficiency assumptions by introducing an equivalent representation that\nencodes both structural heterogeneity and confounding. It further develops a\nbi-directional iterative strategy to alternately refine causal clustering and\nstructure learning, along with a self-supervised regularization that balance\ncross-cluster universality and specificity. Together, these components enable\nconvergence toward interpretable, heterogeneous causal patterns. Theoretically,\nwe show identifiability of heterogeneous causal structures under mild\nconditions. Empirically, HCL achieves superior performance in both clustering\nand structure learning tasks, and recovers biologically meaningful mechanisms\nin real-world single-cell perturbation data, demonstrating its utility for\ndiscovering interpretable, mechanism-level causal heterogeneity.",
      "pdf_url": "http://arxiv.org/pdf/2509.04415v1",
      "arxiv_url": "http://arxiv.org/abs/2509.04415v1",
      "published": "2025-09-04",
      "categories": [
        "cs.LG"
      ]
    },
    {
      "title": "OccTENS: 3D Occupancy World Model via Temporal Next-Scale Prediction",
      "authors": [
        "Bu Jin",
        "Songen Gu",
        "Xiaotao Hu",
        "Yupeng Zheng",
        "Xiaoyang Guo",
        "Qian Zhang",
        "Xiaoxiao Long",
        "Wei Yin"
      ],
      "abstract": "In this paper, we propose OccTENS, a generative occupancy world model that\nenables controllable, high-fidelity long-term occupancy generation while\nmaintaining computational efficiency. Different from visual generation, the\noccupancy world model must capture the fine-grained 3D geometry and dynamic\nevolution of the 3D scenes, posing great challenges for the generative models.\nRecent approaches based on autoregression (AR) have demonstrated the potential\nto predict vehicle movement and future occupancy scenes simultaneously from\nhistorical observations, but they typically suffer from \\textbf{inefficiency},\n\\textbf{temporal degradation} in long-term generation and \\textbf{lack of\ncontrollability}. To holistically address these issues, we reformulate the\noccupancy world model as a temporal next-scale prediction (TENS) task, which\ndecomposes the temporal sequence modeling problem into the modeling of spatial\nscale-by-scale generation and temporal scene-by-scene prediction. With a\n\\textbf{TensFormer}, OccTENS can effectively manage the temporal causality and\nspatial relationships of occupancy sequences in a flexible and scalable way. To\nenhance the pose controllability, we further propose a holistic pose\naggregation strategy, which features a unified sequence modeling for occupancy\nand ego-motion. Experiments show that OccTENS outperforms the state-of-the-art\nmethod with both higher occupancy quality and faster inference time.",
      "pdf_url": "http://arxiv.org/pdf/2509.03887v1",
      "arxiv_url": "http://arxiv.org/abs/2509.03887v1",
      "published": "2025-09-04",
      "categories": [
        "cs.CV"
      ]
    },
    {
      "title": "Causality-guided Prompt Learning for Vision-language Models via Visual Granulation",
      "authors": [
        "Mengyu Gao",
        "Qiulei Dong"
      ],
      "abstract": "Prompt learning has recently attracted much attention for adapting\npre-trained vision-language models (e.g., CLIP) to downstream recognition\ntasks. However, most of the existing CLIP-based prompt learning methods only\nshow a limited ability for handling fine-grained datasets. To address this\nissue, we propose a causality-guided text prompt learning method via visual\ngranulation for CLIP, called CaPL, where the explored visual granulation\ntechnique could construct sets of visual granules for the text prompt to\ncapture subtle discrepancies among different fine-grained classes through\ncasual inference. The CaPL method contains the following two modules: (1) An\nattribute disentanglement module is proposed to decompose visual features into\nnon-individualized attributes (shared by some classes) and individualized\nattributes (specific to single classes) using a Brownian Bridge Diffusion\nModel; (2) A granule learning module is proposed to construct visual granules\nby integrating the aforementioned attributes for recognition under two causal\ninference strategies. Thanks to the learned visual granules, more\ndiscriminative text prompt is expected to be learned. Extensive experimental\nresults on 15 datasets demonstrate that our CaPL method significantly\noutperforms the state-of-the-art prompt learning methods, especially on\nfine-grained datasets.",
      "pdf_url": "http://arxiv.org/pdf/2509.03803v1",
      "arxiv_url": "http://arxiv.org/abs/2509.03803v1",
      "published": "2025-09-04",
      "categories": [
        "cs.CV"
      ]
    },
    {
      "title": "The super learner for time-to-event outcomes: A tutorial",
      "authors": [
        "Ruth H. Keogh",
        "Karla Diaz-Ordaz",
        "Nan van Geloven",
        "Jon Michael Gran",
        "Kamaryn T. Tanner"
      ],
      "abstract": "Estimating risks or survival probabilities conditional on individual\ncharacteristics based on censored time-to-event data is a commonly faced task.\nThis may be for the purpose of developing a prediction model or may be part of\na wider estimation procedure, such as in causal inference. A challenge is that\nit is impossible to know at the outset which of a set of candidate models will\nprovide the best predictions. The super learner is a powerful approach for\nfinding the best model or combination of models ('ensemble') among a\npre-specified set of candidate models or 'learners', which can include\nparametric and machine learning models. Super learners for time-to-event\noutcomes have been developed, but the literature is technical and a reader may\nfind it challenging to gather together the full details of how these methods\nwork and can be implemented. In this paper we provide a practical tutorial on\nsuper learner methods for time-to-event outcomes. An overview of the general\nsteps involved in the super learner is given, followed by details of three\nspecific implementations for time-to-event outcomes. We cover discrete-time and\ncontinuous-time versions of the super learner, as described by Polley and van\nder Laan (2011), Westling et al. (2023) and Munch and Gerds (2024). We compare\nthe properties of the methods and provide information on how they can be\nimplemented in R. The methods are illustrated using an open access data set and\nR code is provided.",
      "pdf_url": "http://arxiv.org/pdf/2509.03315v1",
      "arxiv_url": "http://arxiv.org/abs/2509.03315v1",
      "published": "2025-09-03",
      "categories": [
        "stat.ME"
      ]
    },
    {
      "title": "Bayesian Network Propensity Score to Evaluate Treatment Effects in Observational Studies",
      "authors": [
        "Clelia Di Serio",
        "Federica Cugnata",
        "Pier Luigi Conti",
        "Alberto Briganti",
        "Fulvia Mecatti",
        "Paola Vicard",
        "Paola Maria Vittoria Rancoita"
      ],
      "abstract": "This paper focuses on the Bayesian Network Propensity Score (BNPS), a novel\napproach for estimating treatment effects in observational studies\ncharacterized by unknown (and likely unbalanced) designs and complex dependency\nstructures among covariates. Traditional methods, such as logistic regression,\noften impose rigid parametric assumptions that may lead to misspecification\nerrors, compromising causal inference. Recent classical and machine learning\nalternatives, such as boosted CART, random forests, and Stable Balancing\nWeights, seem to be attractive in a predictive perspective, but they typically\nlack asymptotic properties, such as consistency, efficiency, and valid variance\nestimation. In contrast, the recently proposed BNPS to estimate propensity\nscores uses Bayesian Networks to flexibly model conditional dependencies while\npreserving essential statistical properties such as consistency, asymptotic\nnormality and asymptotic efficiency. Combined with the H\\'ajek estimator, BNPS\nenables robust estimation of the Average Treatment Effect (ATE) in scenarios\nwith strong covariate interactions and unknown data-generating mechanisms.\nThrough extensive simulations across fifteen realistic scenarios and varying\nsample sizes, BNPS consistently outperforms benchmark methods in both empirical\nrejection rates and coverage accuracy. Finally, an application to a real-world\ndataset of 7,162 prostate cancer patients from San Raffaele Hospital (Milan,\nItaly) demonstrates BNPS's practical value in assessing the impact of pelvic\nlymph node dissection on hospitalization duration and biochemical recurrence.\nThe findings support BNPS as a statistically robust, interpretable and\ntransparent alternative for causal inference in complex observational settings,\nenhancing the reliability of evidence from real-world biomedical data.",
      "pdf_url": "http://arxiv.org/pdf/2509.03194v1",
      "arxiv_url": "http://arxiv.org/abs/2509.03194v1",
      "published": "2025-09-03",
      "categories": [
        "stat.ME",
        "stat.AP"
      ]
    },
    {
      "title": "Distribution-valued Causal Machine Learning: Implications of Credit on Spending Patterns",
      "authors": [
        "Cheuk Hang Leung",
        "Yijun Li",
        "Qi Wu"
      ],
      "abstract": "Fintech lending has become a central mechanism through which digital\nplatforms stimulate consumption, offering dynamic, personalized credit limits\nthat directly shape the purchasing power of consumers. Although prior research\nshows that higher limits increase average spending, scalar-based outcomes\nobscure the heterogeneous distributional nature of consumer responses. This\npaper addresses this gap by proposing a new causal inference framework that\nestimates how continuous changes in the credit limit affect the entire\ndistribution of consumer spending. We formalize distributional causal effects\nwithin the Wasserstein space and introduce a robust Distributional Double\nMachine Learning estimator, supported by asymptotic theory to ensure\nconsistency and validity. To implement this estimator, we design a deep\nlearning architecture comprising two components: a Neural Functional Regression\nNet to capture complex, nonlinear relationships between treatments, covariates,\nand distributional outcomes, and a Conditional Normalizing Flow Net to estimate\ngeneralized propensity scores under continuous treatment. Numerical experiments\ndemonstrate that the proposed estimator accurately recovers distributional\neffects in a range of data-generating scenarios. Applying our framework to\ntransaction-level data from a major BigTech platform, we find that increased\ncredit limits primarily shift consumers towards higher-value purchases rather\nthan uniformly increasing spending, offering new insights for personalized\nmarketing strategies and digital consumer finance.",
      "pdf_url": "http://arxiv.org/pdf/2509.03063v1",
      "arxiv_url": "http://arxiv.org/abs/2509.03063v1",
      "published": "2025-09-03",
      "categories": [
        "econ.GN",
        "q-fin.EC"
      ]
    }
  ]
}