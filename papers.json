{
  "last_updated": "2025-09-11T00:50:27.141268",
  "papers": [
    {
      "title": "On the Ambiguities of Incompatibility in Frequentist Inference",
      "authors": [
        "Alessandro Rovetta"
      ],
      "abstract": "The interpretation of the P-value and its monotone transform s=-log2(p), or\nS-value, remains debated despite decades of dedicated literature. Within the\nneo-Fisherian framework, these values are often described as indices of\n(in)compatibility between the observed data and a set of ideal assumptions\n(i.e., the statistical model). In this regard, this paper proposes the\ndistinction between two domains: the model domain, where assumptions are taken\nas perfectly true and every admissible outcome is, by construction, fully\ncompatible with the model; and the real domain, where assumptions may fail and\nface empirical scrutiny. I argue that, although interpreted through an\nobjective numerical index, any level of incompatibility can arise only in the\nlatter domain, where the epistemic status of the model under examination is\nuncertain and a genuine conflict between data and hypotheses can therefore\noccur. The extent to which P- and S-values are taken as indicating\nincompatibility is a matter of contextual judgment. Within this framework,\ndescriptive approaches serve to quantify the numerical values of P and S; these\ncan be interpreted as indicative of a certain degree (or amount) of\nincompatibility between data and hypotheses once causal knowledge of the\ndata-generating process and information about the costs and benefits of related\ndecisions become clearer. Although the distinction between the model domain and\nthe real domain may appear merely theoretical or even philosophical, I argue\nthat this perspective is useful for developing a clear mental representation of\nhow statistical estimates should be evaluated in practical settings and\napplications.",
      "pdf_url": "http://arxiv.org/pdf/2509.07147v1",
      "arxiv_url": "http://arxiv.org/abs/2509.07147v1",
      "published": "2025-09-08",
      "categories": [
        "stat.OT"
      ]
    },
    {
      "title": "MachineLearningLM: Scaling Many-shot In-context Learning via Continued Pretraining",
      "authors": [
        "Haoyu Dong",
        "Pengkun Zhang",
        "Mingzhe Lu",
        "Yanzhen Shen",
        "Guolin Ke"
      ],
      "abstract": "Large language models (LLMs) possess broad world knowledge and strong\ngeneral-purpose reasoning ability, yet they struggle to learn from many\nin-context examples on standard machine learning (ML) tasks, that is, to\nleverage many-shot demonstrations purely via in-context learning (ICL) without\ngradient descent. We introduce MachineLearningLM, a portable\ncontinued-pretraining framework that equips a general-purpose LLM with robust\nin-context ML capability while preserving its general knowledge and reasoning\nfor broader chat workflows.\n  Our pretraining procedure synthesizes ML tasks from millions of structural\ncausal models (SCMs), spanning shot counts up to 1,024. We begin with a\nrandom-forest teacher, distilling tree-based decision strategies into the LLM\nto strengthen robustness in numerical modeling. All tasks are serialized with a\ntoken-efficient prompt, enabling 3x to 6x more examples per context window and\ndelivering up to 50x amortized throughput via batch inference.\n  Despite a modest setup (Qwen-2.5-7B-Instruct with LoRA rank 8),\nMachineLearningLM outperforms strong LLM baselines (e.g., GPT-5-mini) by an\naverage of about 15% on out-of-distribution tabular classification across\nfinance, physics, biology, and healthcare domains. It exhibits a striking\nmany-shot scaling law: accuracy increases monotonically as in-context\ndemonstrations grow from 8 to 1,024. Without any task-specific training, it\nattains random-forest-level accuracy across hundreds of shots. General chat\ncapabilities, including knowledge and reasoning, are preserved: it achieves\n75.4% on MMLU.",
      "pdf_url": "http://arxiv.org/pdf/2509.06806v2",
      "arxiv_url": "http://arxiv.org/abs/2509.06806v2",
      "published": "2025-09-08",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "CausNVS: Autoregressive Multi-view Diffusion for Flexible 3D Novel View Synthesis",
      "authors": [
        "Xin Kong",
        "Daniel Watson",
        "Yannick Str√ºmpler",
        "Michael Niemeyer",
        "Federico Tombari"
      ],
      "abstract": "Multi-view diffusion models have shown promise in 3D novel view synthesis,\nbut most existing methods adopt a non-autoregressive formulation. This limits\ntheir applicability in world modeling, as they only support a fixed number of\nviews and suffer from slow inference due to denoising all frames\nsimultaneously. To address these limitations, we propose CausNVS, a multi-view\ndiffusion model in an autoregressive setting, which supports arbitrary\ninput-output view configurations and generates views sequentially. We train\nCausNVS with causal masking and per-frame noise, using pairwise-relative camera\npose encodings (CaPE) for precise camera control. At inference time, we combine\na spatially-aware sliding-window with key-value caching and noise conditioning\naugmentation to mitigate drift. Our experiments demonstrate that CausNVS\nsupports a broad range of camera trajectories, enables flexible autoregressive\nnovel view synthesis, and achieves consistently strong visual quality across\ndiverse settings. Project page: https://kxhit.github.io/CausNVS.html.",
      "pdf_url": "http://arxiv.org/pdf/2509.06579v1",
      "arxiv_url": "http://arxiv.org/abs/2509.06579v1",
      "published": "2025-09-08",
      "categories": [
        "cs.CV"
      ]
    },
    {
      "title": "Predicting Market Troughs: A Machine Learning Approach with Causal Interpretation",
      "authors": [
        "Peilin Rao",
        "Randall R. Rojas"
      ],
      "abstract": "This paper provides robust, new evidence on the causal drivers of market\ntroughs. We demonstrate that conclusions about these triggers are critically\nsensitive to model specification, moving beyond restrictive linear models with\na flexible DML average partial effect causal machine learning framework. Our\nrobust estimates identify the volatility of options-implied risk appetite and\nmarket liquidity as key causal drivers, relationships misrepresented or\nobscured by simpler models. These findings provide high-frequency empirical\nsupport for intermediary asset pricing theories. This causal analysis is\nenabled by a high-performance nowcasting model that accurately identifies\ncapitulation events in real-time.",
      "pdf_url": "http://arxiv.org/pdf/2509.05922v1",
      "arxiv_url": "http://arxiv.org/abs/2509.05922v1",
      "published": "2025-09-07",
      "categories": [
        "q-fin.ST",
        "econ.EM",
        "stat.ML",
        "91G80, 62P05",
        "J.4"
      ]
    },
    {
      "title": "From Eigenmodes to Proofs: Integrating Graph Spectral Operators with Symbolic Interpretable Reasoning",
      "authors": [
        "Andrew Kiruluta",
        "Priscilla Burity"
      ],
      "abstract": "We introduce Spectral NSR, a fully spectral neuro-symbolic reasoning\nframework that embeds logical rules as spectral templates and performs\ninference directly in the graph spectral domain. By leveraging graph signal\nprocessing (GSP) and frequency-selective filters grounded in the Laplacian\neigenstructure of knowledge graphs, the architecture unifies the\ninterpretability of symbolic reasoning with the scalability and adaptability of\nspectral learning. Beyond the core formulation, we incorporate a comprehensive\nset of extensions, including dynamic graph and basis learning, rational and\ndiffusion filters for sharper spectral selectivity, mixture-of-spectral-experts\nfor modular specialization, proof-guided training with spectral curricula, and\nuncertainty quantification for calibrated confidence. Additional enhancements\nsuch as large language model coupling, co-spectral transfer alignment,\nadversarial robustness, efficient GPU kernels, generalized Laplacians, and\ncausal interventions further expand the versatility of the framework.\n  Empirical evaluation on state-of-the-art reasoning benchmarks such as\nProofWriter and CLUTRR demonstrates that Spectral NSR achieves superior\naccuracy, faster inference, improved robustness to adversarial perturbations,\nand higher interpretability compared to leading baselines including\ntransformers, message-passing neural networks, and neuro-symbolic logic\nprogramming systems. Spectral attribution and proof-band agreement analyses\nconfirm that model decisions align closely with symbolic proof structures,\nwhile transfer experiments validate effective domain adaptation through\nco-spectral alignment. These results establish Spectral NSR as a scalable and\nprincipled foundation for the next generation of reasoning systems, offering\ntransparency, robustness, and generalization beyond conventional approaches.",
      "pdf_url": "http://arxiv.org/pdf/2509.07017v1",
      "arxiv_url": "http://arxiv.org/abs/2509.07017v1",
      "published": "2025-09-07",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ]
    },
    {
      "title": "Causal Clustering for Conditional Average Treatment Effects Estimation and Subgroup Discovery",
      "authors": [
        "Zilong Wang",
        "Turgay Ayer",
        "Shihao Yang"
      ],
      "abstract": "Estimating heterogeneous treatment effects is critical in domains such as\npersonalized medicine, resource allocation, and policy evaluation. A central\nchallenge lies in identifying subpopulations that respond differently to\ninterventions, thereby enabling more targeted and effective decision-making.\nWhile clustering methods are well-studied in unsupervised learning, their\nintegration with causal inference remains limited. We propose a novel framework\nthat clusters individuals based on estimated treatment effects using a learned\nkernel derived from causal forests, revealing latent subgroup structures. Our\napproach consists of two main steps. First, we estimate debiased Conditional\nAverage Treatment Effects (CATEs) using orthogonalized learners via the\nRobinson decomposition, yielding a kernel matrix that encodes sample-level\nsimilarities in treatment responsiveness. Second, we apply kernelized\nclustering to this matrix to uncover distinct, treatment-sensitive\nsubpopulations and compute cluster-level average CATEs. We present this\nkernelized clustering step as a form of regularization within the\nresidual-on-residual regression framework. Through extensive experiments on\nsemi-synthetic and real-world datasets, supported by ablation studies and\nexploratory analyses, we demonstrate the effectiveness of our method in\ncapturing meaningful treatment effect heterogeneity.",
      "pdf_url": "http://arxiv.org/pdf/2509.05775v1",
      "arxiv_url": "http://arxiv.org/abs/2509.05775v1",
      "published": "2025-09-06",
      "categories": [
        "stat.ML",
        "cs.LG"
      ]
    },
    {
      "title": "Bayesian Inference for Confounding Variables and Limited Information",
      "authors": [
        "Ellis Scharfenaker",
        "Duncan K. Foley"
      ],
      "abstract": "A central challenge in statistical inference is the presence of confounding\nvariables that may distort observed associations between treatment and outcome.\nConventional \"causal\" methods, grounded in assumptions such as ignorability,\nexclude the possibility of unobserved confounders, leading to posterior\ninferences that overstate certainty. We develop a Bayesian framework that\nrelaxes these assumptions by introducing entropy-favoring priors over\nhypothesis spaces that explicitly allow for latent confounding variables and\npartial information. Using the case of Simpson's paradox, we demonstrate how\nthis approach produces logically consistent posterior distributions that widen\ncredibly intervals in the presence of potential confounding. Our method\nprovides a generalizable, information-theoretic foundation for more robust\npredictive inference in observational sciences.",
      "pdf_url": "http://arxiv.org/pdf/2509.05520v1",
      "arxiv_url": "http://arxiv.org/abs/2509.05520v1",
      "published": "2025-09-05",
      "categories": [
        "stat.ME",
        "econ.EM"
      ]
    },
    {
      "title": "Semi-supervised inference for treatment heterogeneity",
      "authors": [
        "Yilizhati Anniwaer",
        "Yuqian Zhang"
      ],
      "abstract": "In causal inference, measuring treatment heterogeneity is crucial as it\nprovides scientific insights into how treatments influence outcomes and guides\npersonalized decision-making. In this work, we study semi-supervised settings\nwhere a labeled dataset is accompanied by a large unlabeled dataset, and\ndevelop semi-supervised estimators for two measures of treatment heterogeneity:\nthe total treatment heterogeneity (TTH) and the explained treatment\nheterogeneity (ETH) of a simplified working model. We propose semi-supervised\nestimators for both quantities and demonstrate their improved robustness and\nefficiency compared with supervised methods. For ETH estimation, we show that\ndirect semi-supervised approaches may result in efficiency loss relative to\nsupervised counterparts. To address this, we introduce a re-weighting strategy\nthat assigns data-dependent weights to labeled and unlabeled samples to\noptimize efficiency. The proposed approach guarantees an asymptotic variance no\nlarger than that of the supervised method, ensuring its safe use. We evaluate\nthe performance of the proposed estimators through simulation studies and a\nreal-data application based on an AIDS clinical trial.",
      "pdf_url": "http://arxiv.org/pdf/2509.05048v1",
      "arxiv_url": "http://arxiv.org/abs/2509.05048v1",
      "published": "2025-09-05",
      "categories": [
        "stat.ME"
      ]
    },
    {
      "title": "Detecting extreme event-driven causality",
      "authors": [
        "Siyang Yu",
        "Yu Huang",
        "Zuntao Fu"
      ],
      "abstract": "The occurrence of some extreme events (such as marine heatwaves or\nexceptional circulations) can cause other extreme events (such as heatwave,\ndrought and flood). These concurrent extreme events have a great impact on\nenvironment and human health. However, how to detect and quantify the causes\nand impacts of these extreme events by a data-driven way is still unsolved. In\nthis study, the dynamic system method is extended to develop a method for\ndetecting the causality between extreme events. Taking the coupled\nLorenz-Lorenz systems with extreme event-driven coupling as an example, it is\ndemonstrated that this proposed detecting method is able to capture the extreme\nevent-driven causality, with even better causality detecting performance\nbetween concurrent extreme events. Comparison among three kinds of measured\nseries, full measurements outperform partial ones in event-to-event causality\ndetecting. The successful applicability of our proposed approach in Walker\ncirculation phenomenon indicates that our method contributes a novel way to the\nstudy of causal inference in complex systems. This method offers valuable\ninsights into multi-scale, nonlinear dynamics, particularly in uncovering\nassociations among extreme events.",
      "pdf_url": "http://arxiv.org/pdf/2509.05043v1",
      "arxiv_url": "http://arxiv.org/abs/2509.05043v1",
      "published": "2025-09-05",
      "categories": [
        "physics.ao-ph",
        "math-ph",
        "math.DS",
        "math.MP"
      ]
    },
    {
      "title": "DarkStream: real-time speech anonymization with low latency",
      "authors": [
        "Waris Quamer",
        "Ricardo Gutierrez-Osuna"
      ],
      "abstract": "We propose DarkStream, a streaming speech synthesis model for real-time\nspeaker anonymization. To improve content encoding under strict latency\nconstraints, DarkStream combines a causal waveform encoder, a short lookahead\nbuffer, and transformer-based contextual layers. To further reduce inference\ntime, the model generates waveforms directly via a neural vocoder, thus\nremoving intermediate mel-spectrogram conversions. Finally, DarkStream\nanonymizes speaker identity by injecting a GAN-generated pseudo-speaker\nembedding into linguistic features from the content encoder. Evaluations show\nour model achieves strong anonymization, yielding close to 50% speaker\nverification EER (near-chance performance) on the lazy-informed attack\nscenario, while maintaining acceptable linguistic intelligibility (WER within\n9%). By balancing low-latency, robust privacy, and minimal intelligibility\ndegradation, DarkStream provides a practical solution for privacy-preserving\nreal-time speech communication.",
      "pdf_url": "http://arxiv.org/pdf/2509.04667v1",
      "arxiv_url": "http://arxiv.org/abs/2509.04667v1",
      "published": "2025-09-04",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.LG"
      ]
    }
  ]
}