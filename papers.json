{
  "last_updated": "2025-05-02T00:52:35.686442",
  "papers": [
    {
      "title": "Assessing Racial Disparities in Healthcare Expenditures Using Causal Path-Specific Effects",
      "authors": [
        "Xiaxian Ou",
        "Xinwei He",
        "David Benkeser",
        "Razieh Nabi"
      ],
      "abstract": "Racial disparities in healthcare expenditures are well-documented, yet the\nunderlying drivers remain complex and require further investigation. This study\nemploys causal and counterfactual path-specific effects to quantify how various\nfactors, including socioeconomic status, insurance access, health behaviors,\nand health status, mediate these disparities. Using data from the Medical\nExpenditures Panel Survey, we estimate how expenditures would differ under\ncounterfactual scenarios in which the values of specific mediators were aligned\nacross racial groups along selected causal pathways. A key challenge in this\nanalysis is ensuring robustness against model misspecification while addressing\nthe zero-inflation and right-skewness of healthcare expenditures. For reliable\ninference, we derive asymptotically linear estimators by integrating influence\nfunction-based techniques with flexible machine learning methods, including\nsuper learners and a two-part model tailored to the zero-inflated, right-skewed\nnature of healthcare expenditures.",
      "pdf_url": "http://arxiv.org/pdf/2504.21688v1",
      "arxiv_url": "http://arxiv.org/abs/2504.21688v1",
      "published": "2025-04-30",
      "categories": [
        "stat.AP",
        "stat.ME",
        "stat.ML"
      ]
    },
    {
      "title": "Convergence rate for Nearest Neighbour matching: geometry of the domain and higher-order regularity",
      "authors": [
        "Simon Viel",
        "Lionel Truquet",
        "Ikko Yamane"
      ],
      "abstract": "Estimating some mathematical expectations from partially observed data and in\nparticular missing outcomes is a central problem encountered in numerous fields\nsuch as transfer learning, counterfactual analysis or causal inference.\nMatching estimators, estimators based on k-nearest neighbours, are widely used\nin this context. It is known that the variance of such estimators can converge\nto zero at a parametric rate, but their bias can have a slower rate when the\ndimension of the covariates is larger than 2. This makes analysis of this bias\nparticularly important. In this paper, we provide higher order properties of\nthe bias. In contrast to the existing literature related to this problem, we do\nnot assume that the support of the target distribution of the covariates is\nstrictly included in that of the source, and we analyse two geometric\nconditions on the support that avoid such boundary bias problems. We show that\nthese conditions are much more general than the usual convex support\nassumption, leading to an improvement of existing results. Furthermore, we show\nthat the matching estimator studied by Abadie and Imbens (2006) for the average\ntreatment effect can be asymptotically efficient when the dimension of the\ncovariates is less than 4, a result only known in dimension 1.",
      "pdf_url": "http://arxiv.org/pdf/2504.21633v1",
      "arxiv_url": "http://arxiv.org/abs/2504.21633v1",
      "published": "2025-04-30",
      "categories": [
        "math.ST",
        "stat.TH"
      ]
    },
    {
      "title": "Powerful randomization tests for subgroup analysis",
      "authors": [
        "Yao Zhang",
        "Zijun Gao"
      ],
      "abstract": "Randomization tests are widely used to generate valid $p$-values for testing\nsharp null hypotheses in finite-population causal inference. This article\nextends their application to subgroup analysis. We show that directly testing\nsubgroup null hypotheses may lack power due to small subgroup sizes.\nIncorporating an estimator of the conditional average treatment effect (CATE)\ncan substantially improve power but requires splitting the treatment variables\nbetween estimation and testing to preserve finite-sample validity. To this end,\nwe propose BaR-learner, a Bayesian extension of the popular method R-learner\nfor CATE estimation. BaR-learner imputes the treatment variables reserved for\nrandomization tests, reducing information loss due to sample-splitting.\nFurthermore, we show that the treatment variables most informative for training\nBaR-learner are different from those most valuable for increasing test power.\nMotivated by this insight, we introduce AdaSplit, a sample-splitting procedure\nthat adaptively allocates units between estimation and testing. Simulation\nstudies demonstrate that our method yields more powerful randomization tests\nthan baselines that omit CATE estimation or rely on random sample-splitting. We\nalso apply our method to a blood pressure intervention trial, identifying\npatient subgroups with significant treatment effects.",
      "pdf_url": "http://arxiv.org/pdf/2504.21572v1",
      "arxiv_url": "http://arxiv.org/abs/2504.21572v1",
      "published": "2025-04-30",
      "categories": [
        "stat.ME"
      ]
    },
    {
      "title": "Artificial Intelligence for Personalized Prediction of Alzheimer's Disease Progression: A Survey of Methods, Data Challenges, and Future Directions",
      "authors": [
        "Gulsah Hancerliogullari Koksalmis",
        "Bulent Soykan",
        "Laura J. Brattain",
        "Hsin-Hsiung Huang"
      ],
      "abstract": "Alzheimer's Disease (AD) is marked by significant inter-individual\nvariability in its progression, complicating accurate prognosis and\npersonalized care planning. This heterogeneity underscores the critical need\nfor predictive models capable of forecasting patient-specific disease\ntrajectories. Artificial Intelligence (AI) offers powerful tools to address\nthis challenge by analyzing complex, multi-modal, and longitudinal patient\ndata. This paper provides a comprehensive survey of AI methodologies applied to\npersonalized AD progression prediction. We review key approaches including\nstate-space models for capturing temporal dynamics, deep learning techniques\nlike Recurrent Neural Networks for sequence modeling, Graph Neural Networks\n(GNNs) for leveraging network structures, and the emerging concept of AI-driven\ndigital twins for individualized simulation. Recognizing that data limitations\noften impede progress, we examine common challenges such as high\ndimensionality, missing data, and dataset imbalance. We further discuss\nAI-driven mitigation strategies, with a specific focus on synthetic data\ngeneration using Variational Autoencoders (VAEs) and Generative Adversarial\nNetworks (GANs) to augment and balance datasets. The survey synthesizes the\nstrengths and limitations of current approaches, emphasizing the trend towards\nmultimodal integration and the persistent need for model interpretability and\ngeneralizability. Finally, we identify critical open challenges, including\nrobust external validation, clinical integration, and ethical considerations,\nand outline promising future research directions such as hybrid models, causal\ninference, and federated learning. This review aims to consolidate current\nknowledge and guide future efforts in developing clinically relevant AI tools\nfor personalized AD prognostication.",
      "pdf_url": "http://arxiv.org/pdf/2504.21189v1",
      "arxiv_url": "http://arxiv.org/abs/2504.21189v1",
      "published": "2025-04-29",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.ET"
      ]
    },
    {
      "title": "A Hamiltonian Higher-Order Elasticity Framework for Dynamic Diagnostics(2HOED)",
      "authors": [
        "Ngueuleweu Tiwang Gildas"
      ],
      "abstract": "Machine learning detects patterns, block chain guarantees trust and\nimmutability, and modern causal inference identifies directional linkages, yet\nnone alone exposes the full energetic anatomy of complex systems; the\nHamiltonian Higher Order Elasticity Dynamics(2HOED) framework bridges these\ngaps. Grounded in classical mechanics but extended to Economics order\nelasticity terms, 2HOED represents economic, social, and physical systems as\nenergy-based Hamiltonians whose position, velocity, acceleration, and jerk of\nelasticity jointly determine systemic power, Inertia, policy sensitivity, and\nmarginal responses. Because the formalism is scaling free and coordinate\nagnostic, it transfers seamlessly from financial markets to climate science,\nfrom supply chain logistics to epidemiology, thus any discipline in which\nadaptation and shocks coexist. By embedding standard econometric variables\ninside a Hamiltonian, 2HOED enriches conventional economic analysis with\nrigorous diagnostics of resilience, tipping points, and feedback loops,\nrevealing failure modes invisible to linear models. Wavelet spectra, phase\nspace attractors, and topological persistence diagrams derived from 2HOED\nexpose multistage policy leverage that machine learning detects only\nempirically and block chain secures only after the fact. For economists,\nphysicians and other scientists, the method opens a new causal energetic\nchannel linking biological or mechanical elasticity to macro level outcomes.\nPortable, interpretable, and computationally light, 2HOED turns data streams\ninto dynamical energy maps, empowering decision makers to anticipate crises,\ndesign adaptive policies, and engineer robust systems delivering the predictive\npunch of AI with the explanatory clarity of physics.",
      "pdf_url": "http://arxiv.org/pdf/2504.21062v1",
      "arxiv_url": "http://arxiv.org/abs/2504.21062v1",
      "published": "2025-04-29",
      "categories": [
        "cs.LG",
        "econ.GN",
        "q-fin.EC"
      ]
    },
    {
      "title": "Inference with few treated units",
      "authors": [
        "Luis Alvarez",
        "Bruno Ferman",
        "Kaspar WÃ¼thrich"
      ],
      "abstract": "In many causal inference applications, only one or a few units (or clusters\nof units) are treated. An important challenge in such settings is that standard\ninference methods that rely on asymptotic theory may be unreliable, even when\nthe total number of units is large. This survey reviews and categorizes\ninference methods that are designed to accommodate few treated units,\nconsidering both cross-sectional and panel data methods. We discuss trade-offs\nand connections between different approaches. In doing so, we propose slight\nmodifications to improve the finite-sample validity of some methods, and we\nalso provide theoretical justifications for existing heuristic approaches that\nhave been proposed in the literature.",
      "pdf_url": "http://arxiv.org/pdf/2504.19841v1",
      "arxiv_url": "http://arxiv.org/abs/2504.19841v1",
      "published": "2025-04-28",
      "categories": [
        "econ.EM"
      ]
    },
    {
      "title": "Selective randomization inference for subgroup effects with continuous biomarkers",
      "authors": [
        "Zijun Gao"
      ],
      "abstract": "Randomization tests are a popular method for testing causal effects in\nclinical trials with finite-sample validity. In the presence of heterogeneous\ntreatment effects, it is often of interest to select a subgroup that benefits\nfrom the treatment, frequently by choosing a cutoff for a continuous biomarker.\nHowever, selecting the cutoff and testing the effect on the same data may fail\nto control the type I error. To address this, we propose using \"self-contained\"\nmethods for selecting biomarker-based subgroups (cutoffs) and applying\nconditioning to construct valid randomization tests for the subgroup effect.\nCompared to sample-splitting-based randomization tests, our proposal is fully\ndeterministic, uses the entire selected subgroup for inference, and is thus\nmore powerful. Moreover, we demonstrate scenarios where our procedure achieves\npower comparable to a randomization test with oracle knowledge of the\nbenefiting subgroup. In addition, our procedure is as computationally efficient\nas standard randomization tests. Empirically, we illustrate the effectiveness\nof our method on simulated datasets and the German Breast Cancer Study.",
      "pdf_url": "http://arxiv.org/pdf/2504.19380v1",
      "arxiv_url": "http://arxiv.org/abs/2504.19380v1",
      "published": "2025-04-27",
      "categories": [
        "stat.ME",
        "stat.AP"
      ]
    },
    {
      "title": "ReLU integral probability metric and its applications",
      "authors": [
        "Yuha Park",
        "Kunwoong Kim",
        "Insung Kong",
        "Yongdai Kim"
      ],
      "abstract": "We propose a parametric integral probability metric (IPM) to measure the\ndiscrepancy between two probability measures. The proposed IPM leverages a\nspecific parametric family of discriminators, such as single-node neural\nnetworks with ReLU activation, to effectively distinguish between\ndistributions, making it applicable in high-dimensional settings. By optimizing\nover the parameters of the chosen discriminator class, the proposed IPM\ndemonstrates that its estimators have good convergence rates and can serve as a\nsurrogate for other IPMs that use smooth nonparametric discriminator classes.\nWe present an efficient algorithm for practical computation, offering a simple\nimplementation and requiring fewer hyperparameters. Furthermore, we explore its\napplications in various tasks, such as covariate balancing for causal inference\nand fair representation learning. Across such diverse applications, we\ndemonstrate that the proposed IPM provides strong theoretical guarantees, and\nempirical experiments show that it achieves comparable or even superior\nperformance to other methods.",
      "pdf_url": "http://arxiv.org/pdf/2504.18897v1",
      "arxiv_url": "http://arxiv.org/abs/2504.18897v1",
      "published": "2025-04-26",
      "categories": [
        "stat.ML",
        "cs.LG",
        "stat.ME"
      ]
    },
    {
      "title": "Causality-Driven Neural Network Repair: Challenges and Opportunities",
      "authors": [
        "Fatemeh Vares",
        "Brittany Johnson"
      ],
      "abstract": "Deep Neural Networks (DNNs) often rely on statistical correlations rather\nthan causal reasoning, limiting their robustness and interpretability. While\ntesting methods can identify failures, effective debugging and repair remain\nchallenging. This paper explores causal inference as an approach primarily for\nDNN repair, leveraging causal debugging, counterfactual analysis, and\nstructural causal models (SCMs) to identify and correct failures. We discuss in\nwhat ways these techniques support fairness, adversarial robustness, and\nbackdoor mitigation by providing targeted interventions. Finally, we discuss\nkey challenges, including scalability, generalization, and computational\nefficiency, and outline future directions for integrating causality-driven\ninterventions to enhance DNN reliability.",
      "pdf_url": "http://arxiv.org/pdf/2504.17946v1",
      "arxiv_url": "http://arxiv.org/abs/2504.17946v1",
      "published": "2025-04-24",
      "categories": [
        "cs.LG",
        "D.2.2; I.2.6"
      ]
    },
    {
      "title": "Fast Autoregressive Models for Continuous Latent Generation",
      "authors": [
        "Tiankai Hang",
        "Jianmin Bao",
        "Fangyun Wei",
        "Dong Chen"
      ],
      "abstract": "Autoregressive models have demonstrated remarkable success in sequential data\ngeneration, particularly in NLP, but their extension to continuous-domain image\ngeneration presents significant challenges. Recent work, the masked\nautoregressive model (MAR), bypasses quantization by modeling per-token\ndistributions in continuous spaces using a diffusion head but suffers from slow\ninference due to the high computational cost of the iterative denoising\nprocess. To address this, we propose the Fast AutoRegressive model (FAR), a\nnovel framework that replaces MAR's diffusion head with a lightweight shortcut\nhead, enabling efficient few-step sampling while preserving autoregressive\nprinciples. Additionally, FAR seamlessly integrates with causal Transformers,\nextending them from discrete to continuous token generation without requiring\narchitectural modifications. Experiments demonstrate that FAR achieves\n$2.3\\times$ faster inference than MAR while maintaining competitive FID and IS\nscores. This work establishes the first efficient autoregressive paradigm for\nhigh-fidelity continuous-space image generation, bridging the critical gap\nbetween quality and scalability in visual autoregressive modeling.",
      "pdf_url": "http://arxiv.org/pdf/2504.18391v1",
      "arxiv_url": "http://arxiv.org/abs/2504.18391v1",
      "published": "2025-04-24",
      "categories": [
        "cs.CV",
        "cs.LG"
      ]
    }
  ]
}