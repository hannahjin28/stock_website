{
  "last_updated": "2025-10-18T00:47:08.545591",
  "papers": [
    {
      "title": "Efficient Parallel Samplers for Recurrent-Depth Models and Their Connection to Diffusion Language Models",
      "authors": [
        "Jonas Geiping",
        "Xinyu Yang",
        "Guinan Su"
      ],
      "abstract": "Language models with recurrent depth, also referred to as universal or looped\nwhen considering transformers, are defined by the capacity to increase their\ncomputation through the repetition of layers. Recent efforts in pretraining\nhave demonstrated that these architectures can scale to modern language\nmodeling tasks while exhibiting advantages in reasoning tasks. In this work, we\nexamine the relationship between recurrent-depth models and diffusion language\nmodels. Building on their similarities, we develop a new diffusion forcing\nsampler for these models to accelerate generation. The sampler advances by\ndecoding new tokens at every forward pass of the model, while the latent states\nof these tokens can be further refined in parallel through recurrence.\nTheoretically, generation with our sampler is strictly more expressive than the\nbaseline autoregressive generation using the same time budget on modern\nhardware. Moreover, this sampler, based on principles from diffusion\nliterature, can be directly applied to existing 3.5B recurrent-depth\ntransformers without any tuning, leading to up to a 5x speedup. Consequently,\nour findings not only provide an efficient mechanism for parallelizing the\nextra computation in recurrent-depth models at inference, but also suggest that\nsuch models can be naturally viewed as strong continuous, though causal,\ndiffusion language models.",
      "pdf_url": "http://arxiv.org/pdf/2510.14961v1",
      "arxiv_url": "http://arxiv.org/abs/2510.14961v1",
      "published": "2025-10-16",
      "categories": [
        "cs.LG",
        "cs.CL"
      ]
    },
    {
      "title": "Local Causal Discovery for Statistically Efficient Causal Inference",
      "authors": [
        "Mátyás Schubert",
        "Tom Claassen",
        "Sara Magliacane"
      ],
      "abstract": "Causal discovery methods can identify valid adjustment sets for causal effect\nestimation for a pair of target variables, even when the underlying causal\ngraph is unknown. Global causal discovery methods focus on learning the whole\ncausal graph and therefore enable the recovery of optimal adjustment sets,\ni.e., sets with the lowest asymptotic variance, but they quickly become\ncomputationally prohibitive as the number of variables grows. Local causal\ndiscovery methods offer a more scalable alternative by focusing on the local\nneighborhood of the target variables, but are restricted to statistically\nsuboptimal adjustment sets. In this work, we propose Local Optimal Adjustments\nDiscovery (LOAD), a sound and complete causal discovery approach that combines\nthe computational efficiency of local methods with the statistical optimality\nof global methods. First, LOAD identifies the causal relation between the\ntargets and tests if the causal effect is identifiable by using only local\ninformation. If it is identifiable, it then finds the optimal adjustment set by\nleveraging local causal discovery to infer the mediators and their parents.\nOtherwise, it returns the locally valid parent adjustment sets based on the\nlearned local structure. In our experiments on synthetic and realistic data\nLOAD outperforms global methods in scalability, while providing more accurate\neffect estimation than local methods.",
      "pdf_url": "http://arxiv.org/pdf/2510.14582v1",
      "arxiv_url": "http://arxiv.org/abs/2510.14582v1",
      "published": "2025-10-16",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "CLEAR: Causal Learning Framework For Robust Histopathology Tumor Detection Under Out-Of-Distribution Shifts",
      "authors": [
        "Kieu-Anh Truong Thi",
        "Huy-Hieu Pham",
        "Duc-Trong Le"
      ],
      "abstract": "Domain shift in histopathology, often caused by differences in acquisition\nprocesses or data sources, poses a major challenge to the generalization\nability of deep learning models. Existing methods primarily rely on modeling\nstatistical correlations by aligning feature distributions or introducing\nstatistical variation, yet they often overlook causal relationships. In this\nwork, we propose a novel causal-inference-based framework that leverages\nsemantic features while mitigating the impact of confounders. Our method\nimplements the front-door principle by designing transformation strategies that\nexplicitly incorporate mediators and observed tissue slides. We validate our\nmethod on the CAMELYON17 dataset and a private histopathology dataset,\ndemonstrating consistent performance gains across unseen domains. As a result,\nour approach achieved up to a 7% improvement in both the CAMELYON17 dataset and\nthe private histopathology dataset, outperforming existing baselines. These\nresults highlight the potential of causal inference as a powerful tool for\naddressing domain shift in histopathology image analysis.",
      "pdf_url": "http://arxiv.org/pdf/2510.14273v1",
      "arxiv_url": "http://arxiv.org/abs/2510.14273v1",
      "published": "2025-10-16",
      "categories": [
        "cs.CV"
      ]
    },
    {
      "title": "Exploratory Causal Inference in SAEnce",
      "authors": [
        "Tommaso Mencattini",
        "Riccardo Cadei",
        "Francesco Locatello"
      ],
      "abstract": "Randomized Controlled Trials are one of the pillars of science; nevertheless,\nthey rely on hand-crafted hypotheses and expensive analysis. Such constraints\nprevent causal effect estimation at scale, potentially anchoring on popular yet\nincomplete hypotheses. We propose to discover the unknown effects of a\ntreatment directly from data. For this, we turn unstructured data from a trial\ninto meaningful representations via pretrained foundation models and interpret\nthem via a sparse autoencoder. However, discovering significant causal effects\nat the neural level is not trivial due to multiple-testing issues and effects\nentanglement. To address these challenges, we introduce Neural Effect Search, a\nnovel recursive procedure solving both issues by progressive stratification.\nAfter assessing the robustness of our algorithm on semi-synthetic experiments,\nwe showcase, in the context of experimental ecology, the first successful\nunsupervised causal effect identification on a real-world scientific trial.",
      "pdf_url": "http://arxiv.org/pdf/2510.14073v1",
      "arxiv_url": "http://arxiv.org/abs/2510.14073v1",
      "published": "2025-10-15",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Joint modeling and inference of multiple-subject high-dimensional sparse vector autoregressive models",
      "authors": [
        "Younghoon Kim",
        "Zachary F. Fisher",
        "Vladas Pipiras"
      ],
      "abstract": "The multiple-subject vector autoregression (multi-VAR) model captures\nheterogeneous network Granger causality across subjects by decomposing\nindividual sparse VAR transition matrices into commonly shared and\nsubject-unique paths. The model has been applied to characterize hidden shared\nand unique paths among subjects and has demonstrated performance compared to\nmethods commonly used in psychology and neuroscience. Despite this innovation,\nthe model suffers from using a weighted median for identifying the common\neffects, leading to statistical inefficiency as the convergence rates of the\ncommon and unique paths are determined by the least sparse subject and the\nsmallest sample size across all subjects. We propose a new identifiability\ncondition for the multi-VAR model based on a communication-efficient data\nintegration framework. We show that this approach achieves convergence rates\ntailored to each subject's sparsity level and sample size. Furthermore, we\ndevelop hypothesis tests to assess the nullity and homogeneity of individual\npaths, using Wald-type test statistics constructed from individual debiased\nestimators. A test for the significance of the common paths can also be derived\nthrough the framework. Simulation studies under various heterogeneity scenarios\nand a real data application demonstrate the performance of the proposed method\ncompared to existing benchmark across standard evaluation metrics.",
      "pdf_url": "http://arxiv.org/pdf/2510.14044v1",
      "arxiv_url": "http://arxiv.org/abs/2510.14044v1",
      "published": "2025-10-15",
      "categories": [
        "stat.ME"
      ]
    },
    {
      "title": "Do Large Language Models Show Biases in Causal Learning? Insights from Contingency Judgment",
      "authors": [
        "María Victoria Carro",
        "Denise Alejandra Mester",
        "Francisca Gauna Selasco",
        "Giovanni Franco Gabriel Marraffini",
        "Mario Alejandro Leiva",
        "Gerardo I. Simari",
        "María Vanina Martinez"
      ],
      "abstract": "Causal learning is the cognitive process of developing the capability of\nmaking causal inferences based on available information, often guided by\nnormative principles. This process is prone to errors and biases, such as the\nillusion of causality, in which people perceive a causal relationship between\ntwo variables despite lacking supporting evidence. This cognitive bias has been\nproposed to underlie many societal problems, including social prejudice,\nstereotype formation, misinformation, and superstitious thinking. In this work,\nwe examine whether large language models are prone to developing causal\nillusions when faced with a classic cognitive science paradigm: the contingency\njudgment task. To investigate this, we constructed a dataset of 1,000 null\ncontingency scenarios (in which the available information is not sufficient to\nestablish a causal relationship between variables) within medical contexts and\nprompted LLMs to evaluate the effectiveness of potential causes. Our findings\nshow that all evaluated models systematically inferred unwarranted causal\nrelationships, revealing a strong susceptibility to the illusion of causality.\nWhile there is ongoing debate about whether LLMs genuinely understand causality\nor merely reproduce causal language without true comprehension, our findings\nsupport the latter hypothesis and raise concerns about the use of language\nmodels in domains where accurate causal reasoning is essential for informed\ndecision-making.",
      "pdf_url": "http://arxiv.org/pdf/2510.13985v1",
      "arxiv_url": "http://arxiv.org/abs/2510.13985v1",
      "published": "2025-10-15",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Doing Things with Words: Rethinking Theory of Mind Simulation in Large Language Models",
      "authors": [
        "Agnese Lombardi",
        "Alessandro Lenci"
      ],
      "abstract": "Language is fundamental to human cooperation, facilitating not only the\nexchange of information but also the coordination of actions through shared\ninterpretations of situational contexts. This study explores whether the\nGenerative Agent-Based Model (GABM) Concordia can effectively model Theory of\nMind (ToM) within simulated real-world environments. Specifically, we assess\nwhether this framework successfully simulates ToM abilities and whether GPT-4\ncan perform tasks by making genuine inferences from social context, rather than\nrelying on linguistic memorization. Our findings reveal a critical limitation:\nGPT-4 frequently fails to select actions based on belief attribution,\nsuggesting that apparent ToM-like abilities observed in previous studies may\nstem from shallow statistical associations rather than true reasoning.\nAdditionally, the model struggles to generate coherent causal effects from\nagent actions, exposing difficulties in processing complex social interactions.\nThese results challenge current statements about emergent ToM-like capabilities\nin LLMs and highlight the need for more rigorous, action-based evaluation\nframeworks.",
      "pdf_url": "http://arxiv.org/pdf/2510.13395v1",
      "arxiv_url": "http://arxiv.org/abs/2510.13395v1",
      "published": "2025-10-15",
      "categories": [
        "cs.CL"
      ]
    },
    {
      "title": "Novel Class Discovery for Point Cloud Segmentation via Joint Learning of Causal Representation and Reasoning",
      "authors": [
        "Yang Li",
        "Aming Wu",
        "Zihao Zhang",
        "Yahong Han"
      ],
      "abstract": "In this paper, we focus on Novel Class Discovery for Point Cloud Segmentation\n(3D-NCD), aiming to learn a model that can segment unlabeled (novel) 3D classes\nusing only the supervision from labeled (base) 3D classes. The key to this task\nis to setup the exact correlations between the point representations and their\nbase class labels, as well as the representation correlations between the\npoints from base and novel classes. A coarse or statistical correlation\nlearning may lead to the confusion in novel class inference. lf we impose a\ncausal relationship as a strong correlated constraint upon the learning\nprocess, the essential point cloud representations that accurately correspond\nto the classes should be uncovered. To this end, we introduce a structural\ncausal model (SCM) to re-formalize the 3D-NCD problem and propose a new method,\ni.e., Joint Learning of Causal Representation and Reasoning. Specifically, we\nfirst analyze hidden confounders in the base class representations and the\ncausal relationships between the base and novel classes through SCM. We devise\na causal representation prototype that eliminates confounders to capture the\ncausal representations of base classes. A graph structure is then used to model\nthe causal relationships between the base classes' causal representation\nprototypes and the novel class prototypes, enabling causal reasoning from base\nto novel classes. Extensive experiments and visualization results on 3D and 2D\nNCD semantic segmentation demonstrate the superiorities of our method.",
      "pdf_url": "http://arxiv.org/pdf/2510.13307v1",
      "arxiv_url": "http://arxiv.org/abs/2510.13307v1",
      "published": "2025-10-15",
      "categories": [
        "cs.CV"
      ]
    },
    {
      "title": "DeepCausalMMM: A Deep Learning Framework for Marketing Mix Modeling with Causal Inference",
      "authors": [
        "Aditya Puttaparthi Tirumala"
      ],
      "abstract": "Marketing Mix Modeling (MMM) is a statistical technique used to estimate the\nimpact of marketing activities on business outcomes such as sales, revenue, or\ncustomer visits. Traditional MMM approaches often rely on linear regression or\nBayesian hierarchical models that assume independence between marketing\nchannels and struggle to capture complex temporal dynamics and non-linear\nsaturation effects [@Hanssens2005; @Ng2021Bayesian].\n  DeepCausalMMM is a Python package that addresses these limitations by\ncombining deep learning, causal inference, and advanced marketing science. The\npackage uses Gated Recurrent Units (GRUs) to automatically learn temporal\npatterns such as adstock (carryover effects) and lag, while simultaneously\nlearning statistical dependencies and potential causal structures between\nmarketing channels through Directed Acyclic Graph (DAG) learning\n[@Zheng2018NOTEARS; @Gong2024CausalMMM]. Additionally, it implements Hill\nequation-based saturation curves to model diminishing returns and optimize\nbudget allocation.\n  Key innovations include: (1) a data-driven design where hyperparameters and\ntransformations (e.g., adstock decay, saturation curves) are learned or\nestimated from data with sensible defaults, rather than requiring fixed\nheuristics or manual specification, (2) multi-region modeling with both shared\nand region-specific parameters, (3) robust statistical methods including Huber\nloss and advanced regularization, (4) comprehensive response curve analysis for\nunderstanding channel saturation, and (5) an extensive visualization suite with\n14+ interactive dashboards for business insights.",
      "pdf_url": "http://arxiv.org/pdf/2510.13087v1",
      "arxiv_url": "http://arxiv.org/abs/2510.13087v1",
      "published": "2025-10-15",
      "categories": [
        "cs.LG",
        "stat.ME",
        "stat.ML",
        "62P20, 62M10, 68T05",
        "D.2.2; I.2.6; G.3"
      ]
    },
    {
      "title": "Towards xApp Conflict Evaluation with Explainable Machine Learning and Causal Inference in O-RAN",
      "authors": [
        "Pragya Sharma",
        "Shihua Sun",
        "Shachi Deshpande",
        "Angelos Stavrou",
        "Haining Wang"
      ],
      "abstract": "The Open Radio Access Network (O-RAN) architecture enables a flexible,\nvendor-neutral deployment of 5G networks by disaggregating base station\ncomponents and supporting third-party xApps for near real-time RAN control.\nHowever, the concurrent operation of multiple xApps can lead to conflicting\ncontrol actions, which may cause network performance degradation. In this work,\nwe propose a framework for xApp conflict management that combines explainable\nmachine learning and causal inference to evaluate the causal relationships\nbetween RAN Control Parameters (RCPs) and Key Performance Indicators (KPIs). We\nuse model explainability tools such as SHAP to identify RCPs that jointly\naffect the same KPI, signaling potential conflicts, and represent these\ninteractions as a causal Directed Acyclic Graph (DAG). We then estimate the\ncausal impact of each of these RCPs on their associated KPIs using metrics such\nas Average Treatment Effect (ATE) and Conditional Average Treatment Effect\n(CATE). This approach offers network operators guided insights into identifying\nconflicts and quantifying their impacts, enabling more informed and effective\nconflict resolution strategies across diverse xApp deployments.",
      "pdf_url": "http://arxiv.org/pdf/2510.13031v1",
      "arxiv_url": "http://arxiv.org/abs/2510.13031v1",
      "published": "2025-10-14",
      "categories": [
        "cs.NI",
        "cs.SY",
        "eess.SY"
      ]
    }
  ]
}