{
  "last_updated": "2026-02-02T01:12:04.573838",
  "papers": [
    {
      "title": "Causal Autoregressive Diffusion Language Model",
      "authors": [
        "Junhao Ruan",
        "Bei Li",
        "Yongjing Yin",
        "Pengcheng Huang",
        "Xin Chen",
        "Jingang Wang",
        "Xunliang Cai",
        "Tong Xiao",
        "JingBo Zhu"
      ],
      "abstract": "In this work, we propose Causal Autoregressive Diffusion (CARD), a novel framework that unifies the training efficiency of ARMs with the high-throughput inference of diffusion models. CARD reformulates the diffusion process within a strictly causal attention mask, enabling dense, per-token supervision in a single forward pass. To address the optimization instability of causal diffusion, we introduce a soft-tailed masking schema to preserve local context and a context-aware reweighting mechanism derived from signal-to-noise principles. This design enables dynamic parallel decoding, where the model leverages KV-caching to adaptively generate variable-length token sequences based on confidence. Empirically, CARD outperforms existing discrete diffusion baselines while reducing training latency by 3 $\\times$ compared to block diffusion methods. Our results demonstrate that CARD achieves ARM-level data efficiency while unlocking the latency benefits of parallel generation, establishing a robust paradigm for next-generation efficient LLMs.",
      "pdf_url": "https://arxiv.org/pdf/2601.22031v1",
      "arxiv_url": "http://arxiv.org/abs/2601.22031v1",
      "published": "2026-01-29",
      "categories": [
        "cs.CL"
      ]
    },
    {
      "title": "Causal World Modeling for Robot Control",
      "authors": [
        "Lin Li",
        "Qihang Zhang",
        "Yiming Luo",
        "Shuai Yang",
        "Ruilin Wang",
        "Fei Han",
        "Mingrui Yu",
        "Zelin Gao",
        "Nan Xue",
        "Xing Zhu",
        "Yujun Shen",
        "Yinghao Xu"
      ],
      "abstract": "This work highlights that video world modeling, alongside vision-language pre-training, establishes a fresh and independent foundation for robot learning. Intuitively, video world models provide the ability to imagine the near future by understanding the causality between actions and visual dynamics. Inspired by this, we introduce LingBot-VA, an autoregressive diffusion framework that learns frame prediction and policy execution simultaneously. Our model features three carefully crafted designs: (1) a shared latent space, integrating vision and action tokens, driven by a Mixture-of-Transformers (MoT) architecture, (2) a closed-loop rollout mechanism, allowing for ongoing acquisition of environmental feedback with ground-truth observations, (3) an asynchronous inference pipeline, parallelizing action prediction and motor execution to support efficient control. We evaluate our model on both simulation benchmarks and real-world scenarios, where it shows significant promise in long-horizon manipulation, data efficiency in post-training, and strong generalizability to novel configurations. The code and model are made publicly available to facilitate the community.",
      "pdf_url": "https://arxiv.org/pdf/2601.21998v1",
      "arxiv_url": "http://arxiv.org/abs/2601.21998v1",
      "published": "2026-01-29",
      "categories": [
        "cs.CV",
        "cs.RO"
      ]
    },
    {
      "title": "FBS: Modeling Native Parallel Reading inside a Transformer",
      "authors": [
        "Tongxi Wang"
      ],
      "abstract": "Large language models (LLMs) excel across many tasks, yet inference is still dominated by strictly token-by-token autoregression. Existing acceleration methods largely patch this pipeline and miss core human-reading ingredients: content-adaptive foresight, chunk-structure-aware compute allocation, and train--test consistency for preview/skimming. We propose the \\textbf{Fovea-Block-Skip Transformer} (FBS), which injects a causal, trainable loop into Transformers via Parafovea-Attention Window (PAW), Chunk-Head (CH), and Skip-Gate (SG). Across diverse benchmarks, FBS improves the quality-efficiency trade-off without increasing parameters, and ablations show the three modules are complementary.",
      "pdf_url": "https://arxiv.org/pdf/2601.21708v1",
      "arxiv_url": "http://arxiv.org/abs/2601.21708v1",
      "published": "2026-01-29",
      "categories": [
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Differential Dynamic Causal Nets: Model Construction, Identification and Group Comparisons",
      "authors": [
        "Kang You",
        "Gary Green",
        "Jian Zhang"
      ],
      "abstract": "Pathophysiolpgical modelling of brain systems from microscale to macroscale remains difficult in group comparisons partly because of the infeasibility of modelling the interactions of thousands of neurons at the scales involved. Here, to address the challenge, we present a novel approach to construct differential causal networks directly from electroencephalogram (EEG) data. The proposed network is based on conditionally coupled neuronal circuits which describe the average behaviour of interacting neuron populations that contribute to observed EEG data. In the network, each node represents a parameterised local neural system while directed edges stand for node-wise connections with transmission parameters. The network is hierarchically structured in the sense that node and edge parameters are varying in subjects but follow a mixed-effects model. A novel evolutionary optimisation algorithm for parameter inference in the proposed method is developed using a loss function derived from Chen-Fliess expansions of stochastic differential equations. The method is demonstrated by application to the fitting of coupled Jansen-Rit local models. The performance of the proposed method is evaluated on both synthetic and real EEG data. In the real EEG data analysis, we track changes in the parameters that characterise dynamic causality within brains that demonstrate epileptic activity. We show evidence of network functional disruptions, due to imbalance of excitatory-inhibitory interneurons and altered epileptic brain connectivity, before and during seizure periods.",
      "pdf_url": "https://arxiv.org/pdf/2601.21478v1",
      "arxiv_url": "http://arxiv.org/abs/2601.21478v1",
      "published": "2026-01-29",
      "categories": [
        "q-bio.NC",
        "stat.AP",
        "stat.ML"
      ]
    },
    {
      "title": "Modeling Endogenous Logic: Causal Neuro-Symbolic Reasoning Model for Explainable Multi-Behavior Recommendation",
      "authors": [
        "Yuzhe Chen",
        "Jie Cao",
        "Youquan Wang",
        "Haicheng Tao",
        "Darko B. Vukovic",
        "Jia Wu"
      ],
      "abstract": "Existing multi-behavior recommendations tend to prioritize performance at the expense of explainability, while current explainable methods suffer from limited generalizability due to their reliance on external information. Neuro-Symbolic integration offers a promising avenue for explainability by combining neural networks with symbolic logic rule reasoning. Concurrently, we posit that user behavior chains inherently embody an endogenous logic suitable for explicit reasoning. However, these observational multiple behaviors are plagued by confounders, causing models to learn spurious correlations. By incorporating causal inference into this Neuro-Symbolic framework, we propose a novel Causal Neuro-Symbolic Reasoning model for Explainable Multi-Behavior Recommendation (CNRE). CNRE operationalizes the endogenous logic by simulating a human-like decision-making process. Specifically, CNRE first employs hierarchical preference propagation to capture heterogeneous cross-behavior dependencies. Subsequently, it models the endogenous logic rule implicit in the user's behavior chain based on preference strength, and adaptively dispatches to the corresponding neural-logic reasoning path (e.g., conjunction, disjunction). This process generates an explainable causal mediator that approximates an ideal state isolated from confounding effects. Extensive experiments on three large-scale datasets demonstrate CNRE's significant superiority over state-of-the-art baselines, offering multi-level explainability from model design and decision process to recommendation results.",
      "pdf_url": "https://arxiv.org/pdf/2601.21335v1",
      "arxiv_url": "http://arxiv.org/abs/2601.21335v1",
      "published": "2026-01-29",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Data-Driven Generation of Neutron Star Equations of State Using Variational Autoencoders",
      "authors": [
        "Alex Ross",
        "Tianqi Zhao",
        "Sanjay Reddy"
      ],
      "abstract": "We develop a machine learning model based on a structured variational autoencoder (VAE) framework to reconstruct and generate neutron star (NS) equations of state (EOS). The VAE consists of an encoder network that maps high-dimensional EOS data into a lower-dimensional latent space and a decoder network that reconstructs the full EOS from the latent representation. The latent space includes supervised NS observables derived from the training EOS data, as well as latent random variables corresponding to additional unspecified EOS features learned automatically. Sampling the latent space enables the generation of new, causal, and stable EOS models that satisfy astronomical constraints on the supervised NS observables, while allowing Bayesian inference of the EOS incorporating additional multimessenger data, including gravitational waves from LIGO/Virgo and mass and radius measurements of pulsars. Based on a VAE trained on a Skyrme EOS dataset, we find that a latent space with two supervised NS observables, the maximum mass $(M_{\\max})$ and the canonical radius $(R_{1.4})$, together with one latent random variable controlling the EOS near the crust--core transition, can already reconstruct Skyrme EOSs with high fidelity, achieving mean absolute percentage errors of approximately $(0.15\\%)$ for $(M_{\\max})$ and $(R_{1.4})$ derived from the decoder-reconstructed EOS.",
      "pdf_url": "https://arxiv.org/pdf/2601.21231v1",
      "arxiv_url": "http://arxiv.org/abs/2601.21231v1",
      "published": "2026-01-29",
      "categories": [
        "astro-ph.HE",
        "astro-ph.IM",
        "astro-ph.SR",
        "cs.LG"
      ]
    },
    {
      "title": "Sycophantic Anchors: Localizing and Quantifying User Agreement in Reasoning Models",
      "authors": [
        "Jacek Duszenko"
      ],
      "abstract": "Reasoning models frequently agree with incorrect user suggestions -- a behavior known as sycophancy. However, it is unclear where in the reasoning trace this agreement originates and how strong the commitment is. To localize and quantify this behavior, we introduce \\emph{sycophantic anchors} -- sentences that causally lock models into user agreement. Analyzing over 10,000 counterfactual rollouts on a distilled reasoning model, we show that anchors can be reliably detected and quantified mid-inference. Linear probes distinguish sycophantic anchors with 84.6\\% balanced accuracy, while activation-based regressors predict the magnitude of the commitment ($R^2 = 0.74$). We further observe asymmetry where sycophantic anchors are significantly more distinguishable than correct reasoning anchors, and find that sycophancy builds gradually during reasoning, revealing a potential window for intervention. These results offer sentence-level mechanisms for localizing model misalignment mid-inference.",
      "pdf_url": "https://arxiv.org/pdf/2601.21183v1",
      "arxiv_url": "http://arxiv.org/abs/2601.21183v1",
      "published": "2026-01-29",
      "categories": [
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "MapPFN: Learning Causal Perturbation Maps in Context",
      "authors": [
        "Marvin Sextro",
        "Weronika Kłos",
        "Gabriel Dernbach"
      ],
      "abstract": "Planning effective interventions in biological systems requires treatment-effect models that adapt to unseen biological contexts by identifying their specific underlying mechanisms. Yet single-cell perturbation datasets span only a handful of biological contexts, and existing methods cannot leverage new interventional evidence at inference time to adapt beyond their training data. To meta-learn a perturbation effect estimator, we present MapPFN, a prior-data fitted network (PFN) pretrained on synthetic data generated from a prior over causal perturbations. Given a set of experiments, MapPFN uses in-context learning to predict post-perturbation distributions, without gradient-based optimization. Despite being pretrained on in silico gene knockouts alone, MapPFN identifies differentially expressed genes, matching the performance of models trained on real single-cell data. Our code and data are available at https://github.com/marvinsxtr/MapPFN.",
      "pdf_url": "https://arxiv.org/pdf/2601.21092v1",
      "arxiv_url": "http://arxiv.org/abs/2601.21092v1",
      "published": "2026-01-28",
      "categories": [
        "cs.LG"
      ]
    },
    {
      "title": "Causal Inference in Biomedical Imaging via Functional Linear Structural Equation Models",
      "authors": [
        "Ting Li",
        "Ethan Fan",
        "Tengfei Li",
        "Hongtu Zhu"
      ],
      "abstract": "Understanding the causal effects of organ-specific features from medical imaging on clinical outcomes is essential for biomedical research and patient care. We propose a novel Functional Linear Structural Equation Model (FLSEM) to capture the relationships among clinical outcomes, functional imaging exposures, and scalar covariates like genetics, sex, and age. Traditional methods struggle with the infinite-dimensional nature of exposures and complex covariates. Our FLSEM overcomes these challenges by establishing identifiable conditions using scalar instrumental variables. We develop the Functional Group Support Detection and Root Finding (FGS-DAR) algorithm for efficient variable selection, supported by rigorous theoretical guarantees, including selection consistency and accurate parameter estimation. We further propose a test statistic to test the nullity of the functional coefficient, establishing its null limit distribution. Our approach is validated through extensive simulations and applied to UK Biobank data, demonstrating robust performance in detecting causal relationships from medical imaging.",
      "pdf_url": "https://arxiv.org/pdf/2601.20610v1",
      "arxiv_url": "http://arxiv.org/abs/2601.20610v1",
      "published": "2026-01-28",
      "categories": [
        "stat.ME",
        "math.ST"
      ]
    },
    {
      "title": "Exact Graph Learning via Integer Programming",
      "authors": [
        "Lucas Kook",
        "Søren Wengel Mogensen"
      ],
      "abstract": "Learning the dependence structure among variables in complex systems is a central problem across medical, natural, and social sciences. These structures can be naturally represented by graphs, and the task of inferring such graphs from data is known as graph learning or as causal discovery if the graphs are given a causal interpretation. Existing approaches typically rely on restrictive assumptions about the data-generating process, employ greedy oracle algorithms, or solve approximate formulations of the graph learning problem. As a result, they are either sensitive to violations of central assumptions or fail to guarantee globally optimal solutions. We address these limitations by introducing a nonparametric graph learning framework based on nonparametric conditional independence testing and integer programming. We reformulate the graph learning problem as an integer-programming problem and prove that solving the integer-programming problem provides a globally optimal solution to the original graph learning problem. Our method leverages efficient encodings of graphical separation criteria, enabling the exact recovery of larger graphs than was previously feasible. We provide an implementation in the openly available R package 'glip' which supports learning (acyclic) directed (mixed) graphs and chain graphs. From the resulting output one can compute representations of the corresponding Markov equivalence classes or weak equivalence classes. Empirically, we demonstrate that our approach is faster than other existing exact graph learning procedures for a large fraction of instances and graphs of various sizes. GLIP also achieves state-of-the-art performance on simulated data and benchmark datasets across all aforementioned classes of graphs.",
      "pdf_url": "https://arxiv.org/pdf/2601.20589v1",
      "arxiv_url": "http://arxiv.org/abs/2601.20589v1",
      "published": "2026-01-28",
      "categories": [
        "stat.ME",
        "cs.LG"
      ]
    }
  ]
}