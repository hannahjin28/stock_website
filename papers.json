{
  "last_updated": "2025-03-12T00:47:37.895554",
  "papers": [
    {
      "title": "AR-Diffusion: Asynchronous Video Generation with Auto-Regressive Diffusion",
      "authors": [
        "Mingzhen Sun",
        "Weining Wang",
        "Gen Li",
        "Jiawei Liu",
        "Jiahui Sun",
        "Wanquan Feng",
        "Shanshan Lao",
        "SiYu Zhou",
        "Qian He",
        "Jing Liu"
      ],
      "abstract": "The task of video generation requires synthesizing visually realistic and\ntemporally coherent video frames. Existing methods primarily use asynchronous\nauto-regressive models or synchronous diffusion models to address this\nchallenge. However, asynchronous auto-regressive models often suffer from\ninconsistencies between training and inference, leading to issues such as error\naccumulation, while synchronous diffusion models are limited by their reliance\non rigid sequence length. To address these issues, we introduce Auto-Regressive\nDiffusion (AR-Diffusion), a novel model that combines the strengths of\nauto-regressive and diffusion models for flexible, asynchronous video\ngeneration. Specifically, our approach leverages diffusion to gradually corrupt\nvideo frames in both training and inference, reducing the discrepancy between\nthese phases. Inspired by auto-regressive generation, we incorporate a\nnon-decreasing constraint on the corruption timesteps of individual frames,\nensuring that earlier frames remain clearer than subsequent ones. This setup,\ntogether with temporal causal attention, enables flexible generation of videos\nwith varying lengths while preserving temporal coherence. In addition, we\ndesign two specialized timestep schedulers: the FoPP scheduler for balanced\ntimestep sampling during training, and the AD scheduler for flexible timestep\ndifferences during inference, supporting both synchronous and asynchronous\ngeneration. Extensive experiments demonstrate the superiority of our proposed\nmethod, which achieves competitive and state-of-the-art results across four\nchallenging benchmarks.",
      "pdf_url": "http://arxiv.org/pdf/2503.07418v1",
      "arxiv_url": "http://arxiv.org/abs/2503.07418v1",
      "published": "2025-03-10",
      "categories": [
        "cs.CV"
      ]
    },
    {
      "title": "NFIG: Autoregressive Image Generation with Next-Frequency Prediction",
      "authors": [
        "Zhihao Huang",
        "Xi Qiu",
        "Yukuo Ma",
        "Yifu Zhou",
        "Chi Zhang",
        "Xuelong Li"
      ],
      "abstract": "Autoregressive models have achieved promising results in natural language\nprocessing. However, for image generation tasks, they encounter substantial\nchallenges in effectively capturing long-range dependencies, managing\ncomputational costs, and most crucially, defining meaningful autoregressive\nsequences that reflect natural image hierarchies. To address these issues, we\npresent \\textbf{N}ext-\\textbf{F}requency \\textbf{I}mage \\textbf{G}eneration\n(\\textbf{NFIG}), a novel framework that decomposes the image generation process\ninto multiple frequency-guided stages. Our approach first generates\nlow-frequency components to establish global structure with fewer tokens, then\nprogressively adds higher-frequency details, following the natural spectral\nhierarchy of images. This principled autoregressive sequence not only improves\nthe quality of generated images by better capturing true causal relationships\nbetween image components, but also significantly reduces computational overhead\nduring inference. Extensive experiments demonstrate that NFIG achieves\nstate-of-the-art performance with fewer steps, offering a more efficient\nsolution for image generation, with 1.25$\\times$ speedup compared to VAR-d20\nwhile achieving better performance (FID: 2.81) on the ImageNet-256 benchmark.\nWe hope that our insight of incorporating frequency-domain knowledge to guide\nautoregressive sequence design will shed light on future research. We will make\nour code publicly available upon acceptance of the paper.",
      "pdf_url": "http://arxiv.org/pdf/2503.07076v1",
      "arxiv_url": "http://arxiv.org/abs/2503.07076v1",
      "published": "2025-03-10",
      "categories": [
        "cs.CV",
        "cs.AI",
        "68T07",
        "I.2.10; I.2.6"
      ]
    },
    {
      "title": "AXAI-CDSS : An Affective Explainable AI-Driven Clinical Decision Support System for Cannabis Use",
      "authors": [
        "Tongze Zhang",
        "Tammy Chung",
        "Anind Dey",
        "Sang Won Bae"
      ],
      "abstract": "As cannabis use has increased in recent years, researchers have come to rely\non sophisticated machine learning models to predict cannabis use behavior and\nits impact on health. However, many artificial intelligence (AI) models lack\ntransparency and interpretability due to their opaque nature, limiting their\ntrust and adoption in real-world medical applications, such as clinical\ndecision support systems (CDSS). To address this issue, this paper enhances\nalgorithm explainability underlying CDSS by integrating multiple Explainable\nArtificial Intelligence (XAI) methods and applying causal inference techniques\nto clarify the model' predictive decisions under various scenarios. By\nproviding deeper interpretability of the XAI outputs using Large Language\nModels (LLMs), we provide users with more personalized and accessible insights\nto overcome the challenges posed by AI's \"black box\" nature. Our system\ndynamically adjusts feedback based on user queries and emotional states,\ncombining text-based sentiment analysis with real-time facial emotion\nrecognition to ensure responses are empathetic, context-adaptive, and\nuser-centered. This approach bridges the gap between the learning demands of\ninterpretability and the need for intuitive understanding, enabling\nnon-technical users such as clinicians and clinical researchers to interact\neffectively with AI models.} Ultimately, this approach improves usability,\nenhances perceived trustworthiness, and increases the impact of CDSS in\nhealthcare applications.",
      "pdf_url": "http://arxiv.org/pdf/2503.06463v1",
      "arxiv_url": "http://arxiv.org/abs/2503.06463v1",
      "published": "2025-03-09",
      "categories": [
        "cs.HC"
      ]
    },
    {
      "title": "Causal Discovery and Inference towards Urban Elements and Associated Factors",
      "authors": [
        "Tao Feng",
        "Yunke Zhang",
        "Xiaochen Fan",
        "Huandong Wang",
        "Yong Li"
      ],
      "abstract": "To uncover the city's fundamental functioning mechanisms, it is important to\nacquire a deep understanding of complicated relationships among citizens,\nlocation, and mobility behaviors. Previous research studies have applied direct\ncorrelation analysis to investigate such relationships. Nevertheless, due to\nthe ubiquitous confounding effects, empirical correlation analysis may not\naccurately reflect underlying causal relationships among basic urban elements.\nIn this paper, we propose a novel urban causal computing framework to\ncomprehensively explore causalities and confounding effects among a variety of\nfactors across different types of urban elements. In particular, we design a\nreinforcement learning algorithm to discover the potential causal graph, which\ndepicts the causal relations between urban factors. The causal graph further\nserves as the guidance for estimating causal effects between pair-wise urban\nfactors by propensity score matching. After removing the confounding effects\nfrom correlations, we leverage significance levels of causal effects in\ndownstream urban mobility prediction tasks. Experimental studies on open-source\nurban datasets show that the discovered causal graph demonstrates a\nhierarchical structure, where citizens affect locations, and they both cause\nchanges in urban mobility behaviors. Experimental results in urban mobility\nprediction tasks further show that the proposed method can effectively reduce\nconfounding effects and enhance performance of urban computing tasks.",
      "pdf_url": "http://arxiv.org/pdf/2503.06395v1",
      "arxiv_url": "http://arxiv.org/abs/2503.06395v1",
      "published": "2025-03-09",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Integrating Chain-of-Thought for Multimodal Alignment: A Study on 3D Vision-Language Learning",
      "authors": [
        "Yanjun Chen",
        "Yirong Sun",
        "Xinghao Chen",
        "Jian Wang",
        "Xiaoyu Shen",
        "Wenjie Li",
        "Wei Zhang"
      ],
      "abstract": "Chain-of-Thought (CoT) reasoning has proven effective in natural language\ntasks but remains underexplored in multimodal alignment. This study\ninvestigates its integration into 3D vision-language learning by embedding\nstructured reasoning into alignment training. We introduce the 3D-CoT\nBenchmark, a dataset with hierarchical CoT annotations covering shape\nrecognition, functional inference, and causal reasoning. Through controlled\nexperiments, we compare CoT-structured and standard textual annotations across\nlarge reasoning models (LRMs) and large language models (LLMs). Our evaluation\nemploys a dual-layer framework assessing both intermediate reasoning and final\ninference quality. Extensive experiments demonstrate that CoT significantly\nimproves 3D semantic grounding, with LRMs leveraging CoT more effectively than\nLLMs. Furthermore, we highlight that annotation structure influences\nperformance-explicit reasoning markers aid LLMs, while unmarked CoT better\naligns with LRM inference patterns. Our analyses suggest that CoT is crucial\nfor enhancing multimodal reasoning, with implications beyond 3D tasks.",
      "pdf_url": "http://arxiv.org/pdf/2503.06232v1",
      "arxiv_url": "http://arxiv.org/abs/2503.06232v1",
      "published": "2025-03-08",
      "categories": [
        "cs.CL",
        "cs.CV"
      ]
    },
    {
      "title": "Black Box Causal Inference: Effect Estimation via Meta Prediction",
      "authors": [
        "Lucius E. J. Bynum",
        "Aahlad Manas Puli",
        "Diego Herrero-Quevedo",
        "Nhi Nguyen",
        "Carlos Fernandez-Granda",
        "Kyunghyun Cho",
        "Rajesh Ranganath"
      ],
      "abstract": "Causal inference and the estimation of causal effects plays a central role in\ndecision-making across many areas, including healthcare and economics.\nEstimating causal effects typically requires an estimator that is tailored to\neach problem of interest. But developing estimators can take significant effort\nfor even a single causal inference setting. For example, algorithms for\nregression-based estimators, propensity score methods, and doubly robust\nmethods were designed across several decades to handle causal estimation with\nobserved confounders. Similarly, several estimators have been developed to\nexploit instrumental variables (IVs), including two-stage least-squares (TSLS),\ncontrol functions, and the method-of-moments. In this work, we instead frame\ncausal inference as a dataset-level prediction problem, offloading algorithm\ndesign to the learning process. The approach we introduce, called black box\ncausal inference (BBCI), builds estimators in a black-box manner by learning to\npredict causal effects from sampled dataset-effect pairs. We demonstrate\naccurate estimation of average treatment effects (ATEs) and conditional average\ntreatment effects (CATEs) with BBCI across several causal inference problems\nwith known identification, including problems with less developed estimators.",
      "pdf_url": "http://arxiv.org/pdf/2503.05985v1",
      "arxiv_url": "http://arxiv.org/abs/2503.05985v1",
      "published": "2025-03-07",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.CO",
        "stat.ME",
        "stat.ML"
      ]
    },
    {
      "title": "Integration of aggregated data in causally interpretable meta-analysis by inverse weighting",
      "authors": [
        "Tat-Thang Vo",
        "Tran Trong Khoi Le",
        "Sivem Afach",
        "Stijn Vansteelandt"
      ],
      "abstract": "Obtaining causally interpretable meta-analysis results is challenging when\nthere are differences in the distribution of effect modifiers between eligible\ntrials. To overcome this, recent work on transportability methods has\nconsidered standardizing results of individual studies over the case-mix of a\ntarget population, prior to pooling them as in a classical random-effect\nmeta-analysis. One practical challenge, however, is that case-mix\nstandardization often requires individual participant data (IPD) on outcome,\ntreatments and case-mix characteristics to be fully accessible in every\neligible study, along with IPD case-mix characteristics for a random sample\nfrom the target population. In this paper, we aim to develop novel strategies\nto integrate aggregated-level data from eligible trials with non-accessible IPD\ninto a causal meta-analysis, by extending moment-based methods frequently used\nfor population-adjusted indirect comparison in health technology assessment.\nSince valid inference for these moment-based methods by M-estimation theory\nrequires additional aggregated data that are often unavailable in practice,\ncomputational methods to address this concern are also developed. We assess the\nfinite-sample performance of the proposed approaches by simulated data, and\nthen apply these on real-world clinical data to investigate the effectiveness\nof risankizumab versus ustekinumab among patients with moderate to severe\npsoriasis.",
      "pdf_url": "http://arxiv.org/pdf/2503.05634v1",
      "arxiv_url": "http://arxiv.org/abs/2503.05634v1",
      "published": "2025-03-07",
      "categories": [
        "stat.ME"
      ]
    },
    {
      "title": "Riemannian Metric Learning: Closer to You than You Imagine",
      "authors": [
        "Samuel Gruffaz",
        "Josua Sassen"
      ],
      "abstract": "Riemannian metric learning is an emerging field in machine learning,\nunlocking new ways to encode complex data structures beyond traditional\ndistance metric learning. While classical approaches rely on global distances\nin Euclidean space, they often fall short in capturing intrinsic data geometry.\nEnter Riemannian metric learning: a powerful generalization that leverages\ndifferential geometry to model the data according to their underlying\nRiemannian manifold. This approach has demonstrated remarkable success across\ndiverse domains, from causal inference and optimal transport to generative\nmodeling and representation learning. In this review, we bridge the gap between\nclassical metric learning and Riemannian geometry, providing a structured and\naccessible overview of key methods, applications, and recent advances. We argue\nthat Riemannian metric learning is not merely a technical refinement but a\nfundamental shift in how we think about data representations. Thus, this review\nshould serve as a valuable resource for researchers and practitioners\ninterested in exploring Riemannian metric learning and convince them that it is\ncloser to them than they might imagine-both in theory and in practice.",
      "pdf_url": "http://arxiv.org/pdf/2503.05321v1",
      "arxiv_url": "http://arxiv.org/abs/2503.05321v1",
      "published": "2025-03-07",
      "categories": [
        "stat.ML",
        "cs.LG",
        "math.DG",
        "68T05 (Primary), 58D17 (Secondary)",
        "I.2.6"
      ]
    },
    {
      "title": "Frequency Autoregressive Image Generation with Continuous Tokens",
      "authors": [
        "Hu Yu",
        "Hao Luo",
        "Hangjie Yuan",
        "Yu Rong",
        "Feng Zhao"
      ],
      "abstract": "Autoregressive (AR) models for image generation typically adopt a two-stage\nparadigm of vector quantization and raster-scan ``next-token prediction\",\ninspired by its great success in language modeling. However, due to the huge\nmodality gap, image autoregressive models may require a systematic reevaluation\nfrom two perspectives: tokenizer format and regression direction. In this\npaper, we introduce the frequency progressive autoregressive (\\textbf{FAR})\nparadigm and instantiate FAR with the continuous tokenizer. Specifically, we\nidentify spectral dependency as the desirable regression direction for FAR,\nwherein higher-frequency components build upon the lower one to progressively\nconstruct a complete image. This design seamlessly fits the causality\nrequirement for autoregressive models and preserves the unique spatial locality\nof image data. Besides, we delve into the integration of FAR and the continuous\ntokenizer, introducing a series of techniques to address optimization\nchallenges and improve the efficiency of training and inference processes. We\ndemonstrate the efficacy of FAR through comprehensive experiments on the\nImageNet dataset and verify its potential on text-to-image generation.",
      "pdf_url": "http://arxiv.org/pdf/2503.05305v1",
      "arxiv_url": "http://arxiv.org/abs/2503.05305v1",
      "published": "2025-03-07",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "Kernel-based estimators for functional causal effects",
      "authors": [
        "Yordan P. Raykov",
        "Hengrui Luo",
        "Justin D. Strait",
        "Wasiur R. KhudaBukhsh"
      ],
      "abstract": "We propose causal effect estimators based on empirical Fr\\'{e}chet means and\noperator-valued kernels, tailored to functional data spaces. These methods\naddress the challenges of high-dimensionality, sequential ordering, and model\ncomplexity while preserving robustness to treatment misspecification. Using\nstructural assumptions, we obtain compact representations of potential\noutcomes, enabling scalable estimation of causal effects over time and across\ncovariates. We provide both theoretical, regarding the consistency of\nfunctional causal effects, as well as empirical comparison of a range of\nproposed causal effect estimators.\n  Applications to binary treatment settings with functional outcomes illustrate\nthe framework's utility in biomedical monitoring, where outcomes exhibit\ncomplex temporal dynamics. Our estimators accommodate scenarios with registered\ncovariates and outcomes, aligning them to the Fr\\'{e}chet means, as well as\ncases requiring higher-order representations to capture intricate\ncovariate-outcome interactions. These advancements extend causal inference to\ndynamic and non-linear domains, offering new tools for understanding complex\ntreatment effects in functional data settings.",
      "pdf_url": "http://arxiv.org/pdf/2503.05024v2",
      "arxiv_url": "http://arxiv.org/abs/2503.05024v2",
      "published": "2025-03-06",
      "categories": [
        "stat.ME",
        "cs.LG",
        "math.ST",
        "stat.TH",
        "62G05",
        "G.3"
      ]
    }
  ]
}