{
  "last_updated": "2025-11-30T01:00:51.309750",
  "papers": [
    {
      "title": "Uniform inference for kernel instrumental variable regression",
      "authors": [
        "Marvin Lob",
        "Rahul Singh",
        "Suhas Vijaykumar"
      ],
      "abstract": "Instrumental variable regression is a foundational tool for causal analysis across the social and biomedical sciences. Recent advances use kernel methods to estimate nonparametric causal relationships, with general data types, while retaining a simple closed-form expression. Empirical researchers ultimately need reliable inference on causal estimates; however, uniform confidence sets for the method remain unavailable. To fill this gap, we develop valid and sharp confidence sets for kernel instrumental variable regression, allowing general nonlinearities and data types. Computationally, our bootstrap procedure requires only a single run of the kernel instrumental variable regression estimator. Theoretically, it relies on the same key assumptions. Overall, we provide a practical procedure for inference that substantially increases the value of kernel methods for causal analysis.",
      "pdf_url": "https://arxiv.org/pdf/2511.21603v1",
      "arxiv_url": "http://arxiv.org/abs/2511.21603v1",
      "published": "2025-11-26",
      "categories": [
        "math.ST"
      ]
    },
    {
      "title": "A Sensitivity Analysis Framework for Causal Inference Under Interference",
      "authors": [
        "Matvey Ortyashov",
        "AmirEmad Ghassami"
      ],
      "abstract": "In many applications of causal inference, the treatment received by one unit may influence the outcome of another, a phenomenon referred to as interference. Although there are several frameworks for conducting causal inference in the presence of interference, practitioners often lack the data necessary to adjust for its effects. In this paper, we propose a weighting-based sensitivity analysis framework that can be used to assess the systematic bias arising from ignoring interference. Unlike most of the existing literature, we allow for the presence of unmeasured confounding, and show that the combination of interference and unmeasured confounding is a notable challenge to causal inference. We also study a third factor contributing to systematic bias: lack of transportability. Our framework enables practitioners to assess the impact of these three issues simultaneously through several easily interpretable sensitivity parameters that can reflect a wide range of intuitions about the data.",
      "pdf_url": "https://arxiv.org/pdf/2511.21534v1",
      "arxiv_url": "http://arxiv.org/abs/2511.21534v1",
      "published": "2025-11-26",
      "categories": [
        "stat.ME"
      ]
    },
    {
      "title": "Causal Inference: A Tale of Three Frameworks",
      "authors": [
        "Linbo Wang",
        "Thomas Richardson",
        "James Robins"
      ],
      "abstract": "Causal inference is a central goal across many scientific disciplines. Over the past several decades, three major frameworks have emerged to formalize causal questions and guide their analysis: the potential outcomes framework, structural equation models, and directed acyclic graphs. Although these frameworks differ in language, assumptions, and philosophical orientation, they often lead to compatible or complementary insights. This paper provides a comparative introduction to the three frameworks, clarifying their connections, highlighting their distinct strengths and limitations, and illustrating how they can be used together in practice. The discussion is aimed at researchers and graduate students with some background in statistics or causal inference who are seeking a conceptual foundation for applying causal methods across a range of substantive domains.",
      "pdf_url": "https://arxiv.org/pdf/2511.21516v1",
      "arxiv_url": "http://arxiv.org/abs/2511.21516v1",
      "published": "2025-11-26",
      "categories": [
        "math.ST"
      ]
    },
    {
      "title": "SpatialBench: Benchmarking Multimodal Large Language Models for Spatial Cognition",
      "authors": [
        "Peiran Xu",
        "Sudong Wang",
        "Yao Zhu",
        "Jianing Li",
        "Yunjian Zhang"
      ],
      "abstract": "Spatial cognition is fundamental to real-world multimodal intelligence, allowing models to effectively interact with the physical environment. While multimodal large language models (MLLMs) have made significant strides, existing benchmarks often oversimplify spatial cognition, reducing it to a single-dimensional metric, which fails to capture the hierarchical structure and interdependence of spatial abilities. To address this gap, we propose a hierarchical spatial cognition framework that decomposes spatial intelligence into five progressively complex levels from basic observation to high-level planning. Building upon this taxonomy, we construct SpatialBench, a large-scale, fine-grained benchmark covering 15 tasks aligned with these cognitive levels. To provide a unified evaluation across heterogeneous tasks, we further introduce a high-level capability-oriented metric that reliably assesses a model's overall spatial reasoning ability. Extensive experiments over massive MLLMs reveal distinct performance stratification across cognitive levels: models exhibit strong perceptual grounding yet remain limited in symbolic reasoning, causal inference, and planning. Additional human tests demonstrate that humans perform selective, goal-directed abstraction, while MLLMs tend to over-attend to surface details without coherent spatial intent. Our work establishes the first systematic framework for measuring hierarchical spatial cognition in MLLMs, laying the foundation for future spatially intelligent systems.",
      "pdf_url": "https://arxiv.org/pdf/2511.21471v1",
      "arxiv_url": "http://arxiv.org/abs/2511.21471v1",
      "published": "2025-11-26",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Two-stage Estimation for Causal Inference Involving a Semi-continuous Exposure",
      "authors": [
        "Xiaoya Wang",
        "Richard J. Cook",
        "Yeying Zhu",
        "Tugba Akkaya-Hocagil",
        "R. Colin Carter",
        "Sandra W. Jacobson",
        "Joseph L. Jacobson",
        "Louise M. Ryan"
      ],
      "abstract": "Methods for causal inference are well developed for binary and continuous exposures, but in many settings, the exposure has a substantial mass at zero-such exposures are called semi-continuous. We propose a general causal framework for such semi-continuous exposures, together with a novel two-stage estimation strategy. A two-part propensity structure is introduced for the semi-continuous exposure, with one component for exposure status (exposed vs unexposed) and another for the exposure level among those exposed, and incorporates both into a marginal structural model that disentangles the effects of exposure status and dose. The two-stage procedure sequentially targets the causal dose-response among exposed individuals and the causal effect of exposure status at a reference dose, allowing flexibility in the choice of propensity score methods in the second stage. We establish consistency and asymptotic normality for the resulting estimators, and characterise their limiting values under misspecification of the propensity score models. Simulation studies evaluate finite sample performance and robustness, and an application to a study of prenatal alcohol exposure and child cognition demonstrates how the proposed methods can be used to address a range of scientific questions about both exposure status and exposure intensity.",
      "pdf_url": "https://arxiv.org/pdf/2511.20985v1",
      "arxiv_url": "http://arxiv.org/abs/2511.20985v1",
      "published": "2025-11-26",
      "categories": [
        "stat.ME"
      ]
    },
    {
      "title": "Differentially Private Fisher Randomization Tests for Binary Outcomes",
      "authors": [
        "Qingyang Sun",
        "Jerome P. Reiter"
      ],
      "abstract": "Across many disciplines, causal inference often relies on randomized experiments with binary outcomes. In such experiments, the Fisher randomization test provides exact, assumption-free tests for causal effects. Sometimes the outcomes are sensitive and must be kept confidential, for example, when they comprise physical or mental health measurements. Releasing test statistics or p-values computed with the confidential outcomes can leak information about the individuals in the study. Those responsible for sharing the analysis results may wish to bound this information leakage, which they can do by ensuring the released outputs satisfy differential privacy. In this article, we develop and compare several differentially private versions of the Fisher randomization test for binary outcomes. Specifically, we consider direct perturbation approaches that inject calibrated noise into test statistics or p-values, as well as a mechanism-aware, Bayesian denoising framework that explicitly models the privacy mechanism. We further develop decision-making procedures under privacy constraints, including a Bayes risk-optimal rule and a frequentist-calibrated significance test. Through theoretical results, simulation studies, and an application to the ADAPTABLE clinical trial, we demonstrate that our methods can achieve valid and interpretable causal inference while ensuring the differential privacy guarantee.",
      "pdf_url": "https://arxiv.org/pdf/2511.20884v1",
      "arxiv_url": "http://arxiv.org/abs/2511.20884v1",
      "published": "2025-11-25",
      "categories": [
        "stat.ME"
      ]
    },
    {
      "title": "Physics Steering: Causal Control of Cross-Domain Concepts in a Physics Foundation Model",
      "authors": [
        "Rio Alexa Fear",
        "Payel Mukhopadhyay",
        "Michael McCabe",
        "Alberto Bietti",
        "Miles Cranmer"
      ],
      "abstract": "Recent advances in mechanistic interpretability have revealed that large language models (LLMs) develop internal representations corresponding not only to concrete entities but also distinct, human-understandable abstract concepts and behaviour. Moreover, these hidden features can be directly manipulated to steer model behaviour. However, it remains an open question whether this phenomenon is unique to models trained on inherently structured data (ie. language, images) or if it is a general property of foundation models. In this work, we investigate the internal representations of a large physics-focused foundation model. Inspired by recent work identifying single directions in activation space for complex behaviours in LLMs, we extract activation vectors from the model during forward passes over simulation datasets for different physical regimes. We then compute \"delta\" representations between the two regimes. These delta tensors act as concept directions in activation space, encoding specific physical features. By injecting these concept directions back into the model during inference, we can steer its predictions, demonstrating causal control over physical behaviours, such as inducing or removing some particular physical feature from a simulation. These results suggest that scientific foundation models learn generalised representations of physical principles. They do not merely rely on superficial correlations and patterns in the simulations. Our findings open new avenues for understanding and controlling scientific foundation models and has implications for AI-enabled scientific discovery.",
      "pdf_url": "https://arxiv.org/pdf/2511.20798v1",
      "arxiv_url": "http://arxiv.org/abs/2511.20798v1",
      "published": "2025-11-25",
      "categories": [
        "cs.LG",
        "cs.AI",
        "physics.comp-ph"
      ]
    },
    {
      "title": "Spatio-Temporal Hierarchical Causal Models",
      "authors": [
        "Xintong Li",
        "Haoran Zhang",
        "Xiao Zhou"
      ],
      "abstract": "The abundance of fine-grained spatio-temporal data, such as traffic sensor networks, offers vast opportunities for scientific discovery. However, inferring causal relationships from such observational data remains challenging, particularly due to unobserved confounders that are specific to units (e.g., geographical locations) yet influence outcomes over time. Most existing methods for spatio-temporal causal inference assume that all confounders are observed, an assumption that is often violated in practice. In this paper, we introduce Spatio-Temporal Hierarchical Causal Models (ST-HCMs), a novel graphical framework that extends hierarchical causal modeling to the spatio-temporal domain. At the core of our approach is the Spatio-Temporal Collapse Theorem, which shows that a complex ST-HCM converges to a simpler flat causal model as the amount of subunit data increases. This theoretical result enables a general procedure for causal identification, allowing ST-HCMs to recover causal effects even in the presence of unobserved, time-invariant unit-level confounders, a scenario where standard non-hierarchical models fail. We validate the effectiveness of our framework on both synthetic and real-world datasets, demonstrating its potential for robust causal inference in complex dynamic systems.",
      "pdf_url": "https://arxiv.org/pdf/2511.20558v1",
      "arxiv_url": "http://arxiv.org/abs/2511.20558v1",
      "published": "2025-11-25",
      "categories": [
        "stat.ML",
        "cs.LG"
      ]
    },
    {
      "title": "Block Cascading: Training Free Acceleration of Block-Causal Video Models",
      "authors": [
        "Hmrishav Bandyopadhyay",
        "Nikhil Pinnaparaju",
        "Rahim Entezari",
        "Jim Scott",
        "Yi-Zhe Song",
        "Varun Jampani"
      ],
      "abstract": "Block-causal video generation faces a stark speed-quality trade-off: small 1.3B models manage only 16 FPS while large 14B models crawl at 4.5 FPS, forcing users to choose between responsiveness and quality. Block Cascading significantly mitigates this trade-off through training-free parallelization. Our key insight: future video blocks do not need fully denoised current blocks to begin generation. By starting block generation with partially denoised context from predecessors, we transform sequential pipelines into parallel cascades where multiple blocks denoise simultaneously. With 5 GPUs exploiting temporal parallelism, we achieve ~2x acceleration across all model scales: 1.3B models accelerate from 16 to 30 FPS, 14B models from 4.5 to 12.5 FPS. Beyond inference speed, Block Cascading eliminates overhead from KV-recaching (of ~200ms) during context switches for interactive generation. Extensive evaluations validated against multiple block-causal pipelines demonstrate no significant loss in generation quality when switching from block-causal to Block Cascading pipelines for inference. Project Page: https://hmrishavbandy.github.io/block_cascading_page/",
      "pdf_url": "https://arxiv.org/pdf/2511.20426v1",
      "arxiv_url": "http://arxiv.org/abs/2511.20426v1",
      "published": "2025-11-25",
      "categories": [
        "cs.CV",
        "cs.AI"
      ]
    },
    {
      "title": "CounterVQA: Evaluating and Improving Counterfactual Reasoning in Vision-Language Models for Video Understanding",
      "authors": [
        "Yuefei Chen",
        "Jiang Liu",
        "Xiaodong Lin",
        "Ruixiang Tang"
      ],
      "abstract": "Vision Language Models (VLMs) have recently shown significant advancements in video understanding, especially in feature alignment, event reasoning, and instruction-following tasks. However, their capability for counterfactual reasoning, inferring alternative outcomes under hypothetical conditions, remains underexplored. This capability is essential for robust video understanding, as it requires identifying underlying causal structures and reasoning about unobserved possibilities, rather than merely recognizing observed patterns. To systematically evaluate this capability, we introduce CounterVQA, a video-based benchmark featuring three progressive difficulty levels that assess different aspects of counterfactual reasoning. Through comprehensive evaluation of both state-of-the-art open-source and closed-source models, we uncover a substantial performance gap: while these models achieve reasonable accuracy on simple counterfactual questions, performance degrades significantly on complex multi-hop causal chains. To address these limitations, we develop a post-training method, CFGPT, that enhances a model's visual counterfactual reasoning ability by distilling its counterfactual reasoning capability from the language modality, yielding consistent improvements across all CounterVQA difficulty levels. Dataset and code will be further released.",
      "pdf_url": "https://arxiv.org/pdf/2511.19923v1",
      "arxiv_url": "http://arxiv.org/abs/2511.19923v1",
      "published": "2025-11-25",
      "categories": [
        "cs.CV",
        "cs.CL"
      ]
    }
  ]
}