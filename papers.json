{
  "last_updated": "2025-09-04T00:48:17.171391",
  "papers": [
    {
      "title": "Considerations for Estimating Causal Effects of Informatively Timed Treatments",
      "authors": [
        "Arman Oganisian"
      ],
      "abstract": "Epidemiological studies are often concerned with estimating causal effects of\na sequence of treatment decisions on survival outcomes. In many settings,\ntreatment decisions do not occur at fixed, pre-specified followup times.\nRather, timing varies across subjects in ways that may be informative of\nsubsequent treatment decisions and potential outcomes. Awareness of the issue\nand its potential solutions is lacking in the literature, which motivate this\nwork. Here, we formalize the issue of informative timing, problems associated\nwith ignoring it, and show how g-methods can be used to analyze sequential\ntreatments that are informatively timed. As we describe, in such settings, the\nwaiting times between successive treatment decisions may be properly viewed as\na time-varying confounders. Using synthetic examples, we illustrate how\ng-methods that do not adjust for these waiting times may be biased and how\nadjustment can be done in scenarios where patients may die or be censored in\nbetween treatments. We draw connections between adjustment and identification\nwith discrete-time versus continuous-time models. Finally, we provide\nimplementation guidance and examples using publicly available software. Our\nconcluding message is that 1) considering timing is important for valid\ninference and 2) correcting for informative timing can be done with g-methods\nthat adjust for waiting times between treatments as time-varying confounders.",
      "pdf_url": "http://arxiv.org/pdf/2508.21804v1",
      "arxiv_url": "http://arxiv.org/abs/2508.21804v1",
      "published": "2025-08-29",
      "categories": [
        "stat.ME",
        "cs.LG"
      ]
    },
    {
      "title": "Orientability of Causal Relations in Time Series using Summary Causal Graphs and Faithful Distributions",
      "authors": [
        "Timoth√©e Loranchet",
        "Charles K. Assaad"
      ],
      "abstract": "Understanding causal relations between temporal variables is a central\nchallenge in time series analysis, particularly when the full causal structure\nis unknown. Even when the full causal structure cannot be fully specified,\nexperts often succeed in providing a high-level abstraction of the causal\ngraph, known as a summary causal graph, which captures the main causal\nrelations between different time series while abstracting away micro-level\ndetails. In this work, we present conditions that guarantee the orientability\nof micro-level edges between temporal variables given the background knowledge\nencoded in a summary causal graph and assuming having access to a faithful and\ncausally sufficient distribution with respect to the true unknown graph. Our\nresults provide theoretical guarantees for edge orientation at the micro-level,\neven in the presence of cycles or bidirected edges at the macro-level. These\nfindings offer practical guidance for leveraging SCGs to inform causal\ndiscovery in complex temporal systems and highlight the value of incorporating\nexpert knowledge to improve causal inference from observational time series\ndata.",
      "pdf_url": "http://arxiv.org/pdf/2508.21742v1",
      "arxiv_url": "http://arxiv.org/abs/2508.21742v1",
      "published": "2025-08-29",
      "categories": [
        "cs.AI",
        "stat.ME"
      ]
    },
    {
      "title": "Inferring Effects of Major Events through Discontinuity Forecasting of Population Anxiety",
      "authors": [
        "Siddharth Mangalik",
        "Ojas Deshpande",
        "Adithya V. Ganesan",
        "Sean A. P. Clouston",
        "H. Andrew Schwartz"
      ],
      "abstract": "Estimating community-specific mental health effects of local events is vital\nfor public health policy. While forecasting mental health scores alone offers\nlimited insights into the impact of events on community well-being,\nquasi-experimental designs like the Longitudinal Regression Discontinuity\nDesign (LRDD) from econometrics help researchers derive more effects that are\nmore likely to be causal from observational data. LRDDs aim to extrapolate the\nsize of changes in an outcome (e.g. a discontinuity in running scores for\nanxiety) due to a time-specific event. Here, we propose adapting LRDDs beyond\ntraditional forecasting into a statistical learning framework whereby future\ndiscontinuities (i.e. time-specific shifts) and changes in slope (i.e. linear\ntrajectories) are estimated given a location's history of the score, dynamic\ncovariates (other running assessments), and exogenous variables (static\nrepresentations). Applying our framework to predict discontinuities in the\nanxiety of US counties from COVID-19 events, we found the task was difficult\nbut more achievable as the sophistication of models was increased, with the\nbest results coming from integrating exogenous and dynamic covariates. Our\napproach shows strong improvement ($r=+.46$ for discontinuity and $r = +.65$\nfor slope) over traditional static community representations. Discontinuity\nforecasting raises new possibilities for estimating the idiosyncratic effects\nof potential future or hypothetical events on specific communities.",
      "pdf_url": "http://arxiv.org/pdf/2508.21722v1",
      "arxiv_url": "http://arxiv.org/abs/2508.21722v1",
      "published": "2025-08-29",
      "categories": [
        "cs.LG"
      ]
    },
    {
      "title": "Treatment effects at the margin: Everyone is marginal",
      "authors": [
        "Haotian Deng"
      ],
      "abstract": "This paper develops a framework for identifying treatment effects when a\npolicy simultaneously alters both the incentive to participate and the outcome\nof interest -- such as hiring decisions and wages in response to employment\nsubsidies; or working decisions and wages in response to job trainings. This\nframework was inspired by my PhD project on a Belgian reform that subsidised\nfirst-time hiring, inducing entry by marginal firms yet meanwhile changing the\nwages they pay. Standard methods addressing selection-into-treatment concepts\n(like Heckman selection equations and local average treatment effects), or\nbefore-after comparisons (including simple DiD or RDD), cannot isolate effects\nat this shifting margin where treatment defines who is observed. I introduce\nmarginality-weighted estimands that recover causal effects among policy-induced\nentrants, offering a policy-relevant alternative in settings with endogenous\nselection. This method can thus be applied widely to understanding the economic\nimpacts of public programmes, especially in fields largely relying on\nreduced-form causal inference estimation (e.g. labour economics, development\neconomics, health economics).",
      "pdf_url": "http://arxiv.org/pdf/2508.21583v1",
      "arxiv_url": "http://arxiv.org/abs/2508.21583v1",
      "published": "2025-08-29",
      "categories": [
        "econ.EM",
        "stat.ME"
      ]
    },
    {
      "title": "ORCA: ORchestrating Causal Agent",
      "authors": [
        "Joanie Hayoun Chung",
        "Chaemyung Lim",
        "Sumin Lee",
        "Songseong Kim",
        "Sungbin Lim"
      ],
      "abstract": "Causal inference is essential for decision-making science while the\ncomplexity of the data analysis workflow, ranging from data wrangling to causal\nanalysis, increases substantially as the scale of data grows in complicated\nbusiness environments. Especially, the execution of the workflow in relational\ndatabases by non-experts can result in repetitive bottlenecks which impede\ntimely and responsible business insights. To address this challenge, we propose\nORCA (Orchestrating Causal Agent), an LLM agentic system that can automate\nroutine workflows in RDBMS while preserving expert oversight via human-AI\ninteractions. ORCA orchestrates the full data analysis pipeline: interpreting\nnatural language queries, navigating tables from DB servers, generating proper\nSQL codes, preprocessing data, and configuring modeling processes using causal\ninference libraries. Domain experts still can control the automation through\niterative interactions with ORCA, enabling robust data-driven decision making\nwith less technical expertise in statistical computing. Empirical evaluations\non benchmark and synthetic e-commerce datasets demonstrate competitive\nperformance of ORCA in table understanding, query generation, and cause-effect\nestimation -- achieving over $7\\times$ improvement in estimating average\ntreatment compared to GPT-4o mini.",
      "pdf_url": "http://arxiv.org/pdf/2508.21304v2",
      "arxiv_url": "http://arxiv.org/abs/2508.21304v2",
      "published": "2025-08-29",
      "categories": [
        "cs.DB",
        "cs.MA"
      ]
    },
    {
      "title": "CogVLA: Cognition-Aligned Vision-Language-Action Model via Instruction-Driven Routing & Sparsification",
      "authors": [
        "Wei Li",
        "Renshan Zhang",
        "Rui Shao",
        "Jie He",
        "Liqiang Nie"
      ],
      "abstract": "Recent Vision-Language-Action (VLA) models built on pre-trained\nVision-Language Models (VLMs) require extensive post-training, resulting in\nhigh computational overhead that limits scalability and deployment.We propose\nCogVLA, a Cognition-Aligned Vision-Language-Action framework that leverages\ninstruction-driven routing and sparsification to improve both efficiency and\nperformance. CogVLA draws inspiration from human multimodal coordination and\nintroduces a 3-stage progressive architecture. 1) Encoder-FiLM based\nAggregation Routing (EFA-Routing) injects instruction information into the\nvision encoder to selectively aggregate and compress dual-stream visual tokens,\nforming a instruction-aware latent representation. 2) Building upon this\ncompact visual encoding, LLM-FiLM based Pruning Routing (LFP-Routing)\nintroduces action intent into the language model by pruning\ninstruction-irrelevant visually grounded tokens, thereby achieving token-level\nsparsity. 3) To ensure that compressed perception inputs can still support\naccurate and coherent action generation, we introduce V-L-A Coupled Attention\n(CAtten), which combines causal vision-language attention with bidirectional\naction parallel decoding. Extensive experiments on the LIBERO benchmark and\nreal-world robotic tasks demonstrate that CogVLA achieves state-of-the-art\nperformance with success rates of 97.4% and 70.0%, respectively, while reducing\ntraining costs by 2.5-fold and decreasing inference latency by 2.8-fold\ncompared to OpenVLA. CogVLA is open-sourced and publicly available at\nhttps://github.com/JiuTian-VL/CogVLA.",
      "pdf_url": "http://arxiv.org/pdf/2508.21046v1",
      "arxiv_url": "http://arxiv.org/abs/2508.21046v1",
      "published": "2025-08-28",
      "categories": [
        "cs.CV",
        "cs.RO"
      ]
    },
    {
      "title": "ChainReaction! Structured Approach with Causal Chains as Intermediate Representations for Improved and Explainable Causal Video Question Answering",
      "authors": [
        "Paritosh Parmar",
        "Eric Peh",
        "Basura Fernando"
      ],
      "abstract": "Existing Causal-Why Video Question Answering (VideoQA) models often struggle\nwith higher-order reasoning, relying on opaque, monolithic pipelines that\nentangle video understanding, causal inference, and answer generation. These\nblack-box approaches offer limited interpretability and tend to depend on\nshallow heuristics. We propose a novel, modular framework that explicitly\ndecouples causal reasoning from answer generation, introducing natural language\ncausal chains as interpretable intermediate representations. Inspired by human\ncognitive models, these structured cause-effect sequences bridge low-level\nvideo content with high-level causal reasoning, enabling transparent and\nlogically coherent inference. Our two-stage architecture comprises a Causal\nChain Extractor (CCE) that generates causal chains from video-question pairs,\nand a Causal Chain-Driven Answerer (CCDA) that produces answers grounded in\nthese chains. To address the lack of annotated reasoning traces, we introduce a\nscalable method for generating high-quality causal chains from existing\ndatasets using large language models. We also propose CauCo, a new evaluation\nmetric for causality-oriented captioning. Experiments on three large-scale\nbenchmarks demonstrate that our approach not only outperforms state-of-the-art\nmodels, but also yields substantial gains in explainability, user trust, and\ngeneralization -- positioning the CCE as a reusable causal reasoning engine\nacross diverse domains. Project page:\nhttps://paritoshparmar.github.io/chainreaction/",
      "pdf_url": "http://arxiv.org/pdf/2508.21010v1",
      "arxiv_url": "http://arxiv.org/abs/2508.21010v1",
      "published": "2025-08-28",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.HC",
        "cs.LG"
      ]
    },
    {
      "title": "When Is Causal Inference Possible? A Statistical Test for Unmeasured Confounding",
      "authors": [
        "Muye Liu",
        "Jun Xie"
      ],
      "abstract": "This paper clarifies a fundamental difference between causal inference and\ntraditional statistical inference by formalizing a mathematical distinction\nbetween their respective parameters. We connect two major approaches to causal\ninference, the potential outcomes framework and causal structure graphs, which\nare typically studied separately. While the unconfoundedness assumption in the\npotential outcomes framework cannot be assessed from an observational dataset\nalone, causal structure graphs help explain when causal effects are\nidentifiable through graphical models. We propose a statistical test to assess\nthe unconfoundedness assumption, equivalent to the absence of unmeasured\nconfounding, by comparing two datasets: a randomized controlled trial and an\nobservational study. The test controls the Type I error probability, and we\nanalyze its power under linear models. Our approach provides a practical method\nto evaluate when real-world data are suitable for causal inference.",
      "pdf_url": "http://arxiv.org/pdf/2508.20366v1",
      "arxiv_url": "http://arxiv.org/abs/2508.20366v1",
      "published": "2025-08-28",
      "categories": [
        "stat.ME"
      ]
    },
    {
      "title": "Dynamic Synthetic Controls vs. Panel-Aware Double Machine Learning for Geo-Level Marketing Impact Estimation",
      "authors": [
        "Sang Su Lee",
        "Vineeth Loganathan",
        "Vijay Raghavan"
      ],
      "abstract": "Accurately quantifying geo-level marketing lift in two-sided marketplaces is\nchallenging: the Synthetic Control Method (SCM) often exhibits high power yet\nsystematically under-estimates effect size, while panel-style Double Machine\nLearning (DML) is seldom benchmarked against SCM. We build an open, fully\ndocumented simulator that mimics a typical large-scale geo roll-out: N_unit\nregional markets are tracked for T_pre weeks before launch and for a further\nT_post-week campaign window, allowing all key parameters to be varied by the\nuser and probe both families under five stylized stress tests: 1) curved\nbaseline trends, 2) heterogeneous response lags, 3) treated-biased shocks, 4) a\nnon-linear outcome link, and 5) a drifting control group trend.\n  Seven estimators are evaluated: three standard Augmented SCM (ASC) variants\nand four panel-DML flavors (TWFE, CRE/Mundlak, first-difference, and\nwithin-group). Across 100 replications per scenario, ASC models consistently\ndemonstrate severe bias and near-zero coverage in challenging scenarios\ninvolving nonlinearities or external shocks. By contrast, panel-DML variants\ndramatically reduce this bias and restore nominal 95%-CI coverage, proving far\nmore robust.\n  The results indicate that while ASC provides a simple baseline, it is\nunreliable in common, complex situations. We therefore propose a\n'diagnose-first' framework where practitioners first identify the primary\nbusiness challenge (e.g., nonlinear trends, response lags) and then select the\nspecific DML model best suited for that scenario, providing a more robust and\nreliable blueprint for analyzing geo-experiments.",
      "pdf_url": "http://arxiv.org/pdf/2508.20335v1",
      "arxiv_url": "http://arxiv.org/abs/2508.20335v1",
      "published": "2025-08-28",
      "categories": [
        "cs.LG"
      ]
    },
    {
      "title": "Stochastic Gradients under Nuisances",
      "authors": [
        "Facheng Yu",
        "Ronak Mehta",
        "Alex Luedtke",
        "Zaid Harchaoui"
      ],
      "abstract": "Stochastic gradient optimization is the dominant learning paradigm for a\nvariety of scenarios, from classical supervised learning to modern\nself-supervised learning. We consider stochastic gradient algorithms for\nlearning problems whose objectives rely on unknown nuisance parameters, and\nestablish non-asymptotic convergence guarantees. Our results show that, while\nthe presence of a nuisance can alter the optimum and upset the optimization\ntrajectory, the classical stochastic gradient algorithm may still converge\nunder appropriate conditions, such as Neyman orthogonality. Moreover, even when\nNeyman orthogonality is not satisfied, we show that an algorithm variant with\napproximately orthogonalized updates (with an approximately orthogonalized\ngradient oracle) may achieve similar convergence rates. Examples from\northogonal statistical learning/double machine learning and causal inference\nare discussed.",
      "pdf_url": "http://arxiv.org/pdf/2508.20326v1",
      "arxiv_url": "http://arxiv.org/abs/2508.20326v1",
      "published": "2025-08-28",
      "categories": [
        "stat.ML",
        "cs.LG",
        "math.OC"
      ]
    }
  ]
}