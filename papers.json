{
  "last_updated": "2025-08-05T01:01:00.384358",
  "papers": [
    {
      "title": "Harnessing the Power of Interleaving and Counterfactual Evaluation for Airbnb Search Ranking",
      "authors": [
        "Qing Zhang",
        "Alex Deng",
        "Michelle Du",
        "Huiji Gao",
        "Liwei He",
        "Sanjeev Katariya"
      ],
      "abstract": "Evaluation plays a crucial role in the development of ranking algorithms on\nsearch and recommender systems. It enables online platforms to create\nuser-friendly features that drive commercial success in a steady and effective\nmanner. The online environment is particularly conducive to applying causal\ninference techniques, such as randomized controlled experiments (known as A/B\ntest), which are often more challenging to implement in fields like medicine\nand public policy. However, businesses face unique challenges when it comes to\neffective A/B test. Specifically, achieving sufficient statistical power for\nconversion-based metrics can be time-consuming, especially for significant\npurchases like booking accommodations. While offline evaluations are quicker\nand more cost-effective, they often lack accuracy and are inadequate for\nselecting candidates for A/B test. To address these challenges, we developed\ninterleaving and counterfactual evaluation methods to facilitate rapid online\nassessments for identifying the most promising candidates for A/B tests. Our\napproach not only increased the sensitivity of experiments by a factor of up to\n100 (depending on the approach and metrics) compared to traditional A/B testing\nbut also streamlined the experimental process. The practical insights gained\nfrom usage in production can also benefit organizations with similar interests.",
      "pdf_url": "http://arxiv.org/pdf/2508.00751v1",
      "arxiv_url": "http://arxiv.org/abs/2508.00751v1",
      "published": "2025-08-01",
      "categories": [
        "cs.IR",
        "cs.AI",
        "H.3; G.3"
      ]
    },
    {
      "title": "Multi-Band Variable-Lag Granger Causality: A Unified Framework for Causal Time Series Inference across Frequencies",
      "authors": [
        "Chakattrai Sookkongwaree",
        "Tattep Lakmuang",
        "Chainarong Amornbunchornvej"
      ],
      "abstract": "Understanding causal relationships in time series is fundamental to many\ndomains, including neuroscience, economics, and behavioral science. Granger\ncausality is one of the well-known techniques for inferring causality in time\nseries. Typically, Granger causality frameworks have a strong fix-lag\nassumption between cause and effect, which is often unrealistic in complex\nsystems. While recent work on variable-lag Granger causality (VLGC) addresses\nthis limitation by allowing a cause to influence an effect with different time\nlags at each time point, it fails to account for the fact that causal\ninteractions may vary not only in time delay but also across frequency bands.\nFor example, in brain signals, alpha-band activity may influence another region\nwith a shorter delay than slower delta-band oscillations. In this work, we\nformalize Multi-Band Variable-Lag Granger Causality (MB-VLGC) and propose a\nnovel framework that generalizes traditional VLGC by explicitly modeling\nfrequency-dependent causal delays. We provide a formal definition of MB-VLGC,\ndemonstrate its theoretical soundness, and propose an efficient inference\npipeline. Extensive experiments across multiple domains demonstrate that our\nframework significantly outperforms existing methods on both synthetic and\nreal-world datasets, confirming its broad applicability to any type of time\nseries data. Code and datasets are publicly available.",
      "pdf_url": "http://arxiv.org/pdf/2508.00658v1",
      "arxiv_url": "http://arxiv.org/abs/2508.00658v1",
      "published": "2025-08-01",
      "categories": [
        "cs.AI",
        "cs.LG",
        "econ.EM",
        "stat.ME"
      ]
    },
    {
      "title": "The Missing Parts: Augmenting Fact Verification with Half-Truth Detection",
      "authors": [
        "Yixuan Tang",
        "Jincheng Wang",
        "Anthony K. H. Tung"
      ],
      "abstract": "Fact verification systems typically assess whether a claim is supported by\nretrieved evidence, assuming that truthfulness depends solely on what is\nstated. However, many real-world claims are half-truths, factually correct yet\nmisleading due to the omission of critical context. Existing models struggle\nwith such cases, as they are not designed to reason about what is left unsaid.\nWe introduce the task of half-truth detection, and propose PolitiFact-Hidden, a\nnew benchmark with 15k political claims annotated with sentence-level evidence\nalignment and inferred claim intent. To address this challenge, we present\nTRACER, a modular re-assessment framework that identifies omission-based\nmisinformation by aligning evidence, inferring implied intent, and estimating\nthe causal impact of hidden content. TRACER can be integrated into existing\nfact-checking pipelines and consistently improves performance across multiple\nstrong baselines. Notably, it boosts Half-True classification F1 by up to 16\npoints, highlighting the importance of modeling omissions for trustworthy fact\nverification.",
      "pdf_url": "http://arxiv.org/pdf/2508.00489v1",
      "arxiv_url": "http://arxiv.org/abs/2508.00489v1",
      "published": "2025-08-01",
      "categories": [
        "cs.CL"
      ]
    },
    {
      "title": "Learning to Optimize Feedback for One Million Students: Insights from Multi-Armed and Contextual Bandits in Large-Scale Online Tutoring",
      "authors": [
        "Robin Schmucker",
        "Nimish Pachapurkar",
        "Shanmuga Bala",
        "Miral Shah",
        "Tom Mitchell"
      ],
      "abstract": "We present an online tutoring system that learns to provide effective\nfeedback to students after they answer questions incorrectly. Using data from\none million students, the system learns which assistance action (e.g., one of\nmultiple hints) to provide for each question to optimize student learning.\nEmploying the multi-armed bandit (MAB) framework and offline policy evaluation,\nwe assess 43,000 assistance actions, and identify trade-offs between assistance\npolicies optimized for different student outcomes (e.g., response correctness,\nsession completion). We design an algorithm that for each question decides on a\nsuitable policy training objective to enhance students' immediate second\nattempt success and overall practice session performance. We evaluate the\nresulting MAB policies in 166,000 practice sessions, verifying significant\nimprovements in student outcomes. While MAB policies optimize feedback for the\noverall student population, we further investigate whether contextual bandit\n(CB) policies can enhance outcomes by personalizing feedback based on\nindividual student features (e.g., ability estimates, response times). Using\ncausal inference, we examine (i) how effects of assistance actions vary across\nstudents and (ii) whether CB policies, which leverage such effect\nheterogeneity, outperform MAB policies. While our analysis reveals that some\nactions for some questions exhibit effect heterogeneity, effect sizes may often\nbe too small for CB policies to provide significant improvements beyond what\nwell-optimized MAB policies that deliver the same action to all students\nalready achieve. We discuss insights gained from deploying data-driven systems\nat scale and implications for future refinements. Today, the teaching policies\noptimized by our system support thousands of students daily.",
      "pdf_url": "http://arxiv.org/pdf/2508.00270v1",
      "arxiv_url": "http://arxiv.org/abs/2508.00270v1",
      "published": "2025-08-01",
      "categories": [
        "cs.LG"
      ]
    },
    {
      "title": "Structural Causal Models for Extremes: an Approach Based on Exponent Measures",
      "authors": [
        "Fei Fang",
        "Shuyang Bai",
        "Tiandong Wang"
      ],
      "abstract": "We introduce a new formulation of structural causal models for extremes,\ncalled the extremal structural causal model (eSCM). Unlike conventional\nstructural causal models, where randomness is governed by a probability\ndistribution, eSCMs use an exponent measure--an infinite-mass law that\nnaturally arises in the analysis of multivariate extremes. Central to this\nframework are activation variables, which abstract the single-big-jump\nprinciple, along with additional randomization that enriches the class of eSCM\nlaws. This formulation encompasses all possible laws of directed graphical\nmodels under the recently introduced notion of extremal conditional\nindependence. We also identify an inherent asymmetry in eSCMs under natural\nassumptions, enabling the identifiability of causal directions, a central\nchallenge in causal inference. Finally, we propose a method that utilizes this\ncausal asymmetry and demonstrate its effectiveness in both simulated and real\ndatasets.",
      "pdf_url": "http://arxiv.org/pdf/2508.00223v1",
      "arxiv_url": "http://arxiv.org/abs/2508.00223v1",
      "published": "2025-08-01",
      "categories": [
        "math.ST",
        "stat.ME",
        "stat.TH"
      ]
    },
    {
      "title": "Relative Bias Under Imperfect Identification in Observational Causal Inference",
      "authors": [
        "Melody Huang",
        "Cory McCartan"
      ],
      "abstract": "To conduct causal inference in observational settings, researchers must rely\non certain identifying assumptions. In practice, these assumptions are unlikely\nto hold exactly. This paper considers the bias of selection-on-observables,\ninstrumental variables, and proximal inference estimates under violations of\ntheir identifying assumptions. We develop bias expressions for IV and proximal\ninference that show how violations of their respective assumptions are\namplified by any unmeasured confounding in the outcome variable. We propose a\nset of sensitivity tools that quantify the sensitivity of different\nidentification strategies, and an augmented bias contour plot visualizes the\nrelationship between these strategies. We argue that the act of choosing an\nidentification strategy implicitly expresses a belief about the degree of\nviolations that must be present in alternative identification strategies. Even\nwhen researchers intend to conduct an IV or proximal analysis, a sensitivity\nanalysis comparing different identification strategies can help to better\nunderstand the implications of each set of assumptions. Throughout, we compare\nthe different approaches on a re-analysis of the impact of state surveillance\non the incidence of protest in Communist Poland.",
      "pdf_url": "http://arxiv.org/pdf/2507.23743v1",
      "arxiv_url": "http://arxiv.org/abs/2507.23743v1",
      "published": "2025-07-31",
      "categories": [
        "stat.ME",
        "econ.EM"
      ]
    },
    {
      "title": "Incorporating structural uncertainty in causal decision making",
      "authors": [
        "Maurits Kaptein"
      ],
      "abstract": "Practitioners making decisions based on causal effects typically ignore\nstructural uncertainty. We analyze when this uncertainty is consequential\nenough to warrant methodological solutions (Bayesian model averaging over\ncompeting causal structures). Focusing on bivariate relationships ($X\n\\rightarrow Y$ vs. $X \\leftarrow Y$), we establish that model averaging is\nbeneficial when: (1) structural uncertainty is moderate to high, (2) causal\neffects differ substantially between structures, and (3) loss functions are\nsufficiently sensitive to the size of the causal effect. We prove optimality\nresults of our suggested methodological solution under regularity conditions\nand demonstrate through simulations that modern causal discovery methods can\nprovide, within limits, the necessary quantification. Our framework complements\nexisting robust causal inference approaches by addressing a distinct source of\nuncertainty typically overlooked in practice.",
      "pdf_url": "http://arxiv.org/pdf/2507.23495v1",
      "arxiv_url": "http://arxiv.org/abs/2507.23495v1",
      "published": "2025-07-31",
      "categories": [
        "cs.LG",
        "math.ST",
        "stat.TH"
      ]
    },
    {
      "title": "Causal Reasoning in Pieces: Modular In-Context Learning for Causal Discovery",
      "authors": [
        "Kacper Kadziolka",
        "Saber Salehkaleybar"
      ],
      "abstract": "Causal inference remains a fundamental challenge for large language models.\nRecent advances in internal reasoning with large language models have sparked\ninterest in whether state-of-the-art reasoning models can robustly perform\ncausal discovery-a task where conventional models often suffer from severe\noverfitting and near-random performance under data perturbations. We study\ncausal discovery on the Corr2Cause benchmark using the emergent OpenAI's\no-series and DeepSeek-R model families and find that these reasoning-first\narchitectures achieve significantly greater native gains than prior approaches.\nTo capitalize on these strengths, we introduce a modular in-context pipeline\ninspired by the Tree-of-Thoughts and Chain-of-Thoughts methodologies, yielding\nnearly three-fold improvements over conventional baselines. We further probe\nthe pipeline's impact by analyzing reasoning chain length, complexity, and\nconducting qualitative and quantitative comparisons between conventional and\nreasoning models. Our findings suggest that while advanced reasoning models\nrepresent a substantial leap forward, carefully structured in-context\nframeworks are essential to maximize their capabilities and offer a\ngeneralizable blueprint for causal discovery across diverse domains.",
      "pdf_url": "http://arxiv.org/pdf/2507.23488v1",
      "arxiv_url": "http://arxiv.org/abs/2507.23488v1",
      "published": "2025-07-31",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Causal2Vec: Improving Decoder-only LLMs as Versatile Embedding Models",
      "authors": [
        "Ailiang Lin",
        "Zhuoyun Li",
        "Kotaro Funakoshi"
      ],
      "abstract": "Decoder-only large language models (LLMs) are increasingly used to build\nembedding models that effectively encode the semantic information of natural\nlanguage texts into dense vector representations for various embedding tasks.\nHowever, many existing methods primarily focus on removing the causal attention\nmask in LLMs to enable bidirectional attention, potentially undermining the\nmodel's ability to extract semantic information acquired during pretraining.\nAdditionally, leading unidirectional approaches often rely on extra input text\nto overcome the inherent limitations of causal attention, inevitably increasing\ncomputational costs. In this work, we propose Causal2Vec, a general-purpose\nembedding model tailored to enhance the performance of decoder-only LLMs\nwithout altering their original architectures or introducing significant\ncomputational overhead. Specifically, we first employ a lightweight BERT-style\nmodel to pre-encode the input text into a single Contextual token, which is\nthen prepended to the LLM's input sequence, allowing each token to capture\ncontextualized information even without attending to future tokens.\nFurthermore, to mitigate the recency bias introduced by last-token pooling and\nhelp LLMs better leverage the semantic information encoded in the Contextual\ntoken, we concatenate the last hidden states of Contextual and EOS tokens as\nthe final text embedding. In practice, Causal2Vec achieves state-of-the-art\nperformance on the Massive Text Embeddings Benchmark (MTEB) among models\ntrained solely on publicly available retrieval datasets, while reducing the\nrequired sequence length by up to 85% and inference time by up to 82% compared\nto best-performing methods.",
      "pdf_url": "http://arxiv.org/pdf/2507.23386v1",
      "arxiv_url": "http://arxiv.org/abs/2507.23386v1",
      "published": "2025-07-31",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "ISO-Bench: Benchmarking Multimodal Causal Reasoning in Visual-Language Models through Procedural Plans",
      "authors": [
        "Ananya Sadana",
        "Yash Kumar Lal",
        "Jiawei Zhou"
      ],
      "abstract": "Understanding causal relationships across modalities is a core challenge for\nmultimodal models operating in real-world environments. We introduce ISO-Bench,\na benchmark for evaluating whether models can infer causal dependencies between\nvisual observations and procedural text. Each example presents an image of a\ntask step and a text snippet from a plan, with the goal of deciding whether the\nvisual step occurs before or after the referenced text step. Evaluation results\non ten frontier vision-language models show underwhelming performance: the best\nzero-shot F1 is only 0.57, and chain-of-thought reasoning yields only modest\ngains (up to 0.62 F1), largely behind humans (0.98 F1). Our analysis further\nhighlights concrete directions for improving causal understanding in multimodal\nmodels.",
      "pdf_url": "http://arxiv.org/pdf/2507.23135v1",
      "arxiv_url": "http://arxiv.org/abs/2507.23135v1",
      "published": "2025-07-30",
      "categories": [
        "cs.CL"
      ]
    }
  ]
}