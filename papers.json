{
  "last_updated": "2025-10-08T00:49:10.246191",
  "papers": [
    {
      "title": "Structural Identifiability of Graphical Continuous Lyapunov Models",
      "authors": [
        "Carlos Am√©ndola",
        "Tobias Boege",
        "Benjamin Hollering",
        "Pratik Misra"
      ],
      "abstract": "We prove two characterizations of model equivalence of acyclic graphical\ncontinuous Lyapunov models (GCLMs) with uncorrelated noise. The first result\nshows that two graphs are model equivalent if and only if they have the same\nskeleton and equivalent induced 4-node subgraphs. We also give a\ntransformational characterization via structured edge reversals. The two\ntheorems are Lyapunov analogues of celebrated results for Bayesian networks by\nVerma and Pearl, and Chickering, respectively. Our results have broad\nconsequences for the theory of causal inference of GCLMs. First, we find that\nmodel equivalence classes of acyclic GCLMs refine the corresponding classes of\nBayesian networks. Furthermore, we obtain polynomial-time algorithms to test\nmodel equivalence and structural identifiability of given directed acyclic\ngraphs.",
      "pdf_url": "http://arxiv.org/pdf/2510.04985v1",
      "arxiv_url": "http://arxiv.org/abs/2510.04985v1",
      "published": "2025-10-06",
      "categories": [
        "math.ST",
        "stat.TH",
        "62H22, 60J60 (Primary) 15A24, 62R01, 60J70 (Secondary)"
      ]
    },
    {
      "title": "On Predicting Post-Click Conversion Rate via Counterfactual Inference",
      "authors": [
        "Junhyung Ahn",
        "Sanghack Lee"
      ],
      "abstract": "Accurately predicting conversion rate (CVR) is essential in various\nrecommendation domains such as online advertising systems and e-commerce. These\nsystems utilize user interaction logs, which consist of exposures, clicks, and\nconversions. CVR prediction models are typically trained solely based on\nclicked samples, as conversions can only be determined following clicks.\nHowever, the sparsity of clicked instances necessitates the collection of a\nsubstantial amount of logs for effective model training. Recent works address\nthis issue by devising frameworks that leverage non-clicked samples. While\nthese frameworks aim to reduce biases caused by the discrepancy between clicked\nand non-clicked samples, they often rely on heuristics. Against this\nbackground, we propose a method to counterfactually generate conversion labels\nfor non-clicked samples by using causality as a guiding principle, attempting\nto answer the question, \"Would the user have converted if he or she had clicked\nthe recommended item?\" Our approach is named the Entire Space Counterfactual\nInference Multi-task Model (ESCIM). We initially train a structural causal\nmodel (SCM) of user sequential behaviors and conduct a hypothetical\nintervention (i.e., click) on non-clicked items to infer counterfactual CVRs.\nWe then introduce several approaches to transform predicted counterfactual CVRs\ninto binary counterfactual conversion labels for the non-clicked samples.\nFinally, the generated samples are incorporated into the training process.\nExtensive experiments on public datasets illustrate the superiority of the\nproposed algorithm. Online A/B testing further empirically validates the\neffectiveness of our proposed algorithm in real-world scenarios. In addition,\nwe demonstrate the improved performance of the proposed method on latent\nconversion data, showcasing its robustness and superior generalization\ncapabilities.",
      "pdf_url": "http://arxiv.org/pdf/2510.04816v1",
      "arxiv_url": "http://arxiv.org/abs/2510.04816v1",
      "published": "2025-10-06",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Causality-aware Graph Aggregation Weight Estimator for Popularity Debiasing in Top-K Recommendation",
      "authors": [
        "Yue Que",
        "Yingyi Zhang",
        "Xiangyu Zhao",
        "Chen Ma"
      ],
      "abstract": "Graph-based recommender systems leverage neighborhood aggregation to generate\nnode representations, which is highly sensitive to popularity bias, resulting\nin an echo effect during information propagation. Existing graph-based\ndebiasing solutions refine the aggregation process with attempts such as edge\nreconstruction or weight adjustment. However, these methods remain inadequate\nin fully alleviating popularity bias. Specifically, this is because 1) they\nprovide no insights into graph aggregation rationality, thus lacking an\noptimality guarantee; 2) they fail to well balance the training and debiasing\nprocess, which undermines the effectiveness. In this paper, we propose a novel\napproach to mitigate popularity bias through rational modeling of the graph\naggregation process. We reveal that graph aggregation is a special form of\nbackdoor adjustment in causal inference, where the aggregation weight\ncorresponds to the historical interaction likelihood distribution. Based on\nthis insight, we devise an encoder-decoder architecture, namely Causality-aware\nGraph Aggregation Weight Estimator for Debiasing (CAGED), to approximate the\nunbiased aggregation weight by optimizing the evidence lower bound of the\ninteraction likelihood. In order to enhance the debiasing effectiveness during\nearly training stages, we further design a momentum update strategy that\nincrementally refines the aggregation weight matrix. Extensive experiments on\nthree datasets demonstrate that CAGED outperforms existing graph-based\ndebiasing methods. Our implementation is available at\nhttps://github.com/QueYork/CAGED.",
      "pdf_url": "http://arxiv.org/pdf/2510.04502v1",
      "arxiv_url": "http://arxiv.org/abs/2510.04502v1",
      "published": "2025-10-06",
      "categories": [
        "cs.IR",
        "cs.LG"
      ]
    },
    {
      "title": "REAR: Rethinking Visual Autoregressive Models via Generator-Tokenizer Consistency Regularization",
      "authors": [
        "Qiyuan He",
        "Yicong Li",
        "Haotian Ye",
        "Jinghao Wang",
        "Xinyao Liao",
        "Pheng-Ann Heng",
        "Stefano Ermon",
        "James Zou",
        "Angela Yao"
      ],
      "abstract": "Visual autoregressive (AR) generation offers a promising path toward unifying\nvision and language models, yet its performance remains suboptimal against\ndiffusion models. Prior work often attributes this gap to tokenizer limitations\nand rasterization ordering. In this work, we identify a core bottleneck from\nthe perspective of generator-tokenizer inconsistency, i.e., the AR-generated\ntokens may not be well-decoded by the tokenizer. To address this, we propose\nreAR, a simple training strategy introducing a token-wise regularization\nobjective: when predicting the next token, the causal transformer is also\ntrained to recover the visual embedding of the current token and predict the\nembedding of the target token under a noisy context. It requires no changes to\nthe tokenizer, generation order, inference pipeline, or external models.\nDespite its simplicity, reAR substantially improves performance. On ImageNet,\nit reduces gFID from 3.02 to 1.86 and improves IS to 316.9 using a standard\nrasterization-based tokenizer. When applied to advanced tokenizers, it achieves\na gFID of 1.42 with only 177M parameters, matching the performance with larger\nstate-of-the-art diffusion models (675M).",
      "pdf_url": "http://arxiv.org/pdf/2510.04450v1",
      "arxiv_url": "http://arxiv.org/abs/2510.04450v1",
      "published": "2025-10-06",
      "categories": [
        "cs.CV"
      ]
    },
    {
      "title": "Efficient Latent Variable Causal Discovery: Combining Score Search and Targeted Testing",
      "authors": [
        "Joseph Ramsey",
        "Bryan Andrews"
      ],
      "abstract": "Learning causal structure from observational data is especially challenging\nwhen latent variables or selection bias are present. The Fast Causal Inference\n(FCI) algorithm addresses this setting but often performs exhaustive\nconditional independence tests across many subsets, leading to spurious\nindependence claims, extra or missing edges, and unreliable orientations. We\npresent a family of score-guided mixed-strategy causal search algorithms that\nbuild on this tradition. First, we introduce BOSS-FCI and GRaSP-FCI,\nstraightforward variants of GFCI that substitute BOSS or GRaSP for FGES,\nthereby retaining correctness while incurring different scalability tradeoffs.\nSecond, we develop FCI Targeted-testing (FCIT), a novel mixed-strategy method\nthat improves upon these variants by replacing exhaustive all-subsets testing\nwith targeted tests guided by BOSS, yielding well-formed PAGs with higher\nprecision and efficiency. Finally, we propose a simple heuristic, LV-Dumb (also\nknown as BOSS-POD), which bypasses latent-variable-specific reasoning and\ndirectly returns the PAG of the BOSS DAG. Although not strictly correct in the\nFCI sense, it scales better and often achieves superior accuracy in practice.\nSimulations and real-data analyses demonstrate that BOSS-FCI and GRaSP-FCI\nprovide sound baselines, FCIT improves both efficiency and reliability, and\nLV-Dumb offers a practical heuristic with strong empirical performance.\nTogether, these method highlight the value of score-guided and targeted\nstrategies for scalable latent-variable causal discovery.",
      "pdf_url": "http://arxiv.org/pdf/2510.04263v1",
      "arxiv_url": "http://arxiv.org/abs/2510.04263v1",
      "published": "2025-10-05",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Bridging the Gap Between Multimodal Foundation Models and World Models",
      "authors": [
        "Xuehai He"
      ],
      "abstract": "Humans understand the world through the integration of multiple sensory\nmodalities, enabling them to perceive, reason about, and imagine dynamic\nphysical processes. Inspired by this capability, multimodal foundation models\n(MFMs) have emerged as powerful tools for multimodal understanding and\ngeneration. However, today's MFMs fall short of serving as effective world\nmodels. They lack the essential ability such as perform counterfactual\nreasoning, simulate dynamics, understand the spatiotemporal information,\ncontrol generated visual outcomes, and perform multifaceted reasoning. We\ninvestigates what it takes to bridge the gap between multimodal foundation\nmodels and world models. We begin by improving the reasoning capabilities of\nMFMs through discriminative tasks and equipping MFMs with structured reasoning\nskills, such as causal inference, counterfactual thinking, and spatiotemporal\nreasoning, enabling them to go beyond surface correlations and understand\ndeeper relationships within visual and textual data. Next, we explore\ngenerative capabilities of multimodal foundation models across both image and\nvideo modalities, introducing new frameworks for structured and controllable\ngeneration. Our approaches incorporate scene graphs, multimodal conditioning,\nand multimodal alignment strategies to guide the generation process, ensuring\nconsistency with high-level semantics and fine-grained user intent. We further\nextend these techniques to controllable 4D generation, enabling interactive,\neditable, and morphable object synthesis over time and space.",
      "pdf_url": "http://arxiv.org/pdf/2510.03727v1",
      "arxiv_url": "http://arxiv.org/abs/2510.03727v1",
      "published": "2025-10-04",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CV",
        "cs.LG"
      ]
    },
    {
      "title": "GCVAMD: A Modified CausalVAE Model for Causal Age-related Macular Degeneration Risk Factor Detection and Prediction",
      "authors": [
        "Daeyoung Kim"
      ],
      "abstract": "Age Related Macular Degeneration(AMD) has been one of the most leading causes\nof permanent vision impairment in ophthalmology. Though treatments, such as\nanti VEGF drugs or photodynamic therapies, were developed to slow down the\ndegenerative process of AMD, there is still no specific cure to reverse vision\nloss caused by AMD. Thus, for AMD, detecting existence of risk factors of AMD\nor AMD itself within the patient retina in early stages is a crucial task to\nreduce the possibility of vision impairment. Apart from traditional approaches,\ndeep learning based methods, especially attention mechanism based CNNs and\nGradCAM based XAI analysis on OCT scans, exhibited successful performance in\ndistinguishing AMD retina from normal retinas, making it possible to use AI\ndriven models to aid medical diagnosis and analysis by ophthalmologists\nregarding AMD. However, though having significant success, previous works\nmostly focused on prediction performance itself, not pathologies or underlying\ncausal mechanisms of AMD, which can prohibit intervention analysis on specific\nfactors or even lead to less reliable decisions. Thus, this paper introduces a\nnovel causal AMD analysis model: GCVAMD, which incorporates a modified\nCausalVAE approach that can extract latent causal factors from only raw OCT\nimages. By considering causality in AMD detection, GCVAMD enables causal\ninference such as treatment simulation or intervention analysis regarding major\nrisk factors: drusen and neovascularization, while returning informative latent\ncausal features that can enhance downstream tasks. Results show that through\nGCVAMD, drusen status and neovascularization status can be identified with AMD\ncausal mechanisms in GCVAMD latent spaces, which can in turn be used for\nvarious tasks from AMD detection(classification) to intervention analysis.",
      "pdf_url": "http://arxiv.org/pdf/2510.02781v1",
      "arxiv_url": "http://arxiv.org/abs/2510.02781v1",
      "published": "2025-10-03",
      "categories": [
        "eess.IV",
        "cs.CV"
      ]
    },
    {
      "title": "Disentangling Recall and Reasoning in Transformer Models through Layer-wise Attention and Activation Analysis",
      "authors": [
        "Harshwardhan Fartale",
        "Ashish Kattamuri",
        "Rahul Raja",
        "Arpita Vats",
        "Ishita Prasad",
        "Akshata Kishore Moharir"
      ],
      "abstract": "Transformer-based language models excel at both recall (retrieving memorized\nfacts) and reasoning (performing multi-step inference), but whether these\nabilities rely on distinct internal mechanisms remains unclear. Distinguishing\nrecall from reasoning is crucial for predicting model generalization, designing\ntargeted evaluations, and building safer interventions that affect one ability\nwithout disrupting the other.We approach this question through mechanistic\ninterpretability, using controlled datasets of synthetic linguistic puzzles to\nprobe transformer models at the layer, head, and neuron level. Our pipeline\ncombines activation patching and structured ablations to causally measure\ncomponent contributions to each task type. Across two model families (Qwen and\nLLaMA), we find that interventions on distinct layers and attention heads lead\nto selective impairments: disabling identified \"recall circuits\" reduces\nfact-retrieval accuracy by up to 15\\% while leaving reasoning intact, whereas\ndisabling \"reasoning circuits\" reduces multi-step inference by a comparable\nmargin. At the neuron level, we observe task-specific firing patterns, though\nthese effects are less robust, consistent with neuronal polysemanticity.Our\nresults provide the first causal evidence that recall and reasoning rely on\nseparable but interacting circuits in transformer models. These findings\nadvance mechanistic interpretability by linking circuit-level structure to\nfunctional specialization and demonstrate how controlled datasets and causal\ninterventions can yield mechanistic insights into model cognition, informing\nsafer deployment of large language models.",
      "pdf_url": "http://arxiv.org/pdf/2510.03366v1",
      "arxiv_url": "http://arxiv.org/abs/2510.03366v1",
      "published": "2025-10-03",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Multimodal Function Vectors for Spatial Relations",
      "authors": [
        "Shuhao Fu",
        "Esther Goldberg",
        "Ying Nian Wu",
        "Hongjing Lu"
      ],
      "abstract": "Large Multimodal Models (LMMs) demonstrate impressive in-context learning\nabilities from limited multimodal demonstrations, yet the internal mechanisms\nsupporting such task learning remain opaque. Building on prior work of large\nlanguage models, we show that a small subset of attention heads in the\nvision-language model OpenFlamingo-4B is responsible for transmitting\nrepresentations of spatial relations. The activations of these attention heads,\ntermed function vectors, can be extracted and manipulated to alter an LMM's\nperformance on relational tasks. First, using both synthetic and real image\ndatasets, we apply causal mediation analysis to identify attention heads that\nstrongly influence relational predictions, and extract multimodal function\nvectors that improve zero-shot accuracy at inference time. We further\ndemonstrate that these multimodal function vectors can be fine-tuned with a\nmodest amount of training data, while keeping LMM parameters frozen, to\nsignificantly outperform in-context learning baselines. Finally, we show that\nrelation-specific function vectors can be linearly combined to solve analogy\nproblems involving novel and untrained spatial relations, highlighting the\nstrong generalization ability of this approach. Our results show that LMMs\nencode spatial relational knowledge within localized internal structures, which\ncan be systematically extracted and optimized, thereby advancing our\nunderstanding of model modularity and enhancing control over relational\nreasoning in LMMs.",
      "pdf_url": "http://arxiv.org/pdf/2510.02528v1",
      "arxiv_url": "http://arxiv.org/abs/2510.02528v1",
      "published": "2025-10-02",
      "categories": [
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "How does course recommendation impact student outcomes? Examining directed self-placement with regression discontinuity analysis",
      "authors": [
        "Jason Godfrey"
      ],
      "abstract": "For many students, placement into developmental education becomes a\nself-fulfilling prophecy. Placing college students into developmental education\nsignificantly negatively impacts student attainment, student probability of\npassing, and college credits earned. To combat these negative effects, many\nuniversities are investigating alternative placement mechanisms. Could directed\nself-placement be an effective alternative mechanism? Do students who\nself-place suffer the same negative impacts from placement recommendations as\ntheir traditionally placed counterparts? This paper uses longitudinal data with\ncausal inference methods to examine whether directed self-placement has similar\nnegative impacts on student grades and pass rates as mandatory placement\nschema. We begin with an analysis of over 20,000 student placement records into\none of two different placement tracks for first-year writing. Longitudinal and\ninstitutional data allow us to control for characteristic variables such as\nstudent race, family income, and sex. The results of our regression\ndiscontinuity design show that directed self-placement does not negatively\nimpact student grades or pass rate. This may be an improvement for students who\nplace at or near the threshold for developmental/remedial education; However,\nclass, race, and gender-based statistical differences persist in the program\nat-large, demonstrating that placement technique plays only one part in\nbuilding a more equitable program.",
      "pdf_url": "http://arxiv.org/pdf/2510.03350v1",
      "arxiv_url": "http://arxiv.org/abs/2510.03350v1",
      "published": "2025-10-02",
      "categories": [
        "econ.GN",
        "q-fin.EC",
        "stat.AP"
      ]
    }
  ]
}