{
  "last_updated": "2025-11-21T00:53:22.235868",
  "papers": [
    {
      "title": "Reflexive Evidence-Based Multimodal Learning for Clean Energy Transitions: Causal Insights on Cooking Fuel Access, Urbanization, and Carbon Emissions",
      "authors": [
        "Shan Shan"
      ],
      "abstract": "Achieving Sustainable Development Goal 7 (Affordable and Clean Energy) requires not only technological innovation but also a deeper understanding of the socioeconomic factors influencing energy access and carbon emissions. While these factors are gaining attention, critical questions remain, particularly regarding how to quantify their impacts on energy systems, model their cross-domain interactions, and capture feedback dynamics in the broader context of energy transitions. To address these gaps, this study introduces ClimateAgents, an AI-based framework that combines large language models with domain-specialized agents to support hypothesis generation and scenario exploration. Leveraging 20 years of socioeconomic and emissions data from 265 economies, countries and regions, and 98 indicators drawn from the World Bank database, the framework applies a machine learning based causal inference approach to identify key determinants of carbon emissions in an evidence-based, data driven manner. The analysis highlights three primary drivers: access to clean cooking fuels in rural areas, access to clean cooking fuels in urban areas, and the percentage of population living in urban areas. These findings underscore the critical role of clean cooking technologies and urbanization patterns in shaping emission outcomes. In line with growing calls for evidence-based AI policy, ClimateAgents offers a modular and reflexive learning system that supports the generation of credible and actionable insights for policy. By integrating heterogeneous data modalities, including structured indicators, policy documents, and semantic reasoning, the framework contributes to adaptive policymaking infrastructures that can evolve with complex socio-technical challenges. This approach aims to support a shift from siloed modeling to reflexive, modular systems designed for dynamic, context-aware climate action.",
      "pdf_url": "https://arxiv.org/pdf/2511.15342v1",
      "arxiv_url": "http://arxiv.org/abs/2511.15342v1",
      "published": "2025-11-19",
      "categories": [
        "cs.HC",
        "cs.AI"
      ]
    },
    {
      "title": "Causal Inference in Financial Event Studies",
      "authors": [
        "Paul Goldsmith-Pinkham",
        "Tianshu Lyu"
      ],
      "abstract": "Financial event studies, ubiquitous in finance research, typically use linear factor models with known factors to estimate abnormal returns and identify causal effects of information events. This paper demonstrates that when factor models are misspecified -- an almost certain reality -- traditional event study estimators produce inconsistent estimates of treatment effects. The bias is particularly severe during volatile periods, over long horizons, and when event timing correlates with market conditions. We derive precise conditions for identification and expressions for asymptotic bias. As an alternative, we propose synthetic control methods that construct replicating portfolios from control securities without imposing specific factor structures. Revisiting four empirical applications, we show that some established findings may reflect model misspecification rather than true treatment effects. While traditional methods remain reliable for short-horizon studies with random event timing, our results suggest caution when interpreting long-horizon or volatile-period event studies and highlight the importance of quasi-experimental designs when available.",
      "pdf_url": "https://arxiv.org/pdf/2511.15123v1",
      "arxiv_url": "http://arxiv.org/abs/2511.15123v1",
      "published": "2025-11-19",
      "categories": [
        "econ.EM",
        "econ.GN",
        "q-fin.GN"
      ]
    },
    {
      "title": "Individualized Prediction Bands in Causal Inference with Continuous Treatments",
      "authors": [
        "Max Sampson",
        "Kung-Sik Chan"
      ],
      "abstract": "Individualized treatments are crucial for optimal decision making and treatment allocation, specifically in personalized medicine based on the estimation of an individual's dose-response curve across a continuum of treatment levels, e.g., drug dosage. Current works focus on conditional mean and median estimates, which are useful but do not provide the full picture. We propose viewing causal inference with a continuous treatment as a covariate shift. This allows us to leverage existing weighted conformal prediction methods with both quantile and point estimates to compute individualized uncertainty quantification for dose-response curves. Our method, individualized prediction bands (IPB), is demonstrated via simulations and a real data analysis, which demonstrates the additional medical expenditure caused by continued smoking for selected individuals. The results demonstrate that IPB provides an effective solution to a gap in individual dose-response uncertainty quantification literature.",
      "pdf_url": "https://arxiv.org/pdf/2511.15075v1",
      "arxiv_url": "http://arxiv.org/abs/2511.15075v1",
      "published": "2025-11-19",
      "categories": [
        "stat.ME",
        "stat.AP"
      ]
    },
    {
      "title": "An Estimand-Focused Approach for AUC Estimation, Generalization, and Comparison: From Non-representative Samples to Target Population",
      "authors": [
        "Jiajun Liu",
        "Guangcai Mao",
        "Xiaofei Wang"
      ],
      "abstract": "The area under the ROC curve (AUC) is the standard measure of a biomarker's discriminatory accuracy; however, naive AUC estimates can be misleading when validation cohorts differ from the intended target population. Such covariate shifts commonly arise under biased or non-random sampling, distorting AUC estimations and thus impeding both generalization and cross-study comparison of AUC. We develop an estimand-focused framework for valid AUC estimation and benchmarking under covariate shift. Leveraging balancing ideas from causal inference, we extend calibration weighting to the U-statistic framework for AUC estimation and introduce a family of estimators that accommodate both summary-level and patient-level information; in certain specifications, some of these estimators attain double robustness. Furthermore, we establish asymptotic properties and study their performances across a spectrum of covariate shift severities and calibration choices in comprehensive simulations. Finally, we demonstrate practical utility in the POWER trials by evaluating how baseline stair-climb power (SCP) predicts 6-month survival among advanced non-small-cell lung cancer (NSCLC) patients. Together, the results provide a principled toolkit for anchoring biomarker AUCs to clinically relevant target populations and for comparing them fairly across studies despite distributional differences.",
      "pdf_url": "https://arxiv.org/pdf/2511.14992v1",
      "arxiv_url": "http://arxiv.org/abs/2511.14992v1",
      "published": "2025-11-19",
      "categories": [
        "stat.ME"
      ]
    },
    {
      "title": "Integrating Causal Inference with Graph Neural Networks for Alzheimer's Disease Analysis",
      "authors": [
        "Pranay Kumar Peddi",
        "Dhrubajyoti Ghosh"
      ],
      "abstract": "Deep graph learning has advanced Alzheimer's (AD) disease classification from MRI, but most models remain correlational, confounding demographic and genetic factors with disease specific features. We present Causal-GCN, an interventional graph convolutional framework that integrates do-calculus-based back-door adjustment to identify brain regions exerting stable causal influence on AD progression. Each subject's MRI is represented as a structural connectome where nodes denote cortical and subcortical regions and edges encode anatomical connectivity. Confounders such as age, sec, and APOE4 genotype are summarized via principal components and included in the causal adjustment set. After training, interventions on individual regions are simulated by serving their incoming edges and altering node features to estimate average causal effects on disease probability. Applied to 484 subjects from the ADNI cohort, Causal-GCN achieves performance comparable to baseline GNNs while providing interpretable causal effect rankings that highlight posterior, cingulate, and insular hubs consistent with established AD neuropathology.",
      "pdf_url": "https://arxiv.org/pdf/2511.14922v1",
      "arxiv_url": "http://arxiv.org/abs/2511.14922v1",
      "published": "2025-11-18",
      "categories": [
        "cs.LG",
        "stat.ME"
      ]
    },
    {
      "title": "Uncovering Treatment Effect Heterogeneity in Pragmatic Gerontology Trials",
      "authors": [
        "Changjun Li",
        "Heather Allore",
        "Michael O. Harhay",
        "Fan Li",
        "Guangyu Tong"
      ],
      "abstract": "Detecting heterogeneity in treatment response enriches the interpretation of gerontologic trials. In aging research, estimating the effect of the intervention on clinically meaningful outcomes faces analytical challenges when it is truncated by death. For example, in the Whole Systems Demonstrator trial, a large cluster-randomized study evaluating telecare among older adults, the overall effect of the intervention on quality of life was found to be null. However, this marginal intervention estimate obscures potential heterogeneity of individuals responding to the intervention, particularly among those who survive to the end of follow-up. To explore this heterogeneity, we adopt a causal framework grounded in principal stratification, targeting the Survivor Average Causal Effect (SACE)-the treatment effect among \"always-survivors,\" or those who would survive regardless of treatment assignment. We extend this framework using Bayesian Additive Regression Trees (BART), a nonparametric machine learning method, to flexibly model both latent principal strata and stratum-specific potential outcomes. This enables the estimation of the Conditional SACE (CSACE), allowing us to uncover variation in treatment effects across subgroups defined by baseline characteristics. Our analysis reveals that despite the null average effect, some subgroups experience distinct quality of life benefits (or lack thereof) from telecare, highlighting opportunities for more personalized intervention strategies. This study demonstrates how embedding machine learning methods, such as BART, within a principled causal inference framework can offer deeper insights into trial data with complex features including truncation by death and clustering-key considerations in analyzing pragmatic gerontology trials.",
      "pdf_url": "https://arxiv.org/pdf/2511.14893v1",
      "arxiv_url": "http://arxiv.org/abs/2511.14893v1",
      "published": "2025-11-18",
      "categories": [
        "stat.AP",
        "stat.ME"
      ]
    },
    {
      "title": "Skewness-Robust Causal Discovery in Location-Scale Noise Models",
      "authors": [
        "Daniel Klippert",
        "Alexander Marx"
      ],
      "abstract": "To distinguish Markov equivalent graphs in causal discovery, it is necessary to restrict the structural causal model. Crucially, we need to be able to distinguish cause $X$ from effect $Y$ in bivariate models, that is, distinguish the two graphs $X \\to Y$ and $Y \\to X$. Location-scale noise models (LSNMs), in which the effect $Y$ is modeled based on the cause $X$ as $Y = f(X) + g(X)N$, form a flexible class of models that is general and identifiable in most cases. Estimating these models for arbitrary noise terms $N$, however, is challenging. Therefore, practical estimators are typically restricted to symmetric distributions, such as the normal distribution. As we showcase in this paper, when $N$ is a skewed random variable, which is likely in real-world domains, the reliability of these approaches decreases. To approach this limitation, we propose SkewD, a likelihood-based algorithm for bivariate causal discovery under LSNMs with skewed noise distributions. SkewD extends the usual normal-distribution framework to the skew-normal setting, enabling reliable inference under symmetric and skewed noise. For parameter estimation, we employ a combination of a heuristic search and an expectation conditional maximization algorithm. We evaluate SkewD on novel synthetically generated datasets with skewed noise as well as established benchmark datasets. Throughout our experiments, SkewD exhibits a strong performance and, in comparison to prior work, remains robust under high skewness.",
      "pdf_url": "https://arxiv.org/pdf/2511.14441v1",
      "arxiv_url": "http://arxiv.org/abs/2511.14441v1",
      "published": "2025-11-18",
      "categories": [
        "stat.ML",
        "cs.LG"
      ]
    },
    {
      "title": "InstantViR: Real-Time Video Inverse Problem Solver with Distilled Diffusion Prior",
      "authors": [
        "Weimin Bai",
        "Suzhe Xu",
        "Yiwei Ren",
        "Jinhua Hao",
        "Ming Sun",
        "Wenzheng Chen",
        "He Sun"
      ],
      "abstract": "Video inverse problems are fundamental to streaming, telepresence, and AR/VR, where high perceptual quality must coexist with tight latency constraints. Diffusion-based priors currently deliver state-of-the-art reconstructions, but existing approaches either adapt image diffusion models with ad hoc temporal regularizers - leading to temporal artifacts - or rely on native video diffusion models whose iterative posterior sampling is far too slow for real-time use. We introduce InstantViR, an amortized inference framework for ultra-fast video reconstruction powered by a pre-trained video diffusion prior. We distill a powerful bidirectional video diffusion model (teacher) into a causal autoregressive student that maps a degraded video directly to its restored version in a single forward pass, inheriting the teacher's strong temporal modeling while completely removing iterative test-time optimization. The distillation is prior-driven: it only requires the teacher diffusion model and known degradation operators, and does not rely on externally paired clean/noisy video data. To further boost throughput, we replace the video-diffusion backbone VAE with a high-efficiency LeanVAE via an innovative teacher-space regularized distillation scheme, enabling low-latency latent-space processing. Across streaming random inpainting, Gaussian deblurring and super-resolution, InstantViR matches or surpasses the reconstruction quality of diffusion-based baselines while running at over 35 FPS on NVIDIA A100 GPUs, achieving up to 100 times speedups over iterative video diffusion solvers. These results show that diffusion-based video reconstruction is compatible with real-time, interactive, editable, streaming scenarios, turning high-quality video restoration into a practical component of modern vision systems.",
      "pdf_url": "https://arxiv.org/pdf/2511.14208v1",
      "arxiv_url": "http://arxiv.org/abs/2511.14208v1",
      "published": "2025-11-18",
      "categories": [
        "cs.CV"
      ]
    },
    {
      "title": "Synthetic Survival Control: Extending Synthetic Controls for \"When-If\" Decision",
      "authors": [
        "Jessy Xinyi Han",
        "Devavrat Shah"
      ],
      "abstract": "Estimating causal effects on time-to-event outcomes from observational data is particularly challenging due to censoring, limited sample sizes, and non-random treatment assignment. The need for answering such \"when-if\" questions--how the timing of an event would change under a specified intervention--commonly arises in real-world settings with heterogeneous treatment adoption and confounding. To address these challenges, we propose Synthetic Survival Control (SSC) to estimate counterfactual hazard trajectories in a panel data setting where multiple units experience potentially different treatments over multiple periods. In such a setting, SSC estimates the counterfactual hazard trajectory for a unit of interest as a weighted combination of the observed trajectories from other units. To provide formal justification, we introduce a panel framework with a low-rank structure for causal survival analysis. Indeed, such a structure naturally arises under classical parametric survival models. Within this framework, for the causal estimand of interest, we establish identification and finite sample guarantees for SSC. We validate our approach using a multi-country clinical dataset of cancer treatment outcomes, where the staggered introduction of new therapies creates a quasi-experimental setting. Empirically, we find that access to novel treatments is associated with improved survival, as reflected by lower post-intervention hazard trajectories relative to their synthetic counterparts. Given the broad relevance of survival analysis across medicine, economics, and public policy, our framework offers a general and interpretable tool for counterfactual survival inference using observational data.",
      "pdf_url": "https://arxiv.org/pdf/2511.14133v1",
      "arxiv_url": "http://arxiv.org/abs/2511.14133v1",
      "published": "2025-11-18",
      "categories": [
        "cs.LG",
        "econ.EM",
        "stat.ML"
      ]
    },
    {
      "title": "How to Marginalize in Causal Structure Learning?",
      "authors": [
        "William Zhao",
        "Guy Van den Broeck",
        "Benjie Wang"
      ],
      "abstract": "Bayesian networks (BNs) are a widely used class of probabilistic graphical models employed in numerous application domains. However, inferring the network's graphical structure from data remains challenging. Bayesian structure learners approach this problem by inferring a posterior distribution over the possible directed acyclic graphs underlying the BN. The inference process often requires marginalizing over probability distributions, which is typically done using dynamic programming methods that restrict the set of possible parents for each node. Instead, we present a novel method that utilizes tractable probabilistic circuits to circumvent this restriction. This method utilizes a new learning routine that trains these circuits on both the original distribution and marginal queries. The architecture of probabilistic circuits then inherently allows for fast and exact marginalization on the learned distribution. We then show empirically that utilizing our method to answer marginals allows Bayesian structure learners to improve their performance compared to current methods.",
      "pdf_url": "https://arxiv.org/pdf/2511.14001v1",
      "arxiv_url": "http://arxiv.org/abs/2511.14001v1",
      "published": "2025-11-18",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    }
  ]
}