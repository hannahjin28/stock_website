{
  "last_updated": "2025-06-07T00:53:50.607511",
  "papers": [
    {
      "title": "Generalizable, real-time neural decoding with hybrid state-space models",
      "authors": [
        "Avery Hee-Woon Ryoo",
        "Nanda H. Krishna",
        "Ximeng Mao",
        "Mehdi Azabou",
        "Eva L. Dyer",
        "Matthew G. Perich",
        "Guillaume Lajoie"
      ],
      "abstract": "Real-time decoding of neural activity is central to neuroscience and\nneurotechnology applications, from closed-loop experiments to brain-computer\ninterfaces, where models are subject to strict latency constraints. Traditional\nmethods, including simple recurrent neural networks, are fast and lightweight\nbut often struggle to generalize to unseen data. In contrast, recent\nTransformer-based approaches leverage large-scale pretraining for strong\ngeneralization performance, but typically have much larger computational\nrequirements and are not always suitable for low-resource or real-time\nsettings. To address these shortcomings, we present POSSM, a novel hybrid\narchitecture that combines individual spike tokenization via a cross-attention\nmodule with a recurrent state-space model (SSM) backbone to enable (1) fast and\ncausal online prediction on neural activity and (2) efficient generalization to\nnew sessions, individuals, and tasks through multi-dataset pretraining. We\nevaluate POSSM's decoding performance and inference speed on intracortical\ndecoding of monkey motor tasks, and show that it extends to clinical\napplications, namely handwriting and speech decoding in human subjects.\nNotably, we demonstrate that pretraining on monkey motor-cortical recordings\nimproves decoding performance on the human handwriting task, highlighting the\nexciting potential for cross-species transfer. In all of these tasks, we find\nthat POSSM achieves decoding accuracy comparable to state-of-the-art\nTransformers, at a fraction of the inference cost (up to 9x faster on GPU).\nThese results suggest that hybrid SSMs are a promising approach to bridging the\ngap between accuracy, inference speed, and generalization when training neural\ndecoders for real-time, closed-loop applications.",
      "pdf_url": "http://arxiv.org/pdf/2506.05320v1",
      "arxiv_url": "http://arxiv.org/abs/2506.05320v1",
      "published": "2025-06-05",
      "categories": [
        "q-bio.NC",
        "cs.LG"
      ]
    },
    {
      "title": "MesaNet: Sequence Modeling by Locally Optimal Test-Time Training",
      "authors": [
        "Johannes von Oswald",
        "Nino Scherrer",
        "Seijin Kobayashi",
        "Luca Versari",
        "Songlin Yang",
        "Maximilian Schlegel",
        "Kaitlin Maile",
        "Yanick Schimpf",
        "Oliver Sieberling",
        "Alexander Meulemans",
        "Rif A. Saurous",
        "Guillaume Lajoie",
        "Charlotte Frenkel",
        "Razvan Pascanu",
        "Blaise Agüera y Arcas",
        "João Sacramento"
      ],
      "abstract": "Sequence modeling is currently dominated by causal transformer architectures\nthat use softmax self-attention. Although widely adopted, transformers require\nscaling memory and compute linearly during inference. A recent stream of work\nlinearized the softmax operation, resulting in powerful recurrent neural\nnetwork (RNN) models with constant memory and compute costs such as DeltaNet,\nMamba or xLSTM. These models can be unified by noting that their recurrent\nlayer dynamics can all be derived from an in-context regression objective,\napproximately optimized through an online learning rule. Here, we join this\nline of work and introduce a numerically stable, chunkwise parallelizable\nversion of the recently proposed Mesa layer (von Oswald et al., 2024), and\nstudy it in language modeling at the billion-parameter scale. This layer again\nstems from an in-context loss, but which is now minimized to optimality at\nevery time point using a fast conjugate gradient solver. Through an extensive\nsuite of experiments, we show that optimal test-time training enables reaching\nlower language modeling perplexity and higher downstream benchmark performance\nthan previous RNNs, especially on tasks requiring long context understanding.\nThis performance gain comes at the cost of additional flops spent during\ninference time. Our results are therefore intriguingly related to recent trends\nof increasing test-time compute to improve performance -- here by spending\ncompute to solve sequential optimization problems within the neural network\nitself.",
      "pdf_url": "http://arxiv.org/pdf/2506.05233v1",
      "arxiv_url": "http://arxiv.org/abs/2506.05233v1",
      "published": "2025-06-05",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Causal Effect Identification in lvLiNGAM from Higher-Order Cumulants",
      "authors": [
        "Daniele Tramontano",
        "Yaroslav Kivva",
        "Saber Salehkaleybar Mathias Drton",
        "Negar Kiyavash"
      ],
      "abstract": "This paper investigates causal effect identification in latent variable\nLinear Non-Gaussian Acyclic Models (lvLiNGAM) using higher-order cumulants,\naddressing two prominent setups that are challenging in the presence of latent\nconfounding: (1) a single proxy variable that may causally influence the\ntreatment and (2) underspecified instrumental variable cases where fewer\ninstruments exist than treatments. We prove that causal effects are\nidentifiable with a single proxy or instrument and provide corresponding\nestimation methods. Experimental results demonstrate the accuracy and\nrobustness of our approaches compared to existing methods, advancing the\ntheoretical and practical understanding of causal inference in linear systems\nwith latent confounders.",
      "pdf_url": "http://arxiv.org/pdf/2506.05202v1",
      "arxiv_url": "http://arxiv.org/abs/2506.05202v1",
      "published": "2025-06-05",
      "categories": [
        "stat.ML",
        "cs.LG",
        "stat.ME"
      ]
    },
    {
      "title": "Nonlinear Causal Discovery for Grouped Data",
      "authors": [
        "Konstantin Göbler",
        "Tobias Windisch",
        "Mathias Drton"
      ],
      "abstract": "Inferring cause-effect relationships from observational data has gained\nsignificant attention in recent years, but most methods are limited to scalar\nrandom variables. In many important domains, including neuroscience,\npsychology, social science, and industrial manufacturing, the causal units of\ninterest are groups of variables rather than individual scalar measurements.\nMotivated by these applications, we extend nonlinear additive noise models to\nhandle random vectors, establishing a two-step approach for causal graph\nlearning: First, infer the causal order among random vectors. Second, perform\nmodel selection to identify the best graph consistent with this order. We\nintroduce effective and novel solutions for both steps in the vector case,\ndemonstrating strong performance in simulations. Finally, we apply our method\nto real-world assembly line data with partial knowledge of causal ordering\namong variable groups.",
      "pdf_url": "http://arxiv.org/pdf/2506.05120v1",
      "arxiv_url": "http://arxiv.org/abs/2506.05120v1",
      "published": "2025-06-05",
      "categories": [
        "stat.ML",
        "cs.LG",
        "stat.ME"
      ]
    },
    {
      "title": "Learning Joint Interventional Effects from Single-Variable Interventions in Additive Models",
      "authors": [
        "Armin Kekić",
        "Sergio Hernan Garrido Mejia",
        "Bernhard Schölkopf"
      ],
      "abstract": "Estimating causal effects of joint interventions on multiple variables is\ncrucial in many domains, but obtaining data from such simultaneous\ninterventions can be challenging. Our study explores how to learn joint\ninterventional effects using only observational data and single-variable\ninterventions. We present an identifiability result for this problem, showing\nthat for a class of nonlinear additive outcome mechanisms, joint effects can be\ninferred without access to joint interventional data. We propose a practical\nestimator that decomposes the causal effect into confounded and unconfounded\ncontributions for each intervention variable. Experiments on synthetic data\ndemonstrate that our method achieves performance comparable to models trained\ndirectly on joint interventional data, outperforming a purely observational\nestimator.",
      "pdf_url": "http://arxiv.org/pdf/2506.04945v1",
      "arxiv_url": "http://arxiv.org/abs/2506.04945v1",
      "published": "2025-06-05",
      "categories": [
        "stat.ML",
        "cs.LG"
      ]
    },
    {
      "title": "Bayesian Doubly Robust Causal Inference via Posterior Coupling",
      "authors": [
        "Shunichiro Orihara",
        "Tomotaka Momozaki",
        "Shonosuke Sugasawa"
      ],
      "abstract": "In observational studies, propensity score methods are central for estimating\ncausal effects while adjusting for confounders. Among them, the doubly robust\n(DR) estimator has gained considerable attention because it provides consistent\nestimates when either the propensity score model or the outcome model is\ncorrectly specified. Like other propensity score approaches, the DR estimator\ntypically involves two-step estimation: first, estimating the propensity score\nand outcome models, and then estimating the causal effects using the estimated\nvalues. However, this sequential procedure does not naturally align with the\nBayesian framework, which centers on updating prior beliefs solely through the\nlikelihood. In this manuscript, we propose novel Bayesian DR estimation via\nposterior coupling, which incorporates propensity score information via moment\nconditions directly into the posterior distribution. This design avoids the\nfeedback problem and enables a fully Bayesian interpretation of DR estimation\nwithout requiring two-step estimation. We detail the theoretical properties of\nthe proposed method and demonstrate its advantages over existing Bayesian\napproaches through comprehensive simulation studies and real data applications.",
      "pdf_url": "http://arxiv.org/pdf/2506.04868v1",
      "arxiv_url": "http://arxiv.org/abs/2506.04868v1",
      "published": "2025-06-05",
      "categories": [
        "stat.ME"
      ]
    },
    {
      "title": "OpenAg: Democratizing Agricultural Intelligence",
      "authors": [
        "Srikanth Thudumu",
        "Jason Fisher"
      ],
      "abstract": "Agriculture is undergoing a major transformation driven by artificial\nintelligence (AI), machine learning, and knowledge representation technologies.\nHowever, current agricultural intelligence systems often lack contextual\nunderstanding, explainability, and adaptability, especially for smallholder\nfarmers with limited resources. General-purpose large language models (LLMs),\nwhile powerful, typically lack the domain-specific knowledge and contextual\nreasoning needed for practical decision support in farming. They tend to\nproduce recommendations that are too generic or unrealistic for real-world\napplications. To address these challenges, we present OpenAg, a comprehensive\nframework designed to advance agricultural artificial general intelligence\n(AGI). OpenAg combines domain-specific foundation models, neural knowledge\ngraphs, multi-agent reasoning, causal explainability, and adaptive transfer\nlearning to deliver context-aware, explainable, and actionable insights. The\nsystem includes: (i) a unified agricultural knowledge base that integrates\nscientific literature, sensor data, and farmer-generated knowledge; (ii) a\nneural agricultural knowledge graph for structured reasoning and inference;\n(iii) an adaptive multi-agent reasoning system where AI agents specialize and\ncollaborate across agricultural domains; and (iv) a causal transparency\nmechanism that ensures AI recommendations are interpretable, scientifically\ngrounded, and aligned with real-world constraints. OpenAg aims to bridge the\ngap between scientific knowledge and the tacit expertise of experienced farmers\nto support scalable and locally relevant agricultural decision-making.",
      "pdf_url": "http://arxiv.org/pdf/2506.04571v1",
      "arxiv_url": "http://arxiv.org/abs/2506.04571v1",
      "published": "2025-06-05",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "What Makes Treatment Effects Identifiable? Characterizations and Estimators Beyond Unconfoundedness",
      "authors": [
        "Yang Cai",
        "Alkis Kalavasis",
        "Katerina Mamali",
        "Anay Mehrotra",
        "Manolis Zampetakis"
      ],
      "abstract": "Most of the widely used estimators of the average treatment effect (ATE) in\ncausal inference rely on the assumptions of unconfoundedness and overlap.\nUnconfoundedness requires that the observed covariates account for all\ncorrelations between the outcome and treatment. Overlap requires the existence\nof randomness in treatment decisions for all individuals. Nevertheless, many\ntypes of studies frequently violate unconfoundedness or overlap, for instance,\nobservational studies with deterministic treatment decisions -- popularly known\nas Regression Discontinuity designs -- violate overlap.\n  In this paper, we initiate the study of general conditions that enable the\nidentification of the average treatment effect, extending beyond\nunconfoundedness and overlap. In particular, following the paradigm of\nstatistical learning theory, we provide an interpretable condition that is\nsufficient and nearly necessary for the identification of ATE. Moreover, this\ncondition characterizes the identification of the average treatment effect on\nthe treated (ATT) and can be used to characterize other treatment effects as\nwell. To illustrate the utility of our condition, we present several\nwell-studied scenarios where our condition is satisfied and, hence, we prove\nthat ATE can be identified in regimes that prior works could not capture. For\nexample, under mild assumptions on the data distributions, this holds for the\nmodels proposed by Tan (2006) and Rosenbaum (2002), and the Regression\nDiscontinuity design model introduced by Thistlethwaite and Campbell (1960).\nFor each of these scenarios, we also show that, under natural additional\nassumptions, ATE can be estimated from finite samples.\n  We believe these findings open new avenues for bridging learning-theoretic\ninsights and causal inference methodologies, particularly in observational\nstudies with complex treatment mechanisms.",
      "pdf_url": "http://arxiv.org/pdf/2506.04194v1",
      "arxiv_url": "http://arxiv.org/abs/2506.04194v1",
      "published": "2025-06-04",
      "categories": [
        "math.ST",
        "cs.LG",
        "econ.EM",
        "stat.ME",
        "stat.ML",
        "stat.TH"
      ]
    },
    {
      "title": "N$^2$: A Unified Python Package and Test Bench for Nearest Neighbor-Based Matrix Completion",
      "authors": [
        "Caleb Chin",
        "Aashish Khubchandani",
        "Harshvardhan Maskara",
        "Kyuseong Choi",
        "Jacob Feitelberg",
        "Albert Gong",
        "Manit Paul",
        "Tathagata Sadhukhan",
        "Anish Agarwal",
        "Raaz Dwivedi"
      ],
      "abstract": "Nearest neighbor (NN) methods have re-emerged as competitive tools for matrix\ncompletion, offering strong empirical performance and recent theoretical\nguarantees, including entry-wise error bounds, confidence intervals, and\nminimax optimality. Despite their simplicity, recent work has shown that NN\napproaches are robust to a range of missingness patterns and effective across\ndiverse applications. This paper introduces N$^2$, a unified Python package and\ntestbed that consolidates a broad class of NN-based methods through a modular,\nextensible interface. Built for both researchers and practitioners, N$^2$\nsupports rapid experimentation and benchmarking. Using this framework, we\nintroduce a new NN variant that achieves state-of-the-art results in several\nsettings. We also release a benchmark suite of real-world datasets, from\nhealthcare and recommender systems to causal inference and LLM evaluation,\ndesigned to stress-test matrix completion methods beyond synthetic scenarios.\nOur experiments demonstrate that while classical methods excel on idealized\ndata, NN-based techniques consistently outperform them in real-world settings.",
      "pdf_url": "http://arxiv.org/pdf/2506.04166v1",
      "arxiv_url": "http://arxiv.org/abs/2506.04166v1",
      "published": "2025-06-04",
      "categories": [
        "cs.LG",
        "stat.CO",
        "stat.ML"
      ]
    },
    {
      "title": "Causal Inference with Missing Exposures, Missing Outcomes, and Dependence",
      "authors": [
        "Kirsten E. Landsiedel",
        "Rachel Abbott",
        "Atukunda Mucunguzi",
        "Florence Mwangwa",
        "Elijah Kakande",
        "Edwin D. Charlebois",
        "Carina Marquez",
        "Moses R. Kamya",
        "Laura B. Balzer"
      ],
      "abstract": "Missing data are ubiquitous in public health research. The\nmissing-completely-at-random (MCAR) assumption is often unrealistic and can\nlead to meaningful bias when violated. The missing-at-random (MAR) assumption\ntends to be more reasonable, but guidance on conducting causal analyses under\nMAR is limited when there is missingness on multiple variables. We present a\nseries of causal graphs and identification results to demonstrate the handling\nof missing exposures and outcomes in observational studies. For estimation and\ninference, we highlight the use of targeted minimum loss-based estimation\n(TMLE) with Super Learner to flexibly and robustly address confounding, missing\ndata, and dependence. Our work is motivated by SEARCH-TB's investigation of the\neffect of alcohol consumption on the risk of incident tuberculosis (TB)\ninfection in rural Uganda. This study posed notable challenges due to\nconfounding, missingness on the exposure (alcohol use), missingness on the\nbaseline outcome (defining who was at risk of TB), missingness on the outcome\nat follow-up (capturing who acquired TB), and clustering within households.\nApplication to real data from SEARCH-TB highlighted the real-world consequences\nof the discussed methods. Estimates from TMLE suggested that alcohol use was\nassociated with a 49% increase in the relative risk (RR) of incident TB\ninfection (RR=1.49, 95%CI: 1.39-1.59). These estimates were notably larger and\nmore precise than estimates from inverse probability weighting (RR=1.13, 95%CI:\n1.00-1.27) and unadjusted, complete case analyses (RR=1.18, 95%CI: 0.89-1.57).\nOur work demonstrates the utility of causal models for describing the missing\ndata mechanism and TMLE for flexible inference.",
      "pdf_url": "http://arxiv.org/pdf/2506.03336v1",
      "arxiv_url": "http://arxiv.org/abs/2506.03336v1",
      "published": "2025-06-03",
      "categories": [
        "stat.ME"
      ]
    }
  ]
}