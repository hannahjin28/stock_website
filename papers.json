{
  "last_updated": "2025-10-01T00:56:45.602643",
  "papers": [
    {
      "title": "Efficient Difference-in-Differences Estimation when Outcomes are Missing at Random",
      "authors": [
        "Lorenzo Testa",
        "Edward H. Kennedy",
        "Matthew Reimherr"
      ],
      "abstract": "The Difference-in-Differences (DiD) method is a fundamental tool for causal\ninference, yet its application is often complicated by missing data. Although\nrecent work has developed robust DiD estimators for complex settings like\nstaggered treatment adoption, these methods typically assume complete data and\nfail to address the critical challenge of outcomes that are missing at random\n(MAR) -- a common problem that invalidates standard estimators. We develop a\nrigorous framework, rooted in semiparametric theory, for identifying and\nefficiently estimating the Average Treatment Effect on the Treated (ATT) when\neither pre- or post-treatment (or both) outcomes are missing at random. We\nfirst establish nonparametric identification of the ATT under two minimal sets\nof sufficient conditions. For each, we derive the semiparametric efficiency\nbound, which provides a formal benchmark for asymptotic optimality. We then\npropose novel estimators that are asymptotically efficient, achieving this\ntheoretical bound. A key feature of our estimators is their multiple\nrobustness, which ensures consistency even if some nuisance function models are\nmisspecified. We validate the properties of our estimators and showcase their\nbroad applicability through an extensive simulation study.",
      "pdf_url": "http://arxiv.org/pdf/2509.25009v1",
      "arxiv_url": "http://arxiv.org/abs/2509.25009v1",
      "published": "2025-09-29",
      "categories": [
        "stat.ME",
        "econ.EM",
        "math.ST",
        "stat.TH"
      ]
    },
    {
      "title": "RAPSEM: Identifying Latent Mediators Without Sequential Ignorability via a Rank-Preserving Structural Equation Model",
      "authors": [
        "Sofia Morelli",
        "Roberto Faleh",
        "Holger Brandt"
      ],
      "abstract": "The identification of latent mediator variables is typically conducted using\nstandard structural equation models (SEMs). When SEM is applied to mediation\nanalysis with a causal interpretation, valid inference relies on the strong\nassumption of no unmeasured confounding, that is, all relevant covariates must\nbe included in the analysis. This assumption is often violated in empirical\napplications, leading to biased estimates of direct and indirect effects. We\naddress this limitation by weakening the causal assumptions and proposing a\nprocedure that combines g-estimation with a two-stage method of moments to\nincorporate latent variables, thereby enabling more robust mediation analysis\nin settings common to the social sciences. We establish consistency and\nasymptotic normality of the resulting estimator. Simulation studies demonstrate\nthat the estimator is unbiased across a wide range of settings, robust to\nviolations of its underlying no-effect-modifier assumption, and achieves\nreasonable power to detect medium to large effects for sample sizes above 500,\nwith power increasing as the strength of treatment-covariate interactions\ngrows.",
      "pdf_url": "http://arxiv.org/pdf/2509.23935v1",
      "arxiv_url": "http://arxiv.org/abs/2509.23935v1",
      "published": "2025-09-28",
      "categories": [
        "stat.ME"
      ]
    },
    {
      "title": "Taming Masked Diffusion Language Models via Consistency Trajectory Reinforcement Learning with Fewer Decoding Step",
      "authors": [
        "Jingyi Yang",
        "Guanxu Chen",
        "Xuhao Hu",
        "Jing Shao"
      ],
      "abstract": "Masked diffusion language models (MDLMs) have recently emerged as a promising\nalternative to autoregressive (AR) language models, offering properties such as\nparallel decoding, flexible generation orders, and the potential for fewer\ninference steps. Despite these advantages, decoding strategies and\nreinforcement learning (RL) algorithms tailored for MDLMs remain underexplored.\nA naive approach is to directly transfer techniques well-established for AR\nmodels to MDLMs. However, this raises an immediate question: Is such a naive\ntransfer truly optimal? For example, 1) Block-wise and semi-AR decoding\nstrategies are not employed during the training of MDLMs, so why do they\noutperform full diffusion-style decoding during inference? 2) Applying RL\nalgorithms designed for AR models directly to MDLMs exhibits a\ntraining-inference inconsistency, since MDLM decoding are non-causal\n(parallel). This results in inconsistencies between the rollout trajectory and\nthe optimization trajectory. To address these challenges, we propose EOS Early\nRejection (EOSER) and Ascending Step-Size (ASS) decoding scheduler, which\nunlock the potential of MDLMs to perform full diffusion-style decoding,\nachieving competitive performance with fewer decoding steps. Additionally, we\nintroduce Consistency Trajectory Group Relative Policy Optimization (CJ-GRPO)\nfor taming MDLMs, which emphasizes the consistency between rollout trajectory\nand optimization trajectory, and reduces the optimization errors caused by\nskip-step optimization. We conduct extensive experiments on reasoning tasks,\nsuch as mathematical and planning benchmarks, using LLaDA-8B-Instruct. The\nresults demonstrate that the proposed EOSER and ASS mechanisms, together with\nCJ-GRPO, hold significant promise for effectively and efficiently taming MDLMs.\nCode: https://github.com/yjyddq/EOSER-ASS-RL.",
      "pdf_url": "http://arxiv.org/pdf/2509.23924v1",
      "arxiv_url": "http://arxiv.org/abs/2509.23924v1",
      "published": "2025-09-28",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "One-Shot Multi-Label Causal Discovery in High-Dimensional Event Sequences",
      "authors": [
        "Hugo Math",
        "Robin Sch√∂n",
        "Rainer Lienhart"
      ],
      "abstract": "Understanding causality in event sequences with thousands of sparse event\ntypes is critical in domains such as healthcare, cybersecurity, or vehicle\ndiagnostics, yet current methods fail to scale. We present OSCAR, a one-shot\ncausal autoregressive method that infers per-sequence Markov Boundaries using\ntwo pretrained Transformers as density estimators. This enables efficient,\nparallel causal discovery without costly global CI testing. On a real-world\nautomotive dataset with 29,100 events and 474 labels, OSCAR recovers\ninterpretable causal structures in minutes, while classical methods fail to\nscale, enabling practical scientific diagnostics at production scale.",
      "pdf_url": "http://arxiv.org/pdf/2509.23213v1",
      "arxiv_url": "http://arxiv.org/abs/2509.23213v1",
      "published": "2025-09-27",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "LongLive: Real-time Interactive Long Video Generation",
      "authors": [
        "Shuai Yang",
        "Wei Huang",
        "Ruihang Chu",
        "Yicheng Xiao",
        "Yuyang Zhao",
        "Xianbang Wang",
        "Muyang Li",
        "Enze Xie",
        "Yingcong Chen",
        "Yao Lu",
        "Song Han",
        "Yukang Chen"
      ],
      "abstract": "We present LongLive, a frame-level autoregressive (AR) framework for\nreal-time and interactive long video generation. Long video generation presents\nchallenges in both efficiency and quality. Diffusion and Diffusion-Forcing\nmodels can produce high-quality videos but suffer from low efficiency due to\nbidirectional attention. Causal attention AR models support KV caching for\nfaster inference, but often degrade in quality on long videos due to memory\nchallenges during long-video training. In addition, beyond static prompt-based\ngeneration, interactive capabilities, such as streaming prompt inputs, are\ncritical for dynamic content creation, enabling users to guide narratives in\nreal time. This interactive requirement significantly increases complexity,\nespecially in ensuring visual consistency and semantic coherence during prompt\ntransitions. To address these challenges, LongLive adopts a causal, frame-level\nAR design that integrates a KV-recache mechanism that refreshes cached states\nwith new prompts for smooth, adherent switches; streaming long tuning to enable\nlong video training and to align training and inference (train-long-test-long);\nand short window attention paired with a frame-level attention sink, shorten as\nframe sink, preserving long-range consistency while enabling faster generation.\nWith these key designs, LongLive fine-tunes a 1.3B-parameter short-clip model\nto minute-long generation in just 32 GPU-days. At inference, LongLive sustains\n20.7 FPS on a single NVIDIA H100, achieves strong performance on VBench in both\nshort and long videos. LongLive supports up to 240-second videos on a single\nH100 GPU. LongLive further supports INT8-quantized inference with only marginal\nquality loss.",
      "pdf_url": "http://arxiv.org/pdf/2509.22622v1",
      "arxiv_url": "http://arxiv.org/abs/2509.22622v1",
      "published": "2025-09-26",
      "categories": [
        "cs.CV"
      ]
    },
    {
      "title": "Linear Causal Representation Learning by Topological Ordering, Pruning, and Disentanglement",
      "authors": [
        "Hao Chen",
        "Lin Liu",
        "Yu Guang Wang"
      ],
      "abstract": "Causal representation learning (CRL) has garnered increasing interests from\nthe causal inference and artificial intelligence community, due to its\ncapability of disentangling potentially complex data-generating mechanism into\ncausally interpretable latent features, by leveraging the heterogeneity of\nmodern datasets. In this paper, we further contribute to the CRL literature, by\nfocusing on the stylized linear structural causal model over the latent\nfeatures and assuming a linear mixing function that maps latent features to the\nobserved data or measurements. Existing linear CRL methods often rely on\nstringent assumptions, such as accessibility to single-node interventional data\nor restrictive distributional constraints on latent features and exogenous\nmeasurement noise. However, these prerequisites can be challenging to satisfy\nin certain scenarios. In this work, we propose a novel linear CRL algorithm\nthat, unlike most existing linear CRL methods, operates under weaker\nassumptions about environment heterogeneity and data-generating distributions\nwhile still recovering latent causal features up to an equivalence class. We\nfurther validate our new algorithm via synthetic experiments and an\ninterpretability analysis of large language models (LLMs), demonstrating both\nits superiority over competing methods in finite samples and its potential in\nintegrating causality into AI.",
      "pdf_url": "http://arxiv.org/pdf/2509.22553v1",
      "arxiv_url": "http://arxiv.org/abs/2509.22553v1",
      "published": "2025-09-26",
      "categories": [
        "stat.ML",
        "cs.LG"
      ]
    },
    {
      "title": "Debiased Front-Door Learners for Heterogeneous Effects",
      "authors": [
        "Yonghan Jung"
      ],
      "abstract": "In observational settings where treatment and outcome share unmeasured\nconfounders but an observed mediator remains unconfounded, the front-door (FD)\nadjustment identifies causal effects through the mediator. We study the\nheterogeneous treatment effect (HTE) under FD identification and introduce two\ndebiased learners: FD-DR-Learner and FD-R-Learner. Both attain fast,\nquasi-oracle rates (i.e., performance comparable to an oracle that knows the\nnuisances) even when nuisance functions converge as slowly as n^-1/4. We\nprovide error analyses establishing debiasedness and demonstrate robust\nempirical performance in synthetic studies and a real-world case study of\nprimary seat-belt laws using Fatality Analysis Reporting System (FARS) dataset.\nTogether, these results indicate that the proposed learners deliver reliable\nand sample-efficient HTE estimates in FD scenarios. The implementation is\navailable at https://github.com/yonghanjung/FD-CATE.\n  Keywords: Front-door adjustment; Heterogeneous treatment effects; Debiased\nlearning; Quasi-oracle rates; Causal inference.",
      "pdf_url": "http://arxiv.org/pdf/2509.22531v1",
      "arxiv_url": "http://arxiv.org/abs/2509.22531v1",
      "published": "2025-09-26",
      "categories": [
        "stat.ML",
        "cs.LG"
      ]
    },
    {
      "title": "A Multiplicative Instrumental Variable Model for Data Missing Not-at-Random",
      "authors": [
        "Yunshu Zhang",
        "Chan Park",
        "Jiewen Liu",
        "Yonghoon Lee",
        "Mengxin Yu",
        "James M. Robins",
        "Eric J. Tchetgen Tchetgen"
      ],
      "abstract": "Instrumental variable (IV) methods offer a valuable approach to account for\noutcome data missing not-at-random. A valid missing data instrument is a\nmeasured factor which (i) predicts the nonresponse process and (ii) is\nindependent of the outcome in the underlying population. For point\nidentification, all existing IV methods for missing data including the\ncelebrated Heckman selection model, a priori restrict the extent of selection\nbias on the outcome scale, therefore potentially understating uncertainty due\nto missing data. In this work, we introduce an IV framework which allows the\ndegree of selection bias on the outcome scale to remain completely\nunrestricted. The new approach instead relies for identification on (iii) a key\nmultiplicative selection model, which posits that the instrument and any hidden\ncommon correlate of selection and the outcome, do not interact on the\nmultiplicative scale. Interestingly, we establish that any regular statistical\nfunctional of the missing outcome is nonparametrically identified under\n(i)-(iii) via a single-arm Wald ratio estimand reminiscent of the standard Wald\nratio estimand in causal inference. For estimation and inference, we\ncharacterize the influence function for any functional defined on a\nnonparametric model for the observed data, which we leverage to develop\nsemiparametric multiply robust IV estimators. Several extensions of the methods\nare also considered, including the important practical setting of polytomous\nand continuous instruments. Simulation studies illustrate the favorable finite\nsample performance of proposed methods, which we further showcase in an HIV\nstudy nested within a household health survey study we conducted in Mochudi,\nBotswana, in which interviewer characteristics are used as instruments to\ncorrect for selection bias due to dependent nonresponse in the HIV component of\nthe survey study.",
      "pdf_url": "http://arxiv.org/pdf/2509.22499v1",
      "arxiv_url": "http://arxiv.org/abs/2509.22499v1",
      "published": "2025-09-26",
      "categories": [
        "stat.ME"
      ]
    },
    {
      "title": "Detecting (Un)answerability in Large Language Models with Linear Directions",
      "authors": [
        "Maor Juliet Lavi",
        "Tova Milo",
        "Mor Geva"
      ],
      "abstract": "Large language models (LLMs) often respond confidently to questions even when\nthey lack the necessary information, leading to hallucinated answers. In this\nwork, we study the problem of (un)answerability detection, focusing on\nextractive question answering (QA) where the model should determine if a\npassage contains sufficient information to answer a given question. We propose\na simple approach for identifying a direction in the model's activation space\nthat captures unanswerability and uses it for classification. This direction is\nselected by applying activation additions during inference and measuring their\nimpact on the model's abstention behavior. We show that projecting hidden\nactivations onto this direction yields a reliable score for (un)answerability\nclassification. Experiments on two open-weight LLMs and four extractive QA\nbenchmarks show that our method effectively detects unanswerable questions and\ngeneralizes better across datasets than existing prompt-based and\nclassifier-based approaches. Moreover, the obtained directions extend beyond\nextractive QA to unanswerability that stems from factors, such as lack of\nscientific consensus and subjectivity. Last, causal interventions show that\nadding or ablating the directions effectively controls the abstention behavior\nof the model.",
      "pdf_url": "http://arxiv.org/pdf/2509.22449v1",
      "arxiv_url": "http://arxiv.org/abs/2509.22449v1",
      "published": "2025-09-26",
      "categories": [
        "cs.CL"
      ]
    },
    {
      "title": "Exploratory Semantic Reliability Analysis of Wind Turbine Maintenance Logs using Large Language Models",
      "authors": [
        "Max Malyi",
        "Jonathan Shek",
        "Andre Biscaya"
      ],
      "abstract": "A wealth of operational intelligence is locked within the unstructured\nfree-text of wind turbine maintenance logs, a resource largely inaccessible to\ntraditional quantitative reliability analysis. While machine learning has been\napplied to this data, existing approaches typically stop at classification,\ncategorising text into predefined labels. This paper addresses the gap in\nleveraging modern large language models (LLMs) for more complex reasoning\ntasks. We introduce an exploratory framework that uses LLMs to move beyond\nclassification and perform deep semantic analysis. We apply this framework to a\nlarge industrial dataset to execute four analytical workflows: failure mode\nidentification, causal chain inference, comparative site analysis, and data\nquality auditing. The results demonstrate that LLMs can function as powerful\n\"reliability co-pilots,\" moving beyond labelling to synthesise textual\ninformation and generate actionable, expert-level hypotheses. This work\ncontributes a novel and reproducible methodology for using LLMs as a reasoning\ntool, offering a new pathway to enhance operational intelligence in the wind\nenergy sector by unlocking insights previously obscured in unstructured data.",
      "pdf_url": "http://arxiv.org/pdf/2509.22366v1",
      "arxiv_url": "http://arxiv.org/abs/2509.22366v1",
      "published": "2025-09-26",
      "categories": [
        "cs.CL"
      ]
    }
  ]
}