{
  "last_updated": "2025-08-13T00:56:08.693559",
  "papers": [
    {
      "title": "AURA: A Fine-Grained Benchmark and Decomposed Metric for Audio-Visual Reasoning",
      "authors": [
        "Siminfar Samakoush Galougah",
        "Rishie Raj",
        "Sanjoy Chowdhury",
        "Sayan Nag",
        "Ramani Duraiswami"
      ],
      "abstract": "Current audio-visual (AV) benchmarks focus on final answer accuracy,\noverlooking the underlying reasoning process. This makes it difficult to\ndistinguish genuine comprehension from correct answers derived through flawed\nreasoning or hallucinations. To address this, we introduce AURA (Audio-visual\nUnderstanding and Reasoning Assessment), a benchmark for evaluating the\ncross-modal reasoning capabilities of Audio-Visual Large Language Models\n(AV-LLMs) and Omni-modal Language Models (OLMs). AURA includes questions across\nsix challenging cognitive domains, such as causality, timbre and pitch, tempo\nand AV synchronization, unanswerability, implicit distractions, and skill\nprofiling, explicitly designed to be unanswerable from a single modality. This\nforces models to construct a valid logical path grounded in both audio and\nvideo, setting AURA apart from AV datasets that allow uni-modal shortcuts. To\nassess reasoning traces, we propose a novel metric, AuraScore, which addresses\nthe lack of robust tools for evaluating reasoning fidelity. It decomposes\nreasoning into two aspects: (i) Factual Consistency - whether reasoning is\ngrounded in perceptual evidence, and (ii) Core Inference - the logical validity\nof each reasoning step. Evaluations of SOTA models on AURA reveal a critical\nreasoning gap: although models achieve high accuracy (up to 92% on some tasks),\ntheir Factual Consistency and Core Inference scores fall below 45%. This\ndiscrepancy highlights that models often arrive at correct answers through\nflawed logic, underscoring the need for our benchmark and paving the way for\nmore robust multimodal evaluation.",
      "pdf_url": "http://arxiv.org/pdf/2508.07470v1",
      "arxiv_url": "http://arxiv.org/abs/2508.07470v1",
      "published": "2025-08-10",
      "categories": [
        "cs.CV"
      ]
    },
    {
      "title": "LLM-based Agents for Automated Confounder Discovery and Subgroup Analysis in Causal Inference",
      "authors": [
        "Po-Han Lee",
        "Yu-Cheng Lin",
        "Chan-Tung Ku",
        "Chan Hsu",
        "Pei-Cing Huang",
        "Ping-Hsun Wu",
        "Yihuang Kang"
      ],
      "abstract": "Estimating individualized treatment effects from observational data presents\na persistent challenge due to unmeasured confounding and structural bias.\nCausal Machine Learning (causal ML) methods, such as causal trees and doubly\nrobust estimators, provide tools for estimating conditional average treatment\neffects. These methods have limited effectiveness in complex real-world\nenvironments due to the presence of latent confounders or those described in\nunstructured formats. Moreover, reliance on domain experts for confounder\nidentification and rule interpretation introduces high annotation cost and\nscalability concerns. In this work, we proposed Large Language Model-based\nagents for automated confounder discovery and subgroup analysis that integrate\nagents into the causal ML pipeline to simulate domain expertise. Our framework\nsystematically performs subgroup identification and confounding structure\ndiscovery by leveraging the reasoning capabilities of LLM-based agents, which\nreduces human dependency while preserving interpretability. Experiments on\nreal-world medical datasets show that our proposed approach enhances treatment\neffect estimation robustness by narrowing confidence intervals and uncovering\nunrecognized confounding biases. Our findings suggest that LLM-based agents\noffer a promising path toward scalable, trustworthy, and semantically aware\ncausal inference.",
      "pdf_url": "http://arxiv.org/pdf/2508.07221v1",
      "arxiv_url": "http://arxiv.org/abs/2508.07221v1",
      "published": "2025-08-10",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.MA",
        "stat.AP",
        "stat.ME"
      ]
    },
    {
      "title": "Causal Inference Under Network Interference",
      "authors": [
        "Subhankar Bhadra",
        "Michael Schweinberger"
      ],
      "abstract": "We review and conceptualize recent advances in causal inference under network\ninterference, drawing on a complex and diverse body of work that ranges from\ncausal inference, statistical network analysis, economics, the health sciences,\nand the social sciences. Network interference arises in connected populations\nwhen the treatment assignments of units affect the outcomes of other units.\nExamples include economic, financial, and public health interventions with\nspillover in connected populations, reinforcement learning in connected\npopulations, and advertising on social media. We discuss the design of\nexperiments, targets of causal inference, interpretations and characterizations\nof causal effects, interference tests, and design- and model-based estimators\nof causal effects under network interference. We then contrast inferential\nframeworks based on fixed networks (finite population inference) and random\nnetworks (super population inference) and the generalizability afforded by\nthem. We demonstrate that expected outcomes can depend on the network structure\n(e.g., the absence or presence of superstars and communities) and could be\ndifferent if another network were observed, highlighting the need to understand\nhow network structure affects causal conclusions. We conclude with a selection\nof open problems.",
      "pdf_url": "http://arxiv.org/pdf/2508.06808v1",
      "arxiv_url": "http://arxiv.org/abs/2508.06808v1",
      "published": "2025-08-09",
      "categories": [
        "stat.ME"
      ]
    },
    {
      "title": "Decorrelated feature importance from local sample weighting",
      "authors": [
        "Benedikt Fröhlich",
        "Alison Durst",
        "Merle Behr"
      ],
      "abstract": "Feature importance (FI) statistics provide a prominent and valuable method of\ninsight into the decision process of machine learning (ML) models, but their\neffectiveness has well-known limitations when correlation is present among the\nfeatures in the training data. In this case, the FI often tends to be\ndistributed among all features which are in correlation with the\nresponse-generating signal features. Even worse, if multiple signal features\nare in strong correlation with a noise feature, while being only modestly\ncorrelated with one another, this can result in a noise feature having a\ndistinctly larger FI score than any signal feature. Here we propose local\nsample weighting (losaw) which can flexibly be integrated into many ML\nalgorithms to improve FI scores in the presence of feature correlation in the\ntraining data. Our approach is motivated from inverse probability weighting in\ncausal inference and locally, within the ML model, uses a sample weighting\nscheme to decorrelate a target feature from the remaining features. This\nreduces model bias locally, whenever the effect of a potential signal feature\nis evaluated and compared to others. Moreover, losaw comes with a natural\ntuning parameter, the minimum effective sample size of the weighted population,\nwhich corresponds to an interpretation-prediction-tradeoff, analog to a\nbias-variance-tradeoff as for classical ML tuning parameters. We demonstrate\nhow losaw can be integrated within decision tree-based ML methods and within\nmini-batch training of neural networks. We investigate losaw for random forest\nand convolutional neural networks in a simulation study on settings showing\ndiverse correlation patterns. We found that losaw improves FI consistently.\nMoreover, it often improves prediction accuracy for out-of-distribution, while\nmaintaining a similar accuracy for in-distribution test data.",
      "pdf_url": "http://arxiv.org/pdf/2508.06337v1",
      "arxiv_url": "http://arxiv.org/abs/2508.06337v1",
      "published": "2025-08-08",
      "categories": [
        "stat.ML",
        "cs.LG",
        "stat.ME"
      ]
    },
    {
      "title": "Stochastic Boundaries in Spatial General Equilibrium: A Diffusion-Based Approach to Causal Inference with Spillover Effects",
      "authors": [
        "Tatsuru Kikuchi"
      ],
      "abstract": "This paper introduces a novel framework for causal inference in spatial\neconomics that explicitly models the stochastic transition from partial to\ngeneral equilibrium effects. We develop a Denoising Diffusion Probabilistic\nModel (DDPM) integrated with boundary detection methods from stochastic process\ntheory to identify when and how treatment effects propagate beyond local\nmarkets. Our approach treats the evolution of spatial spillovers as a L\\'evy\nprocess with jump-diffusion dynamics, where the first passage time to critical\nthresholds indicates regime shifts from partial to general equilibrium. Using\nCUSUM-based sequential detection, we identify the spatial and temporal\nboundaries at which local interventions become systemic. Applied to AI adoption\nacross Japanese prefectures, we find that treatment effects exhibit L\\'evy\njumps at approximately 35km spatial scales, with general equilibrium effects\namplifying partial equilibrium estimates by 42\\%. Monte Carlo simulations show\nthat ignoring these stochastic boundaries leads to underestimation of treatment\neffects by 28-67\\%, with particular severity in densely connected economic\nregions. Our framework provides the first rigorous method for determining when\nspatial spillovers necessitate general equilibrium analysis, offering crucial\nguidance for policy evaluation in interconnected economies.",
      "pdf_url": "http://arxiv.org/pdf/2508.06594v1",
      "arxiv_url": "http://arxiv.org/abs/2508.06594v1",
      "published": "2025-08-08",
      "categories": [
        "econ.GN",
        "q-fin.EC"
      ]
    },
    {
      "title": "Llasa+: Free Lunch for Accelerated and Streaming Llama-Based Speech Synthesis",
      "authors": [
        "Wenjie Tian",
        "Xinfa Zhu",
        "Hanke Xie",
        "Zhen Ye",
        "Wei Xue",
        "Lei Xie"
      ],
      "abstract": "Recent progress in text-to-speech (TTS) has achieved impressive naturalness\nand flexibility, especially with the development of large language model\n(LLM)-based approaches. However, existing autoregressive (AR) structures and\nlarge-scale models, such as Llasa, still face significant challenges in\ninference latency and streaming synthesis. To deal with the limitations, we\nintroduce Llasa+, an accelerated and streaming TTS model built on Llasa.\nSpecifically, to accelerate the generation process, we introduce two\nplug-and-play Multi-Token Prediction (MTP) modules following the frozen\nbackbone. These modules allow the model to predict multiple tokens in one AR\nstep. Additionally, to mitigate potential error propagation caused by\ninaccurate MTP, we design a novel verification algorithm that leverages the\nfrozen backbone to validate the generated tokens, thus allowing Llasa+ to\nachieve speedup without sacrificing generation quality. Furthermore, we design\na causal decoder that enables streaming speech reconstruction from tokens.\nExtensive experiments show that Llasa+ achieves a 1.48X speedup without\nsacrificing generation quality, despite being trained only on LibriTTS.\nMoreover, the MTP-and-verification framework can be applied to accelerate any\nLLM-based model. All codes and models are publicly available at\nhttps://github.com/ASLP-lab/LLaSA_Plus.",
      "pdf_url": "http://arxiv.org/pdf/2508.06262v1",
      "arxiv_url": "http://arxiv.org/abs/2508.06262v1",
      "published": "2025-08-08",
      "categories": [
        "cs.SD",
        "eess.AS"
      ]
    },
    {
      "title": "InfoCausalQA:Can Models Perform Non-explicit Causal Reasoning Based on Infographic?",
      "authors": [
        "Keummin Ka",
        "Junhyeong Park",
        "Jahyun Jeon",
        "Youngjae Yu"
      ],
      "abstract": "Recent advances in Vision-Language Models (VLMs) have demonstrated impressive\ncapabilities in perception and reasoning. However, the ability to perform\ncausal inference -- a core aspect of human cognition -- remains underexplored,\nparticularly in multimodal settings. In this study, we introduce InfoCausalQA,\na novel benchmark designed to evaluate causal reasoning grounded in\ninfographics that combine structured visual data with textual context. The\nbenchmark comprises two tasks: Task 1 focuses on quantitative causal reasoning\nbased on inferred numerical trends, while Task 2 targets semantic causal\nreasoning involving five types of causal relations: cause, effect,\nintervention, counterfactual, and temporal. We manually collected 494\ninfographic-text pairs from four public sources and used GPT-4o to generate\n1,482 high-quality multiple-choice QA pairs. These questions were then\ncarefully revised by humans to ensure they cannot be answered based on\nsurface-level cues alone but instead require genuine visual grounding. Our\nexperimental results reveal that current VLMs exhibit limited capability in\ncomputational reasoning and even more pronounced limitations in semantic causal\nreasoning. Their significantly lower performance compared to humans indicates a\nsubstantial gap in leveraging infographic-based information for causal\ninference. Through InfoCausalQA, we highlight the need for advancing the causal\nreasoning abilities of multimodal AI systems.",
      "pdf_url": "http://arxiv.org/pdf/2508.06220v1",
      "arxiv_url": "http://arxiv.org/abs/2508.06220v1",
      "published": "2025-08-08",
      "categories": [
        "cs.CL",
        "cs.AI"
      ]
    },
    {
      "title": "DKG-LLM : A Framework for Medical Diagnosis and Personalized Treatment Recommendations via Dynamic Knowledge Graph and Large Language Model Integration",
      "authors": [
        "Ali Sarabadani",
        "Maryam Abdollahi Shamami",
        "Hamidreza Sadeghsalehi",
        "Borhan Asadi",
        "Saba Hesaraki"
      ],
      "abstract": "Large Language Models (LLMs) have grown exponentially since the release of\nChatGPT. These models have gained attention due to their robust performance on\nvarious tasks, including language processing tasks. These models achieve\nunderstanding and comprehension of tasks by training billions of parameters.\nThe development of these models is a transformative force in enhancing natural\nlanguage understanding and has taken a significant step towards artificial\ngeneral intelligence (AGI). In this study, we aim to present the DKG-LLM\nframework. The DKG-LLM framework introduces a groundbreaking approach to\nmedical diagnosis and personalized treatment recommendations by integrating a\ndynamic knowledge graph (DKG) with the Grok 3 large language model. Using the\nAdaptive Semantic Fusion Algorithm (ASFA), heterogeneous medical data\n(including clinical reports and PubMed articles) and patient records\ndynamically generate a knowledge graph consisting of 15,964 nodes in 13\ndistinct types (e.g., diseases, symptoms, treatments, patient profiles) and\n127,392 edges in 26 relationship types (e.g., causal, therapeutic,\nassociation). ASFA utilizes advanced probabilistic models, Bayesian inference,\nand graph optimization to extract semantic information, dynamically updating\nthe graph with approximately 150 new nodes and edges in each data category\nwhile maintaining scalability with up to 987,654 edges. Real-world datasets,\nincluding MIMIC-III and PubMed, were utilized to evaluate the proposed\narchitecture. The evaluation results show that DKG-LLM achieves a diagnostic\naccuracy of 84.19%. The model also has a treatment recommendation accuracy of\n89.63% and a semantic coverage of 93.48%. DKG-LLM is a reliable and\ntransformative tool that handles noisy data and complex multi-symptom diseases,\nalong with feedback-based learning from physician input.",
      "pdf_url": "http://arxiv.org/pdf/2508.06186v1",
      "arxiv_url": "http://arxiv.org/abs/2508.06186v1",
      "published": "2025-08-08",
      "categories": [
        "cs.CL"
      ]
    },
    {
      "title": "NanoCodec: Towards High-Quality Ultra Fast Speech LLM Inference",
      "authors": [
        "Edresson Casanova",
        "Paarth Neekhara",
        "Ryan Langman",
        "Shehzeen Hussain",
        "Subhankar Ghosh",
        "Xuesong Yang",
        "Ante Jukić",
        "Jason Li",
        "Boris Ginsburg"
      ],
      "abstract": "Large Language Models (LLMs) have significantly advanced audio processing by\nleveraging audio codecs to discretize audio into tokens, enabling the\napplication of language modeling techniques to speech data. However, existing\naudio codecs often operate at high frame rates, leading to slow training and\ninference, particularly for autoregressive models. To address this, there is\ngrowing interest in low frame-rate audio codecs, which reduce the number of\nautoregressive steps required to generate one second of audio. In this paper,\nwe conduct ablation studies to examine the impact of frame rate, bitrate, and\ncausality on codec reconstruction quality. Based on our findings, we introduce\nNanoCodec, a state-of-the-art audio codec that achieves high-quality\ncompression at just 12.5 frames per second (FPS). NanoCodec outperforms related\nworks across various bitrate ranges, establishing a new benchmark for\nlow-latency and efficient Speech LLM training and inference.",
      "pdf_url": "http://arxiv.org/pdf/2508.05835v1",
      "arxiv_url": "http://arxiv.org/abs/2508.05835v1",
      "published": "2025-08-07",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.SD"
      ]
    },
    {
      "title": "MolSnap: Snap-Fast Molecular Generation with Latent Variational Mean Flow",
      "authors": [
        "Md Atik Ahamed",
        "Qiang Ye",
        "Qiang Cheng"
      ],
      "abstract": "Molecular generation conditioned on textual descriptions is a fundamental\ntask in computational chemistry and drug discovery. Existing methods often\nstruggle to simultaneously ensure high-quality, diverse generation and fast\ninference. In this work, we propose a novel causality-aware framework that\naddresses these challenges through two key innovations. First, we introduce a\nCausality-Aware Transformer (CAT) that jointly encodes molecular graph tokens\nand text instructions while enforcing causal dependencies during generation.\nSecond, we develop a Variational Mean Flow (VMF) framework that generalizes\nexisting flow-based methods by modeling the latent space as a mixture of\nGaussians, enhancing expressiveness beyond unimodal priors. VMF enables\nefficient one-step inference while maintaining strong generation quality and\ndiversity. Extensive experiments on four standard molecular benchmarks\ndemonstrate that our model outperforms state-of-the-art baselines, achieving\nhigher novelty (up to 74.5\\%), diversity (up to 70.3\\%), and 100\\% validity\nacross all datasets. Moreover, VMF requires only one number of function\nevaluation (NFE) during conditional generation and up to five NFEs for\nunconditional generation, offering substantial computational efficiency over\ndiffusion-based methods.",
      "pdf_url": "http://arxiv.org/pdf/2508.05411v1",
      "arxiv_url": "http://arxiv.org/abs/2508.05411v1",
      "published": "2025-08-07",
      "categories": [
        "cs.LG"
      ]
    }
  ]
}