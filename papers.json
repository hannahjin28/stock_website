{
  "last_updated": "2025-10-21T00:52:29.186316",
  "papers": [
    {
      "title": "Doubly Robust Estimation of Causal Effects in Strategic Equilibrium Systems",
      "authors": [
        "Sibo Xiao"
      ],
      "abstract": "We introduce the Strategic Doubly Robust (SDR) estimator, a novel framework\nthat integrates strategic equilibrium modeling with doubly robust estimation\nfor causal inference in strategic environments. SDR addresses endogenous\ntreatment assignment arising from strategic agent behavior, maintaining double\nrobustness while incorporating strategic considerations. Theoretical analysis\nconfirms SDR's consistency and asymptotic normality under strategic\nunconfoundedness. Empirical evaluations demonstrate SDR's superior performance\nover baseline methods, achieving 7.6\\%-29.3\\% bias reduction across varying\nstrategic strengths and maintaining robust scalability with agent populations.\nThe framework provides a principled approach for reliable causal inference when\nagents respond strategically to interventions.",
      "pdf_url": "http://arxiv.org/pdf/2510.15555v1",
      "arxiv_url": "http://arxiv.org/abs/2510.15555v1",
      "published": "2025-10-17",
      "categories": [
        "cs.LG"
      ]
    },
    {
      "title": "Semantic4Safety: Causal Insights from Zero-shot Street View Imagery Segmentation for Urban Road Safety",
      "authors": [
        "Huan Chen",
        "Ting Han",
        "Siyu Chen",
        "Zhihao Guo",
        "Yiping Chen",
        "Meiliu Wu"
      ],
      "abstract": "Street-view imagery (SVI) offers a fine-grained lens on traffic risk, yet two\nfundamental challenges persist: (1) how to construct street-level indicators\nthat capture accident-related features, and (2) how to quantify their causal\nimpacts across different accident types. To address these challenges, we\npropose Semantic4Safety, a framework that applies zero-shot semantic\nsegmentation to SVIs to derive 11 interpretable streetscape indicators, and\nintegrates road type as contextual information to analyze approximately 30,000\naccident records in Austin. Specifically, we train an eXtreme Gradient Boosting\n(XGBoost) multi-class classifier and use Shapley Additive Explanations (SHAP)\nto interpret both global and local feature contributions, and then apply\nGeneralized Propensity Score (GPS) weighting and Average Treatment Effect (ATE)\nestimation to control confounding and quantify causal effects. Results uncover\nheterogeneous, accident-type-specific causal patterns: features capturing scene\ncomplexity, exposure, and roadway geometry dominate predictive power; larger\ndrivable area and emergency space reduce risk, whereas excessive visual\nopenness can increase it. By bridging predictive modeling with causal\ninference, Semantic4Safety supports targeted interventions and high-risk\ncorridor diagnosis, offering a scalable, data-informed tool for urban road\nsafety planning.",
      "pdf_url": "http://arxiv.org/pdf/2510.15434v1",
      "arxiv_url": "http://arxiv.org/abs/2510.15434v1",
      "published": "2025-10-17",
      "categories": [
        "cs.CV",
        "cs.LG"
      ]
    },
    {
      "title": "Reflections from Research Roundtables at the Conference on Health, Inference, and Learning (CHIL) 2025",
      "authors": [
        "Emily Alsentzer",
        "Marie-Laure Charpignon",
        "Bill Chen",
        "Niharika D'Souza",
        "Jason Fries",
        "Yixing Jiang",
        "Aparajita Kashyap",
        "Chanwoo Kim",
        "Simon Lee",
        "Aishwarya Mandyam",
        "Ashery Christopher Mbilinyi",
        "Nikita Mehandru",
        "Nitish Nagesh",
        "Brighton Nuwagira",
        "Emma Pierson",
        "Arvind Pillai",
        "Akane Sano",
        "Tanveer Syeda-Mahmood",
        "Shashank Yadav",
        "Elias Adhanom",
        "Muhammad Umar Afza",
        "Amelia Archer",
        "Suhana Bedi",
        "Vasiliki Bikia",
        "Trenton Chang",
        "George H. Chen",
        "Winston Chen",
        "Erica Chiang",
        "Edward Choi",
        "Octavia Ciora",
        "Paz Dozie-Nnamah",
        "Shaza Elsharief",
        "Matthew Engelhard",
        "Ali Eshragh",
        "Jean Feng",
        "Josh Fessel",
        "Scott Fleming",
        "Kei Sen Fong",
        "Thomas Frost",
        "Soham Gadgil",
        "Judy Gichoya",
        "Leeor Hershkovich",
        "Sujeong Im",
        "Bhavya Jain",
        "Vincent Jeanselme",
        "Furong Jia",
        "Qixuan",
        "Jin",
        "Yuxuan Jin",
        "Daniel Kapash",
        "Geetika Kapoor",
        "Behdokht Kiafar",
        "Matthias Kleiner",
        "Stefan Kraft",
        "Annika Kumar",
        "Daeun Kyung",
        "Zhongyuan Liang",
        "Joanna Lin",
        "Qianchu",
        "Liu",
        "Chang Liu",
        "Hongzhou Luan",
        "Chris Lunt",
        "Leopoldo Julían Lechuga López",
        "Matthew B. A. McDermott",
        "Shahriar Noroozizadeh",
        "Connor O'Brien",
        "YongKyung Oh",
        "Mixail Ota",
        "Stephen Pfohl",
        "Meagan Pi",
        "Tanmoy Sarkar Pias",
        "Emma Rocheteau",
        "Avishaan Sethi",
        "Toru Shirakawa",
        "Anita Silver",
        "Neha Simha",
        "Kamile Stankeviciute",
        "Max Sunog",
        "Peter Szolovits",
        "Shengpu Tang",
        "Jialu Tang",
        "Aaron Tierney",
        "John Valdovinos",
        "Byron Wallace",
        "Will Ke Wang",
        "Peter Washington",
        "Jeremy Weiss",
        "Daniel Wolfe",
        "Emily Wong",
        "Hye Sun Yun",
        "Xiaoman Zhang",
        "Xiao Yu Cindy Zhang",
        "Hayoung Jeong",
        "Kaveri A. Thakoor"
      ],
      "abstract": "The 6th Annual Conference on Health, Inference, and Learning (CHIL 2025),\nhosted by the Association for Health Learning and Inference (AHLI), was held in\nperson on June 25-27, 2025, at the University of California, Berkeley, in\nBerkeley, California, USA. As part of this year's program, we hosted Research\nRoundtables to catalyze collaborative, small-group dialogue around critical,\ntimely topics at the intersection of machine learning and healthcare. Each\nroundtable was moderated by a team of senior and junior chairs who fostered\nopen exchange, intellectual curiosity, and inclusive engagement. The sessions\nemphasized rigorous discussion of key challenges, exploration of emerging\nopportunities, and collective ideation toward actionable directions in the\nfield. In total, eight roundtables were held by 19 roundtable chairs on topics\nof \"Explainability, Interpretability, and Transparency,\" \"Uncertainty, Bias,\nand Fairness,\" \"Causality,\" \"Domain Adaptation,\" \"Foundation Models,\" \"Learning\nfrom Small Medical Data,\" \"Multimodal Methods,\" and \"Scalable, Translational\nHealthcare Solutions.\"",
      "pdf_url": "http://arxiv.org/pdf/2510.15217v1",
      "arxiv_url": "http://arxiv.org/abs/2510.15217v1",
      "published": "2025-10-17",
      "categories": [
        "cs.LG"
      ]
    },
    {
      "title": "The Elegant Joint Measurement is Non-Classical in the Triangle Network",
      "authors": [
        "Victor Gitton",
        "Renato Renner"
      ],
      "abstract": "When quantum systems are shared by multiple parties in a network, the\nmeasurement outcomes of the parties can exhibit non-classical correlations,\ni.e., correlations that cannot be obtained if the parties shared classical\nsystems instead. This phenomenon is known as quantum nonlocality and is\ntypically demonstrated in the Bell scenario. However, the Bell scenario is\nfundamentally simpler to investigate than general networks, since the latter\ncome with non-convex optimization problems that are often intractable. The\ntriangle network is one of the simplest networks exhibiting this non-convexity\ndue to the presence of three independent sources. Although some special cases\nof quantum nonlocality are known in the triangle network, general methods to\ncertify classical incompatibility are still lacking, which suggests that our\nunderstanding of networks is still rather limited. For instance, the Elegant\nJoint Measurement (EJM) distribution is a simple and highly-symmetric outcome\ndistribution that can be obtained with quantum systems and measurements in the\ntriangle network. This distribution was conjectured to be non-classical eight\nyears ago. In this article, we provide the first proof of non-classicality of\nthe EJM distribution. To do so, we show how to combine inflation, a causal\ninference technique, with powerful symmetry reductions and Frank-Wolfe\nalgorithms for large-scale optimization. We then use these methods to obtain\ncomputer-assisted proofs of non-classicality in exact arithmetic.",
      "pdf_url": "http://arxiv.org/pdf/2510.15143v1",
      "arxiv_url": "http://arxiv.org/abs/2510.15143v1",
      "published": "2025-10-16",
      "categories": [
        "quant-ph"
      ]
    },
    {
      "title": "Efficient Parallel Samplers for Recurrent-Depth Models and Their Connection to Diffusion Language Models",
      "authors": [
        "Jonas Geiping",
        "Xinyu Yang",
        "Guinan Su"
      ],
      "abstract": "Language models with recurrent depth, also referred to as universal or looped\nwhen considering transformers, are defined by the capacity to increase their\ncomputation through the repetition of layers. Recent efforts in pretraining\nhave demonstrated that these architectures can scale to modern language\nmodeling tasks while exhibiting advantages in reasoning tasks. In this work, we\nexamine the relationship between recurrent-depth models and diffusion language\nmodels. Building on their similarities, we develop a new diffusion forcing\nsampler for these models to accelerate generation. The sampler advances by\ndecoding new tokens at every forward pass of the model, while the latent states\nof these tokens can be further refined in parallel through recurrence.\nTheoretically, generation with our sampler is strictly more expressive than the\nbaseline autoregressive generation using the same time budget on modern\nhardware. Moreover, this sampler, based on principles from diffusion\nliterature, can be directly applied to existing 3.5B recurrent-depth\ntransformers without any tuning, leading to up to a 5x speedup. Consequently,\nour findings not only provide an efficient mechanism for parallelizing the\nextra computation in recurrent-depth models at inference, but also suggest that\nsuch models can be naturally viewed as strong continuous, though causal,\ndiffusion language models.",
      "pdf_url": "http://arxiv.org/pdf/2510.14961v1",
      "arxiv_url": "http://arxiv.org/abs/2510.14961v1",
      "published": "2025-10-16",
      "categories": [
        "cs.LG",
        "cs.CL"
      ]
    },
    {
      "title": "Local Causal Discovery for Statistically Efficient Causal Inference",
      "authors": [
        "Mátyás Schubert",
        "Tom Claassen",
        "Sara Magliacane"
      ],
      "abstract": "Causal discovery methods can identify valid adjustment sets for causal effect\nestimation for a pair of target variables, even when the underlying causal\ngraph is unknown. Global causal discovery methods focus on learning the whole\ncausal graph and therefore enable the recovery of optimal adjustment sets,\ni.e., sets with the lowest asymptotic variance, but they quickly become\ncomputationally prohibitive as the number of variables grows. Local causal\ndiscovery methods offer a more scalable alternative by focusing on the local\nneighborhood of the target variables, but are restricted to statistically\nsuboptimal adjustment sets. In this work, we propose Local Optimal Adjustments\nDiscovery (LOAD), a sound and complete causal discovery approach that combines\nthe computational efficiency of local methods with the statistical optimality\nof global methods. First, LOAD identifies the causal relation between the\ntargets and tests if the causal effect is identifiable by using only local\ninformation. If it is identifiable, it then finds the optimal adjustment set by\nleveraging local causal discovery to infer the mediators and their parents.\nOtherwise, it returns the locally valid parent adjustment sets based on the\nlearned local structure. In our experiments on synthetic and realistic data\nLOAD outperforms global methods in scalability, while providing more accurate\neffect estimation than local methods.",
      "pdf_url": "http://arxiv.org/pdf/2510.14582v1",
      "arxiv_url": "http://arxiv.org/abs/2510.14582v1",
      "published": "2025-10-16",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "CLEAR: Causal Learning Framework For Robust Histopathology Tumor Detection Under Out-Of-Distribution Shifts",
      "authors": [
        "Kieu-Anh Truong Thi",
        "Huy-Hieu Pham",
        "Duc-Trong Le"
      ],
      "abstract": "Domain shift in histopathology, often caused by differences in acquisition\nprocesses or data sources, poses a major challenge to the generalization\nability of deep learning models. Existing methods primarily rely on modeling\nstatistical correlations by aligning feature distributions or introducing\nstatistical variation, yet they often overlook causal relationships. In this\nwork, we propose a novel causal-inference-based framework that leverages\nsemantic features while mitigating the impact of confounders. Our method\nimplements the front-door principle by designing transformation strategies that\nexplicitly incorporate mediators and observed tissue slides. We validate our\nmethod on the CAMELYON17 dataset and a private histopathology dataset,\ndemonstrating consistent performance gains across unseen domains. As a result,\nour approach achieved up to a 7% improvement in both the CAMELYON17 dataset and\nthe private histopathology dataset, outperforming existing baselines. These\nresults highlight the potential of causal inference as a powerful tool for\naddressing domain shift in histopathology image analysis.",
      "pdf_url": "http://arxiv.org/pdf/2510.14273v1",
      "arxiv_url": "http://arxiv.org/abs/2510.14273v1",
      "published": "2025-10-16",
      "categories": [
        "cs.CV"
      ]
    },
    {
      "title": "Exploratory Causal Inference in SAEnce",
      "authors": [
        "Tommaso Mencattini",
        "Riccardo Cadei",
        "Francesco Locatello"
      ],
      "abstract": "Randomized Controlled Trials are one of the pillars of science; nevertheless,\nthey rely on hand-crafted hypotheses and expensive analysis. Such constraints\nprevent causal effect estimation at scale, potentially anchoring on popular yet\nincomplete hypotheses. We propose to discover the unknown effects of a\ntreatment directly from data. For this, we turn unstructured data from a trial\ninto meaningful representations via pretrained foundation models and interpret\nthem via a sparse autoencoder. However, discovering significant causal effects\nat the neural level is not trivial due to multiple-testing issues and effects\nentanglement. To address these challenges, we introduce Neural Effect Search, a\nnovel recursive procedure solving both issues by progressive stratification.\nAfter assessing the robustness of our algorithm on semi-synthetic experiments,\nwe showcase, in the context of experimental ecology, the first successful\nunsupervised causal effect identification on a real-world scientific trial.",
      "pdf_url": "http://arxiv.org/pdf/2510.14073v1",
      "arxiv_url": "http://arxiv.org/abs/2510.14073v1",
      "published": "2025-10-15",
      "categories": [
        "cs.LG",
        "cs.AI"
      ]
    },
    {
      "title": "Joint modeling and inference of multiple-subject high-dimensional sparse vector autoregressive models",
      "authors": [
        "Younghoon Kim",
        "Zachary F. Fisher",
        "Vladas Pipiras"
      ],
      "abstract": "The multiple-subject vector autoregression (multi-VAR) model captures\nheterogeneous network Granger causality across subjects by decomposing\nindividual sparse VAR transition matrices into commonly shared and\nsubject-unique paths. The model has been applied to characterize hidden shared\nand unique paths among subjects and has demonstrated performance compared to\nmethods commonly used in psychology and neuroscience. Despite this innovation,\nthe model suffers from using a weighted median for identifying the common\neffects, leading to statistical inefficiency as the convergence rates of the\ncommon and unique paths are determined by the least sparse subject and the\nsmallest sample size across all subjects. We propose a new identifiability\ncondition for the multi-VAR model based on a communication-efficient data\nintegration framework. We show that this approach achieves convergence rates\ntailored to each subject's sparsity level and sample size. Furthermore, we\ndevelop hypothesis tests to assess the nullity and homogeneity of individual\npaths, using Wald-type test statistics constructed from individual debiased\nestimators. A test for the significance of the common paths can also be derived\nthrough the framework. Simulation studies under various heterogeneity scenarios\nand a real data application demonstrate the performance of the proposed method\ncompared to existing benchmark across standard evaluation metrics.",
      "pdf_url": "http://arxiv.org/pdf/2510.14044v1",
      "arxiv_url": "http://arxiv.org/abs/2510.14044v1",
      "published": "2025-10-15",
      "categories": [
        "stat.ME"
      ]
    },
    {
      "title": "Do Large Language Models Show Biases in Causal Learning? Insights from Contingency Judgment",
      "authors": [
        "María Victoria Carro",
        "Denise Alejandra Mester",
        "Francisca Gauna Selasco",
        "Giovanni Franco Gabriel Marraffini",
        "Mario Alejandro Leiva",
        "Gerardo I. Simari",
        "María Vanina Martinez"
      ],
      "abstract": "Causal learning is the cognitive process of developing the capability of\nmaking causal inferences based on available information, often guided by\nnormative principles. This process is prone to errors and biases, such as the\nillusion of causality, in which people perceive a causal relationship between\ntwo variables despite lacking supporting evidence. This cognitive bias has been\nproposed to underlie many societal problems, including social prejudice,\nstereotype formation, misinformation, and superstitious thinking. In this work,\nwe examine whether large language models are prone to developing causal\nillusions when faced with a classic cognitive science paradigm: the contingency\njudgment task. To investigate this, we constructed a dataset of 1,000 null\ncontingency scenarios (in which the available information is not sufficient to\nestablish a causal relationship between variables) within medical contexts and\nprompted LLMs to evaluate the effectiveness of potential causes. Our findings\nshow that all evaluated models systematically inferred unwarranted causal\nrelationships, revealing a strong susceptibility to the illusion of causality.\nWhile there is ongoing debate about whether LLMs genuinely understand causality\nor merely reproduce causal language without true comprehension, our findings\nsupport the latter hypothesis and raise concerns about the use of language\nmodels in domains where accurate causal reasoning is essential for informed\ndecision-making.",
      "pdf_url": "http://arxiv.org/pdf/2510.13985v1",
      "arxiv_url": "http://arxiv.org/abs/2510.13985v1",
      "published": "2025-10-15",
      "categories": [
        "cs.AI"
      ]
    }
  ]
}