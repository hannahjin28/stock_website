{
  "last_updated": "2026-02-09T01:14:28.900287",
  "papers": [
    {
      "title": "Causal Inference on Stopped Random Walks in Online Advertising",
      "authors": [
        "Jia Yuan Yu"
      ],
      "abstract": "We consider a causal inference problem frequently encountered in online advertising systems, where a publisher (e.g., Instagram, TikTok) interacts repeatedly with human users and advertisers by sporadically displaying to each user an advertisement selected through an auction. Each treatment corresponds to a parameter value of the advertising mechanism (e.g., auction reserve-price), and we want to estimate through experiments the corresponding long-term treatment effect (e.g., annual advertising revenue). In our setting, the treatment affects not only the instantaneous revenue from showing an ad, but also changes each user's interaction-trajectory, and each advertiser's bidding policy -- as the latter is constrained by a finite budget. In particular, each a treatment may even affect the size of the population, since users interact longer with a tolerable advertising mechanism. We drop the classical i.i.d. assumption and model the experiment measurements (e.g., advertising revenue) as a stopped random walk, and use a budget-splitting experimental design, the Anscombe Theorem, a Wald-like equation, and a Central Limit Theorem to construct confidence intervals for the long-term treatment effect.",
      "pdf_url": "https://arxiv.org/pdf/2602.05997v1",
      "arxiv_url": "http://arxiv.org/abs/2602.05997v1",
      "published": "2026-02-05",
      "categories": [
        "stat.ML",
        "cs.LG",
        "stat.ME"
      ]
    },
    {
      "title": "BABE: Biology Arena BEnchmark",
      "authors": [
        "Junting Zhou",
        "Jin Chen",
        "Linfeng Hao",
        "Denghui Cao",
        "Zheyu Wang",
        "Qiguang Chen",
        "Chaoyou Fu",
        "Jiaze Chen",
        "Yuchen Wu",
        "Ge Zhang",
        "Mingxuan Wang",
        "Wenhao Huang",
        "Tong Yang"
      ],
      "abstract": "The rapid evolution of large language models (LLMs) has expanded their capabilities from basic dialogue to advanced scientific reasoning. However, existing benchmarks in biology often fail to assess a critical skill required of researchers: the ability to integrate experimental results with contextual knowledge to derive meaningful conclusions. To address this gap, we introduce BABE(Biology Arena BEnchmark), a comprehensive benchmark designed to evaluate the experimental reasoning capabilities of biological AI systems. BABE is uniquely constructed from peer-reviewed research papers and real-world biological studies, ensuring that tasks reflect the complexity and interdisciplinary nature of actual scientific inquiry. BABE challenges models to perform causal reasoning and cross-scale inference. Our benchmark provides a robust framework for assessing how well AI systems can reason like practicing scientists, offering a more authentic measure of their potential to contribute to biological research.",
      "pdf_url": "https://arxiv.org/pdf/2602.05857v1",
      "arxiv_url": "http://arxiv.org/abs/2602.05857v1",
      "published": "2026-02-05",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "NEX: Neuron Explore-Exploit Scoring for Label-Free Chain-of-Thought Selection and Model Ranking",
      "authors": [
        "Kang Chen",
        "Zhuoka Feng",
        "Sihan Zhao",
        "Kai Xiong",
        "Junjie Nian",
        "Yaoning Wang",
        "Changyi Xiao",
        "Yixin Cao"
      ],
      "abstract": "Large language models increasingly spend inference compute sampling multiple chain-of-thought traces or searching over merged checkpoints. This shifts the bottleneck from generation to selection, often without supervision on the target distribution. We show entropy-based exploration proxies follow an inverted-U with accuracy, suggesting extra exploration can become redundant and induce overthinking. We propose NEX, a white-box label-free unsupervised scoring framework that views reasoning as alternating E-phase (exploration) and X-phase (exploitation). NEX detects E-phase as spikes in newly activated MLP neurons per token from sparse activation caches, then uses a sticky two-state HMM to infer E-X phases and credits E-introduced neurons by whether they are reused in the following X span. These signals yield interpretable neuron weights and a single Good-Mass Fraction score to rank candidate responses and merged variants without task answers. Across reasoning benchmarks and Qwen3 merge families, NEX computed on a small unlabeled activation set predicts downstream accuracy and identifies better variants; we further validate the E-X signal with human annotations and provide causal evidence via \"Effective-vs-Redundant\" neuron transfer.",
      "pdf_url": "https://arxiv.org/pdf/2602.05805v1",
      "arxiv_url": "http://arxiv.org/abs/2602.05805v1",
      "published": "2026-02-05",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "Causal Front-Door Adjustment for Robust Jailbreak Attacks on LLMs",
      "authors": [
        "Yao Zhou",
        "Zeen Song",
        "Wenwen Qiang",
        "Fengge Wu",
        "Shuyi Zhou",
        "Changwen Zheng",
        "Hui Xiong"
      ],
      "abstract": "Safety alignment mechanisms in Large Language Models (LLMs) often operate as latent internal states, obscuring the model's inherent capabilities. Building on this observation, we model the safety mechanism as an unobserved confounder from a causal perspective. Then, we propose the \\textbf{C}ausal \\textbf{F}ront-Door \\textbf{A}djustment \\textbf{A}ttack ({\\textbf{CFA}}$^2$) to jailbreak LLM, which is a framework that leverages Pearl's Front-Door Criterion to sever the confounding associations for robust jailbreaking. Specifically, we employ Sparse Autoencoders (SAEs) to physically strip defense-related features, isolating the core task intent. We further reduce computationally expensive marginalization to a deterministic intervention with low inference complexity. Experiments demonstrate that {CFA}$^2$ achieves state-of-the-art attack success rates while offering a mechanistic interpretation of the jailbreaking process.",
      "pdf_url": "https://arxiv.org/pdf/2602.05444v1",
      "arxiv_url": "http://arxiv.org/abs/2602.05444v1",
      "published": "2026-02-05",
      "categories": [
        "cs.CL"
      ]
    },
    {
      "title": "ProAct: Agentic Lookahead in Interactive Environments",
      "authors": [
        "Yangbin Yu",
        "Mingyu Yang",
        "Junyou Li",
        "Yiming Gao",
        "Feiyu Liu",
        "Yijun Yang",
        "Zichuan Lin",
        "Jiafei Lyu",
        "Yicheng Liu",
        "Zhicong Lu",
        "Deheng Ye",
        "Jie Jiang"
      ],
      "abstract": "Existing Large Language Model (LLM) agents struggle in interactive environments requiring long-horizon planning, primarily due to compounding errors when simulating future states. To address this, we propose ProAct, a framework that enables agents to internalize accurate lookahead reasoning through a two-stage training paradigm. First, we introduce Grounded LookAhead Distillation (GLAD), where the agent undergoes supervised fine-tuning on trajectories derived from environment-based search. By compressing complex search trees into concise, causal reasoning chains, the agent learns the logic of foresight without the computational overhead of inference-time search. Second, to further refine decision accuracy, we propose the Monte-Carlo Critic (MC-Critic), a plug-and-play auxiliary value estimator designed to enhance policy-gradient algorithms like PPO and GRPO. By leveraging lightweight environment rollouts to calibrate value estimates, MC-Critic provides a low-variance signal that facilitates stable policy optimization without relying on expensive model-based value approximation. Experiments on both stochastic (e.g., 2048) and deterministic (e.g., Sokoban) environments demonstrate that ProAct significantly improves planning accuracy. Notably, a 4B parameter model trained with ProAct outperforms all open-source baselines and rivals state-of-the-art closed-source models, while demonstrating robust generalization to unseen environments. The codes and models are available at https://github.com/GreatX3/ProAct",
      "pdf_url": "https://arxiv.org/pdf/2602.05327v1",
      "arxiv_url": "http://arxiv.org/abs/2602.05327v1",
      "published": "2026-02-05",
      "categories": [
        "cs.AI"
      ]
    },
    {
      "title": "FlashBlock: Attention Caching for Efficient Long-Context Block Diffusion",
      "authors": [
        "Zhuokun Chen",
        "Jianfei Cai",
        "Bohan Zhuang"
      ],
      "abstract": "Generating long-form content, such as minute-long videos and extended texts, is increasingly important for modern generative models. Block diffusion improves inference efficiency via KV caching and block-wise causal inference and has been widely adopted in diffusion language models and video generation. However, in long-context settings, block diffusion still incurs substantial overhead from repeatedly computing attention over a growing KV cache. We identify an underexplored property of block diffusion: cross-step redundancy of attention within a block. Our analysis shows that attention outputs from tokens outside the current block remain largely stable across diffusion steps, while block-internal attention varies significantly. Based on this observation, we propose FlashBlock, a cached block-external attention mechanism that reuses stable attention output, reducing attention computation and KV cache access without modifying the diffusion process. Moreover, FlashBlock is orthogonal to sparse attention and can be combined as a complementary residual reuse strategy, substantially improving model accuracy under aggressive sparsification. Experiments on diffusion language models and video generation demonstrate up to 1.44$\\times$ higher token throughput and up to 1.6$\\times$ reduction in attention time, with negligible impact on generation quality. Project page: https://caesarhhh.github.io/FlashBlock/.",
      "pdf_url": "https://arxiv.org/pdf/2602.05305v1",
      "arxiv_url": "http://arxiv.org/abs/2602.05305v1",
      "published": "2026-02-05",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ]
    },
    {
      "title": "Causal Online Learning of Safe Regions in Cloud Radio Access Networks",
      "authors": [
        "Kim Hammar",
        "Tansu Alpcan",
        "Emil Lupu"
      ],
      "abstract": "Cloud radio access networks (RANs) enable cost-effective management of mobile networks by dynamically scaling their capacity on demand. However, deploying adaptive controllers to implement such dynamic scaling in operational networks is challenging due to the risk of breaching service agreements and operational constraints. To mitigate this challenge, we present a novel method for learning the safe operating region of the RAN, i.e., the set of resource allocations and network configurations for which its specification is fulfilled. The method, which we call (C)ausal (O)nline (L)earning, operates in two online phases: an inference phase and an intervention phase. In the first phase, we passively observe the RAN to infer an initial safe region via causal inference and Gaussian process regression. In the second phase, we gradually expand this region through interventional Bayesian learning. We prove that COL ensures that the learned region is safe with a specified probability and that it converges to the full safe region under standard conditions. We experimentally validate COL on a 5G testbed. The results show that COL quickly learns the safe region while incurring low operational cost and being up to 10x more sample-efficient than current state-of-the-art methods for safe learning.",
      "pdf_url": "https://arxiv.org/pdf/2602.05280v1",
      "arxiv_url": "http://arxiv.org/abs/2602.05280v1",
      "published": "2026-02-05",
      "categories": [
        "cs.NI"
      ]
    },
    {
      "title": "Learning virulence-transmission relationships using causal inference",
      "authors": [
        "Sudam Surasinghe",
        "C. Brandon Ogbunugafor"
      ],
      "abstract": "The relationship between traits that influence pathogen virulence and transmission is part of the central canon of the evolution and ecology of infectious disease. However, identifying directional and mechanistic relationships among traits remains a key challenge in various subfields of biology, as models often assume static, fixed links between characteristics. Here, we introduce learning evolutionary trait relationships (LETR), a data-driven framework that applies Granger-causality principles to determine which traits drive others and how these relationships change over time. LETR integrates causal discovery with generative mapping and transfer-operator analysis to link short-term predictability with long-term trait distributions. Using a synthetic myxomatosis virus-host data set, we show that LETR reliably recovers known directional influences, such as virulence driving transmission. Applying the framework to global pandemic (SARS-CoV-2) data, we find that past virulence improves future transmission prediction, while the reverse effect is weak. Invariant-density estimates reveal a long-term trend toward low virulence and transmission, with bimodality in virulence suggesting ecological influences or host heterogeneity. In summary, this study provides a blueprint for learning the relationship between how harmful a pathogen is and how well it spreads, which is highly idiosyncratic and context-dependent. This finding undermines simplistic models and encourages the development of new theory for the constraints underlying pathogen evolution. Further, by uniting causal inference with dynamical modeling, the LETR framework offers a general approach for uncovering mechanistic trait linkages in complex biological systems of various kinds.",
      "pdf_url": "https://arxiv.org/pdf/2602.05196v1",
      "arxiv_url": "http://arxiv.org/abs/2602.05196v1",
      "published": "2026-02-05",
      "categories": [
        "q-bio.PE",
        "q-bio.QM"
      ]
    },
    {
      "title": "ARGaze: Autoregressive Transformers for Online Egocentric Gaze Estimation",
      "authors": [
        "Jia Li",
        "Wenjie Zhao",
        "Shijian Deng",
        "Bolin Lai",
        "Yuheng Wu",
        "RUijia Chen",
        "Jon E. Froehlich",
        "Yuhang Zhao",
        "Yapeng Tian"
      ],
      "abstract": "Online egocentric gaze estimation predicts where a camera wearer is looking from first-person video using only past and current frames, a task essential for augmented reality and assistive technologies. Unlike third-person gaze estimation, this setting lacks explicit head or eye signals, requiring models to infer current visual attention from sparse, indirect cues such as hand-object interactions and salient scene content. We observe that gaze exhibits strong temporal continuity during goal-directed activities: knowing where a person looked recently provides a powerful prior for predicting where they look next. Inspired by vision-conditioned autoregressive decoding in vision-language models, we propose ARGaze, which reformulates gaze estimation as sequential prediction: at each timestep, a transformer decoder predicts current gaze by conditioning on (i) current visual features and (ii) a fixed-length Gaze Context Window of recent gaze target estimates. This design enforces causality and enables bounded-resource streaming inference. We achieve state-of-the-art performance across multiple egocentric benchmarks under online evaluation, with extensive ablations validating that autoregressive modeling with bounded gaze history is critical for robust prediction. We will release our source code and pre-trained models.",
      "pdf_url": "https://arxiv.org/pdf/2602.05132v1",
      "arxiv_url": "http://arxiv.org/abs/2602.05132v1",
      "published": "2026-02-04",
      "categories": [
        "cs.CV"
      ]
    },
    {
      "title": "Causal Representation Meets Stochastic Modeling under Generic Geometry",
      "authors": [
        "Jiaxu Ren",
        "Yixin Wang",
        "Biwei Huang"
      ],
      "abstract": "Learning meaningful causal representations from observations has emerged as a crucial task for facilitating machine learning applications and driving scientific discoveries in fields such as climate science, biology, and physics. This process involves disentangling high-level latent variables and their causal relationships from low-level observations. Previous work in this area that achieves identifiability typically focuses on cases where the observations are either i.i.d. or follow a latent discrete-time process. Nevertheless, many real-world settings require identifying latent variables that are continuous-time stochastic processes (e.g., multivariate point processes). To this end, we develop identifiable causal representation learning for continuous-time latent stochastic point processes. We study its identifiability by analyzing the geometry of the parameter space. Furthermore, we develop MUTATE, an identifiable variational autoencoder framework with a time-adaptive transition module to infer stochastic dynamics. Across simulated and empirical studies, we find that MUTATE can effectively answer scientific questions, such as the accumulation of mutations in genomics and the mechanisms driving neuron spike triggers in response to time-varying dynamics.",
      "pdf_url": "https://arxiv.org/pdf/2602.05033v1",
      "arxiv_url": "http://arxiv.org/abs/2602.05033v1",
      "published": "2026-02-04",
      "categories": [
        "cs.LG",
        "math.AG"
      ]
    }
  ]
}